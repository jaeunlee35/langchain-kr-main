{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16613f46",
   "metadata": {},
   "source": [
    "# FAISS 비동기(Asynchronous)\n",
    "\n",
    "본 튜토리얼은 FAISS DB 를 비동기로 활용하는 방법을 다룹니다.\n",
    "\n",
    "더욱 자세한 사용법은 [Faiss 공식 문서](https://faiss.ai/)를 참조하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bc0ae",
   "metadata": {},
   "source": [
    "faiss-gpu 또는 faiss-cpu 라이브러리를 설치합니다.\n",
    "\n",
    "- CUDA 7.5 이상을 지원하는 GPU가 있는 경우 `faiss-gpu`를 설치합니다.\n",
    "- GPU가 없거나 CPU에서 실행하려는 경우 `faiss-cpu`를 설치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e90789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet faiss-gpu # CUDA 7.5 이상을 지원하는 GPU용 설치\n",
    "# 또는\n",
    "# %pip install --upgrade --quiet faiss-cpu # CPU용 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548bd35",
   "metadata": {},
   "source": [
    "혹은 `dot_env` 라이브러리를 사용하여 `.env` 파일을 만들어 환경 변수로 설정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c0a46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455af5a1",
   "metadata": {},
   "source": [
    "추적을 위한 LangSmith 설정(필수는 아닙니다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGCHAIN_TRACING_V2 환경 변수를 \"true\"로 설정합니다.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# LANGCHAIN_API_KEY 환경 변수를 getpass.getpass() 함수를 통해 입력받은 값으로 설정합니다.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN API KEY 입력\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dae6df",
   "metadata": {},
   "source": [
    "- `TextLoader`를 사용하여 텍스트 데이터를 로드합니다.\n",
    "- `CharacterTextSplitter`를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.\n",
    "- `OpenAIEmbeddings`를 사용하여 문서 임베딩을 생성합니다.\n",
    "- `FAISS` 벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `AVX2`: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우 `AVX2` 를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.\n",
    "- 필요한 경우 `os.environ['FAISS_NO_AVX2'] = '1'` 코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7b919",
   "metadata": {},
   "source": [
    "샘플 데이터(`appendix-keywords.txt`) 일부 파일의 내용을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb983126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# data/appendix-keywords.txt 파일 내용을 읽어서 file 변수에 저장합니다.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# file 변수에 저장된 내용을 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461b4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.\n",
    "# import os\n",
    "#\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = await FAISS.afrom_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b122c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n"
     ]
    }
   ],
   "source": [
    "# 검색할 쿼리 설정\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# 쿼리와 유사한 문서 검색\n",
    "docs = await db.asimilarity_search(query)\n",
    "\n",
    "# 검색된 문서의 내용 출력\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d921a98",
   "metadata": {},
   "source": [
    "## 점수에 기반한 유사도 검색\n",
    "\n",
    "FAISS에는 몇 가지 특정 메서드가 있습니다.\n",
    "\n",
    "그 중 하나는 `similarity_search_with_score`로, 문서뿐만 아니라 쿼리와 문서 간의 거리 점수도 반환할 수 있습니다.\n",
    "\n",
    "반환되는 거리 점수는 L2 거리입니다.\n",
    "\n",
    "따라서 점수가 낮을수록 더 좋은 결과입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fcd9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken', metadata={'source': './data/appendix-keywords.txt'}),\n",
       " 0.29650298)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.\n",
    "docs_and_scores = await db.asimilarity_search_with_score(query)\n",
    "\n",
    "# 검색 결과 중 가장 유사도가 높은 문서와 점수를 가져옵니다.\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26382d",
   "metadata": {},
   "source": [
    "`similarity_search_by_vector` 함수를 사용하면 주어진 임베딩 벡터와 유사한 문서를 검색할 수 있습니다.\n",
    "\n",
    "이 함수는 문자열 대신 **임베딩 벡터** 를 매개변수로 받아들입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0980a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색할 쿼리 설정\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "# 쿼리를 임베딩 벡터로 변환합니다.\n",
    "embedding_vector = await embeddings.aembed_query(query)\n",
    "# 임베딩 벡터를 사용하여 유사도 검색을 수행하고 문서와 점수를 반환합니다.\n",
    "docs_and_scores = await db.asimilarity_search_by_vector(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2288d3",
   "metadata": {},
   "source": [
    "## 저장 및 로드\n",
    "\n",
    "FAISS 인덱스를 저장하고 불러올 수도 있습니다.\n",
    "\n",
    "이는 인덱스를 사용할 때마다 매번 다시 생성할 필요가 없어 유용합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917c1f",
   "metadata": {},
   "source": [
    "- `db.save_local(\"저장할 인덱스 이름\")`를 사용하여 FAISS 인덱스를 로컬 디렉토리에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c69f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에 \"MY_ASYNC_DB_INDEX\"라는 이름으로 데이터베이스를 저장합니다.\n",
    "DB_INDEX = \"MY_ASYNC_DB_INDEX\"\n",
    "db.save_local(DB_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bd181",
   "metadata": {},
   "source": [
    "- `FAISS.load_local(\"불러올 인덱스 이름\", embeddings)`를 사용하여 저장된 FAISS 인덱스를 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28d8991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken', metadata={'source': './data/appendix-keywords.txt'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.\n",
    "new_db = FAISS.load_local(DB_INDEX, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 비동기적으로 검색하여 docs 변수에 할당합니다.\n",
    "docs = await new_db.asimilarity_search(query)\n",
    "\n",
    "docs[0]  # 검색 결과 중 가장 유사한 문서를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a3c32",
   "metadata": {},
   "source": [
    "# 바이트 형태로 직렬화/역직렬화\n",
    "\n",
    "FAISS 인덱스를 직렬화(Serializing)하고 역직렬화(De-Serializing)하는 데에는 `pickle` 함수를 사용하여 저장할 수 있습니다. 하지만, 만약 90MB 크기의 임베딩 모델(예: `sentence-transformers/all-MiniLM-L6-v2` 또는 다른 모델)을 사용한다면, 결과로 생성되는 pickle 파일의 크기는 90MB 이상이 될 것입니다. 이는 모델의 크기 또한 전체 크기에 포함되기 때문입니다.\n",
    "\n",
    "따라서, 아래의 함수들을 사용하는 것이 좋습니다. 이 함수들은 **FAISS 인덱스만 직렬화** 하므로 크기가 훨씬 작아집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a679a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# HuggingFaceEmbeddings 사용하여 임베딩을 생성합니다.\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(docs, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "870c7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 직렬화합니다.\n",
    "serialized_db_index = db.serialize_to_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ee24f",
   "metadata": {},
   "source": [
    "직렬화된 `serialized_db_index` 의 사이즈를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c5f7ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60.81 KB'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 사이즈 측정을 위한 함수를 정의합니다.\n",
    "\n",
    "\n",
    "def get_size(path):\n",
    "    size = sys.getsizeof(path)\n",
    "    if size < 1024:\n",
    "        return f\"{size} bytes\"\n",
    "    elif size < pow(1024, 2):\n",
    "        return f\"{round(size/1024, 2)} KB\"\n",
    "    elif size < pow(1024, 3):\n",
    "        return f\"{round(size/(pow(1024,2)), 2)} MB\"\n",
    "    elif size < pow(1024, 4):\n",
    "        return f\"{round(size/(pow(1024,3)), 2)} GB\"\n",
    "\n",
    "\n",
    "# 직렬화된 FAISS 인덱스의 사이즈를 출력합니다.\n",
    "get_size(serialized_db_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac798b",
   "metadata": {},
   "source": [
    "이번에는 `deserialize_from_bytes` 함수로 역직렬화를 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5c1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화된 인덱스를 로드합니다.\n",
    "deserialized_db = FAISS.deserialize_from_bytes(\n",
    "    embeddings=hf_embeddings,  # 직렬화 할 때의 임베딩과 동일하게 지정\n",
    "    serialized=serialized_db_index,  # 직렬화된 인덱스\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b81226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer', metadata={'source': './data/appendix-keywords.txt'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.\n",
    "docs = deserialized_db.similarity_search(query)\n",
    "\n",
    "# 문서 리스트의 첫 번째 문서를 가져옵니다.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748ffbe",
   "metadata": {},
   "source": [
    "## 병합(merge)\n",
    "\n",
    "FAISS 벡터 저장소를 병합하는 것도 가능합니다.\n",
    "\n",
    "두 개의 FAISS 벡터 저장소를 하나로 합칠 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451ca25",
   "metadata": {},
   "source": [
    "- `FAISS.afrom_texts` 메서드를 사용하여 두 개의 FAISS 데이터베이스 `db1`과 `db2`를 비동기적으로 생성합니다.\n",
    "- `db1`은 `[\"foo\"]` 텍스트 리스트로부터 생성되며, `embeddings`를 사용하여 벡터화됩니다.\n",
    "- `db2`는 `[\"bar\"]` 텍스트 리스트로부터 생성되며, 동일한 `embeddings`를 사용하여 벡터화됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73053ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# db1 생성\n",
    "db1 = FAISS.from_texts([\"LangChain DB 1의 내용\"], embeddings)\n",
    "# db2 생성\n",
    "db2 = FAISS.from_texts([\"DB 2 의 내용\"], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1597e8e",
   "metadata": {},
   "source": [
    "`db1` 의 내용 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ac5e999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5ad3475d-3a95-469c-b868-8ab93b644708': Document(page_content='LangChain DB 1의 내용')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 벡터 데이터베이스의 내부 문서 저장소 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbfbbc",
   "metadata": {},
   "source": [
    "`db2` 의 내용 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "003fbadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3439056f-c28f-4366-baa0-c79a949378c6': Document(page_content='DB 2 의 내용')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db2 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db2.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2ffdb",
   "metadata": {},
   "source": [
    "`db1` 객체의 `merge_from` 메서드를 사용하여 `db2` 객체의 내용을 `db1`에 병합합니다.\n",
    "\n",
    "- `merge_from` 메서드는 `db2`의 데이터를 `db1`에 병합하는 기능을 수행합니다.\n",
    "- 병합 작업 후에는 `db1`에 `db2`의 데이터가 포함됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f617f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.merge_from(db2)  # db1에 db2를 병합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3fb7a2",
   "metadata": {},
   "source": [
    "병합된 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65077a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5ad3475d-3a95-469c-b868-8ab93b644708': Document(page_content='LangChain DB 1의 내용'),\n",
       " '3439056f-c28f-4366-baa0-c79a949378c6': Document(page_content='DB 2 의 내용')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdf5f3",
   "metadata": {},
   "source": [
    "## 필터링\n",
    "\n",
    "FAISS vectorstore는 필터링 기능도 지원할 수 있습니다.\n",
    "\n",
    "FAISS는 기본적으로 필터링을 지원하지 않기 때문에, 이를 수동으로 처리해야 합니다.\n",
    "\n",
    "**방식**\n",
    "\n",
    "1. 이는 먼저 `k`보다 더 많은 결과를 가져온 다음 필터링하는 방식으로 이루어집니다.\n",
    "\n",
    "2. 이 필터는 메타데이터 dict를 입력으로 받아 bool을 반환하는 호출 가능한 함수이거나, 누락된 각 키는 무시되고 존재하는 각 키는 값 목록에 있어야 하는 메타데이터 dict입니다.\n",
    "\n",
    "또한 검색 메서드를 호출할 때 `fetch_k` 매개변수를 설정하여 필터링 전에 가져올 문서 수를 지정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdd954d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "list_of_documents = [\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"bar\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"barbar\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"bar burr\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    # 페이지 내용이 \"bar bruh\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]\n",
    "# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "results_with_scores = db.asimilarity_search_with_score(\n",
    "    \"foo\"\n",
    ")  # \"foo\"와 유사한 문서를 검색하고 점수와 함께 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac2191",
   "metadata": {},
   "source": [
    "이제 동일한 쿼리 호출을 수행하지만, `page = 1`인 경우만 필터링합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4cf6c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}, Score: 0.0\n",
      "Content: bar, Metadata: {'page': 1}, Score: 0.31470414996147156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/7lvqf88d2hxdjf1k1f_0gbs00000gn/T/ipykernel_43974/2456782812.py:2: RuntimeWarning: coroutine 'FAISS.asimilarity_search_with_score' was never awaited\n",
      "  results_with_scores = await db.asimilarity_search_with_score(\"foo\", filter=dict(page=1))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# 유사도 검색을 수행하고 결과를 점수와 함께 가져옵니다. 필터로 page가 1인 문서만 검색합니다.\n",
    "results_with_scores = await db.asimilarity_search_with_score(\"foo\", filter=dict(page=1))\n",
    "for (\n",
    "    doc,\n",
    "    score,\n",
    ") in results_with_scores:  # 검색 결과를 반복하면서 각 문서와 점수를 가져옵니다.\n",
    "    # 문서의 내용, 메타데이터, 점수를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ff90d",
   "metadata": {},
   "source": [
    "## Max Marginal Relevance (MMR)\n",
    "\n",
    "`MMR(Maximal Marginal Relevance)` 방식은 쿼리에 대한 관련 항목을 검색할 때 중복을 피하는 방법 중 하나입니다. 단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 검색된 항목들 사이에 **관련성과 다양성 사이의 균형을 보장**합니다. 이는 **자주 발생할 수 있는, 매우 유사한 항목들만이 검색되는 상황을 방지** 하는 데에 유용합니다.\n",
    "\n",
    "예를 들어, 특정 주제에 대해 정보를 찾고 있다고 가정해봅시다. 가장 관련성 높은 문서만을 반환하는 시스템은 비슷비슷한 정보를 담은 문서들을 제공할 수 있습니다. 하지만, MMR 방식을 사용하면, 검색된 문서들이 해당 주제에 대해 서로 다른 관점이나 새로운 정보를 제공하도록 합니다. 이로써 사용자는 주제에 대해 보다 폭넓은 이해를 할 수 있게 됩니다.\n",
    "\n",
    "MMR은 두 가지 주요 요소, 즉 쿼리에 대한 문서의 관련성과 **이미 선택된 문서들과의 차별성을 동시에 고려** 합니다.\n",
    "\n",
    "1. 첫째로, 쿼리와의 관련성이 높은 문서를 찾는 것이 중요합니다.\n",
    "2. 둘째로, 이미 선택된 문서들과는 다른 새로운 정보나 관점을 제공하는 문서를 찾는 것입니다.\n",
    "\n",
    "이 두 가지 요소의 균형을 맞추는 것이 MMR의 핵심입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e13ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n",
      "Content: bar, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"를 검색어로 사용하여 최대 한계 관련성 검색을 수행하고, 필터로 page가 1인 문서만 검색합니다.\n",
    "results = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\n",
    "for doc in results:  # 검색 결과를 반복합니다.\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52c0bf",
   "metadata": {},
   "source": [
    "`similarity_search` 함수를 호출할 때 `fetch_k` 매개변수를 설정하는 방법에 대한 예시입니다.\n",
    "\n",
    "일반적으로 `fetch_k` 매개변수는 `k` 매개변수보다 훨씬 큰 값으로 설정하는 것이 좋습니다. 그 이유는 `fetch_k` 매개변수가 필터링 전에 가져올 문서의 수를 나타내기 때문입니다.\n",
    "\n",
    "만약 `fetch_k`를 낮은 값으로 설정하면, 필터링할 문서가 충분하지 않을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47acaee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"와 유사한 문서를 검색하고, 'page' 메타데이터가 1인 문서만 필터링하여 가장 유사한 1개의 문서를 반환하되, 4개의 문서를 가져옵니다.\n",
    "results = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\n",
    "for doc in results:  # 검색 결과를 반복하면서 각 문서에 대해 다음을 수행합니다.\n",
    "    # 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2131ee4",
   "metadata": {},
   "source": [
    "## 문서 삭제\n",
    "\n",
    "벡터 저장소에 저장된 문서를 삭제할 수도 있습니다. 이때 삭제할 문서의 ID는 DB 에 저장된 ID와 일치해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded07e0",
   "metadata": {},
   "source": [
    "`db.delete()` 메서드를 사용하여 문서를 삭제합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01757d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'660aafd1-5952-4caf-b55d-e7cc3db10712'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d96635ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 0에 해당하는 문서 저장소 ID를 사용하여 데이터베이스에서 문서를 삭제합니다.\n",
    "db.delete([db.index_to_docstore_id[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
