{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlK44FRyPYaR"
   },
   "source": [
    "# <랭체인LangChain 노트> RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;랭체인LangChain 노트&gt; - LangChain 한국어 튜토리얼🇰🇷</td>\n",
       "      <td>https://wikidocs.net//book/14314</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH01 LangChain 시작하기</td>\n",
       "      <td>https://wikidocs.net/233341</td>\n",
       "      <td># CH01 LangChain 시작하기\\n\\nLangChain은 언어 모델을 활용해...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01. OpenAI API 키 발급 및 테스트</td>\n",
       "      <td>https://wikidocs.net/233342</td>\n",
       "      <td># 01. OpenAI API 키 발급 및 테스트\\n\\n## OpenAI API 키...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02. OpenAI API 사용(GPT-4o 멀티모달)</td>\n",
       "      <td>https://wikidocs.net/233343</td>\n",
       "      <td># 02. OpenAI API 사용(GPT-4o 멀티모달)\\n\\n```\\nCopy#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03. LangChain Expression Language(LCEL)</td>\n",
       "      <td>https://wikidocs.net/233344</td>\n",
       "      <td># 03. LangChain Expression Language(LCEL)\\n\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>CH15 파인튜닝(Fine Tuning)</td>\n",
       "      <td>https://wikidocs.net/233783</td>\n",
       "      <td># CH15 파인튜닝(Fine Tuning)\\n\\n파인튜닝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>CH16 사례(Use Cases)</td>\n",
       "      <td>https://wikidocs.net/233784</td>\n",
       "      <td># CH16 사례(Use Cases)\\n\\nuse cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>01. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools)</td>\n",
       "      <td>https://wikidocs.net/234162</td>\n",
       "      <td># 01. 도구를 활용한 토론 에이전트(Two Agent Debates with T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CH17 LangGraph</td>\n",
       "      <td>https://wikidocs.net/233785</td>\n",
       "      <td># CH17 LangGraph\\n\\nLangGraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>01. Chain of Table for Multiple Tables</td>\n",
       "      <td>https://wikidocs.net/233325</td>\n",
       "      <td># 01. Chain of Table for Multiple Tables\\n\\nCY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0             <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷   \n",
       "1                                 CH01 LangChain 시작하기    \n",
       "2                            01. OpenAI API 키 발급 및 테스트   \n",
       "3                       02. OpenAI API 사용(GPT-4o 멀티모달)   \n",
       "4              03. LangChain Expression Language(LCEL)   \n",
       "..                                                 ...   \n",
       "102                             CH15 파인튜닝(Fine Tuning)   \n",
       "103                                 CH16 사례(Use Cases)   \n",
       "104  01. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools)   \n",
       "105                                     CH17 LangGraph   \n",
       "106             01. Chain of Table for Multiple Tables   \n",
       "\n",
       "                               source  \\\n",
       "0    https://wikidocs.net//book/14314   \n",
       "1         https://wikidocs.net/233341   \n",
       "2         https://wikidocs.net/233342   \n",
       "3         https://wikidocs.net/233343   \n",
       "4         https://wikidocs.net/233344   \n",
       "..                                ...   \n",
       "102       https://wikidocs.net/233783   \n",
       "103       https://wikidocs.net/233784   \n",
       "104       https://wikidocs.net/234162   \n",
       "105       https://wikidocs.net/233785   \n",
       "106       https://wikidocs.net/233325   \n",
       "\n",
       "                                               content  \n",
       "0                                                       \n",
       "1    # CH01 LangChain 시작하기\\n\\nLangChain은 언어 모델을 활용해...  \n",
       "2    # 01. OpenAI API 키 발급 및 테스트\\n\\n## OpenAI API 키...  \n",
       "3    # 02. OpenAI API 사용(GPT-4o 멀티모달)\\n\\n```\\nCopy#...  \n",
       "4    # 03. LangChain Expression Language(LCEL)\\n\\n#...  \n",
       "..                                                 ...  \n",
       "102                   # CH15 파인튜닝(Fine Tuning)\\n\\n파인튜닝  \n",
       "103                  # CH16 사례(Use Cases)\\n\\nuse cases  \n",
       "104  # 01. 도구를 활용한 토론 에이전트(Two Agent Debates with T...  \n",
       "105                      # CH17 LangGraph\\n\\nLangGraph  \n",
       "106  # 01. Chain of Table for Multiple Tables\\n\\nCY...  \n",
       "\n",
       "[107 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 pandas DataFrame으로 로드합니다.\n",
    "df = pd.read_csv('data_list_with_content.csv')\n",
    "\n",
    "# 'content' 열의 NaN 값을 빈 문자열로 대체합니다.\n",
    "df['content'] = df['content'].fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 테디노트 파일의 개수: 825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "# 문서 리스트를 생성합니다.\n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['content']):\n",
    "        continue\n",
    "    chunks = text_splitter.split_text(row['content'])\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(page_content=chunk, metadata={\"title\": row['title'], \"source\": row['source']}))\n",
    "\n",
    "print(f\"분할된 테디노트 파일의 개수: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_openai와 langchain의 필요한 모듈들을 가져옵니다.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# 로컬 파일 저장소를 사용하기 위해 LocalFileStore 인스턴스를 생성합니다.\n",
    "# './cache/' 디렉토리에 데이터를 저장합니다.\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# OpenAI 임베딩 모델 인스턴스를 생성합니다. 모델명으로 \"text-embedding-3-small\"을 사용합니다.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", disallowed_special=())\n",
    "\n",
    "# CacheBackedEmbeddings를 사용하여 임베딩 계산 결과를 캐시합니다.\n",
    "# 이렇게 하면 임베딩을 여러 번 계산할 필요 없이 한 번 계산된 값을 재사용할 수 있습니다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, store, namespace=embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_community 모듈에서 FAISS 클래스를 가져옵니다.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 로컬에 저장할 FAISS 인덱스의 폴더 이름을 지정합니다.\n",
    "FAISS_DB_INDEX = \"teddynote_faiss\"\n",
    "\n",
    "# combined_documents 문서들과 cached_embeddings 임베딩을 사용하여 FAISS 데이터베이스 인스턴스를 생성합니다.\n",
    "db = FAISS.from_documents(documents, cached_embeddings)\n",
    "\n",
    "# 생성된 데이터베이스 인스턴스를 지정한 폴더에 로컬로 저장합니다.\n",
    "db.save_local(folder_path=FAISS_DB_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 클래스의 load_local 메서드를 사용하여 저장된 벡터 인덱스를 로드합니다.\n",
    "db = FAISS.load_local(\n",
    "    FAISS_DB_INDEX,  # 로드할 FAISS 인덱스의 디렉토리 이름\n",
    "    cached_embeddings,  # 임베딩 정보를 제공\n",
    "    allow_dangerous_deserialization=True,  # 역직렬화를 허용하는 옵션\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR을 사용하여 검색을 수행하는 retriever를 생성합니다.\n",
    "faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 20년차 AI 개발자입니다. 당신의 임무는 주어진 질문에 대하여 최대한 문서의 정보를 활용하여 답변하는 것입니다.\n",
    "문서는 Python 코드에 대한 정보를 담고 있습니다. 따라서, 답변을 작성할 때에는 Python 코드에 대한 상세한 code snippet을 포함하여 작성해주세요.\n",
    "최대한 자세하게 답변하고, 한글로 답변해 주세요. 주어진 문서에서 답변을 찾을 수 없는 경우, \"문서에 답변이 없습니다.\"라고 답변해 주세요.\n",
    "답변은 출처(source)를 반드시 표기해 주세요.\n",
    "\n",
    "#참고문서:\n",
    "{context}\n",
    "\n",
    "#질문:\n",
    "{question}\n",
    "\n",
    "#답변: \n",
    "\n",
    "출처:\n",
    "- source1\n",
    "- source2\n",
    "- ...                             \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ").configurable_alternatives(\n",
    "    # 이 필드에 id를 부여합니다.\n",
    "    # 최종 실행 가능한 객체를 구성할 때, 이 id를 사용하여 이 필드를 구성할 수 있습니다.\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    # 기본 키를 설정합니다.\n",
    "    default_key=\"gpt4\",\n",
    "    claude=ChatAnthropic(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    ),\n",
    "    gpt3=ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    ),\n",
    "    ollama=ChatOllama(\n",
    "        model=\"EEVE-Korean-10.8B:long\",\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인을 생성합니다.\n",
    "rag_chain = (\n",
    "    {\"context\": faiss_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate을 사용하는 방법에 대해 설명드리겠습니다. PromptTemplate은 특정 형식의 문자열 템플릿을 정의하고, 이를 기반으로 동적으로 문자열을 생성할 수 있게 해주는 클래스입니다. 주로 변수를 포함한 문자열 템플릿을 정의하고, 이 변수들에 값을 할당하여 최종 문자열을 생성하는 데 사용됩니다.\n",
      "\n",
      "### 기본 사용법\n",
      "\n",
      "1. **PromptTemplate 정의하기**\n",
      "\n",
      "   먼저, 변수를 포함한 문자열 템플릿을 정의합니다. 변수는 중괄호 `{}`로 묶어서 표시합니다.\n",
      "\n",
      "   ```python\n",
      "   from langchain_core.prompts import PromptTemplate\n",
      "   \n",
      "   template = \"{country}의 수도는 어디인가요?\"\n",
      "   ```\n",
      "\n",
      "2. **PromptTemplate 객체 생성하기**\n",
      "\n",
      "   정의한 템플릿 문자열을 사용하여 PromptTemplate 객체를 생성합니다. 이 때, `from_template` 메소드를 사용할 수 있습니다.\n",
      "\n",
      "   ```python\n",
      "   prompt = PromptTemplate.from_template(template)\n",
      "   ```\n",
      "\n",
      "3. **변수에 값 할당하기**\n",
      "\n",
      "   생성된 PromptTemplate 객체에 변수에 해당하는 값을 할당합니다. 이를 위해 `format` 메소드를 사용하고, 변수명을 키워드 인자로 전달합니다.\n",
      "\n",
      "   ```python\n",
      "   prompt = prompt.format(country=\"대한민국\")\n",
      "   ```\n",
      "\n",
      "4. **최종 문자열 생성하기**\n",
      "\n",
      "   변수에 값이 할당된 후, PromptTemplate 객체는 최종 문자열을 포함하게 됩니다. 이 문자열은 PromptTemplate 객체를 문자열로 변환하거나 출력함으로써 확인할 수 있습니다.\n",
      "\n",
      "   ```python\n",
      "   print(prompt)  # '대한민국의 수도는 어디인가요?'\n",
      "   ```\n",
      "\n",
      "### 추가 기능\n",
      "\n",
      "- **변수 유효성 검사**\n",
      "\n",
      "  PromptTemplate을 생성할 때 `input_variables` 인자를 사용하여 템플릿에 포함될 변수들을 명시적으로 지정할 수 있습니다. 이를 통해 변수명의 오타 등을 방지할 수 있습니다.\n",
      "\n",
      "  ```python\n",
      "  prompt = PromptTemplate(\n",
      "      template=template,\n",
      "      input_variables=[\"country\"],\n",
      "  )\n",
      "  ```\n",
      "\n",
      "- **부분 변수 설정**\n",
      "\n",
      "  일부 변수에 미리 값을 할당하고, 나머지 변수에는 나중에 값을 할당하고 싶을 때 `partial_variables` 인자를 사용할 수 있습니다.\n",
      "\n",
      "  ```python\n",
      "  template = \"{country1}과 {country2}의 수도는 각각 어디인가요?\"\n",
      "  prompt = PromptTemplate(\n",
      "      template=template,\n",
      "      input_variables=[\"country1\"],\n",
      "      partial_variables={\"country2\": \"미국\"}\n",
      "  )\n",
      "  ```\n",
      "\n",
      "이러한 방법으로 PromptTemplate을 사용하여 동적으로 문자열을 생성하고, 다양한 상황에 맞게 문자열 포맷을 조정할 수 있습니다.\n",
      "\n",
      "출처: [https://wikidocs.net/233351](https://wikidocs.net/233351)"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.with_config(configurable={\"llm\": \"gpt4\"}).invoke(\n",
    "    \"PromptTemplate 사용방법을 알려주세요\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
