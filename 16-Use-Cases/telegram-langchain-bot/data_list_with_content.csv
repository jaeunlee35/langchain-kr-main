title,source,content
<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷,https://wikidocs.net//book/14314,
CH01 LangChain 시작하기 ,https://wikidocs.net/233341,"# CH01 LangChain 시작하기

LangChain은 언어 모델을 활용해 다양한 애플리케이션을 개발할 수 있는 프레임워크를 말합니다. 이 프레임워크를 통해 언어 모델은 다음과 같은 기능을 수행할 수 있게 됩니다.
- 문맥을 인식하는 기능: LangChain은 언어 모델을 다양한 문맥 소스와 연결합니다. 여기에는 프롬프트 지시사항, 소수의 예시, 응답에 근거한 내용 등이 포함됩니다. 이를 통해 언어 모델은 제공된 정보를 기반으로 더 정확하고 관련성 높은 답변을 생성할 수 있습니다.
- 추론하는 기능: 또한, 언어 모델은 주어진 문맥을 바탕으로 어떠한 답변을 제공하거나, 어떤 조치를 취해야 할지를 스스로 추론할 수 있습니다. 이는 언어 모델이 단순히 정보를 재생산하는 것을 넘어서, 주어진 상황을 분석하고 적절한 해결책을 제시할 수 있음을 의미합니다.
문맥을 인식하는 기능: LangChain은 언어 모델을 다양한 문맥 소스와 연결합니다. 여기에는 프롬프트 지시사항, 소수의 예시, 응답에 근거한 내용 등이 포함됩니다. 이를 통해 언어 모델은 제공된 정보를 기반으로 더 정확하고 관련성 높은 답변을 생성할 수 있습니다.
추론하는 기능: 또한, 언어 모델은 주어진 문맥을 바탕으로 어떠한 답변을 제공하거나, 어떤 조치를 취해야 할지를 스스로 추론할 수 있습니다. 이는 언어 모델이 단순히 정보를 재생산하는 것을 넘어서, 주어진 상황을 분석하고 적절한 해결책을 제시할 수 있음을 의미합니다.
LangChain 을 활용하면 이전에 언급한 기능을 바탕으로검색 증강 생성(RAG) 어플리케이션 제작,구조화된 데이터 분석,챗봇등을 만들 수 있습니다.
더 많은 예제는 유튜브 채널테디노트에서 확인하실 수 있습니다.
## 설치
권장하는 파이썬 버전은3.11버전입니다.
pip 를 이용한 설치
```
Copypip install -r https://raw.githubusercontent.com/teddylee777/langchain-kr/main/requirements.txt
```
## 구성
이 프레임워크는 여러 부분으로 구성되어 있습니다.
- LangChain 라이브러리: Python 및 JavaScript 라이브러리. 다양한 컴포넌트의 인터페이스와 통합, 이러한 컴포넌트를 체인과 에이전트로 결합하는 기본 런타임, 그리고 즉시 사용 가능한 체인과 에이전트의 구현을 포함합니다.
- LangChain 템플릿: 다양한 작업을 위한 쉽게 배포할 수 있는 참조 아키텍처 모음입니다.
- LangServe: LangChain 체인을 REST API로 배포하기 위한 라이브러리입니다.
- LangSmith: 어떤 LLM 프레임워크에도 구축된 체인을 디버그, 테스트, 평가, 모니터링할 수 있게 해주며 LangChain과 원활하게 통합되는 개발자 플랫폼입니다.
- LangGraph: LLM을 사용한 상태유지가 가능한 다중 액터 애플리케이션을 구축하기 위한 라이브러리로, LangChain 위에 구축되었으며 LangChain과 함께 사용하도록 설계되었습니다. 여러 계산 단계에서 다중 체인(또는 액터)을 순환 방식으로 조정할 수 있는 능력을 LangChain 표현 언어에 추가합니다.
LangChain 라이브러리: Python 및 JavaScript 라이브러리. 다양한 컴포넌트의 인터페이스와 통합, 이러한 컴포넌트를 체인과 에이전트로 결합하는 기본 런타임, 그리고 즉시 사용 가능한 체인과 에이전트의 구현을 포함합니다.
LangChain 템플릿: 다양한 작업을 위한 쉽게 배포할 수 있는 참조 아키텍처 모음입니다.
LangServe: LangChain 체인을 REST API로 배포하기 위한 라이브러리입니다.
LangSmith: 어떤 LLM 프레임워크에도 구축된 체인을 디버그, 테스트, 평가, 모니터링할 수 있게 해주며 LangChain과 원활하게 통합되는 개발자 플랫폼입니다.
LangGraph: LLM을 사용한 상태유지가 가능한 다중 액터 애플리케이션을 구축하기 위한 라이브러리로, LangChain 위에 구축되었으며 LangChain과 함께 사용하도록 설계되었습니다. 여러 계산 단계에서 다중 체인(또는 액터)을 순환 방식으로 조정할 수 있는 능력을 LangChain 표현 언어에 추가합니다.
## 개발 용이성✨
컴포넌트의 조립 및 통합🔧
- LangChain은 언어 모델과의 작업을 위한 조립 가능한 도구 및 통합을 제공합니다.
- 컴포넌트는 모듈식으로 설계되어, 사용하기 쉽습니다. 이는 개발자가 LangChain 프레임워크를 자유롭게 활용할 수 있게 해줍니다.
LangChain은 언어 모델과의 작업을 위한 조립 가능한 도구 및 통합을 제공합니다.
컴포넌트는 모듈식으로 설계되어, 사용하기 쉽습니다. 이는 개발자가 LangChain 프레임워크를 자유롭게 활용할 수 있게 해줍니다.
즉시 사용 가능한 체인🚀
- 고수준 작업을 수행하기 위한 컴포넌트의 내장 조합을 제공합니다.
- 이러한 체인은 개발 과정을 간소화하고 속도를 높여줍니다.
고수준 작업을 수행하기 위한 컴포넌트의 내장 조합을 제공합니다.
이러한 체인은 개발 과정을 간소화하고 속도를 높여줍니다.
## 주요 모듈 📌
모델 I/O📃
- 프롬프트 관리, 최적화 및 LLM과의 일반적인 인터페이스와 작업을 위한 유틸리티를 포함합니다.
프롬프트 관리, 최적화 및 LLM과의 일반적인 인터페이스와 작업을 위한 유틸리티를 포함합니다.
검색📚
- '데이터 강화 생성'에 초점을 맞춘 이 모듈은 생성 단계에서 필요한 데이터를 외부 데이터 소스에서 가져오는 작업을 담당합니다.
'데이터 강화 생성'에 초점을 맞춘 이 모듈은 생성 단계에서 필요한 데이터를 외부 데이터 소스에서 가져오는 작업을 담당합니다.
에이전트🤖
- 언어 모델이 어떤 조치를 취할지 결정하고, 해당 조치를 실행하며, 관찰하고, 필요한 경우 반복하는 과정을 포함합니다.
언어 모델이 어떤 조치를 취할지 결정하고, 해당 조치를 실행하며, 관찰하고, 필요한 경우 반복하는 과정을 포함합니다.
LangChain을 활용하면, 언어 모델 기반 애플리케이션의 개발을 보다 쉽게 시작할 수 있으며, 필요에 맞게 기능을 맞춤 설정하고, 다양한 데이터 소스와 통합하여 복잡한 작업을 처리할 수 있게 됩니다."
01. OpenAI API 키 발급 및 테스트,https://wikidocs.net/233342,"# 01. OpenAI API 키 발급 및 테스트

## OpenAI API 키 발급 및 설정
1) OpenAI API 키 발급
OpenAI API 웹사이트에 접속합니다.
- 링크:https://platform.openai.com/docs/overview
링크:https://platform.openai.com/docs/overview
우측 상단""Sign Up""을 눌러회원가입을 진행합니다.
(만약, 이미 가입되어 있는 상태라면""Log in"" 버튼을 눌러 로그인합니다.

우측 상단톱니바퀴(Setting) 를 눌러 설정으로 이동합니다.

왼쪽""Billing""메뉴에서""Payment methods""를 클릭하여신용카드를 등록합니다.

신용카드를 등록했다면 아래와 같이 등록된 신용카드가 목록에 나타납니다.

""Add to credit balance""버튼을 눌러 사용할만큼의 미화(달러) 를 입력합니다.

금액은$5부터 추가가 가능합니다. (즉, 최소결제금액인 $5 이상은 결제를 해야 합니다)
금액을 입력한 후""Continue""를 눌러 결제를 진행합니다.

왼쪽의""Limits""탭에서 월간 사용한도를 설정할 수 있습니다.
- ""Set a monthly budge"": 월간 사용한도를 지정합니다. 이 금액에 도달하면 더이상 과금하지 않고 API 는 사용을 멈춥니다.
- ""Set an email notification threshold"": 이메일이 발송되는 요금을 지정할 수 있습니다. 이 금액에 도달하면 이메일이 발송됩니다.
""Set a monthly budge"": 월간 사용한도를 지정합니다. 이 금액에 도달하면 더이상 과금하지 않고 API 는 사용을 멈춥니다.
""Set an email notification threshold"": 이메일이 발송되는 요금을 지정할 수 있습니다. 이 금액에 도달하면 이메일이 발송됩니다.

우측프로필 이미지클릭 -""Your profile""

API Key 관리 메뉴로 접속합니다.
- https://platform.openai.com/api-keys
https://platform.openai.com/api-keys
""Create new secret key""를 클릭합니다.

Name과프로젝트를 입력합니다. (별도 생성한 프로젝트가 없다면 Default project 를 설정합니다)

우측""Copy""버튼을 눌러 키를 복사합니다.
주의!!!
- 키가 유출되면 다른 사람이 내 API KEY 를 사용하여 GPT 를 사용할 수 있으며, 결제는 제 지갑에서 결제됩니다.
- 절대키는 타인에게 공유하지 마시고, 안전한 곳에 보관하세요! (암호라고 생각하세요)
키가 유출되면 다른 사람이 내 API KEY 를 사용하여 GPT 를 사용할 수 있으며, 결제는 제 지갑에서 결제됩니다.
절대키는 타인에게 공유하지 마시고, 안전한 곳에 보관하세요! (암호라고 생각하세요)

2).env파일 설정
프로젝트 루트 디렉토리에.env파일을 생성합니다.
.env파일에OPENAI_API_KEY=방금복사한 키를 입력한 뒤 Ctrl + S 를 눌러 저장하고 파일을 닫습니다.

```
Copy# LangChain 업데이트!pip install -r https://raw.githubusercontent.com/teddylee777/langchain-kr/main/requirements.txt
```
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일# 설치: pip install python-dotenvfromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
API Key 가 잘 설정되었는지 확인합니다.
```
Copyimportosprint(f""[API KEY]\n{os.environ['OPENAI_API_KEY']}"")
```"
02. LangSmith 추적 설정,https://wikidocs.net/250954,"# 02. LangSmith 추적 설정

## LangSmith 추적 설정하기
LangSmith는LLM 애플리케이션 개발, 모니터링 및 테스트를 위한 플랫폼입니다. 
프로젝트나 LangChain 학습을 시작하시는 분들이라면 LangSmith는 꼭 설정 후 진행하는 것을 추천 드립니다.
## LangSmith 의 추적기능
추적은 LLM 애플리케이션의 동작을 이해하기 위한 강력한 도구입니다. LangSmith는 LangChain 사용 여부와 관계없이 동급 최고의 추적 기능을 제공합니다.
추적은 다음과 같은 문제를 추적하는 데 도움이 될 수 있습니다.
- 예상치 못한 최종 결과
- 에이전트가 루핑되는 이유
- 체인이 예상보다 느린 이유
- 에이전트가 각 단계에서 사용한 토큰 수
예상치 못한 최종 결과
에이전트가 루핑되는 이유
체인이 예상보다 느린 이유
에이전트가 각 단계에서 사용한 토큰 수
프로젝트 단위로 실행 카운트, Error 발생률, 토큰 사용량, 과금 정보등을 확인할 수 있습니다.

프로젝트를 클릭하면 실행된 모든 Run 이 나타납니다.


1개의 실행을 한 뒤 retrieve 된 문서의 검색 결과 뿐만 아니라, GPT 의 입출력 내용에 대해서 자세하게 기록합니다.
따라서, 문서의 검색된 내용을 확인 후 검색 알고리즘을 변경해야할지 혹은 프롬프트를 변경해야할지 판단하는데 도움이 됩니다.
뿐만 아니라, 상단에는 1개의 실행(Run) 이 걸린 시간(약 30초)와 사용된 토큰(5,104) 등이 표기가 되고, 토큰에 마우스 호버를 하게 되면 청구 금액까지 표기해 줍니다.
## LangSmith 추적 사용하기
추적을 사용하는 방법은 매우 간단합니다.
https://smith.langchain.com/ 으로 접속하여 회원가입을 진행합니다.
가입후 이메일 인증하는 절차를 진행해야 합니다.
왼쪽 톱니바퀴(Setting) - 가운데 ""Personal"" - ""Create API Key"" 를 눌러 API 키를 발급 받습니다.

Description 에 본인이 알 수 있는 설명을 넣고 Create API Key 버튼을 클릭하여 생성합니다.

생성한 키를 복사한 뒤 다음 단계로 진행합니다.
(주의!) 생성한 키를 유출하지 않도록 안전한 곳에 복사해 두세요.

먼저,.env파일에 LangSmith 에서 발급받은 키와 프로젝트 정보를 입력합니다.
- LANGCHAIN_TRACING_V2:""true""로 설정하면 추적을 시작합니다.
- LANGCHAIN_ENDPOINT:https://api.smith.langchain.com변경하지 않습니다.
- LANGCHAIN_API_KEY: 이전 단계에서발급받은 키를 입력합니다.
- LANGCHAIN_PROJECT:프로젝트 명을 기입하면 해당 프로젝트 그룹으로 모든 실행(Run) 이 추적됩니다.
LANGCHAIN_TRACING_V2:""true""로 설정하면 추적을 시작합니다.
LANGCHAIN_ENDPOINT:https://api.smith.langchain.com변경하지 않습니다.
LANGCHAIN_API_KEY: 이전 단계에서발급받은 키를 입력합니다.
LANGCHAIN_PROJECT:프로젝트 명을 기입하면 해당 프로젝트 그룹으로 모든 실행(Run) 이 추적됩니다.

## Jupyter Notebook 혹은 코드에서 추적을 활성화 하기
추적을 활성화 하는 방법은 매우 간단합니다.
환경 변수만 설정하면 됩니다.
.env 에 설정한 내용을 불러옵니다.
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
만약 설정한 추적이 활성화 되어 있고, API KEY 와 프로젝트 명이 제대로 설정되어 있다면, 이걸로도 충분합니다.
하지만, 프로젝트 명을 변경하거나, 추적을 변경하고 싶을 때는 아래의 코드로 변경할 수 있습니다.
```
Copyimportos

os.environ[""LANGCHAIN_TRACING_V2""] =""true""os.environ[""LANGCHAIN_ENDPOINT""] =""https://api.smith.langchain.com""os.environ[""LANGCHAIN_PROJECT""] =""LangChain 프로젝트명""os.environ[""LANGCHAIN_API_KEY""] =""LangChain API KEY 입력""
```
## langchain-teddynote
langchain 관련 기능을 보다 더 편리하게 사용하기 위한 목적으로langchain-teddynote패키지를 만들었습니다.
설치코드 (터미널에서 실행 혹은 Jupyter Notebook 에서 실행)
```
Copypip install langchain-teddynote
```
.env파일에LangSmith API 키가 설정되어 있어야 합니다.(LANGCHAIN_API_KEY)
```
Copyfromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""원하는 프로젝트명"")
```
출력예시
```
CopyLangSmith 추적을 시작합니다.[프로젝트명]랭체인 튜토리얼 프로젝트
```
추척을 원하지 않을 때는 다음과 같이추적을 끌 수있습니다.
```
Copyfromlangchain_teddynoteimportlogging# set_enable=False 로 지정하면 추적을 하지 않습니다.logging.langsmith(""랭체인 튜토리얼 프로젝트"", set_enable=False)
```"
03. OpenAI API 사용(GPT-4o 멀티모달),https://wikidocs.net/233343,"# 03. OpenAI API 사용(GPT-4o 멀티모달)

```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copy# LangSmith 추적을 설정합니다. https://smith.langchain.com# .env 파일에 LANGCHAIN_API_KEY를 입력합니다.# !pip install -qU langchain-teddynotefromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""CH01-Basic"")
```
```
LangSmith 추적을 시작합니다.
[프로젝트명]
CH01-Basic
```
## ChatOpenAI
OpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.
객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.
temperature
- 사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.
사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.
max_tokens
- 채팅 완성에서 생성할 토큰의 최대 개수입니다.
채팅 완성에서 생성할 토큰의 최대 개수입니다.
model_name: 적용 가능한 모델 리스트
-gpt-3.5-turbo-gpt-4-turbo-gpt-4o

- 링크: https://platform.openai.com/docs/models
링크: https://platform.openai.com/docs/models
```
Copyfromlangchain_openaiimportChatOpenAI# 객체 생성llm = ChatOpenAI(
    temperature=0.1,# 창의성 (0.0 ~ 2.0)model_name=""gpt-4o"",# 모델명)# 질의내용question =""대한민국의 수도는 어디인가요?""# 질의print(f""[답변]:{llm.invoke(question)}"")
```
```
[답변]: content='대한민국의 수도는 서울입니다. 서울은 대한민국의 정치, 경제, 문화의 중심지로서 많은 인구와 다양한 명소를 자랑하는 도시입니다.' response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 16, 'total_tokens': 52}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None} id='run-dcda4cfa-3143-4982-b24c-4a25ce0a447e-0' usage_metadata={'input_tokens': 16, 'output_tokens': 36, 'total_tokens': 52}
```
```
Copy# 질의내용question =""대한민국의 수도는 어디인가요?""# 질의response = llm.invoke(question)
```
```
Copyresponse
```
```
AIMessage(content='대한민국의 수도는 서울입니다. 서울은 대한민국의 정치, 경제, 문화의 중심지로서 많은 인구와 다양한 명소를 자랑하는 도시입니다.', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 16, 'total_tokens': 52}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_aa87380ac5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3296402a-f47b-4ace-88cd-b74efb7465fb-0', usage_metadata={'input_tokens': 16, 'output_tokens': 36, 'total_tokens': 52})
```
```
Copyresponse.content
```
```
'대한민국의 수도는 서울입니다. 서울은 대한민국의 정치, 경제, 문화의 중심지로서 많은 인구와 다양한 명소를 자랑하는 도시입니다.'
```
```
Copyresponse.response_metadata
```
```
{'token_usage': {'completion_tokens': 36,  'prompt_tokens': 16,  'total_tokens': 52}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_aa87380ac5', 'finish_reason': 'stop', 'logprobs': None}
```
주어진 텍스트에 대한 모델의토큰 확률의 로그 값을 의미합니다. 토큰이란 문장을 구성하는 개별 단어나 문자 등의 요소를 의미하고, 확률은모델이 그 토큰을 예측할 확률을 나타냅니다.
```
Copy# 객체 생성llm_with_logprob = ChatOpenAI(
    temperature=0.1,# 창의성 (0.0 ~ 2.0)max_tokens=2048,# 최대 토큰수model_name=""gpt-3.5-turbo"",# 모델명).bind(logprobs=True)
```
```
Copy# 질의내용question =""대한민국의 수도는 어디인가요?""# 질의response = llm_with_logprob.invoke(question)
```
```
Copy# 결과 출력response.response_metadata
```
```
{'token_usage': {'completion_tokens': 15,  'prompt_tokens': 24,  'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [{'token': '대',    'bytes': [235, 140, 128],    'logprob': -0.03859115,    'top_logprobs': []},   {'token': '한',    'bytes': [237, 149, 156],    'logprob': -5.5122365e-07,    'top_logprobs': []},   {'token': '\\xeb\\xaf',    'bytes': [235, 175],    'logprob': -2.8160932e-06,    'top_logprobs': []},   {'token': '\\xbc', 'bytes': [188], 'logprob': 0.0, 'top_logprobs': []},   {'token': '\\xea\\xb5',    'bytes': [234, 181],    'logprob': -6.704273e-07,    'top_logprobs': []},   {'token': '\\xad', 'bytes': [173], 'logprob': 0.0, 'top_logprobs': []},   {'token': '의',    'bytes': [236, 157, 152],    'logprob': -6.2729996e-06,    'top_logprobs': []},   {'token': ' 수',    'bytes': [32, 236, 136, 152],    'logprob': -5.5122365e-07,    'top_logprobs': []},   {'token': '도',    'bytes': [235, 143, 132],    'logprob': -5.5122365e-07,    'top_logprobs': []},   {'token': '는',    'bytes': [235, 138, 148],    'logprob': -1.9361265e-07,    'top_logprobs': []},   {'token': ' 서',    'bytes': [32, 236, 132, 156],    'logprob': -5.080963e-06,    'top_logprobs': []},   {'token': '\\xec\\x9a',    'bytes': [236, 154],    'logprob': 0.0,    'top_logprobs': []},   {'token': '\\xb8', 'bytes': [184], 'logprob': 0.0, 'top_logprobs': []},   {'token': '입니다',    'bytes': [236, 158, 133, 235, 139, 136, 235, 139, 164],    'logprob': -0.13815464,    'top_logprobs': []},   {'token': '.',    'bytes': [46],    'logprob': -9.0883464e-07,    'top_logprobs': []}]}}
```
스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.
```
Copy# 스트림 방식으로 질의# answer 에 스트리밍 답변의 결과를 받습니다.answer = llm.stream(""대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!"")
```
```
Copy# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)fortokeninanswer:print(token.content, end="""", flush=True)
```
```
물론입니다! 대한민국에는 아름다운 관광지가 많이 있습니다. 다음은 그 중 10곳과 그 주소입니다:

1. **경복궁**
   - 주소: 서울특별시 종로구 사직로 161

2. **부산 해운대 해수욕장**
   - 주소: 부산광역시 해운대구 우동

3. **제주도 한라산 국립공원**
   - 주소: 제주특별자치도 제주시 1100로 2070-61

4. **경주 불국사**
   - 주소: 경상북도 경주시 불국로 385

5. **설악산 국립공원**
   - 주소: 강원도 속초시 설악산로 833

6. **남이섬**
   - 주소: 강원도 춘천시 남산면 남이섬길 1

7. **안동 하회마을**
   - 주소: 경상북도 안동시 풍천면 하회종가길 40

8. **전주 한옥마을**
   - 주소: 전라북도 전주시 완산구 기린대로 99

9. **서울 남산타워 (N서울타워)**
   - 주소: 서울특별시 용산구 남산공원길 105

10. **보성 녹차밭 대한다원**
    - 주소: 전라남도 보성군 보성읍 녹차로 763-67

이 관광지들은 각기 다른 매력을 가지고 있어 다양한 경험을 할 수 있습니다. 즐거운 여행 되세요!
```
```
Copyfromlangchain_teddynote.messagesimportstream_response# 스트림 방식으로 질의# answer 에 스트리밍 답변의 결과를 받습니다.answer = llm.stream(""대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!"")
stream_response(answer)
```
```
물론입니다! 대한민국에는 아름다운 관광지가 많이 있습니다. 다음은 그 중 10곳과 그 주소입니다:

1. **경복궁**
   - 주소: 서울특별시 종로구 사직로 161

2. **부산 해운대 해수욕장**
   - 주소: 부산광역시 해운대구 우동

3. **제주도 한라산 국립공원**
   - 주소: 제주특별자치도 제주시 1100로 2070-61

4. **경주 불국사**
   - 주소: 경상북도 경주시 불국로 385

5. **설악산 국립공원**
   - 주소: 강원도 속초시 설악산로 833

6. **남이섬**
   - 주소: 강원도 춘천시 남산면 남이섬길 1

7. **전주 한옥마을**
   - 주소: 전라북도 전주시 완산구 기린대로 99

8. **안동 하회마을**
   - 주소: 경상북도 안동시 풍천면 하회종가길 40

9. **서울 남산타워 (N서울타워)**
   - 주소: 서울특별시 용산구 남산공원길 105

10. **순천만 국가정원**
    - 주소: 전라남도 순천시 국가정원1호길 47

이 관광지들은 각기 다른 매력을 가지고 있어 다양한 경험을 할 수 있습니다. 즐거운 여행 되세요!
```
## 멀티모달 모델(이미지 인식)
멀티모달은 여러 가지 형태의 정보(모달)를 통합하여 처리하는 기술이나 접근 방식을 의미합니다. 이는 다음과 같은 다양한 데이터 유형을 포함할 수 있습니다.
- 텍스트: 문서, 책, 웹 페이지 등의 글자로 된 정보
- 이미지: 사진, 그래픽, 그림 등 시각적 정보
- 오디오: 음성, 음악, 소리 효과 등의 청각적 정보
- 비디오: 동영상 클립, 실시간 스트리밍 등 시각적 및 청각적 정보의 결합
텍스트: 문서, 책, 웹 페이지 등의 글자로 된 정보
이미지: 사진, 그래픽, 그림 등 시각적 정보
오디오: 음성, 음악, 소리 효과 등의 청각적 정보
비디오: 동영상 클립, 실시간 스트리밍 등 시각적 및 청각적 정보의 결합
gpt-4o나gpt-4-turbo모델은 이미지 인식 기능(Vision) 이 추가되어 있는 모델입니다.
```
Copyfromlangchain_teddynote.modelsimportMultiModalfromlangchain_teddynote.messagesimportstream_response# 객체 생성llm = ChatOpenAI(
    temperature=0.1,# 창의성 (0.0 ~ 2.0)max_tokens=2048,# 최대 토큰수model_name=""gpt-4o"",# 모델명)# 멀티모달 객체 생성multimodal_llm = MultiModal(llm)
```
```
Copy# 샘플 이미지 주소(웹사이트로 부터 바로 인식)IMAGE_URL =""https://t3.ftcdn.net/jpg/03/77/33/96/360_F_377339633_Rtv9I77sSmSNcev8bEcnVxTHrXB4nRJ5.jpg""# 이미지 파일로 부터 질의answer = multimodal_llm.stream(IMAGE_URL)# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)stream_response(answer)
```

```
이 이미지는 표 형식의 데이터 테이블을 보여줍니다. 테이블의 제목은 ""TABLE 001: LOREM IPSUM DOLOR AMIS ENIMA ACCUMER TUNA""입니다. 테이블은 다섯 개의 열과 여덟 개의 행으로 구성되어 있습니다.

열 제목은 다음과 같습니다:
1. Loremis
2. Amis terim
3. Gato lepis
4. Tortores

각 행의 데이터는 다음과 같습니다:
1. Lorem dolor siamet: 8,288, 123%, YES, $89
2. Consecter odio: 123, 87%, NO, $129
3. Gatoque accums: 1,005, 12%, NO, $199
4. Sed hac enim rem: 56, 69%, N/A, $199
5. Rempus tortor just: 5,554, 18%, NO, $999
6. Klimas nsecter: 455, 56%, NO, $245
7. Babiask atque accu: 1,222, 2%, YES, $977
8. Enim rem kos: 5,002, 91%, N/A, $522

표 하단에는 작은 글씨로 Lorem ipsum 텍스트가 포함되어 있습니다.
```
```
Copy# 로컬 PC 에 저장되어 있는 이미지의 경로 입력IMAGE_PATH_FROM_FILE =""./images/sample-image.png""# 이미지 파일로 부터 질의(스트림 방식)answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)stream_response(answer)
```

```
이미지 설명 대체 텍스트:

이미지에는 ""FIRST OPENAI DEVDAY EVENT""라는 제목이 상단에 크게 표시되어 있습니다. 이벤트 날짜는 2023년 11월 6일입니다. 주요 업데이트 항목으로는 GPT 4 Turbo, 128k Tokens, Custom GPTs, Assistant API, Price Reduction이 나열되어 있습니다.

이미지 왼쪽 상단에는 ""ASTRA TECHZ"" 로고가 있습니다.

이미지 중앙에는 ""MAIN UPDATES SUMMARISED""라는 제목 아래 주요 업데이트 내용이 요약되어 있습니다. 각 항목 옆에는 체크 표시가 있으며, 세부 내용은 다음과 같습니다:

- Token Length: 128K
- Custom GPTs: Private or Public
- Multi Modal: Img, Video, Voice
- JSON Mode: Guaranteed
- Assistant API: Developers
- Text 2 Speech: Beta Release
- Natural Voice Options: 6 Voices
- GPT Store: Revenue Shared
- Conversation Threading: Per Conversation
- File Uploading: Multiple
- API Price Reduction: 2.5x - 3.5x
- Code Interpreter: Built In
- Function Calling: Built In

이미지 하단에는 ""visit www.astratechz.com to build AI solutions""라는 문구가 있습니다.
```
## System, User 프롬프트 수정
```
Copysystem_prompt =""""""당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다. 
당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다.""""""user_prompt =""""""당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요.""""""# 멀티모달 객체 생성multimodal_llm_with_prompt = MultiModal(
    llm, system_prompt=system_prompt, user_prompt=user_prompt
)
```
```
Copy# 로컬 PC 에 저장되어 있는 이미지의 경로 입력IMAGE_PATH_FROM_FILE =""https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png""# 이미지 파일로 부터 질의(스트림 방식)answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)stream_response(answer)
```

```
주어진 재무제표를 바탕으로 몇 가지 흥미로운 사실을 정리해 보았습니다:

1. **유동자산의 변화**:
   - 제 19기(2019년) 유동자산은 8,349,633백만원으로, 제 18기(2018년) 8,602,837백만원에 비해 감소하였습니다.
   - 특히 현금 및 현금성 자산이 제 18기 1,690,862백만원에서 제 19기 1,002,263백만원으로 크게 감소하였습니다.

2. **매출채권**:
   - 매출채권은 제 18기 4,004,920백만원에서 제 19기 3,981,935백만원으로 소폭 감소하였습니다.

3. **기타수취채권**:
   - 기타수취채권은 제 18기 321,866백만원에서 제 19기 366,141백만원으로 증가하였습니다.

4. **비유동자산의 증가**:
   - 비유동자산은 제 18기 15,127,741백만원에서 제 19기 18,677,453백만원으로 크게 증가하였습니다.
   - 특히, 재고자산이 제 18기 2,426,364백만원에서 제 19기 2,670,294백만원으로 증가하였습니다.

5. **기타유동자산**:
   - 기타유동자산은 제 18기 156,538백만원에서 제 19기 207,596백만원으로 증가하였습니다.

6. **기타장기수취채권**:
   - 기타장기수취채권은 제 18기 118,086백만원에서 제 19기 505,489백만원으로 크게 증가하였습니다.

이러한 변화들은 회사의 자산 구조와 재무 상태에 중요한 영향을 미칠 수 있으며, 특히 현금성 자산의 감소와 비유동자산의 증가가 눈에 띕니다. 이는 회사의 유동성 관리와 장기 투자 전략에 대한 추가적인 분석이 필요함을 시사합니다.
```"
04. LangChain Expression Language(LCEL),https://wikidocs.net/233344,"# 04. LangChain Expression Language(LCEL)

## 기본 예시: 프롬프트 + 모델 + 출력 파서
가장 기본적이고 일반적인 사용 사례는 prompt 템플릿과 모델을 함께 연결하는 것입니다. 이것이 어떻게 작동하는지 보기 위해, 각 나라별 수도를 물어보는 Chain을 생성해 보겠습니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copy# LangSmith 추적을 설정합니다. https://smith.langchain.com# !pip install -qU langchain-teddynotefromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""CH01-Basic"")
```
```
LangSmith 추적을 시작합니다.
[프로젝트명]
CH01-Basic
```
## 프롬프트 템플릿의 활용
PromptTemplate
- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다
- 사용법
- template: 템플릿 문자열입니다. 이 문자열 내에서 중괄호{}는 변수를 나타냅니다.
- input_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.
사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다
사용법
template: 템플릿 문자열입니다. 이 문자열 내에서 중괄호{}는 변수를 나타냅니다.
input_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.
input_variables
- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.
input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.
```
Copyfromlangchain_teddynote.messagesimportstream_response# 스트리밍 출력fromlangchain_core.promptsimportPromptTemplate
```
from_template()메소드를 사용하여 PromptTemplate 객체 생성
```
Copy# template 정의template =""{country}의 수도는 어디인가요?""# from_template 메소드를 이용하여 PromptTemplate 객체 생성prompt_template = PromptTemplate.from_template(template)
prompt_template
```
```
PromptTemplate(input_variables=['country'], template='{country}의 수도는 어디인가요?')
```
```
Copy# prompt 생성prompt = prompt_template.format(country=""대한민국"")
prompt
```
```
'대한민국의 수도는 어디인가요?'
```
```
Copy# prompt 생성prompt = prompt_template.format(country=""미국"")
prompt
```
```
'미국의 수도는 어디인가요?'
```
```
Copyfromlangchain_openaiimportChatOpenAI

model = ChatOpenAI(
    model=""gpt-3.5-turbo"",
    max_tokens=2048,
    temperature=0.1,
)
```
## Chain 생성

여기서 우리는 LCEL을 사용하여 다양한 구성 요소를 단일 체인으로 결합합니다
```
Copychain= prompt | model | output_parser
```
|기호는unix 파이프 연산자와 유사하며, 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달합니다.
이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달됩니다. 각 구성 요소를 개별적으로 살펴보면 무슨 일이 일어나고 있는지 이해할 수 있습니다.
```
Copy# prompt 를 PromptTemplate 객체로 생성합니다.prompt = PromptTemplate.from_template(""{topic} 에 대해 쉽게 설명해주세요."")

model = ChatOpenAI()

chain = prompt | model
```
- python 딕셔너리 형태로 입력값을 전달합니다.(키: 값)
- invoke() 함수 호출 시, 입력값을 전달합니다.
python 딕셔너리 형태로 입력값을 전달합니다.(키: 값)
invoke() 함수 호출 시, 입력값을 전달합니다.
```
Copy# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'으로 설정합니다.input= {""topic"":""인공지능 모델의 학습 원리""}
```
```
Copy# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.chain.invoke(input)
```
```
AIMessage(content='인공지능 모델의 학습 원리는 데이터를 이용하여 패턴을 학습하는 것입니다. 모델은 입력 데이터를 받아들이고 내부적으로 가중치를 조정하여 원하는 결과를 출력합니다. 학습 과정에서 모델은 입력 데이터와 정답 데이터를 이용하여 오차를 계산하고 이 오차를 최소화하는 방향으로 가중치를 업데이트합니다. 이렇게 반복적으로 학습을 진행하면 모델은 입력 데이터로부터 패턴을 학습하여 정확한 결과를 예측하게 됩니다. 이러한 학습 원리를 통해 인공지능 모델은 데이터를 이용하여 스스로 학습하고 문제를 해결할 수 있습니다.', response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 33, 'total_tokens': 247}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7f8a08f4-51ba-4d14-b9d2-2e092be3e7aa-0', usage_metadata={'input_tokens': 33, 'output_tokens': 214, 'total_tokens': 247})
```
아래는 스트리밍을 출력하는 예시 입니다.
```
Copy# 스트리밍 출력을 위한 요청answer = chain.stream(input)# 스트리밍 출력stream_response(answer)
```
```
인공지능 모델의 학습 원리는 데이터를 입력으로 받아서 패턴을 학습하고 이를 기반으로 예측이나 분류를 수행하는 과정입니다. 

학습 과정은 크게 입력층, 은닉층, 출력층으로 구성된 인공신경망을 사용합니다. 입력층에서 데이터를 받아 은닉층을 거쳐 출력층으로 결과를 출력하는 구조입니다.

이때, 모델은 주어진 데이터를 통해 가중치를 조정하고 오차를 최소화하는 방향으로 학습을 진행합니다. 이를 위해 주어진 데이터에 대해 예측을 수행하고 실제 값과 비교하여 오차를 계산한 후, 이 오차를 줄이기 위해 가중치를 업데이트합니다.

이러한 반복적인 과정을 통해 모델은 데이터 간의 패턴을 학습하고 새로운 데이터에 대해 정확한 예측을 수행할 수 있게 됩니다. 이렇게 학습된 모델은 새로운 데이터에 대해 일반화된 예측을 할 수 있습니다.
```
```
Copyfromlangchain_core.output_parsersimportStrOutputParser

output_parser = StrOutputParser()
```
Chain 에 출력파서를 추가합니다.
```
Copy# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.chain = prompt | model | output_parser
```
```
Copy# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.input= {""topic"":""인공지능 모델의 학습 원리""}
chain.invoke(input)
```
```
'인공지능 모델의 학습 원리는 데이터를 입력으로 받아서 패턴을 학습하는 것입니다. 모델은 입력 데이터를 받아서 내부적으로 가중치를 조절하면서 원하는 결과를 출력하도록 학습됩니다. 이때, 모델은 입력 데이터와 출력 데이터 간의 관계를 학습하여 새로운 입력 데이터에 대한 출력을 예측할 수 있게 됩니다. 이 과정은 반복적으로 이루어지며, 모델은 학습을 통해 점차적으로 정확도를 향상시킵니다. 이러한 방식으로 인공지능 모델은 주어진 데이터를 기반으로 판단하고 예측하는 능력을 향상시킬 수 있습니다.'
```
```
Copy# 스트리밍 출력을 위한 요청answer = chain.stream(input)# 스트리밍 출력stream_response(answer)
```
```
인공지능 모델의 학습 원리는 데이터를 이용해서 패턴을 학습하는 과정입니다. 먼저 모델은 입력 데이터를 받아서 처리하고, 이때 입력 데이터와 정답 데이터를 비교하여 오차를 계산합니다. 이 오차를 최소화하기 위해 모델은 가중치와 편향을 조정하면서 점차적으로 정확한 패턴을 학습해나갑니다. 이런 과정을 반복하여 모델이 데이터에 대해 정확한 예측을 할 수 있도록 학습시키는 것이 인공지능 모델의 핵심 원리입니다.
```
- 아래의 프롬프트 내용을 얼마든지변경하여 테스트 해볼 수 있습니다.
- model_name역시 변경하여 테스트가 가능합니다.
아래의 프롬프트 내용을 얼마든지변경하여 테스트 해볼 수 있습니다.
model_name역시 변경하여 테스트가 가능합니다.
```
Copytemplate =""""""
당신은 영어를 가르치는 10년차 영어 선생님입니다. 상황에 [FORMAT]에 영어 회화를 작성해 주세요.

상황:
{question}

FORMAT:
- 영어 회화:
- 한글 해석:
""""""# 프롬프트 템플릿을 이용하여 프롬프트를 생성합니다.prompt = PromptTemplate.from_template(template)# ChatOpenAI 챗모델을 초기화합니다.model = ChatOpenAI(model_name=""gpt-4-turbo"")# 문자열 출력 파서를 초기화합니다.output_parser = StrOutputParser()
```
```
Copy# 체인을 구성합니다.chain = prompt | model | output_parser
```
```
Copy# 완성된 Chain을 실행하여 답변을 얻습니다.# 스트리밍 출력을 위한 요청answer = chain.stream({""question"":""저는 식당에 가서 음식을 주문하고 싶어요""})# 스트리밍 출력stream_response(answer)
```
```
영어 회화:
- Hello, could I see the menu, please? 
- I'd like to order the grilled salmon and a side of mashed potatoes.
- Could I have a glass of water as well?
- Thank you!

한글 해석:
- 안녕하세요, 메뉴판 좀 볼 수 있을까요?
- 구운 연어와 매시드 포테이토를 주문하고 싶어요.
- 물 한 잔도 주실 수 있나요?
- 감사합니다!
```
```
Copy# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.# 스트리밍 출력을 위한 요청answer = chain.stream({""question"":""미국에서 피자 주문""})# 스트리밍 출력stream_response(answer)
```
```
영어 회화:
- Employee: ""Hello, Tony's Pizza. How can I help you?""
- Customer: ""Hi, I'd like to place an order for delivery, please.""
- Employee: ""Sure thing! What would you like to order?""
- Customer: ""I'll have a large pepperoni pizza with extra cheese and a side of garlic bread.""
- Employee: ""Anything to drink?""
- Customer: ""Yes, a 2-liter bottle of Coke, please.""
- Employee: ""Alright, your total comes to $22.50. Can I have your delivery address?""
- Customer: ""It's 742 Evergreen Terrace.""
- Employee: ""Thank you. Your order will be there in about 30-45 minutes. Is there anything else I can help you with?""
- Customer: ""No, that's everything. Thank you!""
- Employee: ""Thank you for choosing Tony's Pizza. Have a great day!""

한글 해석:
- 직원: ""안녕하세요, 토니의 피자입니다. 어떻게 도와드릴까요?""
- 고객: ""안녕하세요, 배달 주문하고 싶은데요.""
- 직원: ""네, 무엇을 주문하시겠어요?""
- 고객: ""큰 사이즈의 페퍼로니 피자에 치즈 추가하고, 마늘빵 하나 주세요.""
- 직원: ""음료는 드릴까요?""
- 고객: ""네, 콜라 2리터 한 병 주세요.""
- 직원: ""알겠습니다, 합계는 $22.50입니다. 배달 주소를 알려주시겠어요?""
- 고객: ""742 에버그린 테라스입니다.""
- 직원: ""감사합니다. 주문하신 음식은 대략 30-45분 내에 도착할 예정입니다. 다른 도움이 필요하신가요?""
- 고객: ""아니요, 이게 다예요. 감사합니다!""
- 직원: ""토니의 피자를 선택해주셔서 감사합니다. 좋은 하루 되세요!""
```"
05. LCEL 인터페이스,https://wikidocs.net/233345,"# 05. LCEL 인터페이스

## LCEL 인터페이스
사용자 정의 체인을 가능한 쉽게 만들 수 있도록,Runnable프로토콜을 구현했습니다.
Runnable프로토콜은 대부분의 컴포넌트에 구현되어 있습니다.
이는 표준 인터페이스로, 사용자 정의 체인을 정의하고 표준 방식으로 호출하는 것을 쉽게 만듭니다.
표준 인터페이스에는 다음이 포함됩니다.
- stream: 응답의 청크를 스트리밍합니다.
- invoke: 입력에 대해 체인을 호출합니다.
- batch: 입력 목록에 대해 체인을 호출합니다.
stream: 응답의 청크를 스트리밍합니다.
invoke: 입력에 대해 체인을 호출합니다.
batch: 입력 목록에 대해 체인을 호출합니다.
비동기 메소드도 있습니다.
- astream: 비동기적으로 응답의 청크를 스트리밍합니다.
- ainvoke: 비동기적으로 입력에 대해 체인을 호출합니다.
- abatch: 비동기적으로 입력 목록에 대해 체인을 호출합니다.
- astream_log: 최종 응답뿐만 아니라 발생하는 중간 단계를 스트리밍합니다.
astream: 비동기적으로 응답의 청크를 스트리밍합니다.
ainvoke: 비동기적으로 입력에 대해 체인을 호출합니다.
abatch: 비동기적으로 입력 목록에 대해 체인을 호출합니다.
astream_log: 최종 응답뿐만 아니라 발생하는 중간 단계를 스트리밍합니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copy# LangSmith 추적을 설정합니다. https://smith.langchain.comfromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""CH01-Basic"")
```
LCEL 문법을 사용하여 chain 을 생성합니다.
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain_core.promptsimportPromptTemplatefromlangchain_core.output_parsersimportStrOutputParser# ChatOpenAI 모델을 인스턴스화합니다.model = ChatOpenAI()# 주어진 토픽에 대한 농담을 요청하는 프롬프트 템플릿을 생성합니다.prompt = PromptTemplate.from_template(""{topic} 에 대하여 3문장으로 설명해줘."")# 프롬프트와 모델을 연결하여 대화 체인을 생성합니다.chain = prompt | model | StrOutputParser()
```
## stream: 실시간 출력
이 함수는chain.stream메서드를 사용하여 주어진 토픽에 대한 데이터 스트림을 생성하고, 이 스트림을 반복하여 각 데이터의 내용(content)을 즉시 출력합니다.end=""""인자는 출력 후 줄바꿈을 하지 않도록 설정하며,flush=True인자는 출력 버퍼를 즉시 비우도록 합니다.
```
Copy# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.fortokeninchain.stream({""topic"":""멀티모달""}):# 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.print(token, end="""", flush=True)
```
```
멀티모달은 여러 가지 다른 형태의 커뮤니케이션 수단을 통해 정보를 전달하고 상호작용하는 기술을 의미합니다. 예를 들어 음성, 텍스트, 이미지, 동영상 등 다양한 매체를 활용하여 사용자와 상호작용할 수 있습니다. 멀티모달 기술은 사용자 경험을 향상시키고 정보 전달의 효율성을 높이는데 도움을 줄 수 있습니다.
```
## invoke: 호출
chain객체의invoke메서드는 주제를 인자로 받아 해당 주제에 대한 처리를 수행합니다.
```
Copy# chain 객체의 invoke 메서드를 호출하고, 'ChatGPT'라는 주제로 딕셔너리를 전달합니다.chain.invoke({""topic"":""ChatGPT""})
```
```
'ChatGPT는 OpenAI에서 개발한 대화형 인공지능 모델로, 다양한 주제에 대한 대화를 자연스럽게 이어나갈 수 있습니다. 사용자들은 ChatGPT를 통해 질문에 답변을 받거나 대화를 이어가며 새로운 정보를 습득할 수 있습니다. 또한 ChatGPT는 사용자의 입력을 학습하여 점차적으로 더욱 유창하고 자연스러운 대화를 제공합니다.'
```
## batch: 배치(단위 실행)
함수chain.batch는 여러 개의 딕셔너리를 포함하는 리스트를 인자로 받아, 각 딕셔너리에 있는topic키의 값을 사용하여 일괄 처리를 수행합니다.
```
Copy# 주어진 토픽 리스트를 batch 처리하는 함수 호출chain.batch([{""topic"":""ChatGPT""}, {""topic"":""Instagram""}])
```
```
['ChatGPT는 인공지능 챗봇으로 자연어 처리 기술을 사용하여 대화를 수행합니다. 사용자들과 자연스럽게 상호작용하며 다양한 주제에 대해 대화할 수 있습니다. ChatGPT는 정보 제공, 질문 응답, 상담 및 엔터테인먼트 등 다양한 용도로 활용될 수 있습니다.', 'Instagram은 사진과 동영상을 공유하고 다른 사람들과 소통하는 소셜 미디어 플랫폼이다. 해시태그를 통해 관심사나 주제별로 사진을 검색하고 팔로워들과 소통할 수 있다. 인기 있는 인플루언서나 브랜드가 활발하게 활동하는 플랫폼으로 세계적으로 인기가 높다.']
```
max_concurrency매개변수를 사용하여 동시 요청 수를 설정할 수 있습니다
config딕셔너리는max_concurrency키를 통해 동시에 처리할 수 있는 최대 작업 수를 설정합니다. 여기서는 최대 3개의 작업을 동시에 처리하도록 설정되어 있습니다.
```
Copychain.batch(
    [
        {""topic"":""ChatGPT""},
        {""topic"":""Instagram""},
        {""topic"":""멀티모달""},
        {""topic"":""프로그래밍""},
        {""topic"":""머신러닝""},
    ],
    config={""max_concurrency"":3},
)
```
```
['ChatGPT는 인공지능 챗봇으로, 자연어 처리 기술을 사용하여 대화 상대와 상호작용합니다. 사용자의 질문에 응답하고 대화를 이어가며 다양한 주제에 대해 대화할 수 있습니다. ChatGPT는 사용자와 자연스럽게 대화를 나누는 데 도움을 줄 뿐만 아니라 정보를 제공하고 문제 해결을 돕기도 합니다.', 'Instagram은 사진과 동영상을 공유하고 다른 사람들과 소통할 수 있는 소셜 미디어 플랫폼이다. 다양한 필터와 편집 기능을 제공하여 사용자가 쉽게 멋진 사진을 업로드할 수 있으며 해시태그를 통해 관심사에 맞는 콘텐츠를 찾을 수 있다. 인기 있는 인플루언서들의 활동과 광고가 많이 이루어지는 플랫폼이기도 하다.', '멀티모달은 여러 가지의 다른 형태의 정보를 함께 제공하거나 처리하는 기술이다. 이는 텍스트, 이미지, 음성, 비디오 등 여러 형태의 데이터를 통합하여 효과적으로 전달하고 상호작용할 수 있게 한다. 멀티모달은 사용자 경험을 향상시키고 정보를 보다 쉽게 이해하고 활용할 수 있도록 도와준다.', '프로그래밍은 컴퓨터에게 실행할 작업을 지시하는 일종의 커뮤니케이션 방법이다. 이를 위해 프로그래머가 사용하는 언어는 컴퓨터가 이해할 수 있는 형태여야 하며, 문법과 로직을 통해 원하는 결과를 얻을 수 있다. 프로그래밍을 통해 소프트웨어를 개발하고 문제를 해결하는 등 다양한 분야에서 활용할 수 있다.', '머신러닝은 컴퓨터 시스템이 데이터에서 학습하고 패턴을 발견하여 예측하거나 결정을 내리는 인공지능의 한 분야입니다. 이를 통해 컴퓨터는 사람의 개입 없이 스스로 학습하고 문제를 해결할 수 있습니다. 머신러닝은 이미지 및 음성 인식, 자율 주행 자동차, 헬스케어 등 다양한 분야에서 활용되고 있습니다.']
```
## async stream: 비동기 스트림
함수chain.astream은 비동기 스트림을 생성하며, 주어진 토픽에 대한 메시지를 비동기적으로 처리합니다.
비동기 for 루프(async for)를 사용하여 스트림에서 메시지를 순차적으로 받아오고,print함수를 통해 메시지의 내용(s.content)을 즉시 출력합니다.end=""""는 출력 후 줄바꿈을 하지 않도록 설정하며,flush=True는 출력 버퍼를 강제로 비워 즉시 출력되도록 합니다.
```
Copy# 비동기 스트림을 사용하여 'YouTube' 토픽의 메시지를 처리합니다.asyncfortokeninchain.astream({""topic"":""YouTube""}):# 메시지 내용을 출력합니다. 줄바꿈 없이 바로 출력하고 버퍼를 비웁니다.print(token, end="""", flush=True)
```
```
YouTube 는 동영상을 공유하고 시청할 수 있는 온라인 동영상 플랫폼이다. 누구나 자신의 동영상을 업로드하여 다른 사람들과 공유할 수 있고, 영상 콘텐츠를 시청하며 다양한 정보나 즐길거리를 찾을 수 있다. 또한 유명한 크리에이터들의 영상을 통해 엔터테인먼트와 정보를 얻을 수 있다.
```
## async invoke: 비동기 호출
chain객체의ainvoke메서드는 비동기적으로 주어진 인자를 사용하여 작업을 수행합니다. 여기서는topic이라는 키와NVDA(엔비디아의 티커) 라는 값을 가진 딕셔너리를 인자로 전달하고 있습니다. 이 메서드는 특정 토픽에 대한 처리를 비동기적으로 요청하는 데 사용될 수 있습니다.
```
Copy# 비동기 체인 객체의 'ainvoke' 메서드를 호출하여 'NVDA' 토픽을 처리합니다.my_process = chain.ainvoke({""topic"":""NVDA""})
```
```
Copy# 비동기로 처리되는 프로세스가 완료될 때까지 기다립니다.awaitmy_process
```
```
'NVDA는 엔비디아의 주식 코드로, 미국의 반도체 기업인 엔비디아(NVIDIA)의 주식을 말합니다. 엔비디아는 그래픽 처리 유닛(GPU)을 전문으로 하는 기업으로, 인공지능, 가상현실, 자율주행차 등 다양한 분야에서 기술을 제공하고 있습니다. NVDA 주식은 기술 산업의 성장과 함께 높은 수익을 창출하고 있습니다.'
```
## async batch: 비동기 배치
함수abatch는 비동기적으로 일련의 작업을 일괄 처리합니다.
이 예시에서는chain객체의abatch메서드를 사용하여topic에 대한 작업을 비동기적으로 처리하고 있습니다.
await키워드는 해당 비동기 작업이 완료될 때까지 기다리는 데 사용됩니다.
```
Copy# 주어진 토픽에 대해 비동기적으로 일괄 처리를 수행합니다.my_abatch_process = chain.abatch(
    [{""topic"":""YouTube""}, {""topic"":""Instagram""}, {""topic"":""Facebook""}]
)
```
```
Copy# 비동기로 처리되는 일괄 처리 프로세스가 완료될 때까지 기다립니다.awaitmy_abatch_process
```
```
['YouTube는 동영상 공유 플랫폼으로 사용자들이 영상을 업로드하고 시청할 수 있는 서비스입니다. 다양한 콘텐츠를 제공하며 사용자는 무료로 영상을 시청할 수 있습니다. 유명한 유튜버들이 활동하고 수익을 창출할 수 있는 플랫폼으로도 알려져 있습니다.', '인스타그램은 사진과 동영상을 공유하는 소셜 미디어 플랫폼으로, 사용자들은 다양한 필터와 효과를 이용해 자신의 콘텐츠를 멋지게 꾸밀 수 있습니다. 또한 팔로워들과 소통하고, 다른 사용자의 게시물을 좋아하거나 댓글을 남기며 커뮤니케이션을 할 수 있습니다. 인스타그램은 비즈니스나 개인 브랜딩에도 활용되며, 많은 사람들이 일상 속 소소한 순간부터 특별한 순간까지를 공유하고 있습니다.', 'Facebook은 미국의 소셜 네트워크 서비스로, 사용자들이 커뮤니케이션하고 정보를 공유할 수 있는 플랫폼이다. 현재 전 세계적으로 약 30억 명 이상의 사용자가 활동하고 있으며, 광고 및 비즈니스 활동에도 널리 활용되고 있다. 또한 개인정보 보호 문제와 가짜 뉴스 등 여러 논란을 빚어왔으나, 여전히 많은 사람들이 이용하고 있는 대표적인 SNS 서비스이다.']
```
## Parallel: 병렬성
LangChain Expression Language가 병렬 요청을 지원하는 방법을 살펴봅시다.
예를 들어,RunnableParallel을 사용할 때(자주 사전 형태로 작성됨), 각 요소를 병렬로 실행합니다.
langchain_core.runnables모듈의RunnableParallel클래스를 사용하여 두 가지 작업을 병렬로 실행하는 예시를 보여줍니다.
ChatPromptTemplate.from_template메서드를 사용하여 주어진country에 대한수도와면적을 구하는 두 개의 체인(chain1,chain2)을 만듭니다.
이 체인들은 각각model과 파이프(|) 연산자를 통해 연결됩니다. 마지막으로,RunnableParallel클래스를 사용하여 이 두 체인을capital와area이라는 키로 결합하여 동시에 실행할 수 있는combined객체를 생성합니다.
```
Copyfromlangchain_core.runnablesimportRunnableParallel# {country} 의 수도를 물어보는 체인을 생성합니다.chain1 = (
    PromptTemplate.from_template(""{country} 의 수도는 어디야?"")
    | model
    | StrOutputParser()
)# {country} 의 면적을 물어보는 체인을 생성합니다.chain2 = (
    PromptTemplate.from_template(""{country} 의 면적은 얼마야?"")
    | model
    | StrOutputParser()
)# 위의 2개 체인을 동시에 생성하는 병렬 실행 체인을 생성합니다.combined = RunnableParallel(capital=chain1, area=chain2)
```
chain1.invoke()함수는chain1객체의invoke메서드를 호출합니다.
이때,country이라는 키에대한민국라는 값을 가진 딕셔너리를 인자로 전달합니다.
```
Copy# chain1 를 실행합니다.chain1.invoke({""country"":""대한민국""})
```
```
'대한민국의 수도는 서울이다.'
```
이번에는chain2.invoke()를 호출합니다.country키에 다른 국가인미국을 전달합니다.
```
Copy# chain2 를 실행합니다.chain2.invoke({""country"":""미국""})
```
```
'미국의 면적은 약 9,826,675 제곱 킬로미터입니다.'
```
combined객체의invoke메서드는 주어진country에 대한 처리를 수행합니다.
이 예제에서는대한민국라는 주제를invoke메서드에 전달하여 실행합니다.
```
Copy# 병렬 실행 체인을 실행합니다.combined.invoke({""country"":""대한민국""})
```
```
{'capital': '대한민국의 수도는 서울입니다.', 'area': '대한민국의 면적은 약 100,363.4 제곱 킬로미터 입니다.'}
```
병렬 처리는 다른 실행 가능한 코드와 결합될 수 있습니다.
배치와 병렬 처리를 사용해 보도록 합시다.
chain1.batch함수는 여러 개의 딕셔너리를 포함하는 리스트를 인자로 받아, 각 딕셔너리에 있는 ""topic"" 키에 해당하는 값을 처리합니다. 이 예시에서는 ""대한민국""와 ""미국""라는 두 개의 토픽을 배치 처리하고 있습니다.
```
Copy# 배치 처리를 수행합니다.chain1.batch([{""country"":""대한민국""}, {""country"":""미국""}])
```
```
['대한민국의 수도는 서울이에요.', '미국의 수도는 워싱턴 D.C.입니다.']
```
chain2.batch함수는 여러 개의 딕셔너리를 리스트 형태로 받아, 일괄 처리(batch)를 수행합니다.
이 예시에서는대한민국와미국라는 두 가지 국가에 대한 처리를 요청합니다.
```
Copy# 배치 처리를 수행합니다.chain2.batch([{""country"":""대한민국""}, {""country"":""미국""}])
```
```
['대한민국의 총 면적은 약 100,363 제곱킬로미터 입니다.', '미국의 면적은 약 9,834,000km² 입니다.']
```
combined.batch함수는 주어진 데이터를 배치로 처리하는 데 사용됩니다. 이 예시에서는 두 개의 딕셔너리 객체를 포함하는 리스트를 인자로 받아 각각대한민국와미국두 나라에 대한 데이터를 배치 처리합니다.
```
Copy# 주어진 데이터를 배치로 처리합니다.combined.batch([{""country"":""대한민국""}, {""country"":""미국""}])
```
```
[{'capital': '대한민국의 수도는 서울이다.', 'area': '대한민국의 면적은 약 100,363km² 입니다.'}, {'capital': '미국의 수도는 워싱턴 D.C.입니다.', 'area': '미국의 면적은 약 9,833,520 km² 입니다.'}]
```"
06. Runnable,https://wikidocs.net/233346,"# 06. Runnable

```
Copy# .env 파일을 읽어서 환경변수로 설정fromdotenvimportload_dotenv# 토큰 정보로드load_dotenv()
```
```
True
```
```
Copy# LangSmith 추적을 설정합니다. https://smith.langchain.comfromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""CH01-Basic"")
```
## 데이터를 효과적으로 전달하는 방법
- RunnablePassthrough는 입력을 변경하지 않거나 추가 키를 더하여 전달할 수 있습니다.
- RunnablePassthrough()가 단독으로 호출되면, 단순히 입력을 받아 그대로 전달합니다.
- RunnablePassthrough.assign(...)방식으로 호출되면, 입력을 받아 assign 함수에 전달된 추가 인수를 추가합니다.
RunnablePassthrough는 입력을 변경하지 않거나 추가 키를 더하여 전달할 수 있습니다.
RunnablePassthrough()가 단독으로 호출되면, 단순히 입력을 받아 그대로 전달합니다.
RunnablePassthrough.assign(...)방식으로 호출되면, 입력을 받아 assign 함수에 전달된 추가 인수를 추가합니다.
```
Copyfromlangchain_core.promptsimportPromptTemplatefromlangchain_openaiimportChatOpenAI# prompt 와 llm 을 생성합니다.prompt = PromptTemplate.from_template(""{num} 의 10배는?"")
llm = ChatOpenAI(temperature=0)# chain 을 생성합니다.chain = prompt | llm
```
chain 을invoke()하여 실행할 때는 입력 데이터의 타입이 딕셔너리여야 합니다.
```
Copy# chain 을 실행합니다.chain.invoke({""num"":5})
```
```
AIMessage(content='50입니다.', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 16, 'total_tokens': 19}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-069c8df3-97d1-413b-be57-376657b4e79a-0')
```
하지만, langchain 라이브러리가 업데이트 되면서 1개의 변수만 템플릿에 포함하고 있다면, 값만 전달하는 것도 가능합니다.
```
Copy# chain 을 실행합니다.chain.invoke(5)
```
```
AIMessage(content='50입니다.', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 16, 'total_tokens': 19}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7e44f965-5ea7-4e64-bd7c-174d235253d1-0')
```
아래는RunnablePassthrough를 사용한 예제입니다.
RunnablePassthrough는runnable객체이며,runnable객체는invoke()메소드를 사용하여 별도 실행이 가능합니다.
```
Copyfromlangchain_core.runnablesimportRunnablePassthrough# runnableRunnablePassthrough().invoke({""num"":10})
```
```
{'num': 10}
```
아래는RunnablePassthrough로 체인을 구성하는 예제입니다.
```
Copyrunnable_chain = {""num"": RunnablePassthrough()} | prompt | ChatOpenAI()# dict 값이 RunnablePassthrough() 로 변경되었습니다.runnable_chain.invoke(10)
```
```
AIMessage(content='100입니다.', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 16, 'total_tokens': 19}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9aa7c4e6-1fb4-4ae8-8bd1-ae2eaf95f101-0')
```
다음은RunnablePassthrough.assign()을 사용하는 경우와 비교한 결과입니다.
```
CopyRunnablePassthrough().invoke({""num"":1})
```
```
{'num': 1}
```
RunnablePassthrough.assign()
- 입력 값으로 들어온 값의 key/value 쌍과 새롭게 할당된 key/value 쌍을 합칩니다.
입력 값으로 들어온 값의 key/value 쌍과 새롭게 할당된 key/value 쌍을 합칩니다.
```
Copy# 입력 키: num, 할당(assign) 키: new_num(RunnablePassthrough.assign(new_num=lambdax: x[""num""] *3)).invoke({""num"":1})
```
```
{'num': 1, 'new_num': 3}
```
## RunnableParallel
```
Copyfromlangchain_core.runnablesimportRunnableParallel# RunnableParallel 인스턴스를 생성합니다. 이 인스턴스는 여러 Runnable 인스턴스를 병렬로 실행할 수 있습니다.runnable = RunnableParallel(# RunnablePassthrough 인스턴스를 'passed' 키워드 인자로 전달합니다. 이는 입력된 데이터를 그대로 통과시키는 역할을 합니다.passed=RunnablePassthrough(),# 'extra' 키워드 인자로 RunnablePassthrough.assign을 사용하여, 'mult' 람다 함수를 할당합니다. 이 함수는 입력된 딕셔너리의 'num' 키에 해당하는 값을 3배로 증가시킵니다.extra=RunnablePassthrough.assign(mult=lambdax: x[""num""] *3),# 'modified' 키워드 인자로 람다 함수를 전달합니다. 이 함수는 입력된 딕셔너리의 'num' 키에 해당하는 값에 1을 더합니다.modified=lambdax: x[""num""] +1,
)# runnable 인스턴스에 {'num': 1} 딕셔너리를 입력으로 전달하여 invoke 메소드를 호출합니다.runnable.invoke({""num"":1})
```
```
{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}
```
Chain 도 RunnableParallel 적용할 수 있습니다.
```
Copychain1 = (
    {""country"": RunnablePassthrough()}
    | PromptTemplate.from_template(""{country} 의 수도는?"")
    | ChatOpenAI()
)
chain2 = (
    {""country"": RunnablePassthrough()}
    | PromptTemplate.from_template(""{country} 의 면적은?"")
    | ChatOpenAI()
)
```
```
Copycombined_chain = RunnableParallel(capital=chain1, area=chain2)
combined_chain.invoke(""대한민국"")
```
```
{'capital': AIMessage(content='서울특별시입니다.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 19, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-98237e47-6f36-42d9-af0f-a8bf71d74111-0'), 'area': AIMessage(content='대한민국의 총 면적은 약 100,363.4km² 입니다.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 20, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-96550ba9-3fae-4219-95e1-69abe4004bd4-0')}
```
## RunnableLambda
RunnableLambda 를 사용하여 사용자 정의 함수를 맵핑할 수 있습니다.
```
Copyfromlangchain_core.runnablesimportRunnableLambdadefextra(x):print(x)
    ext = x[""extra""]returnint(ext[""num""]) *int(ext[""mult""])
```
```
Copy(runnable | RunnableLambda(extra)).invoke({""num"":3})
```
```
{'passed': {'num': 3}, 'extra': {'num': 3, 'mult': 9}, 'modified': 4}
```
```
27
```
itemgetter를 사용하여 특정 키를 추출합니다.
```
Copyfromoperatorimportitemgetterfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnableLambdafromlangchain_openaiimportChatOpenAI# 문장의 길이를 반환하는 함수입니다.deflength_function(text):returnlen(text)# 두 문장의 길이를 곱한 값을 반환하는 함수입니다.def_multiple_length_function(text1, text2):returnlen(text1) *len(text2)# _multiple_length_function 함수를 사용하여 두 문장의 길이를 곱한 값을 반환하는 함수입니다.defmultiple_length_function(_dict):return_multiple_length_function(_dict[""text1""], _dict[""text2""])


prompt = ChatPromptTemplate.from_template(""{a} + {b} 는 무엇인가요?"")
model = ChatOpenAI()

chain1 = prompt | model

chain = (
    {""a"": itemgetter(""word1"") | RunnableLambda(length_function),""b"": {""text1"": itemgetter(""word1""),""text2"": itemgetter(""word2"")}
        | RunnableLambda(multiple_length_function),
    }
    | prompt
    | model
)
```
```
Copychain.invoke({""word1"":""hello"",""word2"":""world""})
```
```
AIMessage(content='5 + 25 = 30입니다.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 22, 'total_tokens': 31}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-11e21866-f47b-461a-8842-ceeaa083dcb2-0')
```"
CH02 프롬프트(Prompt),https://wikidocs.net/233347,"# CH02 프롬프트(Prompt)

프롬프트 단계는 검색기에서 검색된 문서들을 바탕으로언어 모델이 사용할 질문이나 명령을 생성하는 과정입니다. 이 단계는검색된 정보를 바탕으로 최종 사용자의 질문에 가장 잘 대응할 수 있는 응답을 생성하기 위해 필수적인 단계입니다.
## 프롬프트의 필요성
문맥(Context) 설정: 프롬프트는 언어 모델이 특정 문맥에서 작동하도록 설정하는 역할을 합니다. 이를 통해 모델은 제공된 정보를 바탕으로 보다 정확하고 관련성 높은 답변을 생성할 수 있습니다.
문맥(Context) 설정: 프롬프트는 언어 모델이 특정 문맥에서 작동하도록 설정하는 역할을 합니다. 이를 통해 모델은 제공된 정보를 바탕으로 보다 정확하고 관련성 높은 답변을 생성할 수 있습니다.
정보 통합: 여러 문서에서 검색된 정보는 서로 다른 관점이나 내용을 포함할 수 있습니다. 프롬프트 단계에서 이러한 정보를 통합하고, 모델이 이를 효율적으로 활용할 수 있는 형식으로 조정합니다.
정보 통합: 여러 문서에서 검색된 정보는 서로 다른 관점이나 내용을 포함할 수 있습니다. 프롬프트 단계에서 이러한 정보를 통합하고, 모델이 이를 효율적으로 활용할 수 있는 형식으로 조정합니다.
응답 품질 향상: 질문에 대한 모델의 응답 품질은 프롬프트의 구성에 크게 의존합니다. 잘 구성된 프롬프트는 모델이 보다 정확하고 유용한 정보를 제공하게 돕습니다.
응답 품질 향상: 질문에 대한 모델의 응답 품질은 프롬프트의 구성에 크게 의존합니다. 잘 구성된 프롬프트는 모델이 보다 정확하고 유용한 정보를 제공하게 돕습니다.
## RAG 프롬프트 구조
- 지시사항(Instruction)
- 질문(사용자 입력 질문)
- 문맥(검색된 정보)
지시사항(Instruction)
질문(사용자 입력 질문)
문맥(검색된 정보)
```
Copy당신은질문-답변(Question-Answer)Task를수행한는AI어시스턴트입니다.검색된문맥(context)를사용하여질문(question)에답하세요.만약,문맥(context)으로부터답을찾을수없다면'모른다'고말하세요.한국어로대답하세요.#Question:{이곳에사용자가입력한질문이삽입됩니다}#Context:{이곳에검색된정보가삽입됩니다}
```
## 프롬프트의 중요성
프롬프트 단계는 RAG 시스템에서 중요한 역할을 합니다.
이 단계를 통해 언어 모델은 사용자의 질문에 대해최적화된 방식으로 응답을 생성할 수 있으며,시스템 전체의 성능과 사용자 만족도에 직접적인 영향을 미칩니다. 프롬프트가 잘 구성되어 있지 않으면, 모델이 비효율적으로 작동할 수 있으며, 결과적으로 사용자의 요구에 부응하지 못하는 응답을 생성할 가능성이 높아집니다.
## 참고
- 프롬프트
- LangChain Prompts
프롬프트
LangChain Prompts"
01. 프롬프트(Prompt),https://wikidocs.net/233351,"# 01. 프롬프트(Prompt)

## PromptTemplate
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
```
Copy# LangSmith 추적을 설정합니다. https://smith.langchain.comfromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""CH02-Prompt"")
```
LLM 객체를 정의합니다.
```
Copyfromlangchain_openaiimportChatOpenAI

llm = ChatOpenAI()
```
- 치환될 변수를{ 변수 }로 묶어서 템플릿을 정의합니다.
치환될 변수를{ 변수 }로 묶어서 템플릿을 정의합니다.
```
Copyfromlangchain_core.promptsimportPromptTemplate# template 정의. {country}는 변수로, 이후에 값이 들어갈 자리를 의미template =""{country}의 수도는 어디인가요?""# from_template 메소드를 이용하여 PromptTemplate 객체 생성prompt = PromptTemplate.from_template(template)
prompt
```
```
PromptTemplate(input_variables=['country'], template='{country}의 수도는 어디인가요?')
```
country변수에 값을 넣어서 문장을 생성할 수 있습니다.
```
Copy# prompt 생성. format 메소드를 이용하여 변수에 값을 넣어줌prompt = prompt.format(country=""대한민국"")
prompt
```
```
'대한민국의 수도는 어디인가요?'
```
```
Copy# template 정의template =""{country}의 수도는 어디인가요?""# from_template 메소드를 이용하여 PromptTemplate 객체 생성prompt = PromptTemplate.from_template(template)# chain 생성chain = prompt | llm
```
```
Copy# country 변수에 입력된 값이 자동으로 치환되어 수행됨chain.invoke(""대한민국"").content
```
```
'대한민국의 수도는 서울입니다.'
```
추가 유효성 검사를 위해input_variables를 명시적으로 지정하세요.
이러한 변수는 인스턴스화 중에 템플릿 문자열에 있는 변수와 비교하여 불일치하는 경우 예외를 발생시킵니다.
```
Copy# template 정의template =""{country}의 수도는 어디인가요?""# PromptTemplate 객체를 활용하여 prompt_template 생성prompt = PromptTemplate(
    template=template,
    input_variables=[""country""],
)

prompt
```
```
PromptTemplate(input_variables=['country'], template='{country}의 수도는 어디인가요?')
```
```
Copy# prompt 생성prompt.format(country=""대한민국"")
```
```
'대한민국의 수도는 어디인가요?'
```
```
Copy# template 정의template =""{country1}과 {country2}의 수도는 각각 어디인가요?""# PromptTemplate 객체를 활용하여 prompt_template 생성prompt = PromptTemplate(
    template=template,
    input_variables=[""country1""],
    partial_variables={""country2"":""미국""# dictionary 형태로 partial_variables를 전달},
)

prompt
```
```
PromptTemplate(input_variables=['country1'], partial_variables={'country2': '미국'}, template='{country1}과 {country2}의 수도는 각각 어디인가요?')
```
```
Copyprompt.format(country1=""대한민국"")
```
```
'대한민국과 미국의 수도는 각각 어디인가요?'
```
```
Copyprompt_partial = prompt.partial(country2=""캐나다"")
prompt_partial
```
```
PromptTemplate(input_variables=['country1'], partial_variables={'country2': '캐나다'}, template='{country1}과 {country2}의 수도는 각각 어디인가요?')
```
```
Copyprompt_partial.format(country1=""대한민국"")
```
```
'대한민국과 캐나다의 수도는 각각 어디인가요?'
```
```
Copychain = prompt_partial | llm
```
```
Copychain.invoke(""대한민국"").content
```
```
'대한민국의 수도는 서울이고, 캐나다의 수도는 오타와입니다.'
```
```
Copychain.invoke({""country1"":""대한민국"",""country2"":""호주""}).content
```
```
'대한민국의 수도는 서울이고 호주의 수도는 캔버라입니다.'
```
partial을 사용하는 일반적인 용도는 함수를 부분적으로 사용하는 것입니다. 이 사용 사례는항상 공통된 방식으로 가져오고 싶은 변수가 있는 경우입니다.
대표적인 예가날짜나 시간입니다.
항상 현재 날짜가 표시되기를 원하는 프롬프트가 있다고 가정해 보겠습니다. 프롬프트에 하드 코딩할 수도 없고, 다른 입력 변수와 함께 전달하는 것도 번거롭습니다. 이 경우 항상 현재날짜를 반환하는 함수를 사용하여 프롬프트를 부분적으로 변경할 수 있으면 매우 편리합니다.
다음의 코드는 오늘 날짜를 구하는 파이썬 코드입니다.
```
Copyfromdatetimeimportdatetime# 오늘 날짜를 출력datetime.now().strftime(""%B %d"")
```
```
'June 17'
```
```
Copy# 날짜를 반환하는 함수 정의defget_today():returndatetime.now().strftime(""%B %d"")
```
```
Copyprompt = PromptTemplate(
    template=""오늘의 날짜는 {today} 입니다. 오늘이 생일인 유명인 {n}명을 나열해 주세요. 생년월일을 표기해주세요."",
    input_variables=[""n""],
    partial_variables={""today"": get_today# dictionary 형태로 partial_variables를 전달},
)
```
```
Copy# prompt 생성prompt.format(n=3)
```
```
'오늘의 날짜는 June 17 입니다. 오늘이 생일인 유명인 3명을 나열해 주세요. 생년월일을 표기해주세요.'
```
```
Copy# chain 을 생성합니다.chain = prompt | llm
```
```
Copy# chain 을 실행 후 결과를 확인합니다.print(chain.invoke(3).content)
```
```
1. Venus Williams - 1980년 6월 17일
2. Paulina Rubio - 1971년 6월 17일
3. Will Forte - 1970년 6월 17일
```
```
Copy# chain 을 실행 후 결과를 확인합니다.print(chain.invoke({""today"":""Jan 02"",""n"":3}).content)
```
```
1. 김유정 (1999년 1월 2일)
2. 김태희 (1980년 3월 29일)
3. 조승우 (1980년 1월 2일)
```
## 파일로부터 template 읽어오기
```
Copyfromlangchain_core.promptsimportload_prompt

prompt = load_prompt(""prompts/fruit_color.yaml"")
prompt
```
```
PromptTemplate(input_variables=['fruit'], template='{fruit}의 색깔이 뭐야?')
```
```
Copyprompt.format(fruit=""사과"")
```
```
'사과의 색깔이 뭐야?'
```
```
Copyprompt2 = load_prompt(""prompts/capital.yaml"")print(prompt2.format(country=""대한민국""))
```
```
대한민국의 수도에 대해서 알려주세요.
수도의 특징을 다음의 양식에 맞게 정리해 주세요.
300자 내외로 작성해 주세요.
한글로 작성해 주세요.
----
[양식]
1. 면적
2. 인구
3. 역사적 장소
4. 특산품

#Answer:
```
## ChatPromptTemplate
ChatPromptTemplate은 대화목록을 프롬프트로 주입하고자 할 때 활용할 수 있습니다.
메시지는 튜플(tuple) 형식으로 구성하며, (role,message) 로 구성하여 리스트로 생성할 수 있습니다.
role-""system"": 시스템 설정 메시지 입니다. 주로 전역설정과 관련된 프롬프트입니다.
-""human"": 사용자 입력 메시지 입니다.
-""ai"": AI 의 답변 메시지입니다.
```
Copyfromlangchain_core.promptsimportChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [# role, message(""system"",""당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다.""),
        (""human"",""반가워요!""),
        (""ai"",""안녕하세요! 무엇을 도와드릴까요?""),
        (""human"",""{user_input}""),
    ]
)# 챗 message 를 생성합니다.messages = chat_template.format_messages(
    name=""테디"", user_input=""당신의 이름은 무엇입니까?"")
messages
```
```
[SystemMessage(content='당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 테디 입니다.'), HumanMessage(content='반가워요!'), AIMessage(content='안녕하세요! 무엇을 도와드릴까요?'), HumanMessage(content='당신의 이름은 무엇입니까?')]
```
생성한 메시지를 바로 주입하여 결과를 받을 수 있습니다.
```
Copyllm.invoke(messages).content
```
```
'제 이름은 테디입니다. 어떻게 도와드릴까요?'
```
이번에는 체인을 생성해 보겠습니다.
```
Copychain = chat_template | llm
```
```
Copychain.invoke({""name"":""Teddy"",""user_input"":""당신의 이름은 무엇입니까?""}).content
```
```
'제 이름은 Teddy입니다. 어떻게 도와드릴까요?'
```
## MessagePlaceholder
또한 LangChain은 포맷하는 동안 렌더링할 메시지를 완전히 제어할 수 있는MessagePlaceholder를 제공합니다.
메시지 프롬프트 템플릿에 어떤 역할을 사용해야 할지 확실하지 않거나 서식 지정 중에 메시지 목록을 삽입하려는 경우 유용할 수 있습니다.
```
Copyfromlangchain_core.promptsimport(
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)

human_prompt =""Summarize our conversation so far in {word_count} words.""human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)

chat_prompt = ChatPromptTemplate.from_messages(
    [MessagesPlaceholder(variable_name=""conversation""), human_message_template]
)
```
conversation대화목록을 나중에 추가하고자 할 때MessagesPlaceholder를 사용할 수 있습니다.
```
Copychat_prompt.format(
    word_count=5,
    conversation=[
        (""human"",""안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다.""),
        (""ai"",""반가워요! 앞으로 잘 부탁 드립니다.""),
    ],
)
```
```
'Human: 안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다.\nAI: 반가워요! 앞으로 잘 부탁 드립니다.\nHuman: Summarize our conversation so far in 5 words.'
```
```
Copy# chain 생성chain = chat_prompt | llm
```
```
Copy# chain 실행 및 결과확인chain.invoke(
    {""word_count"":5,""conversation"": [
            (""human"",""안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다."",
            ),
            (""ai"",""반가워요! 앞으로 잘 부탁 드립니다.""),
        ],
    }
)
```
```
AIMessage(content='테디 새로 입사, 반가워요!', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 81, 'total_tokens': 97}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-44c68399-4712-4c69-bb6a-48220438cb8f-0')
```"
02. 퓨샷 프롬프트(FewShotPromptTemplate),https://wikidocs.net/233348,"# 02. 퓨샷 프롬프트(FewShotPromptTemplate)

```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
```
Copy# LangSmith 추적을 설정합니다. https://smith.langchain.comfromlangchain_teddynoteimportlogging# 프로젝트 이름을 입력합니다.logging.langsmith(""CH02-Prompt"")
```
```
LangSmith 추적을 시작합니다.
[프로젝트명]
CH02-Prompt
```
## FewShotPromptTemplate
```
Copyfromlangchain_openaiimportChatOpenAI# 객체 생성llm = ChatOpenAI(
    temperature=0,# 창의성model_name=""gpt-4-turbo"",# 모델명)# 질의내용question =""대한민국의 수도는 뭐야?""# 질의print(llm.invoke(question).content)
```
```
대한민국의 수도는 서울입니다.
```
```
Copyfromlangchain_core.prompts.few_shotimportFewShotPromptTemplatefromlangchain_core.promptsimportPromptTemplate

examples = [
    {""question"":""스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?"",""answer"":""""""이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 스티브 잡스는 몇 살에 사망했나요?
중간 답변: 스티브 잡스는 56세에 사망했습니다.
추가 질문: 아인슈타인은 몇 살에 사망했나요?
중간 답변: 아인슈타인은 76세에 사망했습니다.
최종 답변은: 아인슈타인
"""""",
    },
    {""question"":""네이버의 창립자는 언제 태어났나요?"",""answer"":""""""이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 네이버의 창립자는 누구인가요?
중간 답변: 네이버는 이해진에 의해 창립되었습니다.
추가 질문: 이해진은 언제 태어났나요?
중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.
최종 답변은: 1967년 6월 22일
"""""",
    },
    {""question"":""율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?"",""answer"":""""""이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 율곡 이이의 어머니는 누구인가요?
중간 답변: 율곡 이이의 어머니는 신사임당입니다.
추가 질문: 신사임당은 언제 태어났나요?
중간 답변: 신사임당은 1504년에 태어났습니다.
추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?
중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.
최종 답변은: 연산군
"""""",
    },
    {""question"":""올드보이와 기생충의 감독이 같은 나라 출신인가요?"",""answer"":""""""이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 올드보이의 감독은 누구인가요?
중간 답변: 올드보이의 감독은 박찬욱입니다.
추가 질문: 박찬욱은 어느 나라 출신인가요?
중간 답변: 박찬욱은 대한민국 출신입니다.
추가 질문: 기생충의 감독은 누구인가요?
중간 답변: 기생충의 감독은 봉준호입니다.
추가 질문: 봉준호는 어느 나라 출신인가요?
중간 답변: 봉준호는 대한민국 출신입니다.
최종 답변은: 예
"""""",
    },
]
```
```
Copyexample_prompt = PromptTemplate.from_template(""Question:\n{question}\nAnswer:\n{answer}"")print(example_prompt.format(**examples[0]))
```
```
Question:
스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?
Answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 스티브 잡스는 몇 살에 사망했나요?
중간 답변: 스티브 잡스는 56세에 사망했습니다.
추가 질문: 아인슈타인은 몇 살에 사망했나요?
중간 답변: 아인슈타인은 76세에 사망했습니다.
최종 답변은: 아인슈타인
```
```
Copyprompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix=""Question:\n{question}\nAnswer:"",
    input_variables=[""question""],
)

question =""Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?""final_prompt = prompt.format(question=question)print(final_prompt)
```
```
Question:
스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?
Answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 스티브 잡스는 몇 살에 사망했나요?
중간 답변: 스티브 잡스는 56세에 사망했습니다.
추가 질문: 아인슈타인은 몇 살에 사망했나요?
중간 답변: 아인슈타인은 76세에 사망했습니다.
최종 답변은: 아인슈타인


Question:
네이버의 창립자는 언제 태어났나요?
Answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 네이버의 창립자는 누구인가요?
중간 답변: 네이버는 이해진에 의해 창립되었습니다.
추가 질문: 이해진은 언제 태어났나요?
중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.
최종 답변은: 1967년 6월 22일


Question:
율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?
Answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 율곡 이이의 어머니는 누구인가요?
중간 답변: 율곡 이이의 어머니는 신사임당입니다.
추가 질문: 신사임당은 언제 태어났나요?
중간 답변: 신사임당은 1504년에 태어났습니다.
추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?
중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.
최종 답변은: 연산군


Question:
올드보이와 기생충의 감독이 같은 나라 출신인가요?
Answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 올드보이의 감독은 누구인가요?
중간 답변: 올드보이의 감독은 박찬욱입니다.
추가 질문: 박찬욱은 어느 나라 출신인가요?
중간 답변: 박찬욱은 대한민국 출신입니다.
추가 질문: 기생충의 감독은 누구인가요?
중간 답변: 기생충의 감독은 봉준호입니다.
추가 질문: 봉준호는 어느 나라 출신인가요?
중간 답변: 봉준호는 대한민국 출신입니다.
최종 답변은: 예


Question:
Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?
Answer:
```
```
Copy# 결과 출력print(llm.invoke(final_prompt).content)
```
```
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: Google은 언제 창립되었나요?
중간 답변: Google은 1998년에 창립되었습니다.
추가 질문: Bill Gates는 언제 태어났나요?
중간 답변: Bill Gates는 1955년 10월 28일에 태어났습니다.
최종 답변은: 1998년에 Bill Gates의 나이는 43세입니다.
```
```
Copyprompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix=""Question:\n{question}\nAnswer:"",
    input_variables=[""question""],
)# chain 생성chain = prompt | llm# 결과 출력print(chain.invoke(question).content)
```
```
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: Google은 언제 창립되었나요?
중간 답변: Google은 1998년에 창립되었습니다.
추가 질문: Bill Gates는 언제 태어났나요?
중간 답변: Bill Gates는 1955년 10월 28일에 태어났습니다.
최종 답변은: 1998년에 Bill Gates의 나이는 43세입니다.
```
## Example Selector
예제가 많은 경우 프롬프트에 포함할 예제를 선택해야 할 수도 있습니다. Example Selector 는 이 작업을 담당하는 클래스입니다.
- [참고] https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/
[참고] https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/
```
Copyfromlangchain_core.example_selectorsimport(
    MaxMarginalRelevanceExampleSelector,
    SemanticSimilarityExampleSelector,
)fromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_chromaimportChroma# Vector DB 생성 (저장소 이름, 임베딩 클래스)chroma = Chroma(""example_selector"", OpenAIEmbeddings())

example_selector = SemanticSimilarityExampleSelector.from_examples(# 여기에는 선택 가능한 예시 목록이 있습니다.examples,# 여기에는 의미적 유사성을 측정하는 데 사용되는 임베딩을 생성하는 임베딩 클래스가 있습니다.OpenAIEmbeddings(),# 여기에는 임베딩을 저장하고 유사성 검색을 수행하는 데 사용되는 VectorStore 클래스가 있습니다.Chroma,# 이것은 생성할 예시의 수입니다.k=1,
)# 입력과 가장 유사한 예시를 선택합니다.selected_examples = example_selector.select_examples({""question"": question})

question =""Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?""print(f""입력에 가장 유사한 예시:\n{question}\n"")forexampleinselected_examples:print(f'question:\n{example[""question""]}')print(f'answer:\n{example[""answer""]}')
```
```
입력에 가장 유사한 예시:
Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?

question:
네이버의 창립자는 언제 태어났나요?
answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 네이버의 창립자는 누구인가요?
중간 답변: 네이버는 이해진에 의해 창립되었습니다.
추가 질문: 이해진은 언제 태어났나요?
중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.
최종 답변은: 1967년 6월 22일
```
```
Copyprompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix=""Question:\n{question}\nAnswer:"",
    input_variables=[""question""],
)

question =""Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?""example_selector_prompt = prompt.format(question=question)print(example_selector_prompt)
```
```
Question:
네이버의 창립자는 언제 태어났나요?
Answer:
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: 네이버의 창립자는 누구인가요?
중간 답변: 네이버는 이해진에 의해 창립되었습니다.
추가 질문: 이해진은 언제 태어났나요?
중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.
최종 답변은: 1967년 6월 22일


Question:
Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?
Answer:
```
이번에는 ExampleSelector 를 사용하여 FewShotPromptTemplate 을 생성합니다.
```
Copyprompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix=""Question:\n{question}\nAnswer:"",
    input_variables=[""question""],
)# 체인 생성chain = prompt | llm
```
```
Copy# 결과 출력question =""Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?""print(chain.invoke(question).content)
```
```
이 질문에 추가 질문이 필요한가요: 예.
추가 질문: Google이 창립된 연도는 언제인가요?
중간 답변: Google은 1998년에 창립되었습니다.
추가 질문: Bill Gates는 언제 태어났나요?
중간 답변: Bill Gates는 1955년 10월 28일에 태어났습니다.
최종 계산: 1998년 - 1955년 = 43살
최종 답변은: 43살
```
## FewShotChatMessagePromptTemplate
```
Copyexamples = [
    {""instruction"":""당신은 회의록 작성 전문가 입니다. 주어진 정보를 바탕으로 회의록을 작성해 주세요"",""input"":""2023년 12월 25일, XYZ 회사의 마케팅 전략 회의가 오후 3시에 시작되었다. 회의에는 마케팅 팀장인 김수진, 디지털 마케팅 담당자인 박지민, 소셜 미디어 관리자인 이준호가 참석했다. 회의의 주요 목적은 2024년 상반기 마케팅 전략을 수립하고, 새로운 소셜 미디어 캠페인에 대한 아이디어를 논의하는 것이었다. 팀장인 김수진은 최근 시장 동향에 대한 간략한 개요를 제공했으며, 이어서 각 팀원이 자신의 분야에서의 전략적 아이디어를 발표했다."",""answer"":""""""
회의록: XYZ 회사 마케팅 전략 회의
일시: 2023년 12월 25일
장소: XYZ 회사 회의실
참석자: 김수진 (마케팅 팀장), 박지민 (디지털 마케팅 담당자), 이준호 (소셜 미디어 관리자)

1. 개회
   - 회의는 김수진 팀장의 개회사로 시작됨.
   - 회의의 목적은 2024년 상반기 마케팅 전략 수립 및 새로운 소셜 미디어 캠페인 아이디어 논의.

2. 시장 동향 개요 (김수진)
   - 김수진 팀장은 최근 시장 동향에 대한 분석을 제시.
   - 소비자 행동 변화와 경쟁사 전략에 대한 통찰 공유.

3. 디지털 마케팅 전략 (박지민)
   - 박지민은 디지털 마케팅 전략에 대해 발표.
   - 온라인 광고와 SEO 최적화 방안에 중점을 둠.

4. 소셜 미디어 캠페인 (이준호)
   - 이준호는 새로운 소셜 미디어 캠페인에 대한 아이디어를 제안.
   - 인플루언서 마케팅과 콘텐츠 전략에 대한 계획을 설명함.

5. 종합 논의
   - 팀원들 간의 아이디어 공유 및 토론.
   - 각 전략에 대한 예산 및 자원 배분에 대해 논의.

6. 마무리
   - 다음 회의 날짜 및 시간 확정.
   - 회의록 정리 및 배포는 박지민 담당.
"""""",
    },
    {""instruction"":""당신은 요약 전문가 입니다. 다음 주어진 정보를 바탕으로 내용을 요약해 주세요"",""input"":""이 문서는 '지속 가능한 도시 개발을 위한 전략'에 대한 20페이지 분량의 보고서입니다. 보고서는 지속 가능한 도시 개발의 중요성, 현재 도시화의 문제점, 그리고 도시 개발을 지속 가능하게 만들기 위한 다양한 전략을 포괄적으로 다루고 있습니다. 이 보고서는 또한 성공적인 지속 가능한 도시 개발 사례를 여러 국가에서 소개하고, 이러한 사례들을 통해 얻은 교훈을 요약하고 있습니다."",""answer"":""""""
문서 요약: 지속 가능한 도시 개발을 위한 전략 보고서

- 중요성: 지속 가능한 도시 개발이 필수적인 이유와 그에 따른 사회적, 경제적, 환경적 이익을 강조.
- 현 문제점: 현재의 도시화 과정에서 발생하는 주요 문제점들, 예를 들어 환경 오염, 자원 고갈, 불평등 증가 등을 분석.
- 전략: 지속 가능한 도시 개발을 달성하기 위한 다양한 전략 제시. 이에는 친환경 건축, 대중교통 개선, 에너지 효율성 증대, 지역사회 참여 강화 등이 포함됨.
- 사례 연구: 전 세계 여러 도시의 성공적인 지속 가능한 개발 사례를 소개. 예를 들어, 덴마크의 코펜하겐, 일본의 요코하마 등의 사례를 통해 실현 가능한 전략들을 설명.
- 교훈: 이러한 사례들에서 얻은 주요 교훈을 요약. 강조된 교훈에는 다각적 접근의 중요성, 지역사회와의 협력, 장기적 계획의 필요성 등이 포함됨.

이 보고서는 지속 가능한 도시 개발이 어떻게 현실적이고 효과적인 형태로 이루어질 수 있는지에 대한 심도 있는 분석을 제공합니다.
"""""",
    },
    {""instruction"":""당신은 문장 교정 전문가 입니다. 다음 주어진 문장을 교정해 주세요"",""input"":""우리 회사는 새로운 마케팅 전략을 도입하려고 한다. 이를 통해 고객과의 소통이 더 효과적이 될 것이다."",""answer"":""본 회사는 새로운 마케팅 전략을 도입함으로써, 고객과의 소통을 보다 효과적으로 개선할 수 있을 것으로 기대된다."",
    },
]
```
```
Copyfromlangchain.promptsimportChatPromptTemplate, FewShotChatMessagePromptTemplatefromlangchain_core.example_selectorsimport(
    SemanticSimilarityExampleSelector,
)fromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_chromaimportChroma

chroma = Chroma(""fewshot_chat"", OpenAIEmbeddings())

example_prompt = ChatPromptTemplate.from_messages(
    [
        (""human"",""{instruction}:\n{input}""),
        (""ai"",""{answer}""),
    ]
)

example_selector = SemanticSimilarityExampleSelector.from_examples(# 여기에는 선택 가능한 예시 목록이 있습니다.examples,# 여기에는 의미적 유사성을 측정하는 데 사용되는 임베딩을 생성하는 임베딩 클래스가 있습니다.OpenAIEmbeddings(),# 여기에는 임베딩을 저장하고 유사성 검색을 수행하는 데 사용되는 VectorStore 클래스가 있습니다.chroma,# 이것은 생성할 예시의 수입니다.k=1,
)

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
)
```
fewshot 예제와 example selector를 사용하여 유사한 예제 1개를 선택합니다.
```
Copyquestion = {""instruction"":""회의록을 작성해 주세요"",""input"":""2023년 12월 26일, ABC 기술 회사의 제품 개발 팀은 새로운 모바일 애플리케이션 프로젝트에 대한 주간 진행 상황 회의를 가졌다. 이 회의에는 프로젝트 매니저인 최현수, 주요 개발자인 황지연, UI/UX 디자이너인 김태영이 참석했다. 회의의 주요 목적은 프로젝트의 현재 진행 상황을 검토하고, 다가오는 마일스톤에 대한 계획을 수립하는 것이었다. 각 팀원은 자신의 작업 영역에 대한 업데이트를 제공했고, 팀은 다음 주까지의 목표를 설정했다."",
}

example_selector.select_examples(question)
```
```
[{'answer': '\n회의록: XYZ 회사 마케팅 전략 회의\n일시: 2023년 12월 25일\n장소: XYZ 회사 회의실\n참석자: 김수진 (마케팅 팀장), 박지민 (디지털 마케팅 담당자), 이준호 (소셜 미디어 관리자)\n\n1. 개회\n   - 회의는 김수진 팀장의 개회사로 시작됨.\n   - 회의의 목적은 2024년 상반기 마케팅 전략 수립 및 새로운 소셜 미디어 캠페인 아이디어 논의.\n\n2. 시장 동향 개요 (김수진)\n   - 김수진 팀장은 최근 시장 동향에 대한 분석을 제시.\n   - 소비자 행동 변화와 경쟁사 전략에 대한 통찰 공유.\n\n3. 디지털 마케팅 전략 (박지민)\n   - 박지민은 디지털 마케팅 전략에 대해 발표.\n   - 온라인 광고와 SEO 최적화 방안에 중점을 둠.\n\n4. 소셜 미디어 캠페인 (이준호)\n   - 이준호는 새로운 소셜 미디어 캠페인에 대한 아이디어를 제안.\n   - 인플루언서 마케팅과 콘텐츠 전략에 대한 계획을 설명함.\n\n5. 종합 논의\n   - 팀원들 간의 아이디어 공유 및 토론.\n   - 각 전략에 대한 예산 및 자원 배분에 대해 논의.\n\n6. 마무리\n   - 다음 회의 날짜 및 시간 확정.\n   - 회의록 정리 및 배포는 박지민 담당.\n',  'input': '2023년 12월 25일, XYZ 회사의 마케팅 전략 회의가 오후 3시에 시작되었다. 회의에는 마케팅 팀장인 김수진, 디지털 마케팅 담당자인 박지민, 소셜 미디어 관리자인 이준호가 참석했다. 회의의 주요 목적은 2024년 상반기 마케팅 전략을 수립하고, 새로운 소셜 미디어 캠페인에 대한 아이디어를 논의하는 것이었다. 팀장인 김수진은 최근 시장 동향에 대한 간략한 개요를 제공했으며, 이어서 각 팀원이 자신의 분야에서의 전략적 아이디어를 발표했다.',  'instruction': '당신은 회의록 작성 전문가 입니다. 주어진 정보를 바탕으로 회의록을 작성해 주세요'}]
```
```
Copyfinal_prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""You are a helpful assistant."",
        ),
        few_shot_prompt,
        (""human"",""{instruction}\n{input}""),
    ]
)
```
```
Copy# chain 생성chain = final_prompt | llm# 실행 및 결과 출력print(chain.invoke(question).content)
```
```
회의록: ABC 기술 회사 제품 개발 팀 주간 진행 상황 회의
일시: 2023년 12월 26일
장소: ABC 기술 회사 회의실
참석자: 최현수 (프로젝트 매니저), 황지연 (주요 개발자), 김태영 (UI/UX 디자이너)

1. 개회
   - 최현수 프로젝트 매니저가 회의를 개회하고, 회의의 목적을 설명함.
   - 목적: 프로젝트의 현재 진행 상황 검토 및 다가오는 마일스톤에 대한 계획 수립.

2. 프로젝트 진행 상황 보고
   - 황지연 개발자가 개발 진행 상황에 대해 보고.
     - 현재 개발 중인 기능들의 상태와 최근 해결된 문제점들에 대해 설명.
     - 다음 주까지 완료 목표로 설정된 기능들을 공유.
   - 김태영 디자이너가 UI/UX 디자인 진행 상황에 대해 보고.
     - 최근 완성된 디자인 요소들과 사용자 피드백을 바탕으로 한 개선 사항들을 공유.
     - 다음 단계의 디자인 작업 계획을 설명.

3. 마일스톤 계획 논의
   - 최현수가 다가오는 마일스톤에 대한 전체적인 계획을 제시.
   - 팀원들과 함께 중요한 마일스톤 달성을 위한 구체적인 작업 분담과 일정 조정을 논의.

4. 다음 주 목표 설정
   - 각 팀원이 다음 주까지의 구체적인 목표를 설정하고 공유.
   - 황지연은 특정 기능 개발 완료를, 김태영은 새로운 디자인 프로토타입 제작을 목표로 함.

5. 문제점 및 지원 요청
   - 팀원들이 현재 직면한 문제점을 공유하고 필요한 지원 사항을 요청.
   - 최현수가 문제 해결을 위한 지원 방안을 모색하고 필요한 자원을 조정.

6. 마무리
   - 다음 회의 날짜 및 시간 확정.
   - 회의록 정리 및 배포는 최현수 매니저가 담당.

회의는 모든 안건이 충분히 논의되고 각 팀원의 목표가 명확히 설정된 후 종료됨.
```
유사도 계산시instruction과input을 사용하고 있습니다. 하지만,instruction만 사용하여 검색시 제대로된 유사도 결과가 나오지 않습니다.
이를 해결하기 위해 커스텀 유사도 계산을 위한 클래스를 정의합니다.
아래는 잘못 검색된 결과의 예시입니다.
```
Copyquestion = {""instruction"":""회의록을 작성해 주세요"",
}

example_selector.select_examples(question)
```
```
[{'answer': '본 회사는 새로운 마케팅 전략을 도입함으로써, 고객과의 소통을 보다 효과적으로 개선할 수 있을 것으로 기대된다.',  'input': '우리 회사는 새로운 마케팅 전략을 도입하려고 한다. 이를 통해 고객과의 소통이 더 효과적이 될 것이다.',  'instruction': '당신은 문장 교정 전문가 입니다. 다음 주어진 문장을 교정해 주세요'}]
```
```
Copy# 커스텀 하지 않은 기본 예제 선택기를 사용했을 때 결과example_selector.select_examples({""instruction"":""다음 문장을 요약해 주세요""})
```
```
[{'answer': '본 회사는 새로운 마케팅 전략을 도입함으로써, 고객과의 소통을 보다 효과적으로 개선할 수 있을 것으로 기대된다.',  'input': '우리 회사는 새로운 마케팅 전략을 도입하려고 한다. 이를 통해 고객과의 소통이 더 효과적이 될 것이다.',  'instruction': '당신은 문장 교정 전문가 입니다. 다음 주어진 문장을 교정해 주세요'}]
```
```
Copyfromlangchain_teddynote.promptsimportCustomExampleSelector# 커스텀 예제 선택기 생성custom_selector = CustomExampleSelector(examples, OpenAIEmbeddings())# 커스텀 예제 선택기를 사용했을 때 결과custom_selector.select_examples({""instruction"":""다음 문장을 교정 작성해 주세요""})
```
```
[{'instruction': '당신은 문장 교정 전문가 입니다. 다음 주어진 문장을 교정해 주세요',  'input': '우리 회사는 새로운 마케팅 전략을 도입하려고 한다. 이를 통해 고객과의 소통이 더 효과적이 될 것이다.',  'answer': '본 회사는 새로운 마케팅 전략을 도입함으로써, 고객과의 소통을 보다 효과적으로 개선할 수 있을 것으로 기대된다.'}]
```
```
Copyexample_prompt = ChatPromptTemplate.from_messages(
    [
        (""human"",""{instruction}:\n{input}""),
        (""ai"",""{answer}""),
    ]
)

custom_fewshot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=custom_selector,# 커스텀 예제 선택기 사용example_prompt=example_prompt,# 예제 프롬프트 사용)

custom_prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""You are a helpful assistant."",
        ),
        few_shot_prompt,
        (""human"",""{instruction}\n{input}""),
    ]
)
```
```
Copy# chain 을 생성합니다.chain = custom_prompt | llm
```
```
Copy# 스트리밍 출력을 하기 위한 함수를 불러옵니다.fromlangchain_teddynote.messagesimportstream_response
```
```
Copyquestion = {""instruction"":""회의록을 작성해 주세요"",""input"":""2023년 12월 26일, ABC 기술 회사의 제품 개발 팀은 새로운 모바일 애플리케이션 프로젝트에 대한 주간 진행 상황 회의를 가졌다. 이 회의에는 프로젝트 매니저인 최현수, 주요 개발자인 황지연, UI/UX 디자이너인 김태영이 참석했다. 회의의 주요 목적은 프로젝트의 현재 진행 상황을 검토하고, 다가오는 마일스톤에 대한 계획을 수립하는 것이었다. 각 팀원은 자신의 작업 영역에 대한 업데이트를 제공했고, 팀은 다음 주까지의 목표를 설정했다."",
}# 실행 및 결과 출력_ = stream_response(chain.stream(question))
```
```
회의록: ABC 기술 회사 제품 개발 팀 주간 진행 상황 회의
일시: 2023년 12월 26일
장소: ABC 기술 회사 회의실
참석자: 최현수 (프로젝트 매니저), 황지연 (주요 개발자), 김태영 (UI/UX 디자이너)

1. 개회
   - 최현수 프로젝트 매니저가 회의를 개회하고, 회의의 목적을 설명함.
   - 목적: 프로젝트의 현재 진행 상황 검토 및 다가오는 마일스톤에 대한 계획 수립.

2. 프로젝트 진행 상황 보고
   - 황지연 개발자가 현재 개발 진행 상황에 대해 보고.
     - 최근 완료된 기능과 현재 개발 중인 기능에 대한 상세 설명.
     - 발생한 문제점 및 해결 방안에 대해 논의.
   - 김태영 디자이너가 UI/UX 디자인 진행 상황에 대해 보고.
     - 새로운 디자인 요소의 적용 상황과 사용자 피드백에 대한 내용 공유.
     - 다음 단계의 디자인 계획 발표.

3. 다가오는 마일스톤 계획
   - 최현수가 다음 주까지의 주요 목표와 마일스톤을 제시.
     - 개발 팀과 디자인 팀이 협력하여 특정 기능을 완성할 계획.
     - 테스트 계획 및 사용자 테스트 준비에 대한 논의.

4. 문제 해결 및 리소스 조정
   - 팀원들이 직면한 기술적, 디자인적 문제에 대해 논의.
   - 필요한 추가 리소스나 지원에 대해 토론.

5. 다음 회의 일정 및 목표 설정
   - 다음 회의 날짜 및 시간 확정.
   - 각 팀원에게 다음 회의까지의 구체적인 작업 목표 할당.

6. 마무리
   - 최현수 매니저가 회의를 마무리하며, 팀의 노력을 치하.
   - 회의록 정리 및 배포는 황지연 개발자가 담당하기로 함.
```
```
Copyquestion = {""instruction"":""문서를 요약해 주세요"",""input"":""이 문서는 '2023년 글로벌 경제 전망'에 관한 30페이지에 달하는 상세한 보고서입니다. 보고서는 세계 경제의 현재 상태, 주요 국가들의 경제 성장률, 글로벌 무역 동향, 그리고 다가오는 해에 대한 경제 예측을 다룹니다. 이 보고서는 또한 다양한 경제적, 정치적, 환경적 요인들이 세계 경제에 미칠 영향을 분석하고 있습니다."",
}# 실행 및 결과 출력_ = stream_response(chain.stream(question))
```
```
문서 요약: 2023년 글로벌 경제 전망 보고서

- 현재 상태: 세계 경제의 현재 상황에 대한 분석, 최근의 경제적 변동과 그 원인들을 설명.
- 경제 성장률: 주요 국가들의 경제 성장률을 검토하며, 각국의 경제 회복력과 성장 동력을 평가.
- 글로벌 무역: 전 세계적인 무역 동향과 주요 무역 파트너 간의 관계 변화에 대해 설명.
- 경제 예측: 2023년도에 대한 경제 예측을 제시하며, 예상되는 주요 경제 이벤트와 그 영향을 분석.
- 영향 요인: 경제적, 정치적, 환경적 요인들이 세계 경제에 미칠 영향을 심층 분석.

이 보고서는 다가오는 해의 글로벌 경제 전망을 이해하는 데 필요한 중요한 정보와 분석을 제공하며, 다양한 요인들이 경제에 미치는 복합적인 영향을 평가합니다.
```
```
Copyquestion = {""instruction"":""문장을 교정해 주세요"",""input"":""회사는 올해 매출이 증가할 것으로 예상한다. 새로운 전략이 잘 작동하고 있다."",
}# 실행 및 결과 출력_ = stream_response(chain.stream(question))
```
```
회사는 올해 매출이 증가할 것으로 예상하고 있으며, 새로운 전략이 잘 작동하고 있는 것으로 보인다.
```"
03. LangChain Hub,https://wikidocs.net/233349,"# 03. LangChain Hub

다음은 LangChain Hub 에서 프롬프트를 받아서 실행하는 예제입니다.
아래 주소에서 LangChain Hub 프롬프트를 확인할 수 있습니다.
받아오는 방법은 프롬프트 repo 의 아이디 값을 가져 올 수 있고, commit id 를 붙여서 특정 버전에 대한 프롬프트를 받아올 수도 있습니다.
## Hub로부터 Prompt 받아오기
```
Copyfromlangchainimporthub# 가장 최신 버전의 프롬프트를 가져옵니다.prompt = hub.pull(""rlm/rag-prompt"")
```
```
Copy# 프롬프트 내용 출력print(prompt)
```
```
input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:""))]
```
```
Copy# 특정 버전의 프롬프트를 가져오려면 버전 해시를 지정하세요prompt = hub.pull(""rlm/rag-prompt:50442af1"")
prompt
```
```
ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:""))])
```
## Prompt Hub 에 자신의 프롬프트 등록
```
Copyfromlangchain.promptsimportChatPromptTemplate


prompt = ChatPromptTemplate.from_template(""주어진 내용을 바탕으로 다음 문장을 요약하세요. 답변은 반드시 한글로 작성하세요\n\nCONTEXT: {context}\n\nSUMMARY:"")
prompt
```
```
ChatPromptTemplate(input_variables=['context'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='주어진 내용을 바탕으로 다음 문장을 요약하세요. 답변은 반드시 한글로 작성하세요\n\nCONTEXT: {context}\n\nSUMMARY:'))])
```
```
Copyfromlangchainimporthub# 프롬프트를 허브에 업로드합니다.hub.push(""teddynote/simple-summary-korean"", prompt)
```
다음은 Hub 에 성공적으로 업로드 된 후 출력입니다.
아이디/프롬프트명/해시
출력: 'https://smith.langchain.com/hub/teddynote/simple-summary-korean/0e296563'
```
Copyfromlangchainimporthub# 프롬프트를 허브로부터 가져옵니다.pulled_prompt = hub.pull(""teddynote/simple-summary-korean"")
```
```
Copy# 프롬프트 내용 출력print(pulled_prompt)
```
```
input_variables=['context'] metadata={'lc_hub_owner': 'teddynote', 'lc_hub_repo': 'simple-summary-korean', 'lc_hub_commit_hash': '0e296563564b581e5ad77089b035596246c2b96046f8db0503355dd3c275d056'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='주어진 내용을 바탕으로 다음 문장을 요약하세요. 답변은 반드시 한글로 작성하세요\n\nCONTEXT: {context}\n\nSUMMARY:'))]
```"
04. 개인화된 프롬프트(Hub에 업로드),https://wikidocs.net/233350,"# 04. 개인화된 프롬프트(Hub에 업로드)

LangChain 아이디를 입력합니다.
```
Copy# Owner 지정PROMPT_OWNER =""teddynote""
```
## 요약: Stuff Documents
```
Copyfromlangchainimporthubfromlangchain.promptsimportPromptTemplate

prompt_title =""summary-stuff-documents""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt_template =""""""Please summarize the sentence according to the following REQUEST.
REQUEST:
1. Summarize the main points in bullet points.
2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.
3. Use various emojis to make the summary more interesting.
4. DO NOT include any unnecessary information.

CONTEXT:
{context}

SUMMARY:""
""""""prompt = PromptTemplate.from_template(prompt_template)
prompt
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
## Map Prompt
```
Copyfromlangchainimporthubfromlangchain.promptsimportPromptTemplate

prompt_title =""map-prompt""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt_template =""""""You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.
Please provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format. 
The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. 
Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. 
The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.

GIVEN DOCUMENTS:
{docs}

FORMAT:
1. main theme 1
2. main theme 2
3. main theme 3
...

CAUTION:
- DO NOT list more than 5 main themes.

Helpful Answer:
""""""prompt = PromptTemplate.from_template(prompt_template)
prompt
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
## Reduce Prompt
```
Copyfromlangchainimporthubfromlangchain.promptsimportPromptTemplate

prompt_title =""reduce-prompt""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt_template =""""""You are a helpful expert in summary writing.
You are given numbered lists of summaries.
Extract top 10 most important insights from the summaries.
Then, write a summary of the insights in KOREAN.

LIST OF SUMMARIES:
{doc_summaries}

Helpful Answer:
""""""prompt = PromptTemplate.from_template(prompt_template)
prompt
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
```
Copyfromlangchainimporthubfromlangchain.promptsimportPromptTemplate

prompt_title =""chain-of-density-reduce-korean""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt_template =""""""You are a helpful expert in summary writing. You are given lists of summaries.
Please sum up previously summarized sentences according to the following REQUEST.
REQUEST:
1. Summarize the main points in bullet points in KOREAN.
2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.
3. Use various emojis to make the summary more interesting.
4. MOST IMPORTANT points should be organized at the top of the list.
5. DO NOT include any unnecessary information.

LIST OF SUMMARIES:
{doc_summaries}

Helpful Answer: """"""prompt = PromptTemplate.from_template(prompt_template)
prompt
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
## Metadata Tagger
```
Copyfromlangchainimporthubfromlangchain.promptsimportChatPromptTemplate

prompt_title =""metadata-tagger""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt_template =""""""Given the following product review, conduct a comprehensive analysis to extract key aspects mentioned by the customer, with a focus on evaluating the product's design and distinguishing between positive aspects and areas for improvement. 
Identify primary features or attributes of the product that the customer appreciated or highlighted, specifically looking for mentions related to the feel of the keys, sound produced by the keys, overall user experience, charging aspect, and the design of the product, etc. 
Assess the overall tone of the review (positive, neutral, or negative) based on the sentiment expressed about these attributes. 
Additionally, provide a detailed evaluation of the design, outline the positive aspects that the customer enjoyed, and note any areas of improvement or disappointment mentioned. 
Extract the customer's rating of the product on a scale of 1 to 5, as indicated at the beginning of the review. 
Summarize your findings in a structured JSON format, including an array of keywords, evaluations for design, satisfaction points, improvement areas, the assessed tone, and the numerical rating. 

INPUT:
{input}

""""""prompt = ChatPromptTemplate.from_template(prompt_template)
prompt
```
```
ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=""Given the following product review, conduct a comprehensive analysis to extract key aspects mentioned by the customer, with a focus on evaluating the product's design and distinguishing between positive aspects and areas for improvement. \nIdentify primary features or attributes of the product that the customer appreciated or highlighted, specifically looking for mentions related to the feel of the keys, sound produced by the keys, overall user experience, charging aspect, and the design of the product, etc. \nAssess the overall tone of the review (positive, neutral, or negative) based on the sentiment expressed about these attributes. \nAdditionally, provide a detailed evaluation of the design, outline the positive aspects that the customer enjoyed, and note any areas of improvement or disappointment mentioned. \nExtract the customer's rating of the product on a scale of 1 to 5, as indicated at the beginning of the review. \nSummarize your findings in a structured JSON format, including an array of keywords, evaluations for design, satisfaction points, improvement areas, the assessed tone, and the numerical rating. \n\nINPUT:\n{input}\n\n""))])
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
## Chain of Density 요약
```
Copyfromlangchainimporthubfromlangchain.promptsimportChatPromptTemplate

prompt_title =""chain-of-density-korean""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt = ChatPromptTemplate.from_template(""""""Article: {ARTICLE}
You will generate increasingly concise, entity-dense summaries of the above article. 

Repeat the following 2 steps 5 times. 

Step 1. Identify 1-3 informative entities ("";"" delimited) from the article which are missing from the previously generated summary. 
Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. 

A missing entity is:
- relevant to the main story, 
- specific yet concise (100 words or fewer), 
- novel (not in the previous summary), 
- faithful (present in the article), 
- anywhere (can be located anywhere in the article).

Guidelines:

- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., ""this article discusses"") to reach ~200 words.
- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.
- Make space with fusion, compression, and removal of uninformative phrases like ""the article discusses"".
- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. 
- Missing entities can appear anywhere in the new summary.
- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. 

Remember, use the exact same number of words for each summary.
Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are ""Missing_Entities"" and ""Denser_Summary"".
Use only KOREAN language to reply."""""")
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
## Chain of Density (Korean) - 2
```
Copyfromlangchainimporthubfromlangchain.promptsimportChatPromptTemplate

prompt_title =""chain-of-density-map-korean""# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)prompt = ChatPromptTemplate.from_template(""""""Article: {ARTICLE}
You will generate increasingly concise, entity-dense summaries of the above article. 

Repeat the following 2 steps 3 times. 

Step 1. Identify 1-3 informative entities ("";"" delimited) from the article which are missing from the previously generated summary. 
Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. 

A missing entity is:
- relevant to the main story, 
- specific yet concise (100 words or fewer), 
- novel (not in the previous summary), 
- faithful (present in the article), 
- anywhere (can be located anywhere in the article).

Guidelines:

- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., ""this article discusses"") to reach ~200 words.
- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.
- Make space with fusion, compression, and removal of uninformative phrases like ""the article discusses"".
- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. 
- Missing entities can appear anywhere in the new summary.
- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. 

Remember, use the exact same number of words for each summary.
Answer ""Missing Entities"" and ""Denser_Summary"" as in TEXT format.
Use only KOREAN language to reply."""""")
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt)
```
## RAG 문서 프롬프트
```
Copyprompt_title =""rag-prompt-korean""
```
```
Copysystem =""""""당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.
검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.
한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. Don't narrate the answer, just answer the question. Let's think step-by-step.""""""human =""""""#Question: 
{question} 

#Context: 
{context} 

#Answer:""""""fromlangchain.promptsimportChatPromptTemplate


prompt = ChatPromptTemplate.from_messages([(""system"", system), (""human"", human)])
```
```
Copyhub.push(f""{PROMPT_OWNER}/{prompt_title}"", prompt, parent_commit_hash=""latest"")
```
```
'https://smith.langchain.com/hub/teddynote/rag-prompt-korean/4214de29'
```"
CH03 출력 파서(Output Parsers),https://wikidocs.net/233771,"# CH03 출력 파서(Output Parsers)

출력 파서(Output Parsers)는 LLM의 출력을 받아 더 적합한 형식으로 변환하는 역할을 담당합니다. 이는 LLM을 사용하여어떤 형태의 구조화된 데이터를 생성할 때 매우 유용합니다.
LangChain OutputParsers의 눈에 띄는 이점 중 하나는 많은 종류의 다양한 출력 파서를 보유하고 있을 뿐만 아니라, 많은 출력 파서들이스트리밍을 지원한다는 것입니다."
01. Pydantic 출력 파서(PydanticOutputParser),https://wikidocs.net/233786,"# 01. Pydantic 출력 파서(PydanticOutputParser)

PydanticOutputParser는 언어 모델의 출력을 더 구조화된 정보로 변환하는 데 도움이 되는 클래스입니다. 언어 모델에서 나온 단순 텍스트를 넘어서, 사용자가 필요로 하는 정보를 명확하고 체계적인 형태로 제공할 수 있습니다.
이 클래스를 활용함으로써, 언어 모델의 출력을 특정 데이터 모델에 맞게 변환하여, 더 용이하게 정보를 처리하고 활용할 수 있게 됩니다.
PydanticOutputParser에는 주로 두 가지 핵심 메서드가 구현되어야 합니다.
- get_format_instructions(): 이 메서드는 언어 모델이 출력해야 할 정보의 형식을 정의하는 지침을 제공합니다. 예를 들어, 언어 모델이 출력해야 할 데이터의 필드와 그 형태를 설명하는 지침을 문자열로 반환할 수 있습니다. 이 지침은 사용자가 원하는 출력 형식을 정확히 이해하고 준비하는 데 도움을 줍니다.
- parse(): 이 메서드는 언어 모델의 출력(문자열로 가정)을 받아들여 이를 특정 구조로 분석하고 변환합니다. Pydantic와 같은 도구를 사용하여, 입력된 문자열을 사전 정의된 스키마에 따라 검증하고, 해당 스키마를 따르는 데이터 구조로 변환합니다. 이 과정을 통해 언어 모델의 출력을 더 사용하기 쉬운 형태로 가공할 수 있습니다.
get_format_instructions(): 이 메서드는 언어 모델이 출력해야 할 정보의 형식을 정의하는 지침을 제공합니다. 예를 들어, 언어 모델이 출력해야 할 데이터의 필드와 그 형태를 설명하는 지침을 문자열로 반환할 수 있습니다. 이 지침은 사용자가 원하는 출력 형식을 정확히 이해하고 준비하는 데 도움을 줍니다.
parse(): 이 메서드는 언어 모델의 출력(문자열로 가정)을 받아들여 이를 특정 구조로 분석하고 변환합니다. Pydantic와 같은 도구를 사용하여, 입력된 문자열을 사전 정의된 스키마에 따라 검증하고, 해당 스키마를 따르는 데이터 구조로 변환합니다. 이 과정을 통해 언어 모델의 출력을 더 사용하기 쉬운 형태로 가공할 수 있습니다.
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.promptsimportPromptTemplatefromlangchain.output_parsersimportPydanticOutputParserfromlangchain_core.pydantic_v1importBaseModel, Field


llm = ChatOpenAI(temperature=0)
```
다음은 이메일 본문 예시입니다.
```
Copyemail_conversation =""""""From: 김철수 (chulsoo.kim@bikecorporation.me)
To: 이은채 (eunchae@teddyinternational.me)
Subject: ""ZENESIS"" 자전거 유통 협력 및 미팅 일정 제안

안녕하세요, 이은채 대리님,

저는 바이크코퍼레이션의 김철수 상무입니다. 최근 보도자료를 통해 귀사의 신규 자전거 ""ZENESIS""에 대해 알게 되었습니다. 바이크코퍼레이션은 자전거 제조 및 유통 분야에서 혁신과 품질을 선도하는 기업으로, 이 분야에서의 장기적인 경험과 전문성을 가지고 있습니다.

ZENESIS 모델에 대한 상세한 브로슈어를 요청드립니다. 특히 기술 사양, 배터리 성능, 그리고 디자인 측면에 대한 정보가 필요합니다. 이를 통해 저희가 제안할 유통 전략과 마케팅 계획을 보다 구체화할 수 있을 것입니다.

또한, 협력 가능성을 더 깊이 논의하기 위해 다음 주 화요일(1월 15일) 오전 10시에 미팅을 제안합니다. 귀사 사무실에서 만나 이야기를 나눌 수 있을까요?

감사합니다.

김철수
상무이사
바이크코퍼레이션
""""""
```
위와 같은 이메일 내용이 주어졌을 때 아래의 PyDantic 스타일로 정의된 클래스를 사용하여 이메일의 정보를 파싱해 보겠습니다.
```
CopyclassEmailSummary(BaseModel):
    person:str= Field(description=""메일을 보낸 사람"")
    email:str= Field(description=""메일을 보낸 사람의 이메일 주소"")
    subject:str= Field(description=""메일 제목"")
    summary:str= Field(description=""메일 본문을 요약한 텍스트"")
    date:str= Field(description=""메일 본문에 언급된 미팅 날짜와 시간"")# PydanticOutputParser 생성parser = PydanticOutputParser(pydantic_object=EmailSummary)
```
프롬프트를 정의합니다.
question: 유저의 질문을 받습니다.
email_conversation: 이메일 본문의 내용을 입력합니다.
format: 형식을 지정합니다.
```
Copyprompt = PromptTemplate.from_template(""""""
You are a helpful assistant. Please answer the following questions in KOREAN.

QUESTION:
{question}

EMAIL CONVERSATION:
{email_conversation}

FORMAT:
{format}
"""""")# format 에 PydanticOutputParser의 format을 추가prompt = prompt.partial(format=parser.get_format_instructions())
```
다음으로는 Chain 을 생성합니다.
```
Copychain = prompt | llm
```
체인을 실행하고 결과를 확인합니다.
```
Copy# chain 을 실행하고 결과를 출력합니다.response = chain.invoke(
    {""email_conversation"": email_conversation,""question"":""이메일 내용중 주요 내용을 추출해 주세요."",
    }
)# 결과는 JSON 형태로 출력됩니다.print(response.content)
```
```
{
  ""person"": ""김철수"",
  ""email"": ""chulsoo.kim@bikecorporation.me"",
  ""subject"": ""\""ZENESIS\"" 자전거 유통 협력 및 미팅 일정 제안"",
  ""summary"": ""바이크코퍼레이션의 김철수 상무가 이은채 대리님에게 ZENESIS 모델에 대한 상세한 정보를 요청하고, 유통 전략과 마케팅 계획을 논의하기 위한 미팅을 제안하는 이메일입니다."",
  ""date"": ""2023-01-15 오전 10시""
}
```
마지막으로parser를 사용하여 결과를 파싱하고EmailSummary객체로 변환합니다.
```
Copyparser.parse(response.content)
```
```
EmailSummary(person='김철수', email='chulsoo.kim@bikecorporation.me', subject='""ZENESIS"" 자전거 유통 협력 및 미팅 일정 제안', summary='바이크코퍼레이션의 김철수 상무가 이은채 대리님에게 ZENESIS 모델에 대한 상세한 정보를 요청하고, 유통 전략과 마케팅 계획을 논의하기 위한 미팅을 제안하는 이메일입니다.', date='2023-01-15 오전 10시')
```
## parser 가 결합된 체인 생성
출력 결과를 객체로 생성할 수 있습니다.
```
Copy# 출력 파서를 추가하여 전체 체인을 재구성합니다.chain = prompt | llm | parser
```
```
Copy# chain 을 실행하고 결과를 출력합니다.response = chain.invoke(
    {""email_conversation"": email_conversation,""question"":""이메일 내용중 주요 내용을 추출해 주세요."",
    }
)# 결과는 EmailSummary 객체 형태로 출력됩니다.response
```
```
EmailSummary(person='김철수', email='chulsoo.kim@bikecorporation.me', subject='""ZENESIS"" 자전거 유통 협력 및 미팅 일정 제안', summary=""바이크코퍼레이션의 김철수 상무가 귀사의 'ZENESIS' 자전거에 대한 관심을 표현하고, 상세한 브로슈어 요청 및 미팅 제안을 하였습니다."", date='2023-01-15 10:00 AM')
```"
02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser),https://wikidocs.net/233787,"# 02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser)

CommaSeparatedListOutputParser는 쉼표로 구분된 항목 목록을 반환할 필요가 있을 때 유용합니다.
이 출력 파서를 사용하면, 사용자가 입력한 데이터나 요청한 정보를 쉼표로 구분하여 명확하고 간결한 목록 형태로 제공받을 수 있습니다.
예를 들어, 여러 개의 데이터 포인트, 이름, 항목 또는 다른 종류의 값들을 나열할 때 이를 통해 효과적으로 정보를 정리하고 사용자에게 전달할 수 있습니다.
이 방법은 정보를 구조화하고, 가독성을 높이며, 특히 데이터를 다루거나 리스트 형태의 결과를 요구하는 경우에 매우 유용합니다.
- PromptTemplate를 사용하여 사용자 입력에 따라 동적으로 프롬프트를 생성합니다. 이때,subject를 입력 변수로 받고,format_instructions를 부분 변수로 설정합니다.
PromptTemplate를 사용하여 사용자 입력에 따라 동적으로 프롬프트를 생성합니다. 이때,subject를 입력 변수로 받고,format_instructions를 부분 변수로 설정합니다.
```
Copyfromlangchain.output_parsersimportCommaSeparatedListOutputParserfromlangchain.promptsimportPromptTemplatefromlangchain_openaiimportChatOpenAI# 콤마로 구분된 리스트 출력 파서 초기화output_parser = CommaSeparatedListOutputParser()# 출력 형식 지침 가져오기format_instructions = output_parser.get_format_instructions()# 프롬프트 템플릿 설정prompt = PromptTemplate(# 주제에 대한 다섯 가지를 나열하라는 템플릿template=""List five {subject}.\n{format_instructions}"",
    input_variables=[""subject""],# 입력 변수로 'subject' 사용# 부분 변수로 형식 지침 사용partial_variables={""format_instructions"": format_instructions},
)# ChatOpenAI 모델 초기화model = ChatOpenAI(temperature=0)# 프롬프트, 모델, 출력 파서를 연결하여 체인 생성chain = prompt | model | output_parser
```
- 체인을 실행하고 결과를 출력합니다.
체인을 실행하고 결과를 출력합니다.
```
Copychain.invoke(
    {""subject"":""대한민국 관광명소""}
)# ""대한민국 관광명소""에 대한 체인 호출 실행
```
```
['경복궁', '남산타워', '부산 해운대해수욕장', '제주도', '경주여행']
```
- chain.stream을 사용하여 ""대한민국 관광명소""에 대한 스트림을 반복 처리합니다.
- 반복 중 스트림의 결과를 출력합니다.
chain.stream을 사용하여 ""대한민국 관광명소""에 대한 스트림을 반복 처리합니다.
반복 중 스트림의 결과를 출력합니다.
```
Copy# 아이스크림 맛에 대한 스트림을 순회합니다.forsinchain.stream({""subject"":""대한민국 관광명소""}):print(s)# 스트림의 내용을 출력합니다.
```
```
['경복궁']
['남산타워']
['부산 해운대해수욕장']
['제주도']
['경주여행']
```"
03. 구조화된 출력 파서(StructuredOuputParser),https://wikidocs.net/233788,"# 03. 구조화된 출력 파서(StructuredOuputParser)

이 출력 파서는 여러 필드를 반환하고자 할 때 사용할 수 있습니다. Pydantic/JSON 파서가 더 강력하지만, 이는 덜 강력한 모델에 유용합니다.
```
Copyfromlangchain.output_parsersimportResponseSchema, StructuredOutputParserfromlangchain.promptsimportPromptTemplatefromlangchain_openaiimportChatOpenAI
```
- ResponseSchema클래스를 사용하여 사용자의 질문에 대한 답변과 사용된 소스(웹사이트)에 대한 설명을 포함하는 응답 스키마를 정의합니다.
- StructuredOutputParser를response_schemas를 사용하여 초기화하여, 정의된 응답 스키마에 따라 출력을 구조화합니다.
ResponseSchema클래스를 사용하여 사용자의 질문에 대한 답변과 사용된 소스(웹사이트)에 대한 설명을 포함하는 응답 스키마를 정의합니다.
StructuredOutputParser를response_schemas를 사용하여 초기화하여, 정의된 응답 스키마에 따라 출력을 구조화합니다.
```
Copy# 사용자의 질문에 대한 답변response_schemas = [
    ResponseSchema(name=""answer"", description=""사용자의 질문에 대한 답변""),
    ResponseSchema(
        name=""source"",
        description=""사용자의 질문에 답하기 위해 사용된 출처, 웹사이트 이여야 합니다."",
    ),
]# 응답 스키마를 기반으로 한 구조화된 출력 파서 초기화output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
```
우리는 이제 응답이 어떻게 포맷되어야 하는지에 대한 지시사항이 포함된 문자열을 받게 되며, 그것을 우리의 프롬프트에 삽입합니다.
- output_parser.get_format_instructions()함수를 호출하여 포맷 지시사항을 가져옵니다.
- PromptTemplate클래스를 사용하여 사용자의 질문에 최대한 답변하는 프롬프트 템플릿을 생성합니다.
- 템플릿에는question이라는 입력 변수와format_instructions라는 부분 변수가 포함됩니다.
output_parser.get_format_instructions()함수를 호출하여 포맷 지시사항을 가져옵니다.
PromptTemplate클래스를 사용하여 사용자의 질문에 최대한 답변하는 프롬프트 템플릿을 생성합니다.
템플릿에는question이라는 입력 변수와format_instructions라는 부분 변수가 포함됩니다.
```
Copy# 출력 형식 지시사항을 파싱합니다.format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(# 사용자의 질문에 최대한 답변하도록 템플릿을 설정합니다.template=""answer the users question as best as possible.\n{format_instructions}\n{question}"",# 입력 변수로 'question'을 사용합니다.input_variables=[""question""],# 부분 변수로 'format_instructions'을 사용합니다.partial_variables={""format_instructions"": format_instructions},
)
```
```
Copymodel = ChatOpenAI(temperature=0)# ChatOpenAI 모델 초기화chain = prompt | model | output_parser# 프롬프트, 모델, 출력 파서를 연결
```
```
Copy# 대한민국의 수도가 무엇인지 질문합니다.chain.invoke({""question"":""대한민국의 수도는 어디인가요?""})
```
```
{'answer': '서울', 'source': 'https://ko.wikipedia.org/wiki/%EC%84%9C%EC%9A%B8'}
```
- chain.stream메소드를 사용하여 ""세종대왕의 업적은 무엇인가요?"" 라는 질문에 대한 스트림을 반복 처리합니다.
- 반복 과정에서 스트림의 결과를 출력합니다.
chain.stream메소드를 사용하여 ""세종대왕의 업적은 무엇인가요?"" 라는 질문에 대한 스트림을 반복 처리합니다.
반복 과정에서 스트림의 결과를 출력합니다.
```
Copyforsinchain.stream({""question"":""세종대왕의 업적은 무엇인가요?""}):# 스트리밍 출력print(s)
```
```
{'answer': '세종대왕은 한글을 창제하고 문화 발전에 큰 기여를 한 대표적 인물입니다.', 'source': 'https://ko.wikipedia.org/wiki/%EC%84%B8%EC%A2%85%EB%8C%80%EC%99%95'}
```"
04. JSON 출력 파서(JsonOutputParser),https://wikidocs.net/233789,"# 04. JSON 출력 파서(JsonOutputParser)

이 출력 파서는 사용자가 원하는 JSON 스키마를 지정할 수 있게 해주며, 그 스키마에 맞게 LLM에서 데이터를 조회하여 결과를 도출해줍니다.
LLM이 데이터를 정확하고 효율적으로 처리하여 원하는 형태의 JSON을 생성하기 위해서는, 모델의 용량이 충분해야 한다는 점을 기억해야 합니다.
데이터 모델을 정의할 때는 Pydantic과 같은 도구를 사용하여, 스키마가 잘 정의되고 검증될 수 있도록 할 수 있습니다.
이렇게 하면 사용자가 특정 데이터 구조를 기반으로 정보를 요청하고, 모델이 그 요구사항에 맞는 데이터를 정확하게 제공할 수 있게 됩니다.
```
CopyfromtypingimportListfromlangchain.promptsimportPromptTemplatefromlangchain_core.output_parsersimportJsonOutputParserfromlangchain_core.pydantic_v1importBaseModel, Fieldfromlangchain_openaiimportChatOpenAI
```
```
Copymodel = ChatOpenAI(temperature=0)# ChatOpenAI 모델 초기화 및 온도 설정
```
원하는 출력 구조를 정의합니다.
```
Copy# 원하는 데이터 구조를 정의합니다.classTopic(BaseModel):
    description:str= Field(description=""Concise description about topic"")
    hashtags:str= Field(description=""Some keywords in hashtag format"")
```
- JsonOutputParser를 사용하여 파서를 설정하고, 프롬프트 템플릿에 지시사항을 주입합니다.
- PromptTemplate을 이용하여 사용자 질의에 대한 답변을 생성하는 템플릿을 구성합니다. 이때, 질의(query)와 포맷 지시사항(format_instructions)을 변수로 사용합니다.
- prompt,model,parser를 연결하여 처리 체인을 구성합니다.
- 처리 체인을 호출하여query에 대한 응답을 생성합니다.
JsonOutputParser를 사용하여 파서를 설정하고, 프롬프트 템플릿에 지시사항을 주입합니다.
PromptTemplate을 이용하여 사용자 질의에 대한 답변을 생성하는 템플릿을 구성합니다. 이때, 질의(query)와 포맷 지시사항(format_instructions)을 변수로 사용합니다.
prompt,model,parser를 연결하여 처리 체인을 구성합니다.
처리 체인을 호출하여query에 대한 응답을 생성합니다.
```
Copy# 질의 작성query =""온난화에 대해 알려주세요.""# 파서를 설정하고 프롬프트 템플릿에 지시사항을 주입합니다.parser = JsonOutputParser(pydantic_object=Topic)

prompt = PromptTemplate(# 사용자 쿼리에 답하십시오.template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],# 입력 변수 설정# 부분 변수에 형식 지시사항 설정partial_variables={""format_instructions"": parser.get_format_instructions()},
)

chain = prompt | model | parser# 체인을 구성합니다.chain.invoke({""query"": query})# 체인을 호출하여 쿼리 실행
```
```
{'description': '온난화는 지구 온도가 상승하는 현상을 의미하며, 이는 대기 중 온실가스 농도 증가와 인간 활동에 의해 발생합니다. 이로 인해 극지방의 빙하가 녹아 해수면 상승, 기후 변화 등의 문제가 발생할 수 있습니다.', 'hashtags': '#온난화 #기후변화 #온실가스'}
```
## Without Pydantic
Pydantic 없이도 이 기능을 사용할 수 있습니다. 이 경우 JSON을 반환하도록 요청하지만, 스키마가 어떻게 되어야 하는지에 대한 구체적인 정보는 제공하지 않습니다.
- JsonOutputParser클래스의 인스턴스를parser변수에 할당합니다.
- PromptTemplate클래스를 사용하여 사용자 쿼리에 대한 프롬프트를 생성합니다. 이때,template매개변수에는 사용자 쿼리를 처리할 템플릿 문자열을,input_variables에는 템플릿에서 사용할 변수 목록을,partial_variables에는parser에서 제공하는 형식 지침을 포함합니다.
- prompt,model,parser를 연결하여chain을 구성합니다.
- chain.invoke메소드를 호출하여{""query"": query}매개변수를 전달함으로써, 사용자 쿼리에 대한 처리를 시작합니다.
JsonOutputParser클래스의 인스턴스를parser변수에 할당합니다.
PromptTemplate클래스를 사용하여 사용자 쿼리에 대한 프롬프트를 생성합니다. 이때,template매개변수에는 사용자 쿼리를 처리할 템플릿 문자열을,input_variables에는 템플릿에서 사용할 변수 목록을,partial_variables에는parser에서 제공하는 형식 지침을 포함합니다.
prompt,model,parser를 연결하여chain을 구성합니다.
chain.invoke메소드를 호출하여{""query"": query}매개변수를 전달함으로써, 사용자 쿼리에 대한 처리를 시작합니다.
```
Copy# 질의 작성query =""온난화에 대해 알려주세요. 온난화에 대한 설명은 `description`에, 관련 키워드는 `hashtags`에 담아주세요.""parser = JsonOutputParser()# JSON 출력 파서 초기화prompt = PromptTemplate(# 사용자 쿼리에 답변하는 템플릿template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],# 입력 변수로 'query' 사용# 부분 변수로 포맷 지시사항 설정partial_variables={""format_instructions"": parser.get_format_instructions()},
)

chain = prompt | model | parser# 프롬프트, 모델, 파서를 연결하는 체인 생성chain.invoke({""query"": query})# 체인을 호출하여 농담 쿼리 처리
```
```
{'description': '온난화는 지구 온도가 상승하는 현상을 의미합니다. 이는 대기 중 온실가스 농도 증가로 인해 발생하며, 지구 기후와 생태계에 심각한 영향을 미칩니다.', 'hashtags': ['온난화', '기후변화', '지구온난화', '온실가스']}
```"
05. 데이터프레임 출력 파서(PandasDataFrameOutputParser),https://wikidocs.net/233790,"# 05. 데이터프레임 출력 파서(PandasDataFrameOutputParser)

Pandas DataFrame은 Python 프로그래밍 언어에서 널리 사용되는 데이터 구조로, 데이터 조작 및 분석을 위해 흔히 사용됩니다. 구조화된 데이터를 다루기 위한 포괄적인 도구 세트를 제공하여, 데이터 정제, 변환 및 분석과 같은 작업에 다양하게 활용될 수 있습니다.
이 출력 파서는 사용자가 임의의 Pandas DataFrame을 지정하고 해당 DataFrame에서 데이터를 추출하여 형식화된 사전 형태로 데이터를 조회할 수 있는 LLM을 요청할 수 있게 해줍니다.
```
CopyimportpprintfromtypingimportAny,Dictimportpandasaspdfromlangchain.output_parsersimport(
    PandasDataFrameOutputParser,
)# Pandas 데이터프레임 출력 파서fromlangchain.promptsimportPromptTemplate# 프롬프트 템플릿fromlangchain_openaiimportChatOpenAI# ChatOpenAI
```
```
Copy# ChatOpenAI 모델 초기화model = ChatOpenAI(temperature=0)
```
- format_parser_output함수는 파서 출력을 사전 형식으로 변환하고 이를 예쁘게 출력합니다.
- 출력된 각 키에 대해, 해당 키의 값을.to_dict()메소드를 사용하여 사전 형식으로 변환합니다.
- pprint.PrettyPrinter클래스를 사용하여 변환된 출력을 가독성 높게 출력합니다.
format_parser_output함수는 파서 출력을 사전 형식으로 변환하고 이를 예쁘게 출력합니다.
출력된 각 키에 대해, 해당 키의 값을.to_dict()메소드를 사용하여 사전 형식으로 변환합니다.
pprint.PrettyPrinter클래스를 사용하여 변환된 출력을 가독성 높게 출력합니다.
```
Copy# 문서화 목적으로만 사용됩니다.defformat_parser_output(parser_output:Dict[str,Any]) ->None:forkeyinparser_output.keys():# 파서 출력의 키들을 순회합니다.# 각 키의 값을 딕셔너리로 변환합니다.parser_output[key] = parser_output[key].to_dict()# 예쁘게 출력합니다.returnpprint.PrettyPrinter(width=4, compact=True).pprint(parser_output)
```
- titanic.csv데이터를 읽어온 뒤 DataFrame 을 로드하여df변수에 할당합니다.
- PandasDataFrameOutputParser를 사용하여 DataFrame을 파싱합니다.
titanic.csv데이터를 읽어온 뒤 DataFrame 을 로드하여df변수에 할당합니다.
PandasDataFrameOutputParser를 사용하여 DataFrame을 파싱합니다.
```
Copy# 원하는 Pandas DataFrame을 정의합니다.df = pd.read_csv(""./data/titanic.csv"")
df.head()
```
```
Copy# 파서를 설정하고 프롬프트 템플릿에 지시사항을 주입합니다.parser = PandasDataFrameOutputParser(dataframe=df)
```
컬럼 작업을 수행하는 예제입니다.
- 사용자 쿼리를 처리하기 위한 프롬프트를 설정합니다.
- PromptTemplate을 사용하여 쿼리 포맷과 관련 지시사항을 포함한 템플릿을 생성합니다.
- parser.get_format_instructions()함수를 호출하여 포맷 지시사항을 가져옵니다.
- 프롬프트, 모델, 파서를 연결하여 처리 체인을 구성합니다.
- chain.invoke메소드를 사용하여df_query에 대한 처리를 실행합니다.
- format_parser_output함수를 호출하여 파서의 출력을 포맷합니다.
사용자 쿼리를 처리하기 위한 프롬프트를 설정합니다.
PromptTemplate을 사용하여 쿼리 포맷과 관련 지시사항을 포함한 템플릿을 생성합니다.
parser.get_format_instructions()함수를 호출하여 포맷 지시사항을 가져옵니다.
프롬프트, 모델, 파서를 연결하여 처리 체인을 구성합니다.
chain.invoke메소드를 사용하여df_query에 대한 처리를 실행합니다.
format_parser_output함수를 호출하여 파서의 출력을 포맷합니다.
```
Copy# 열 작업 예시입니다.df_query =""Retrieve the passenges ages.""# 프롬프트 설정prompt = PromptTemplate(# 사용자 쿼리에 답합니다.template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],# 입력 변수 설정partial_variables={""format_instructions"": parser.get_format_instructions()
    },# 부분 변수 설정)

chain = prompt | model | parser# 체인 생성parser_output = chain.invoke({""query"": df_query})# 체인 실행format_parser_output(parser_output)# 파서 출력 형식 지정
```
```
{'Age': {0: 22.0,
         1: 38.0,
         2: 26.0,
         3: 35.0,
         4: 35.0,
         5: nan,
         6: 54.0,
         7: 2.0,
         8: 27.0,
         9: 14.0,
         10: 4.0,
         11: 58.0,
         12: 20.0,
         13: 39.0,
         14: 14.0,
         15: 55.0,
         16: 2.0,
         17: nan,
         18: 31.0,
         19: nan}}
```
데이터 프레임의 첫 번째 행을 검색하는 쿼리를 처리합니다.
- PromptTemplate을 사용하여 사용자 쿼리에 대한 프롬프트를 설정합니다. 이때, 쿼리와 포맷 지시사항을 변수로 사용합니다.
- parser.get_format_instructions()함수를 호출하여 포맷 지시사항을 가져옵니다.
- 프롬프트, 모델, 파서를 연결하여 처리 체인을 구성합니다.
- 처리 체인을 통해 쿼리를 실행하고,chain.invoke함수를 사용하여 쿼리 결과를 처리합니다.
- format_parser_output함수를 사용하여 파서의 출력 결과를 포맷합니다.
PromptTemplate을 사용하여 사용자 쿼리에 대한 프롬프트를 설정합니다. 이때, 쿼리와 포맷 지시사항을 변수로 사용합니다.
parser.get_format_instructions()함수를 호출하여 포맷 지시사항을 가져옵니다.
프롬프트, 모델, 파서를 연결하여 처리 체인을 구성합니다.
처리 체인을 통해 쿼리를 실행하고,chain.invoke함수를 사용하여 쿼리 결과를 처리합니다.
format_parser_output함수를 사용하여 파서의 출력 결과를 포맷합니다.
```
Copy# 행 연산 예시입니다.df_query =""Retrieve the first row.""# 프롬프트 설정prompt = PromptTemplate(# 사용자 질의에 답합니다.template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],# 입력 변수partial_variables={""format_instructions"": parser.get_format_instructions()
    },# 부분 변수)

chain = prompt | model | parser# 체인 생성parser_output = chain.invoke({""query"": df_query})# 체인 실행format_parser_output(parser_output)# 파서 출력 형식 지정
```
```
{'0': {'Age': 22.0,
       'Cabin': nan,
       'Embarked': 'S',
       'Fare': 7.25,
       'Name': 'Braund, '
               'Mr. '
               'Owen '
               'Harris',
       'Parch': 0,
       'PassengerId': 1,
       'Pclass': 3,
       'Sex': 'male',
       'SibSp': 1,
       'Survived': 0,
       'Ticket': 'A/5 '
                 '21171'}}
```
Pandas DataFrame의 특정 열에서 일부 행의 평균을 검색하는 작업 예제입니다.
- PromptTemplate을 사용하여 쿼리 포맷과 관련 지시사항을 포함한 프롬프트를 구성합니다.
- parser.get_format_instructions()을 호출하여 포맷 지시사항을 가져옵니다.
- 프롬프트, 모델, 파서를 연결하여 처리 체인을 구성합니다.
- 처리 체인을 통해 사용자 쿼리(df_query)에 대한 파싱 작업을 수행합니다.
- 파싱된 출력 결과를 출력합니다.
PromptTemplate을 사용하여 쿼리 포맷과 관련 지시사항을 포함한 프롬프트를 구성합니다.
parser.get_format_instructions()을 호출하여 포맷 지시사항을 가져옵니다.
프롬프트, 모델, 파서를 연결하여 처리 체인을 구성합니다.
처리 체인을 통해 사용자 쿼리(df_query)에 대한 파싱 작업을 수행합니다.
파싱된 출력 결과를 출력합니다.
```
Copy# row 0 ~ 4의 평균 나이를 구합니다.df[""Age""].head().mean()
```
```
31.2
```
```
Copy# 임의의 Pandas DataFrame 작업 예시, 행의 수를 제한합니다.df_query =""Retrieve the average of the ages from row 0 to 4.""# 프롬프트 설정prompt = PromptTemplate(# 사용자 쿼리에 답합니다.template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],# 입력 변수 설정partial_variables={""format_instructions"": parser.get_format_instructions()
    },# 부분 변수 설정)

chain = prompt | model | parser# 체인 생성parser_output = chain.invoke({""query"": df_query})# 체인 실행print(parser_output)# 결과 출력
```
```
{'mean': 31.2}
```
- 사용자 쿼리에 대한 답변을 생성하기 위해PromptTemplate을 설정합니다.
- PromptTemplate에서는 쿼리와 포맷 지시사항을 포함한 템플릿을 정의합니다.
- 포맷 지시사항은parser.get_format_instructions()을 호출하여 얻습니다.
- prompt,model,parser를 연결하여 처리 체인을 구성합니다.
- 처리 체인을 통해 주어진 쿼리(df_query)에 대한 파싱 결과를 생성합니다.
사용자 쿼리에 대한 답변을 생성하기 위해PromptTemplate을 설정합니다.
PromptTemplate에서는 쿼리와 포맷 지시사항을 포함한 템플릿을 정의합니다.
포맷 지시사항은parser.get_format_instructions()을 호출하여 얻습니다.
prompt,model,parser를 연결하여 처리 체인을 구성합니다.
처리 체인을 통해 주어진 쿼리(df_query)에 대한 파싱 결과를 생성합니다.
```
Copy# 잘못 형식화된 쿼리의 예시입니다.df_query =""Retrieve the average Fare.""# 프롬프트 설정합니다.prompt = PromptTemplate(# 사용자 쿼리에 답합니다.template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],# 입력 변수를 지정합니다.# 부분 변수를 설정합니다.partial_variables={""format_instructions"": parser.get_format_instructions()},
)

chain = prompt | model | parser# 체인을 구성합니다.parser_output = chain.invoke({""query"": df_query})# 체인을 호출하여 결과를 얻습니다.
```
```
Copy# 결과 출력출력parser_output
```
```
{'mean': 22.19937}
```
```
Copy# 결과 검증df[""Fare""].mean()
```
```
22.19937
```"
06. 날짜 형식 출력 파서(DatetimeOutputParser),https://wikidocs.net/233791,"# 06. 날짜 형식 출력 파서(DatetimeOutputParser)

이 OutputParser는 LLM 출력을datetime형식으로 파싱하는 데 사용할 수 있습니다.
```
Copyfromlangchain.output_parsersimportDatetimeOutputParserfromlangchain.promptsimportPromptTemplatefromlangchain_openaiimportOpenAI
```
- DatetimeOutputParser클래스를 사용하여 출력 포맷을 파싱합니다.
- 사용자의 질문에 답변하는 템플릿을 정의합니다.
- PromptTemplate.from_template함수를 사용하여 템플릿과 포맷 지시사항을 결합합니다.
DatetimeOutputParser클래스를 사용하여 출력 포맷을 파싱합니다.
사용자의 질문에 답변하는 템플릿을 정의합니다.
PromptTemplate.from_template함수를 사용하여 템플릿과 포맷 지시사항을 결합합니다.
```
Copyoutput_parser = DatetimeOutputParser()# 날짜 및 시간 출력 파서 초기화template =""""""Answer the users question:

{question}

{format_instructions}""""""# 사용자 질문에 대한 답변 템플릿prompt = PromptTemplate.from_template(
    template,
    partial_variables={""format_instructions"": output_parser.get_format_instructions()
    },# 포맷 지침을 템플릿에 적용)
```
```
Copy# 프롬프트 내용을 출력prompt
```
```
PromptTemplate(input_variables=['question'], partial_variables={'format_instructions': ""Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 0739-06-25T17:26:36.848419Z, 0750-01-27T22:48:22.968426Z, 1964-07-09T13:36:18.520912Z\n\nReturn ONLY this string, no other words!""}, template='Answer the users question:\n\n{question}\n\n{format_instructions}')
```
체인을 생성합니다.
```
Copychain = (
    prompt | OpenAI() | output_parser
)# 프롬프트를 OpenAI로 전달하고 결과를 파싱합니다.
```
체인에invoke()메서드를 활용하여 쿼리를 실행합니다.
```
Copy# 체인을 호출하여 질문에 대한 답변을 생성합니다.output = chain.invoke({""question"":""Google 이 창업한 연도는?""})
```
결과를 출력합니다.
```
Copyprint(output)# 출력 결과를 표시합니다.
```
```
1998-09-04 00:00:00
```"
07. 열거형 출력 파서(EnumOutputParser),https://wikidocs.net/233792,"# 07. 열거형 출력 파서(EnumOutputParser)

Enum 출력 파서의 사용 방법을 알아봅시다!
```
Copyfromlangchain.output_parsers.enumimportEnumOutputParser
```
- enum모듈을 사용하여Colors클래스를 정의합니다.
- Colors클래스는Enum을 상속받으며,RED,GREEN,BLUE세 가지 색상 값을 가집니다.
enum모듈을 사용하여Colors클래스를 정의합니다.
Colors클래스는Enum을 상속받으며,RED,GREEN,BLUE세 가지 색상 값을 가집니다.
```
CopyfromenumimportEnumclassColors(Enum):
    RED =""빨간색""GREEN =""초록색""BLUE =""파란색""
```
```
Copy# EnumOutputParser 인스턴스 생성parser = EnumOutputParser(enum=Colors)
```
- 프롬프트에 사람의 정보({person})와 파싱 지침({instructions})을 포함시킵니다.
- parser.get_format_instructions()함수를 호출하여 파싱 지침을 가져옵니다.
- 프롬프트,ChatOpenAI모델, 파서를 연결하여 처리 체인을 구성합니다.
프롬프트에 사람의 정보({person})와 파싱 지침({instructions})을 포함시킵니다.
parser.get_format_instructions()함수를 호출하여 파싱 지침을 가져옵니다.
프롬프트,ChatOpenAI모델, 파서를 연결하여 처리 체인을 구성합니다.
```
Copyfromlangchain_core.promptsimportPromptTemplatefromlangchain_openaiimportChatOpenAI# 프롬프트 템플릿을 생성합니다.prompt = PromptTemplate.from_template(""""""다음의 물체는 어떤 색깔인가요?

Object: {object}

Instructions: {instructions}""""""# 파서에서 지시사항 형식을 가져와 부분적으로 적용합니다.).partial(instructions=parser.get_format_instructions())# 프롬프트와 ChatOpenAI, 파서를 연결합니다.chain = prompt | ChatOpenAI() | parser
```
- chain.invoke함수를 사용하여 ""하늘""에 대한 정보를 요청합니다.
chain.invoke함수를 사용하여 ""하늘""에 대한 정보를 요청합니다.
```
Copyresponse = chain.invoke({""object"":""하늘""})# ""하늘"" 에 대한 체인 호출 실행print(response)
```
```
Colors.BLUE
```
결과를 확인합니다."
08. 출력 수정 파서(OutputFixingParser),https://wikidocs.net/233793,"# 08. 출력 수정 파서(OutputFixingParser)

OutputFixingParser는 출력 파싱 과정에서 발생할 수 있는 오류를 자동으로 수정하는 기능을 제공합니다. 이 파서는 기본적으로 다른 파서, 예를 들어PydanticOutputParser를 래핑하고, 이 파서가 처리할 수 없는 형식의 출력이나 오류를 반환할 경우, 추가적인 LLM 호출을 통해 오류를 수정하도록 설계되었습니다.
이러한 접근 방식의 핵심은, 첫 번째 시도에서 스키마를 준수하지 않는 결과가 나올 경우,OutputFixingParser가 자동으로 형식이 잘못된 출력을 인식하고, 이를 수정하기 위한 새로운 명령어와 함께 모델에 다시 제출한다는 것입니다. 이 과정에서, 수정을 위한 명령어는 오류를 정확히 지적하고, 올바른 형식으로 데이터를 재구성할 수 있도록 구체적인 지시를 포함해야 합니다.
예를 들어,PydanticOutputParser를 사용하여 특정 데이터 스키마를 준수하는 출력을 생성하려고 했지만, 일부 필드가 누락되었거나 데이터 유형이 잘못된 경우가 발생할 수 있습니다. 이때OutputFixingParser는 다음 단계로, 해당 오류를 수정하는 지시를 포함한 새로운 요청을 LLM에 제출합니다. LLM은 이 지시를 바탕으로 오류를 수정한 새로운 출력을 생성하게 됩니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.output_parsersimportPydanticOutputParserfromlangchain_core.pydantic_v1importBaseModel, FieldfromtypingimportListclassActor(BaseModel):
    name:str= Field(description=""name of an actor"")
    film_names:List[str] = Field(
        description=""list of names of films they starred in"")


actor_query =""Generate the filmography for a random actor.""parser = PydanticOutputParser(pydantic_object=Actor)
```
```
Copy# 잘못된 형식을 일부러 입력misformatted =""{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}""# 잘못된 형식으로 입력된 데이터를 파싱하려고 시도parser.parse(misformatted)# 오류 출력
```
OutputFixingParser를 사용하여 잘못된 형식을 바로 잡도록 하겠습니다.
```
Copyfromlangchain.output_parsersimportOutputFixingParser

new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())
```
```
Copy# 잘못된 형식의 출력misformatted
```
```
""{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}""
```
```
Copy# OutputFixingParser 를 사용하여 잘못된 형식의 출력을 파싱actor = new_parser.parse(misformatted)
```
```
Copy# 파싱된 결과actor
```
```
Actor(name='Tom Hanks', film_names=['Forrest Gump'])
```"
CH04 모델(Model),https://wikidocs.net/233772,"# CH04 모델(Model)

모델 혹은 LLM(Large Language Model) 단계는이전 프롬프트 단계에서 구성된 입력을 기반으로 대규모 언어 모델을 활용하여 응답을 생성하는 과정입니다. 이 단계는 RAG 시스템의 핵심적인 부분으로,언어 모델의 능력을 최대한 활용하여 사용자의 질문에 대해 정확하고 자연스러운 답변을 생성합니다.
## LLM의 필요성
사용자 의도 이해: LLM은 다양한 언어의 구조와 의미를 깊이 이해하고 있으며, 이를 바탕으로 복잡한 질문에 답할 수 있습니다. 자연어 이해(NLU)와 자연어 생성(NLG) 능력이 결합되어, 보다 자연스럽고 유익한 응답을 제공할 수 있습니다.
사용자 의도 이해: LLM은 다양한 언어의 구조와 의미를 깊이 이해하고 있으며, 이를 바탕으로 복잡한 질문에 답할 수 있습니다. 자연어 이해(NLU)와 자연어 생성(NLG) 능력이 결합되어, 보다 자연스럽고 유익한 응답을 제공할 수 있습니다.
문맥적 적응성: LLM은 주어진문맥을 고려하여 응답을 생성합니다. 이는 사용자의 질문에 더욱 정확하게 대응할 수 있습니다. 또한, 사전학습된 지식외 사용자가 제공한 정보에 기반한 답변을 문맥을 참고하여 답변하도록 합니다.
문맥적 적응성: LLM은 주어진문맥을 고려하여 응답을 생성합니다. 이는 사용자의 질문에 더욱 정확하게 대응할 수 있습니다. 또한, 사전학습된 지식외 사용자가 제공한 정보에 기반한 답변을 문맥을 참고하여 답변하도록 합니다.
## LLM의 중요성
LLM 단계는 사용자의 질문에 대한 답변의 질과 자연스러움을 결정짓는 핵심 요소입니다. 이 단계에서 LLM은 지금까지의 모든 데이터와 정보를 종합하여 사용자의 질문에 최적화된 답변을 생성합니다.LLM의 성능은 RAG 시스템의 전체적인 성능과 사용자 만족도에 직접적으로 영향을 미치며, 이는 RAG 시스템을 사용하는 많은 응용 분야에서 매우 중요한 역할을 합니다.
## 코드
OpenAIGPT-4o활용
```
Copy# 단계 7: 언어모델(LLM) 생성# OpenAI 의 GPT-4o 모델 을 생성합니다.llm = ChatOpenAI(model_name=""gpt-4o"")
```
AnthropicClaude3 Sonnet활용
```
Copyfromlangchain_anthropicimportChatAnthropic# 단계 7: 언어모델(LLM) 생성# Anthropic 의 Claude 모델 을 생성합니다.llm = ChatAnthropic(model=""claude-3-sonnet-20240229"")
```
로컬 모델(llama3-8b) 활용
```
Copyfromlangchain_community.chat_modelsimportChatOllama# 단계 7: 언어모델(LLM) 생성# LangChain이 지원하는 Ollama(로컬) 모델을 사용합니다.llm = ChatOllama(model=""llama3:8b"")
```"
01. OpenAI GPT 모델,https://wikidocs.net/233795,"# 01. OpenAI GPT 모델

```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
## ChatOpenAI
OpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.
객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.
temperature
- 사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.
사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.
max_tokens
- 채팅 완성에서 생성할 토큰의 최대 개수입니다.
채팅 완성에서 생성할 토큰의 최대 개수입니다.
model_name: 적용 가능한 모델 리스트
- 링크: https://platform.openai.com/docs/models
링크: https://platform.openai.com/docs/models
```
Copyfromlangchain.chat_modelsimportChatOpenAI# 객체 생성llm = ChatOpenAI(
    temperature=0,# 창의성 (0.0 ~ 2.0)max_tokens=2048,# 최대 토큰수model_name=""gpt-3.5-turbo"",# 모델명)# 질의내용question =""대한민국의 수도는 뭐야?""# 질의print(f""[답변]:{llm.predict(question)}"")
```
```
[답변]: 대한민국의 수도는 서울입니다.
```
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain_anthropicimportChatAnthropic

chatgpt = ChatOpenAI()
anthropic = ChatAnthropic()
```
## 프롬프트 템플릿의 활용
PromptTemplate
- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다
- 사용법
- template: 템플릿 문자열입니다. 이 문자열 내에서 중괄호{}는 변수를 나타냅니다.
- input_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.
사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다
사용법
template: 템플릿 문자열입니다. 이 문자열 내에서 중괄호{}는 변수를 나타냅니다.
input_variables: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.
input_variables
- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.
- 사용법: 리스트 형식으로 변수 이름을 정의합니다.
input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.
사용법: 리스트 형식으로 변수 이름을 정의합니다.
```
Copyfromlangchain.promptsimportPromptTemplate# 질문 템플릿 형식 정의template =""{country}의 수도는 뭐야?""# 템플릿 완성prompt = PromptTemplate.from_template(template=template)
prompt
```
```
PromptTemplate(input_variables=['country'], template='{country}의 수도는 뭐야?')
```
LLMChain
- LLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다
- 사용법
- prompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.
- llm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.
LLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다
사용법
prompt: 앞서 정의한 PromptTemplate 객체를 사용합니다.
llm: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.
```
Copyfromlangchain.chainsimportLLMChain# 연결된 체인(Chain)객체 생성llm_chain = LLMChain(prompt=prompt, llm=llm)
```
run()함수로 템플릿 프롬프트 실행
```
Copy# 체인 실행: run()llm_chain.run(country=""일본"")
```
```
일본의 수도는 도쿄입니다.
```
```
Copy# 체인 실행: run()llm_chain.run(country=""캐나다"")
```
```
캐나다의 수도는 오타와(Ottawa)입니다.
```
predict()함수로run()함수를 대체할 수 있습니다.
```
Copyllm_chain.predict(country=""대한민국"")
```
```
대한민국의 수도는 서울입니다.
```
apply()함수로 여러개의 입력에 대한 처리를 한 번에 수행할 수 있습니다.
```
Copyinput_list = [{""country"":""호주""}, {""country"":""중국""}, {""country"":""네덜란드""}]

llm_chain.apply(input_list)
```
```
[{'text': '호주의 수도는 캔버라입니다.'}, {'text': '중국의 수도는 베이징(北京)입니다.'}, {'text': '네덜란드의 수도는 암스테르담(Amsterdam)입니다.'}]
```
text키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.
이를 반복문으로 출력한다면 다음과 같습니다.
```
Copy# input_list 에 대한 결과 반환result = llm_chain.apply(input_list)# 반복문으로 결과 출력forresinresult:print(res[""text""].strip())
```
```
호주의 수도는 캔버라입니다.
중국의 수도는 베이징(北京)입니다.
네덜란드의 수도는 암스테르담(Amsterdam)입니다.
```
generate()는 문자열 대신에 LLMResult를 반환하는 점을 제외하고는 apply와 유사합니다.
LLMResult는 토큰 사용량과 종료 이유와 같은 유용한 생성 정보를 자주 포함하고 있습니다.
```
Copy# 입력값input_list = [{""country"":""호주""}, {""country"":""중국""}, {""country"":""네덜란드""}]# input_list 에 대한 결과 반환generated_result = llm_chain.generate(input_list)print(generated_result)
```
```
generations=[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='호주의 수도는 캔버라입니다.'))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.'))], [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.'))]] llm_output={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 58, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('7127dd75-908c-4825-965f-0847e5fc4cba')), RunInfo(run_id=UUID('46f48115-2c43-4fc9-b020-d08ed599eed6')), RunInfo(run_id=UUID('630bebd8-9550-4bc1-89db-e832ff9a96cf'))]
```
```
Copy# 답변 출력generated_result.generations
```
```
[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='호주의 수도는 캔버라입니다.'))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.'))], [ChatGeneration(text='네덜란드의 수도는 암스테르담(Amsterdam)입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='네덜란드의 수도는 암스테르담(Amsterdam)입니다.'))]]
```
```
Copy# 토큰 사용량 출력generated_result.llm_output
```
```
{'token_usage': {'completion_tokens': 57,  'prompt_tokens': 58,  'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo'}
```
```
Copy# run ID 출력generated_result.run
```
```
[RunInfo(run_id=UUID('7127dd75-908c-4825-965f-0847e5fc4cba')), RunInfo(run_id=UUID('46f48115-2c43-4fc9-b020-d08ed599eed6')), RunInfo(run_id=UUID('630bebd8-9550-4bc1-89db-e832ff9a96cf'))]
```
```
Copy# 답변 출력forgeningenerated_result.generations:print(gen[0].text.strip())
```
```
호주의 수도는 캔버라입니다.
중국의 수도는 베이징(北京)입니다.
네덜란드의 수도는 암스테르담(Amsterdam)입니다.
```
2개 이상의 변수를 적용하여 템플릿을 생성할 수 있습니다.
이번에는 2개 이상의 변수(input_variables) 를 활용하여 템플릿 구성을 해보겠습니다.
```
Copy# 질문 템플릿 형식 정의template =""{area1} 와 {area2} 의 시차는 몇시간이야?""# 템플릿 완성prompt = PromptTemplate.from_template(template)
prompt
```
```
PromptTemplate(input_variables=['area1', 'area2'], template='{area1} 와 {area2} 의 시차는 몇시간이야?')
```
```
Copy# 연결된 체인(Chain)객체 생성llm_chain = LLMChain(prompt=prompt, llm=llm)
```
```
Copy# 체인 실행: run()print(llm_chain.run(area1=""서울"", area2=""파리""))
```
```
서울과 파리의 시차는 8시간입니다. 서울이 파리보다 8시간 앞서 있습니다.
```
```
Copyinput_list = [
    {""area1"":""파리"",""area2"":""뉴욕""},
    {""area1"":""서울"",""area2"":""하와이""},
    {""area1"":""켄버라"",""area2"":""베이징""},
]# 반복문으로 결과 출력result = llm_chain.apply(input_list)forresinresult:print(res[""text""].strip())
```
```
파리와 뉴욕의 시차는 일반적으로 6시간입니다. 파리가 뉴욕보다 6시간 앞서 있습니다. 예를 들어, 파리가 오전 9시라면 뉴욕은 오전 3시입니다.
서울과 하와이의 시차는 서울이 하와이보다 19시간 빠릅니다. 예를 들어, 서울이 오전 9시라면 하와이는 전날 오후 2시입니다.
켄버라와 베이징의 시차는 2시간입니다. 켄버라는 오스트레일리아의 수도로 UTC+10 시간대에 위치하고, 베이징은 중국의 수도로 UTC+8 시간대에 위치합니다.
```
## 스트리밍(streaming)
스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.
다음과 같이streaming=True로 설정하고 스트리밍으로 답변을 받기 위한StreamingStdOutCallbackHandler()을 콜백으로 지정합니다.
```
Copyfromlangchain.callbacks.streaming_stdoutimportStreamingStdOutCallbackHandler# 객체 생성llm = ChatOpenAI(
    temperature=0,# 창의성 (0.0 ~ 2.0)max_tokens=2048,# 최대 토큰수model_name=""gpt-3.5-turbo"",# 모델명streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)
```
```
Copy# 질의내용question =""대한민국에 대해서 300자 내외로 최대한 상세히 알려줘""# 스트리밍으로 답변 출력response = llm.predict(question)
```
```
대한민국은 동아시아에 위치한 나라로, 한반도의 남쪽에 위치하고 있다. 수도는 서울이며, 인구는 약 5,100만 명으로 세계에서 27번째로 인구가 많은 나라이다. 대한민국은 민주공화국으로, 대통령이 국가의 수장이며 국회에서 법률을 제정한다.

대한민국은 경제적으로 선진화된 나라로, 세계에서 12번째로 큰 경제를 가지고 있다. 주요 산업은 제조업, 서비스업, IT 등이다. 대한민국은 세계에서 가장 빠른 인터넷 속도를 가지고 있으며, IT 기술과 전자제품 생산 분야에서 세계적으로 유명하다.

문화적으로는 한국의 전통문화와 현대 문화가 조화롭게 공존한다. 한국은 한글을 사용하는 유일한 나라로, 한글은 세계에서 가장 과학적이고 효율적인 문자체계로 알려져 있다. 또한 한국은 한류 열풍으로 유명하며, 한국의 음식, 음악, 드라마 등이 전 세계에서 사랑받고 있다.

대한민국은 자연환경도 다양하다. 산과 강, 바다 등이 풍부하며, 자연경관이 아름답다. 한라산, 설악산 등의 산은 등산객들에게 인기가 많고, 제주도는 관광지로 유명하다. 또한 한반도는 사계절의 변화가 뚜렷하며, 봄에는 벚꽃, 가을에는 단풍이 아름답다.

대한민국은 국제적으로도 중요한 위치를 가지고 있다. 북한과의 관계와 분단 문제, 동북아시아의 안보 문제 등이 주요 이슈이다. 또한 대한민국은 세계적으로 인정받는 문화, 스포츠, 영화 등의 분야에서 활약하고 있다.

이렇듯 대한민국은 역사, 문화, 경제, 자연환경 등 다양한 면에서 매력적인 나라이다.
```"
02. 캐싱(Cache),https://wikidocs.net/233796,"# 02. 캐싱(Cache)

```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
## 캐싱(Caching)
LangChain은 LLM을 위한 선택적 캐싱 레이어를 제공합니다.
이는 두 가지 이유로 유용합니다:
- 동일한 완료를 여러 번 요청하는 경우 LLM 공급자에 대한API 호출 횟수를 줄여 비용을 절감할 수 있습니다.
- LLM 제공업체에 대한API 호출 횟수를 줄여 애플리케이션의 속도를 높일 수있습니다.
동일한 완료를 여러 번 요청하는 경우 LLM 공급자에 대한API 호출 횟수를 줄여 비용을 절감할 수 있습니다.
LLM 제공업체에 대한API 호출 횟수를 줄여 애플리케이션의 속도를 높일 수있습니다.
모델과 프롬프트를 생성합니다
```
Copyfromlangchain_openaiimportChatOpenAI# 모델을 생성합니다.llm = ChatOpenAI()
```
```
Copyfromlangchain.promptsimportPromptTemplate

prompt = PromptTemplate.from_template(""{country} 에 대해서 200자 내외로 요약해줘"")
prompt
```
```
PromptTemplate(input_variables=['country'], template='{country} 에 대해서 200자 내외로 요약해줘')
```
```
Copychain = prompt | llm
```
```
Copy%time response = chain.invoke({""country"":""한국""})print(response.content)
```
```
CPU times: user 23.3 ms, sys: 3.41 ms, total: 26.7 ms
Wall time: 6.25 s
한국은 동아시아에 위치한 대한민국과 조선민주주의인민공화국으로 이루어진 반도 국가이다. 서울을 수도로 하는 대한민국은 민주주의를 기반으로 한 고도의 경제 발전을 이룩해낸 선진국가이며 K-pop, K-drama 등의 문화 콘텐츠로 세계적으로 인기를 끌고 있다. 반면, 북한은 독재체제를 유지하며 국제사회와의 갈등을 겪고 있으며 핵무기 문제로 인해 국제사회의 관심을 받고 있다. 두 국가는 과거 한반도 분단으로 인해 한계가 있지만 평화와 번영을 위해 노력하고 있다. 한국은 독특한 문화와 역사, 풍부한 전통을 간직한 독특한 나라로서 전 세계적으로 주목받는 나라이다.
```
## SQLite Cache
```
Copyfromlangchain.cacheimportSQLiteCachefromlangchain.globalsimportset_llm_cache

set_llm_cache(SQLiteCache(database_path=""my_llm_cache.db""))
```
```
Copy%time response = chain.invoke({""country"":""한국""})print(response.content)
```
```
CPU times: user 7.73 ms, sys: 804 µs, total: 8.53 ms
Wall time: 8.01 ms
한국은 동아시아에 위치한 고대 역사와 현대 문화가 공존하는 나라이다. 1953년 한반도 전쟁 이후 분단된 한반도 남쪽에 위치하며, 수도는 서울이다. 한국은 삼국시대부터 고려, 조선, 일제 강점기, 한국전쟁을 거쳐 현재의 대한민국이 되었다. 한국은 K-pop, K-drama와 같은 문화적 산업뿐만 아니라 IT 기술과 자동차 산업에서도 세계적인 기업들을 배출하고 있다. 또한 한국은 불교, 기독교, 천주교 등 다양한 종교가 공존하며, 한국음식인 김치, 불고기 등도 세계적으로 유명하다. 현재 한반도에서는 남북한 간의 관계가 긴장되어 있지만, 평화롭게 공존하고 발전하기를 희망하는 노력을 계속하고 있다.
```"
03. 토큰 사용량 확인,https://wikidocs.net/233797,"# 03. 토큰 사용량 확인

```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
특정 호출에 대한 토큰 사용량을 추적하는 방법에 대해 설명합니다.
이 기능은 현재 OpenAI API 에만 구현되어 있습니다.
먼저 단일 Chat 모델 호출에 대한 토큰 사용량을 추적하는 매우 간단한 예를 살펴보겠습니다.
```
Copyfromlangchain.callbacksimportget_openai_callbackfromlangchain_openaiimportChatOpenAI
```
```
Copyllm = ChatOpenAI(model_name=""gpt-4"")
```
```
Copywithget_openai_callback()ascb:
    result = llm.invoke(""대한민국의 수도는 어디야?"")print(cb)
```
```
Tokens Used: 37
    Prompt Tokens: 22
    Completion Tokens: 15
Successful Requests: 1
Total Cost (USD): $0.0015599999999999998
```
```
Copywithget_openai_callback()ascb:
    result = llm.invoke(""대한민국의 수도는 어디야?"")
    result = llm.invoke(""대한민국의 수도는 어디야?"")print(f""총 사용된 토큰수: \t\t{cb.total_tokens}"")print(f""프롬프트에 사용된 토큰수: \t{cb.prompt_tokens}"")print(f""답변에 사용된 토큰수: \t{cb.completion_tokens}"")print(f""호출에 청구된 금액(USD): \t${cb.total_cost}"")
```
```
총 사용된 토큰수:      74
프롬프트에 사용된 토큰수:  44
답변에 사용된 토큰수:    30
호출에 청구된 금액(USD):    $0.0031199999999999995
```"
04. 모델 직렬화(Serialization) - 저장 및 불러오기,https://wikidocs.net/233798,"# 04. 모델 직렬화(Serialization) - 저장 및 불러오기

```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
LangChain은 직렬화(Serialization) 체계를 공유합니다.
is_lc_serializable클래스 메서드로 실행하여 LangChain 클래스가 직렬화 가능한지 확인할 수 있습니다.
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.llms.loadingimportload_llm
```
```
Copy# 직렬화가 가능한지 체크합니다.print(f""ChatOpenAI:{ChatOpenAI.is_lc_serializable()}"")
```
```
ChatOpenAI: True
```
```
Copyllm = ChatOpenAI(model=""gpt-3.5-turbo"", temperature=0)
```
```
Copyfromlangchain.promptsimportPromptTemplate

prompt = PromptTemplate.from_template(""{fruit}의 색상이 무엇입니까?"")
prompt
```
```
PromptTemplate(input_variables=['fruit'], template='{fruit}의 색상이 무엇입니까?')
```
```
Copychain = prompt | llm
```
```
Copychain.is_lc_serializable()
```
```
True
```
## dumpd: 모델 저장
직렬화 가능한 모든 객체는 딕셔너리 또는 json 문자열로 직렬화할 수 있습니다.
```
Copyfromlangchain.loadimportdumpd

dumped_chain = dumpd(chain)
dumped_chain
```
```
{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1,   'type': 'constructor',   'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],   'kwargs': {'input_variables': ['fruit'],    'template': '{fruit}의 색상이 무엇입니까?',    'template_format': 'f-string',    'partial_variables': {}},   'name': 'PromptTemplate',   'graph': {'nodes': [{'id': 0,      'type': 'schema',      'data': {'title': 'PromptInput',       'type': 'object',       'properties': {'fruit': {'title': 'Fruit', 'type': 'string'}}}},     {'id': 1,      'type': 'runnable',      'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],       'name': 'PromptTemplate'}},     {'id': 2,      'type': 'schema',      'data': {'title': 'PromptTemplateOutput',       'anyOf': [{'$ref': '#/definitions/StringPromptValue'},        {'$ref': '#/definitions/ChatPromptValueConcrete'}],       'definitions': {'StringPromptValue': {'title': 'StringPromptValue',         'description': 'String prompt value.',         'type': 'object',         'properties': {'text': {'title': 'Text', 'type': 'string'},          'type': {'title': 'Type',           'default': 'StringPromptValue',           'enum': ['StringPromptValue'],           'type': 'string'}},         'required': ['text']},        'AIMessage': {'title': 'AIMessage',         'description': 'Message from an AI.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'ai',           'enum': ['ai'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'HumanMessage': {'title': 'HumanMessage',         'description': 'Message from a human.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'human',           'enum': ['human'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'ChatMessage': {'title': 'ChatMessage',         'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'chat',           'enum': ['chat'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'role': {'title': 'Role', 'type': 'string'}},         'required': ['content', 'role']},        'SystemMessage': {'title': 'SystemMessage',         'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'system',           'enum': ['system'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content']},        'FunctionMessage': {'title': 'FunctionMessage',         'description': 'Message for passing the result of executing a function back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'function',           'enum': ['function'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content', 'name']},        'ToolMessage': {'title': 'ToolMessage',         'description': 'Message for passing the result of executing a tool back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'tool',           'enum': ['tool'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},         'required': ['content', 'tool_call_id']},        'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',         'description': 'Chat prompt value which explicitly lists out the message types it accepts.\nFor use in external schemas.',         'type': 'object',         'properties': {'messages': {'title': 'Messages',           'type': 'array',           'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},             {'$ref': '#/definitions/HumanMessage'},             {'$ref': '#/definitions/ChatMessage'},             {'$ref': '#/definitions/SystemMessage'},             {'$ref': '#/definitions/FunctionMessage'},             {'$ref': '#/definitions/ToolMessage'}]}},          'type': {'title': 'Type',           'default': 'ChatPromptValueConcrete',           'enum': ['ChatPromptValueConcrete'],           'type': 'string'}},         'required': ['messages']}}}}],    'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}},  'middle': [],  'last': {'lc': 1,   'type': 'constructor',   'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],   'kwargs': {'model': 'gpt-3.5-turbo',    'temperature': 0.0,    'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}},   'name': 'ChatOpenAI',   'graph': {'nodes': [{'id': 0,      'type': 'schema',      'data': {'title': 'ChatOpenAIInput',       'anyOf': [{'type': 'string'},        {'$ref': '#/definitions/StringPromptValue'},        {'$ref': '#/definitions/ChatPromptValueConcrete'},        {'type': 'array',         'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},           {'$ref': '#/definitions/HumanMessage'},           {'$ref': '#/definitions/ChatMessage'},           {'$ref': '#/definitions/SystemMessage'},           {'$ref': '#/definitions/FunctionMessage'},           {'$ref': '#/definitions/ToolMessage'}]}}],       'definitions': {'StringPromptValue': {'title': 'StringPromptValue',         'description': 'String prompt value.',         'type': 'object',         'properties': {'text': {'title': 'Text', 'type': 'string'},          'type': {'title': 'Type',           'default': 'StringPromptValue',           'enum': ['StringPromptValue'],           'type': 'string'}},         'required': ['text']},        'AIMessage': {'title': 'AIMessage',         'description': 'Message from an AI.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'ai',           'enum': ['ai'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'HumanMessage': {'title': 'HumanMessage',         'description': 'Message from a human.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'human',           'enum': ['human'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'ChatMessage': {'title': 'ChatMessage',         'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'chat',           'enum': ['chat'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'role': {'title': 'Role', 'type': 'string'}},         'required': ['content', 'role']},        'SystemMessage': {'title': 'SystemMessage',         'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'system',           'enum': ['system'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content']},        'FunctionMessage': {'title': 'FunctionMessage',         'description': 'Message for passing the result of executing a function back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'function',           'enum': ['function'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content', 'name']},        'ToolMessage': {'title': 'ToolMessage',         'description': 'Message for passing the result of executing a tool back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'tool',           'enum': ['tool'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},         'required': ['content', 'tool_call_id']},        'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',         'description': 'Chat prompt value which explicitly lists out the message types it accepts.\nFor use in external schemas.',         'type': 'object',         'properties': {'messages': {'title': 'Messages',           'type': 'array',           'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},             {'$ref': '#/definitions/HumanMessage'},             {'$ref': '#/definitions/ChatMessage'},             {'$ref': '#/definitions/SystemMessage'},             {'$ref': '#/definitions/FunctionMessage'},             {'$ref': '#/definitions/ToolMessage'}]}},          'type': {'title': 'Type',           'default': 'ChatPromptValueConcrete',           'enum': ['ChatPromptValueConcrete'],           'type': 'string'}},         'required': ['messages']}}}},     {'id': 1,      'type': 'runnable',      'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],       'name': 'ChatOpenAI'}},     {'id': 2,      'type': 'schema',      'data': {'title': 'ChatOpenAIOutput',       'anyOf': [{'$ref': '#/definitions/AIMessage'},        {'$ref': '#/definitions/HumanMessage'},        {'$ref': '#/definitions/ChatMessage'},        {'$ref': '#/definitions/SystemMessage'},        {'$ref': '#/definitions/FunctionMessage'},        {'$ref': '#/definitions/ToolMessage'}],       'definitions': {'AIMessage': {'title': 'AIMessage',         'description': 'Message from an AI.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'ai',           'enum': ['ai'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'HumanMessage': {'title': 'HumanMessage',         'description': 'Message from a human.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'human',           'enum': ['human'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'ChatMessage': {'title': 'ChatMessage',         'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'chat',           'enum': ['chat'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'role': {'title': 'Role', 'type': 'string'}},         'required': ['content', 'role']},        'SystemMessage': {'title': 'SystemMessage',         'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'system',           'enum': ['system'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content']},        'FunctionMessage': {'title': 'FunctionMessage',         'description': 'Message for passing the result of executing a function back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'function',           'enum': ['function'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content', 'name']},        'ToolMessage': {'title': 'ToolMessage',         'description': 'Message for passing the result of executing a tool back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'tool',           'enum': ['tool'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},         'required': ['content', 'tool_call_id']}}}}],    'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}},  'name': None}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0,    'type': 'schema',    'data': {'title': 'PromptInput',     'type': 'object',     'properties': {'fruit': {'title': 'Fruit', 'type': 'string'}}}},   {'id': 1,    'type': 'runnable',    'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],     'name': 'PromptTemplate'}},   {'id': 2,    'type': 'runnable',    'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],     'name': 'ChatOpenAI'}},   {'id': 3,    'type': 'schema',    'data': {'title': 'ChatOpenAIOutput',     'anyOf': [{'$ref': '#/definitions/AIMessage'},      {'$ref': '#/definitions/HumanMessage'},      {'$ref': '#/definitions/ChatMessage'},      {'$ref': '#/definitions/SystemMessage'},      {'$ref': '#/definitions/FunctionMessage'},      {'$ref': '#/definitions/ToolMessage'}],     'definitions': {'AIMessage': {'title': 'AIMessage',       'description': 'Message from an AI.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'ai',         'enum': ['ai'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},       'required': ['content']},      'HumanMessage': {'title': 'HumanMessage',       'description': 'Message from a human.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'human',         'enum': ['human'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},       'required': ['content']},      'ChatMessage': {'title': 'ChatMessage',       'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'chat',         'enum': ['chat'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'role': {'title': 'Role', 'type': 'string'}},       'required': ['content', 'role']},      'SystemMessage': {'title': 'SystemMessage',       'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'system',         'enum': ['system'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'}},       'required': ['content']},      'FunctionMessage': {'title': 'FunctionMessage',       'description': 'Message for passing the result of executing a function back to a model.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'function',         'enum': ['function'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'}},       'required': ['content', 'name']},      'ToolMessage': {'title': 'ToolMessage',       'description': 'Message for passing the result of executing a tool back to a model.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'tool',         'enum': ['tool'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},       'required': ['content', 'tool_call_id']}}}}],  'edges': [{'source': 0, 'target': 1},   {'source': 2, 'target': 3},   {'source': 1, 'target': 2}]}}
```
```
Copyimportpickle# fuit_chain.pkl 파일로 직렬화된 체인을 저장합니다.withopen(""fruit_chain.pkl"",""wb"")asf:
    pickle.dump(dumped_chain, f)
```
## load: 저장한 모델 불러오기
```
Copyimportpicklewithopen(""fruit_chain.pkl"",""rb"")asf:
    loaded_chain = pickle.load(f)

loaded_chain
```
```
{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1,   'type': 'constructor',   'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],   'kwargs': {'input_variables': ['fruit'],    'template': '{fruit}의 색상이 무엇입니까?',    'template_format': 'f-string',    'partial_variables': {}},   'name': 'PromptTemplate',   'graph': {'nodes': [{'id': 0,      'type': 'schema',      'data': {'title': 'PromptInput',       'type': 'object',       'properties': {'fruit': {'title': 'Fruit', 'type': 'string'}}}},     {'id': 1,      'type': 'runnable',      'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],       'name': 'PromptTemplate'}},     {'id': 2,      'type': 'schema',      'data': {'title': 'PromptTemplateOutput',       'anyOf': [{'$ref': '#/definitions/StringPromptValue'},        {'$ref': '#/definitions/ChatPromptValueConcrete'}],       'definitions': {'StringPromptValue': {'title': 'StringPromptValue',         'description': 'String prompt value.',         'type': 'object',         'properties': {'text': {'title': 'Text', 'type': 'string'},          'type': {'title': 'Type',           'default': 'StringPromptValue',           'enum': ['StringPromptValue'],           'type': 'string'}},         'required': ['text']},        'AIMessage': {'title': 'AIMessage',         'description': 'Message from an AI.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'ai',           'enum': ['ai'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'HumanMessage': {'title': 'HumanMessage',         'description': 'Message from a human.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'human',           'enum': ['human'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'ChatMessage': {'title': 'ChatMessage',         'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'chat',           'enum': ['chat'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'role': {'title': 'Role', 'type': 'string'}},         'required': ['content', 'role']},        'SystemMessage': {'title': 'SystemMessage',         'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'system',           'enum': ['system'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content']},        'FunctionMessage': {'title': 'FunctionMessage',         'description': 'Message for passing the result of executing a function back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'function',           'enum': ['function'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content', 'name']},        'ToolMessage': {'title': 'ToolMessage',         'description': 'Message for passing the result of executing a tool back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'tool',           'enum': ['tool'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},         'required': ['content', 'tool_call_id']},        'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',         'description': 'Chat prompt value which explicitly lists out the message types it accepts.\nFor use in external schemas.',         'type': 'object',         'properties': {'messages': {'title': 'Messages',           'type': 'array',           'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},             {'$ref': '#/definitions/HumanMessage'},             {'$ref': '#/definitions/ChatMessage'},             {'$ref': '#/definitions/SystemMessage'},             {'$ref': '#/definitions/FunctionMessage'},             {'$ref': '#/definitions/ToolMessage'}]}},          'type': {'title': 'Type',           'default': 'ChatPromptValueConcrete',           'enum': ['ChatPromptValueConcrete'],           'type': 'string'}},         'required': ['messages']}}}}],    'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}},  'middle': [],  'last': {'lc': 1,   'type': 'constructor',   'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],   'kwargs': {'model': 'gpt-3.5-turbo',    'temperature': 0.0,    'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}},   'name': 'ChatOpenAI',   'graph': {'nodes': [{'id': 0,      'type': 'schema',      'data': {'title': 'ChatOpenAIInput',       'anyOf': [{'type': 'string'},        {'$ref': '#/definitions/StringPromptValue'},        {'$ref': '#/definitions/ChatPromptValueConcrete'},        {'type': 'array',         'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},           {'$ref': '#/definitions/HumanMessage'},           {'$ref': '#/definitions/ChatMessage'},           {'$ref': '#/definitions/SystemMessage'},           {'$ref': '#/definitions/FunctionMessage'},           {'$ref': '#/definitions/ToolMessage'}]}}],       'definitions': {'StringPromptValue': {'title': 'StringPromptValue',         'description': 'String prompt value.',         'type': 'object',         'properties': {'text': {'title': 'Text', 'type': 'string'},          'type': {'title': 'Type',           'default': 'StringPromptValue',           'enum': ['StringPromptValue'],           'type': 'string'}},         'required': ['text']},        'AIMessage': {'title': 'AIMessage',         'description': 'Message from an AI.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'ai',           'enum': ['ai'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'HumanMessage': {'title': 'HumanMessage',         'description': 'Message from a human.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'human',           'enum': ['human'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'ChatMessage': {'title': 'ChatMessage',         'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'chat',           'enum': ['chat'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'role': {'title': 'Role', 'type': 'string'}},         'required': ['content', 'role']},        'SystemMessage': {'title': 'SystemMessage',         'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'system',           'enum': ['system'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content']},        'FunctionMessage': {'title': 'FunctionMessage',         'description': 'Message for passing the result of executing a function back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'function',           'enum': ['function'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content', 'name']},        'ToolMessage': {'title': 'ToolMessage',         'description': 'Message for passing the result of executing a tool back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'tool',           'enum': ['tool'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},         'required': ['content', 'tool_call_id']},        'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',         'description': 'Chat prompt value which explicitly lists out the message types it accepts.\nFor use in external schemas.',         'type': 'object',         'properties': {'messages': {'title': 'Messages',           'type': 'array',           'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},             {'$ref': '#/definitions/HumanMessage'},             {'$ref': '#/definitions/ChatMessage'},             {'$ref': '#/definitions/SystemMessage'},             {'$ref': '#/definitions/FunctionMessage'},             {'$ref': '#/definitions/ToolMessage'}]}},          'type': {'title': 'Type',           'default': 'ChatPromptValueConcrete',           'enum': ['ChatPromptValueConcrete'],           'type': 'string'}},         'required': ['messages']}}}},     {'id': 1,      'type': 'runnable',      'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],       'name': 'ChatOpenAI'}},     {'id': 2,      'type': 'schema',      'data': {'title': 'ChatOpenAIOutput',       'anyOf': [{'$ref': '#/definitions/AIMessage'},        {'$ref': '#/definitions/HumanMessage'},        {'$ref': '#/definitions/ChatMessage'},        {'$ref': '#/definitions/SystemMessage'},        {'$ref': '#/definitions/FunctionMessage'},        {'$ref': '#/definitions/ToolMessage'}],       'definitions': {'AIMessage': {'title': 'AIMessage',         'description': 'Message from an AI.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'ai',           'enum': ['ai'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'HumanMessage': {'title': 'HumanMessage',         'description': 'Message from a human.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'human',           'enum': ['human'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'example': {'title': 'Example',           'default': False,           'type': 'boolean'}},         'required': ['content']},        'ChatMessage': {'title': 'ChatMessage',         'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'chat',           'enum': ['chat'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'role': {'title': 'Role', 'type': 'string'}},         'required': ['content', 'role']},        'SystemMessage': {'title': 'SystemMessage',         'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'system',           'enum': ['system'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content']},        'FunctionMessage': {'title': 'FunctionMessage',         'description': 'Message for passing the result of executing a function back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'function',           'enum': ['function'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'}},         'required': ['content', 'name']},        'ToolMessage': {'title': 'ToolMessage',         'description': 'Message for passing the result of executing a tool back to a model.',         'type': 'object',         'properties': {'content': {'title': 'Content',           'anyOf': [{'type': 'string'},            {'type': 'array',             'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},          'additional_kwargs': {'title': 'Additional Kwargs',           'type': 'object'},          'type': {'title': 'Type',           'default': 'tool',           'enum': ['tool'],           'type': 'string'},          'name': {'title': 'Name', 'type': 'string'},          'id': {'title': 'Id', 'type': 'string'},          'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},         'required': ['content', 'tool_call_id']}}}}],    'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}},  'name': None}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0,    'type': 'schema',    'data': {'title': 'PromptInput',     'type': 'object',     'properties': {'fruit': {'title': 'Fruit', 'type': 'string'}}}},   {'id': 1,    'type': 'runnable',    'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],     'name': 'PromptTemplate'}},   {'id': 2,    'type': 'runnable',    'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],     'name': 'ChatOpenAI'}},   {'id': 3,    'type': 'schema',    'data': {'title': 'ChatOpenAIOutput',     'anyOf': [{'$ref': '#/definitions/AIMessage'},      {'$ref': '#/definitions/HumanMessage'},      {'$ref': '#/definitions/ChatMessage'},      {'$ref': '#/definitions/SystemMessage'},      {'$ref': '#/definitions/FunctionMessage'},      {'$ref': '#/definitions/ToolMessage'}],     'definitions': {'AIMessage': {'title': 'AIMessage',       'description': 'Message from an AI.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'ai',         'enum': ['ai'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},       'required': ['content']},      'HumanMessage': {'title': 'HumanMessage',       'description': 'Message from a human.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'human',         'enum': ['human'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},       'required': ['content']},      'ChatMessage': {'title': 'ChatMessage',       'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'chat',         'enum': ['chat'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'role': {'title': 'Role', 'type': 'string'}},       'required': ['content', 'role']},      'SystemMessage': {'title': 'SystemMessage',       'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\nof input messages.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'system',         'enum': ['system'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'}},       'required': ['content']},      'FunctionMessage': {'title': 'FunctionMessage',       'description': 'Message for passing the result of executing a function back to a model.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'function',         'enum': ['function'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'}},       'required': ['content', 'name']},      'ToolMessage': {'title': 'ToolMessage',       'description': 'Message for passing the result of executing a tool back to a model.',       'type': 'object',       'properties': {'content': {'title': 'Content',         'anyOf': [{'type': 'string'},          {'type': 'array',           'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},        'type': {'title': 'Type',         'default': 'tool',         'enum': ['tool'],         'type': 'string'},        'name': {'title': 'Name', 'type': 'string'},        'id': {'title': 'Id', 'type': 'string'},        'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},       'required': ['content', 'tool_call_id']}}}}],  'edges': [{'source': 0, 'target': 1},   {'source': 2, 'target': 3},   {'source': 1, 'target': 2}]}}
```
```
Copyfromlangchain.load.loadimportload


loaded_chain = load(loaded_chain)
```
```
Copyprint(loaded_chain.invoke({""fruit"":""사과""}))
```
```
content='사과의 색상은 주로 빨간색이지만, 녹색, 노란색, 주황색 등 다양한 색상의 사과도 있습니다.'
```"
05. 구글 생성 AI(Google Generative AI),https://wikidocs.net/233799,"# 05. 구글 생성 AI(Google Generative AI)

Google AI의gemini와gemini-vision모델뿐만 아니라 다른 생성 모델에 접근하려면langchain-google-genai통합 패키지의ChatGoogleGenerativeAI클래스를 사용하면 됩니다.
```
Copy%pip install --upgrade --quiet  langchain-google-genai pillow
```
- 링크에서 API KEY를 발급받아주세요.
- 사용자의 Google API 키를 환경 변수GOOGLE_API_KEY로 설정합니다.
링크에서 API KEY를 발급받아주세요.
사용자의 Google API 키를 환경 변수GOOGLE_API_KEY로 설정합니다.
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
langchain_google_genai 패키지에서 ChatGoogleGenerativeAI 클래스를 가져옵니다.
- ChatGoogleGenerativeAI 클래스는 Google의 Generative AI 모델을 사용하여 대화형 AI 시스템을 구현하는 데 사용됩니다.
- 이 클래스를 통해 사용자는 Google의 대화형 AI 모델과 상호 작용할 수 있습니다.
- 모델과의 대화는 채팅 형식으로 이루어지며, 사용자의 입력에 따라 모델이 적절한 응답을 생성합니다.
- ChatGoogleGenerativeAI 클래스는 LangChain 프레임워크와 통합되어 있어, 다른 LangChain 컴포넌트와 함께 사용할 수 있습니다.
ChatGoogleGenerativeAI 클래스는 Google의 Generative AI 모델을 사용하여 대화형 AI 시스템을 구현하는 데 사용됩니다.
이 클래스를 통해 사용자는 Google의 대화형 AI 모델과 상호 작용할 수 있습니다.
모델과의 대화는 채팅 형식으로 이루어지며, 사용자의 입력에 따라 모델이 적절한 응답을 생성합니다.
ChatGoogleGenerativeAI 클래스는 LangChain 프레임워크와 통합되어 있어, 다른 LangChain 컴포넌트와 함께 사용할 수 있습니다.
```
Copyfromlangchain_google_genaiimportChatGoogleGenerativeAI
```
- ChatGoogleGenerativeAI클래스를 사용하여 ""gemini-pro"" 모델을 기반으로 언어 모델(llm)을 초기화합니다.
- invoke메서드를 호출하여 ""Write a ballad about LangChain""이라는 프롬프트에 대한 응답을 생성합니다.
- 생성된 응답의 내용을result.content를 통해 출력합니다.
ChatGoogleGenerativeAI클래스를 사용하여 ""gemini-pro"" 모델을 기반으로 언어 모델(llm)을 초기화합니다.
invoke메서드를 호출하여 ""Write a ballad about LangChain""이라는 프롬프트에 대한 응답을 생성합니다.
생성된 응답의 내용을result.content를 통해 출력합니다.
```
Copy# ChatGoogleGenerativeAI 언어 모델을 ""gemini-pro"" 모델로 초기화합니다.llm = ChatGoogleGenerativeAI(model=""gemini-pro"")# 언어 모델에 ""자연어처리에 대해서 간략히 설명해 줘""는 프롬프트를 전달하여 결과를 생성합니다.result = llm.invoke(""자연어처리에 대해서 간략히 설명해 줘"")print(result.content)# 생성된 결과의 내용을 출력합니다.
```
```
자연어처리(Natural Language Processing, NLP)는 컴퓨터가 인간의 자연어를 이해하고 처리할 수 있도록 하는 인공지능(AI)의 한 분야입니다.

NLP의 목표는 다음과 같습니다.

* **텍스트 이해:** 텍스트의 의미, 감정, 주제를 파악합니다.
* **텍스트 생성:** 사람처럼 보이는 텍스트를 생성합니다.
* **텍스트 분석:** 텍스트에서 패턴, 트렌드, 통찰력을 찾습니다.

NLP의 주요 응용 분야는 다음과 같습니다.

* **자연어 검색:** 검색 엔진에서 의미적 검색을 수행합니다.
* **머신 번역:** 언어 간에 텍스트를 번역합니다.
* **챗봇:** 인간과 자연어로 대화하는 가상 조수를 만듭니다.
* **감성 분석:** 텍스트의 감정을 식별합니다.
* **문서 요약:** 긴 문서에서 핵심 요점을 추출합니다.

NLP는 텍스트 기반 데이터를 이해하고 활용하여 인간과 컴퓨터 간의 상호 작용을 개선하고, 비즈니스 인사이트를 제공하며, 의사 결정을 지원하는 데 사용됩니다.
```
Gemini는 현재SystemMessage를 지원하지 않습니다.
그러나convert_system_message_to_human을 True로 설정하면, 첫 번째 human message에SystemMessage를 추가할 수 있습니다.
이러한 동작을 원한다면,convert_system_message_to_human을 True로 설정하면 됩니다.
- ChatGoogleGenerativeAI모델을 ""gemini-pro""로 초기화하고,convert_system_message_to_human옵션을 True로 설정합니다.
- SystemMessage를 사용하여 모델에게 ""yes 또는 no로만 대답하라""는 지시를 전달합니다.
- HumanMessage를 사용하여 모델에게 ""사과는 과일인가요?""라는 질문을 전달합니다.
- 모델은 주어진 시스템 메시지와 사용자 메시지를 기반으로 질문에 대한 간단한 yes/no 답변을 생성할 것입니다.
ChatGoogleGenerativeAI모델을 ""gemini-pro""로 초기화하고,convert_system_message_to_human옵션을 True로 설정합니다.
SystemMessage를 사용하여 모델에게 ""yes 또는 no로만 대답하라""는 지시를 전달합니다.
HumanMessage를 사용하여 모델에게 ""사과는 과일인가요?""라는 질문을 전달합니다.
모델은 주어진 시스템 메시지와 사용자 메시지를 기반으로 질문에 대한 간단한 yes/no 답변을 생성할 것입니다.
```
Copyfromlangchain_core.messagesimportHumanMessage, SystemMessage

model = ChatGoogleGenerativeAI(
    model=""gemini-pro"",# 사용할 모델을 지정합니다.convert_system_message_to_human=True,# 시스템 메시지를 사람 메시지로 변환할지 여부를 설정합니다.)

model.invoke(
    [# 시스템 메시지로 ""yes 또는 no로만 대답하세요.""라는 내용을 전달합니다.SystemMessage(content=""Answer only yes or no.""),# 사람 메시지로 ""사과는 과일인가요?""라는 질문을 전달합니다.HumanMessage(content=""Is apple a fruit?""),
    ]
)
```
```
AIMessage(content='Yes')
```
## Streaming and Batching
ChatGoogleGenerativeAI는 기본적으로 스트리밍과 배치 처리를 지원합니다.
아래는 그 예시입니다.
- llm객체의stream메서드를 사용하여 ""Google 의 기업 역사에 대해서 markdown 형식으로 작성해 줘"" 라는 프롬프트에 대한 응답을 생성합니다.
- 생성된 응답은 청크(chunk) 단위로 반환되며, 각 청크는 하나 이상의 토큰을 포함할 수 있습니다.
- 반환된 각 청크의 내용(content)을 출력합니다.
llm객체의stream메서드를 사용하여 ""Google 의 기업 역사에 대해서 markdown 형식으로 작성해 줘"" 라는 프롬프트에 대한 응답을 생성합니다.
생성된 응답은 청크(chunk) 단위로 반환되며, 각 청크는 하나 이상의 토큰을 포함할 수 있습니다.
반환된 각 청크의 내용(content)을 출력합니다.
```
Copy# LLM을 사용하여 ""LLM에 대한 limerick을 작성하라""는 프롬프트를 전달하고, 생성된 텍스트를 청크 단위로 스트리밍합니다.forchunkinllm.stream(""Google 의 기업 역사에 대해서 markdown 형식으로 작성해 줘""):print(chunk.content, end="""", flush=True)# 각 청크의 내용을 출력합니다.# 각 청크는 하나 이상의 ""토큰""을 포함할 수 있습니다.
```
```
## Google의 기업 역사

### 초기 역사 (1996-2004)

* **1996년:** 스탠포드 대학 박사 과정생인 래리 페이지와 세르게이 브린이 ""백링크 분석"" 알고리즘을 개발하여 웹 페이지의 관련성을 평가함.
* **1997년:** ""BackRub""이라는 검색 엔진을 출시함.
* **1998년:** 회사명을 ""Google""로 변경함.
* **2000년:** 주식 공개(IPO)를 통해 16억 달러를 조달함.
* **2001년:** Gmail, AdWords, Google News를 출시함.
* **2003년:** AdSense, Google Maps, Google Earth를 출시함.

### 성장과 확장 (2005-2014)

* **2005년:** YouTube를 인수함.
* **2006년:** Google Apps를 출시하여 비즈니스 및 교육용 소프트웨어를 제공함.
* **2007년:** Android 모바일 운영 체제를 출시함.
* **2008년:** Google Chrome 브라우저를 출시함.
* **2011년:** Google+ 소셜 네트워크를 출시함.
* **2012년:** Google 드라이브 클라우드 스토리지 서비스를 출시함.

### 재편성과 혁신 (2015-현재)

* **2015년:** 알파벳(Alphabet)이라는 지주 회사를 설립하여 Google을 자회사로 편입함.
* **2016년:** Waymo 자율 주행 차량 회사를 설립함.
* **2017년:** Google Home 스마트 스피커를 출시함.
* **2018년:** Google Pixel 스마트폰을 출시함.
* **2020년:** Google Meet 비디오 회의 플랫폼을 출시함.
* **2021년:** Google Stadia 클라우드 게임 서비스를 출시함.

### 주요 인수 및 파트너십

Google은 성장 과정에서 여러 주요 인수와 파트너십을 체결하였습니다.

* **인수:** YouTube, Android, Waze, Fitbit
* **파트너십:** Facebook, Twitter, Microsoft

### 중요한 이정표

* 2000년: 10억 번째 웹 페이지 색인화
* 2004년: 1조 번째 검색 수행
* 2008년: Android 출시
* 2010년: 1조 번째 YouTube 조회수
* 2015년: Alphabet 설립
* 2019년: 1조 번째 Google 어시스턴트 요청

### 현재 상태

오늘날 Google은 세계에서 가장 큰 검색 엔진 및 기술 회사 중 하나입니다. 검색, 클라우드 컴퓨팅, 소프트웨어, 하드웨어, AI 등 다양한 제품과 서비스를 제공합니다. Google은 또한 자율 주행 차량, 의학 연구, 양자 컴퓨팅 등 첨단 기술 분야에 투자하고 있습니다.
```
## Gemini Prompting FAQs
이 문서가 작성된 시점(2023/12/12)을 기준으로, Gemini는 수용하는 프롬프트의 유형과 구조에 몇 가지 제한 사항이 있습니다.
구체적으로:
멀티모달(이미지) 입력을 제공할 때, ""human""(사용자) 유형의 메시지는 최대 1개로 제한됩니다. 여러 개의 메시지를 전달할 수 없습니다(단, 단일 human 메시지에는 여러 개의 콘텐츠 항목이 포함될 수 있음).
멀티모달(이미지) 입력을 제공할 때, ""human""(사용자) 유형의 메시지는 최대 1개로 제한됩니다. 여러 개의 메시지를 전달할 수 없습니다(단, 단일 human 메시지에는 여러 개의 콘텐츠 항목이 포함될 수 있음).
System 메시지는 허용되지 않습니다.
System 메시지는 허용되지 않습니다.
일반적인 채팅 대화의 경우, 메시지는 human/ai/human/ai의 교차 패턴을 따라야 합니다. 2개의 AI 또는 human 메시지를 연속으로 제공할 수 없습니다.
일반적인 채팅 대화의 경우, 메시지는 human/ai/human/ai의 교차 패턴을 따라야 합니다. 2개의 AI 또는 human 메시지를 연속으로 제공할 수 없습니다.
LLM의 안전성 검사를 위반하는 메시지는 차단될 수 있습니다. 이 경우, 모델은 빈 응답을 반환합니다.
LLM의 안전성 검사를 위반하는 메시지는 차단될 수 있습니다. 이 경우, 모델은 빈 응답을 반환합니다.
Gemini 모델에는 기본 안전 설정이 있지만, 이를 재정의할 수 있습니다.
만약 모델로부터 많은 ""Safety Warnings""를 받고 있다면, 모델의safety_settings속성을 조정해 볼 수 있습니다.
예를 들어, 위험한 내용에 대한 안전 차단을 해제하려면 다음과 같이 LLM을 구성할 수 있습니다:
langchain_google_genai 라이브러리를 사용하여 Google Generative AI 모델인 ""gemini-pro""를 초기화합니다.
- ChatGoogleGenerativeAI클래스를 사용하여llm객체를 생성합니다.
- model매개변수를 통해 사용할 모델을 지정합니다.
- safety_settings매개변수를 통해 유해 콘텐츠 관련 설정을 조정합니다.
- HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT카테고리에 대해HarmBlockThreshold.BLOCK_NONE값을 설정하여 위험한 콘텐츠에 대한 차단을 비활성화합니다.
ChatGoogleGenerativeAI클래스를 사용하여llm객체를 생성합니다.
model매개변수를 통해 사용할 모델을 지정합니다.
safety_settings매개변수를 통해 유해 콘텐츠 관련 설정을 조정합니다.
HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT카테고리에 대해HarmBlockThreshold.BLOCK_NONE값을 설정하여 위험한 콘텐츠에 대한 차단을 비활성화합니다.
```
Copyfromlangchain_google_genaiimport(
    ChatGoogleGenerativeAI,
    HarmBlockThreshold,
    HarmCategory,
)

llm = ChatGoogleGenerativeAI(# 사용할 모델을 ""gemini-pro""로 지정합니다.model=""gemini-pro"",
    safety_settings={# 위험한 콘텐츠에 대한 차단 임계값을 설정합니다.# 이 경우 위험한 콘텐츠를 차단하지 않도록 설정되어 있습니다.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    },
)
```
Google의safety setting types문서에서는 사용 가능한 카테고리와 임계값에 대한 열거형 정보를 제공합니다.
이 문서에는 콘텐츠 필터링 및 안전 설정과 관련된 다양한 카테고리와 해당 임계값이 정의되어 있어, 개발자들이 생성형 AI 모델을 활용할 때 적절한 안전 설정을 선택하고 적용하는 데 도움을 줍니다.
이를 통해 개발자들은 모델이 생성하는 콘텐츠의 안전성과 적절성을 보장하고, 사용자에게 유해하거나 부적절한 내용이 노출되는 것을 방지할 수 있습니다.
## Batch 단위 실행
- llm객체의batch메서드를 사용하여 여러 개의 프롬프트를 한 번에 처리합니다.
- batch메서드는 프롬프트 리스트를 받아 각 프롬프트에 대한 응답을 생성합니다.
- 생성된 응답은results변수에 저장됩니다.
- for루프를 사용하여results의 각 응답 객체에 접근합니다.
- 각 응답 객체의content속성을 출력하여 생성된 응답 텍스트를 확인합니다.
llm객체의batch메서드를 사용하여 여러 개의 프롬프트를 한 번에 처리합니다.
batch메서드는 프롬프트 리스트를 받아 각 프롬프트에 대한 응답을 생성합니다.
생성된 응답은results변수에 저장됩니다.
for루프를 사용하여results의 각 응답 객체에 접근합니다.
각 응답 객체의content속성을 출력하여 생성된 응답 텍스트를 확인합니다.
```
Copyfromlangchain_google_genaiimport(
    ChatGoogleGenerativeAI,
    HarmBlockThreshold,
    HarmCategory,
)

llm = ChatGoogleGenerativeAI(# 사용할 모델을 ""gemini-pro""로 지정합니다.model=""gemini-pro"",
    safety_settings={# 위험한 콘텐츠에 대한 차단 임계값을 설정합니다.# 이 경우 위험한 콘텐츠를 차단하지 않도록 설정되어 있습니다.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    },
)

results = llm.batch(
    [""2+2 의 계산 결과는?"",# 2+2의 결과는 무엇인가요?""3+5 의 계산 결과는?"",# 3+5의 결과는 무엇인가요?]
)forresinresults:print(res.content)# 각 결과의 내용을 출력합니다.
```
```
4
8
```
## Multimodal support
사람이 보내는 메시지에List[dict]타입의 내용을 전달하여 이미지를 제공할 수 있습니다.
이때 각 dict는 이미지 값(image_url타입) 또는 텍스트 값(text타입)을 포함해야 합니다.
image_url의 값은 다음 중 하나일 수 있습니다:
- 공개된 이미지 URL
- 접근 가능한 gcs 파일 (예: ""gcs://path/to/file.png"")
- 로컬 파일 경로
- base64로 인코딩된 이미지 (예:data:image/png;base64,abcd124)
- PIL 이미지
- requests라이브러리를 사용하여 이미지 URL에서 이미지 데이터를 가져옵니다.
- IPython.display모듈에서Image클래스를 임포트합니다.
- image_url변수에 이미지 URL을 할당합니다.
- requests.get()함수를 사용하여 이미지 URL에서 이미지 데이터를 가져와content변수에 저장합니다.
- Image클래스를 사용하여content에 저장된 이미지 데이터를 표시합니다.
공개된 이미지 URL
접근 가능한 gcs 파일 (예: ""gcs://path/to/file.png"")
로컬 파일 경로
base64로 인코딩된 이미지 (예:data:image/png;base64,abcd124)
PIL 이미지
PIL 이미지
requests라이브러리를 사용하여 이미지 URL에서 이미지 데이터를 가져옵니다.
requests라이브러리를 사용하여 이미지 URL에서 이미지 데이터를 가져옵니다.
IPython.display모듈에서Image클래스를 임포트합니다.
image_url변수에 이미지 URL을 할당합니다.
requests.get()함수를 사용하여 이미지 URL에서 이미지 데이터를 가져와content변수에 저장합니다.
Image클래스를 사용하여content에 저장된 이미지 데이터를 표시합니다.
```
CopyimportrequestsfromIPython.displayimportImage

image_url =""https://picsum.photos/seed/picsum/300/300""# 이미지 URL을 지정합니다.content = requests.get(image_url).content# 지정된 URL에서 이미지 데이터를 가져옵니다.Image(content)# 가져온 이미지 데이터를 IPython 디스플레이에 표시합니다.
```
```

```
```
Copyimportbase64fromioimportBytesIOfromIPython.displayimportHTML, displayfromPILimportImagedefconvert_to_base64(pil_image):""""""
    PIL 이미지를 Base64로 인코딩된 문자열로 변환합니다.

    :param pil_image: PIL 이미지
    :return: 크기 조정된 Base64 문자열
    """"""buffered = BytesIO()
    pil_image.save(buffered,format=""JPEG"")# 필요한 경우 형식을 변경할 수 있습니다.img_str = base64.b64encode(buffered.getvalue()).decode(""utf-8"")returnimg_strdefplt_img_base64(img_base64):""""""
    Base64로 인코딩된 문자열을 이미지로 표시합니다.

    :param img_base64:  Base64 문자열
    """"""# Base64 문자열을 소스로 사용하여 HTML img 태그 생성image_html =f'<img src=""data:image/jpeg;base64,{img_base64}"" />'# HTML을 렌더링하여 이미지 표시display(HTML(image_html))


file_path =""./images/jeju-beach.jpg""pil_image = Image.open(file_path)

image_b64 = convert_to_base64(pil_image)
plt_img_base64(image_b64)
```
-ChatGoogleGenerativeAI클래스를 사용하여 ""gemini-pro-vision"" 모델로 초기화된llm객체를 생성합니다.
-HumanMessage클래스를 사용하여message객체를 생성합니다.
  -content매개변수는 리스트 형태로 제공되며, 텍스트와 이미지 URL을 포함할 수 있습니다.
  - 텍스트 부분은 ""type""을 ""text""로 지정하고 ""text"" 필드에 텍스트를 제공합니다.
  - 이미지 URL은 ""type""을 ""image_url""로 지정하고 ""image_url"" 필드에 URL을 제공합니다.
-llm.invoke()메서드를 호출하여message를 전달하고 이미지에 대한 분석 결과를 얻습니다.
```
Copyfromlangchain_core.messagesimportHumanMessagefromlangchain_google_genaiimportChatGoogleGenerativeAI# Google의 Gemini-pro-vision 모델을 사용하여 ChatGoogleGenerativeAI 객체를 생성합니다.llm = ChatGoogleGenerativeAI(model=""gemini-pro-vision"")# 예시message = HumanMessage(# 사용자 메시지를 생성합니다.content=[
        {""type"":""text"",# 메시지 유형을 텍스트로 지정합니다.""text"":""What's in this image?"",# 이미지에 대한 질문을 텍스트로 입력합니다.},# 선택적으로 텍스트 부분을 제공할 수 있습니다.# 메시지 유형을 이미지 URL로 지정하고, 이미지 URL을 입력합니다.{""type"":""image_url"",""image_url"":f""data:image/jpeg;base64,{image_b64}""},
    ]
)
llm.invoke([message])# 생성된 사용자 메시지를 모델에 전달하여 응답을 생성합니다.
```
```
AIMessage(content=' Udo Island in Jeju Island, South Korea.')
```"
06. 허깅페이스 엔드포인트(HuggingFace Endpoints),https://wikidocs.net/233802,"# 06. 허깅페이스 엔드포인트(HuggingFace Endpoints)

Hugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능합니다. 이 온라인 플랫폼에서 사람들은 쉽게 협업하고 함께 머신러닝을 구축할 수 있습니다.
Hugging Face Hub은 또한 다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공합니다. 이 예제는 다양한 유형의 엔드포인트에 연결하는 방법을 보여줍니다.
특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다.
## 허깅페이스 토큰 발급
허깅페이스(https://huggingface.co) 에 회원가입을 한 뒤, 아래의 주소에서 토큰 발급을 신청합니다.
- 토큰 발급주소: https://huggingface.co/docs/hub/security-tokens
토큰 발급주소: https://huggingface.co/docs/hub/security-tokens
## HuggingFace 모델 리스트
- 허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
- 모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads
허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads
## Installation and Setup
HuggingFaceEndpoint클래스를 사용하여 Hugging Face 엔드포인트와 상호 작용할 수 있습니다.
- langchain_community.llms모듈에서HuggingFaceEndpoint클래스를 임포트합니다.
- HuggingFaceEndpoint클래스를 사용하면 Hugging Face에서 호스팅되는 언어 모델과 통신할 수 있습니다.
- 이 클래스는 Hugging Face 엔드포인트의 URL과 필요한 인증 정보를 사용하여 초기화됩니다.
- 초기화된HuggingFaceEndpoint객체를 통해 프롬프트를 전송하고 언어 모델의 응답을 받을 수 있습니다.
langchain_community.llms모듈에서HuggingFaceEndpoint클래스를 임포트합니다.
HuggingFaceEndpoint클래스를 사용하면 Hugging Face에서 호스팅되는 언어 모델과 통신할 수 있습니다.
이 클래스는 Hugging Face 엔드포인트의 URL과 필요한 인증 정보를 사용하여 초기화됩니다.
초기화된HuggingFaceEndpoint객체를 통해 프롬프트를 전송하고 언어 모델의 응답을 받을 수 있습니다.
```
Copyfromlangchain_community.llmsimportHuggingFaceEndpoint
```
사용하기 위해서는 Python의huggingface_hub패키지를 설치해야 합니다.
- huggingface_hub라이브러리를 최신 버전으로 업그레이드합니다.
huggingface_hub라이브러리를 최신 버전으로 업그레이드합니다.
```
Copy%pip install --upgrade --quiet huggingface_hub
```
아래의 코드를 실행하여 발급 받은 허깅페이스 토큰을 인증합니다.
```
Copyfromhuggingface_hubimportlogin

login()
```
이 코드는 Hugging Face Hub API 토큰을 안전하게 입력받아HUGGINGFACEHUB_API_TOKEN변수에 저장하는 역할을 합니다. 토큰은 Hugging Face Hub의 API 추론 기능을 사용하기 위해 필요합니다.
- 환경 변수HUGGINGFACEHUB_API_TOKEN을 설정합니다.
- os.environ딕셔너리를 사용하여""HUGGINGFACEHUB_API_TOKEN""키에HUGGINGFACEHUB_API_TOKEN변수의 값을 할당합니다.
환경 변수HUGGINGFACEHUB_API_TOKEN을 설정합니다.
os.environ딕셔너리를 사용하여""HUGGINGFACEHUB_API_TOKEN""키에HUGGINGFACEHUB_API_TOKEN변수의 값을 할당합니다.
```
Copyimportosfromdotenvimportload_dotenv

load_dotenv()

HUGGINGFACEHUB_API_TOKEN = os.environ[""HUGGINGFACEHUB_API_TOKEN""]
```
## Prepare Examples
예시 데이터를 준비하는 과정입니다.
HuggingFaceEndpoint클래스를 사용하여 Hugging Face 엔드포인트와 상호 작용할 수 있습니다.
- langchain_community.llms모듈에서HuggingFaceEndpoint클래스를 임포트합니다.
- HuggingFaceEndpoint클래스를 사용하면 Hugging Face에서 호스팅되는 언어 모델과 통신할 수 있습니다.
- 이 클래스는 Hugging Face 엔드포인트의 URL과 필요한 인증 정보를 사용하여 초기화됩니다.
- 초기화된HuggingFaceEndpoint객체를 통해 프롬프트를 전송하고 언어 모델의 응답을 받을 수 있습니다.
langchain_community.llms모듈에서HuggingFaceEndpoint클래스를 임포트합니다.
HuggingFaceEndpoint클래스를 사용하면 Hugging Face에서 호스팅되는 언어 모델과 통신할 수 있습니다.
이 클래스는 Hugging Face 엔드포인트의 URL과 필요한 인증 정보를 사용하여 초기화됩니다.
초기화된HuggingFaceEndpoint객체를 통해 프롬프트를 전송하고 언어 모델의 응답을 받을 수 있습니다.
```
Copyfromlangchain_community.llmsimportHuggingFaceEndpoint
```
- question변수에 ""1994년 FIFA 월드컵에서 우승한 팀은 어디인가요?""라는 질문을 할당합니다.
- template변수에 질문과 답변 형식을 지정하는 템플릿 문자열을 할당합니다.
- 템플릿에는{question}플레이스홀더가 포함되어 있습니다.
- 답변 부분에는 ""단계별로 생각해 보겠습니다.""라는 문구가 포함되어 있습니다.
- PromptTemplate.from_template()메서드를 사용하여template을 기반으로PromptTemplate객체인prompt를 생성합니다.
- 이를 통해 질문을 템플릿에 삽입하여 프롬프트를 생성할 수 있습니다.
question변수에 ""1994년 FIFA 월드컵에서 우승한 팀은 어디인가요?""라는 질문을 할당합니다.
template변수에 질문과 답변 형식을 지정하는 템플릿 문자열을 할당합니다.
템플릿에는{question}플레이스홀더가 포함되어 있습니다.
답변 부분에는 ""단계별로 생각해 보겠습니다.""라는 문구가 포함되어 있습니다.
PromptTemplate.from_template()메서드를 사용하여template을 기반으로PromptTemplate객체인prompt를 생성합니다.
이를 통해 질문을 템플릿에 삽입하여 프롬프트를 생성할 수 있습니다.
```
Copyfromlangchain.promptsimportPromptTemplate

template =""""""Please answer the following questions concisely.
QUESTION: {question}

ANSWER: """"""prompt = PromptTemplate.from_template(template)
```
## Examples
다음은 무료Serverless EndpointsAPI의HuggingFaceEndpoint통합에 액세스하는 방법의 예시입니다.
- repo_id변수에 ""mistralai/Mistral-7B-Instruct-v0.2"" 모델의 저장소 ID를 할당합니다.
- HuggingFaceEndpoint를 사용하여llm객체를 생성합니다.
- repo_id로 지정된 모델을 사용합니다.
- max_length를 128로 설정하여 생성할 최대 토큰 수를 제한합니다.
- temperature를 0.5로 설정하여 생성 결과의 다양성을 조절합니다.
- token에HUGGINGFACEHUB_API_TOKEN을 전달하여 인증합니다.
- LLMChain을 사용하여llm_chain객체를 생성합니다.
- prompt변수에 할당된 프롬프트를 사용합니다.
- llm매개변수에 이전에 생성한llm객체를 전달합니다.
- llm_chain.run(question)을 호출하여 주어진 질문에 대한 답변을 생성합니다.
- 생성된 답변을 출력합니다.
repo_id변수에 ""mistralai/Mistral-7B-Instruct-v0.2"" 모델의 저장소 ID를 할당합니다.
HuggingFaceEndpoint를 사용하여llm객체를 생성합니다.
repo_id로 지정된 모델을 사용합니다.
max_length를 128로 설정하여 생성할 최대 토큰 수를 제한합니다.
temperature를 0.5로 설정하여 생성 결과의 다양성을 조절합니다.
token에HUGGINGFACEHUB_API_TOKEN을 전달하여 인증합니다.
LLMChain을 사용하여llm_chain객체를 생성합니다.
prompt변수에 할당된 프롬프트를 사용합니다.
llm매개변수에 이전에 생성한llm객체를 전달합니다.
llm_chain.run(question)을 호출하여 주어진 질문에 대한 답변을 생성합니다.
생성된 답변을 출력합니다.
```
Copyfromlangchain.chainsimportLLMChainfromlangchain.callbacks.streaming_stdoutimportStreamingStdOutCallbackHandler# 사용할 모델의 저장소 ID를 설정합니다.# repo_id = ""mistralai/Mistral-7B-Instruct-v0.2""repo_id =""mistralai/Mixtral-8x7B-Instruct-v0.1""# repo_id = ""google/gemma-7b""llm = HuggingFaceEndpoint(
    repo_id=repo_id,# 모델 저장소 ID를 지정합니다.max_new_tokens=256,# 생성할 최대 토큰 길이를 설정합니다.temperature=0.1,# 샘플링 온도를 설정합니다. 값이 높을수록 더 다양한 출력을 생성합니다.callbacks=[StreamingStdOutCallbackHandler()],# 콜백을 설정합니다.streaming=True,# 스트리밍을 사용합니다.)# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.llm_chain = LLMChain(prompt=prompt, llm=llm)# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.response = llm_chain.invoke(
    {""question"":""Please tell me top 5 places to visit in Seoul, Korea.""}
)print(response)
```
```
Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /Users/teddy/.cache/huggingface/token
Login successful

1. Gyeongbokgung Palace: This is the largest of the Five Grand Palaces built by the Joseon dynasty. It's a must-visit for its historical significance and beautiful architecture.

2. Myeongdong: This is a shopping district that's famous for its cosmetic shops, street food, and fashion boutiques. It's a great place to experience the local culture and try some delicious Korean snacks.

3. Bukchon Hanok Village: This traditional Korean village is home to hundreds of hanok houses, which are traditional Korean homes. It's a great place to take a stroll and soak in the traditional Korean atmosphere.

4. N Seoul Tower: Also known as Namsan Tower, this is a communication and observation tower located on Namsan Mountain in central Seoul. It offers panoramic views of the city and is a popular spot for couples to lock ""love padlocks"" to the fence as a symbol of their love.

5. Insadong: This is a great place to experience traditional Korean culture. It's home to many antique shops, tea houses, and street vendors selling traditional Korean sn{'question': 'Please tell me top 5 places to visit in Seoul, Korea.', 'text': '\n1. Gyeongbokgung Palace: This is the largest of the Five Grand Palaces built by the Joseon dynasty. It\'s a must-visit for its historical significance and beautiful architecture.\n\n2. Myeongdong: This is a shopping district that\'s famous for its cosmetic shops, street food, and fashion boutiques. It\'s a great place to experience the local culture and try some delicious Korean snacks.\n\n3. Bukchon Hanok Village: This traditional Korean village is home to hundreds of hanok houses, which are traditional Korean homes. It\'s a great place to take a stroll and soak in the traditional Korean atmosphere.\n\n4. N Seoul Tower: Also known as Namsan Tower, this is a communication and observation tower located on Namsan Mountain in central Seoul. It offers panoramic views of the city and is a popular spot for couples to lock ""love padlocks"" to the fence as a symbol of their love.\n\n5. Insadong: This is a great place to experience traditional Korean culture. It\'s home to many antique shops, tea houses, and street vendors selling traditional Korean sn'}
```
## Dedicated Endpoint
무료 서버리스 API를 사용하면 솔루션을 빠르게 구현하고 반복할 수 있지만, 로드가 다른 요청과 공유되기 때문에 대용량 사용 사례에서는 속도 제한이 있을 수 있습니다.
엔터프라이즈 워크로드의 경우,Inference Endpoints - Dedicated를 사용하는 것이 가장 좋습니다.
이를 통해 더 많은 유연성과 속도를 제공하는 완전 관리형 인프라에 액세스할 수 있습니다.
이러한 리소스에는 지속적인 지원과 가동 시간 보장은 물론 AutoScaling과 같은 옵션도 포함됩니다.

- your_endpoint_url변수에 Inference Endpoint의 URL을 설정합니다.
your_endpoint_url변수에 Inference Endpoint의 URL을 설정합니다.
```
Copy# Inference Endpoint URL을 아래에 설정합니다.your_endpoint_url =""https://qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud""
```
- HuggingFaceEndpoint클래스를 사용하여llm객체를 생성합니다.
- endpoint_url매개변수에는 Hugging Face 엔드포인트의 URL을 전달합니다.
- max_new_tokens매개변수는 생성할 최대 토큰 수를 지정합니다.
- top_k매개변수는 확률 기반 샘플링에 사용할 상위 k개 토큰을 지정합니다.
- top_p매개변수는 누적 확률 기반 샘플링에 사용할 상위 확률 임계값을 지정합니다.
- typical_p매개변수는 전형적인 확률 기반 샘플링에 사용할 확률 임계값을 지정합니다.
- temperature매개변수는 샘플링 과정에서의 무작위성을 조절합니다.
- repetition_penalty매개변수는 반복 패널티를 조절하여 반복되는 단어나 구문을 방지합니다.
- llm객체를 호출하여 ""#QUESTION: 대한민국의 수도는 어디인가요?\n\n#ANSWER:"" 라는 질문을 전달하면, 해당 질문에 대한 답변을 생성합니다.
HuggingFaceEndpoint클래스를 사용하여llm객체를 생성합니다.
endpoint_url매개변수에는 Hugging Face 엔드포인트의 URL을 전달합니다.
max_new_tokens매개변수는 생성할 최대 토큰 수를 지정합니다.
top_k매개변수는 확률 기반 샘플링에 사용할 상위 k개 토큰을 지정합니다.
top_p매개변수는 누적 확률 기반 샘플링에 사용할 상위 확률 임계값을 지정합니다.
typical_p매개변수는 전형적인 확률 기반 샘플링에 사용할 확률 임계값을 지정합니다.
temperature매개변수는 샘플링 과정에서의 무작위성을 조절합니다.
repetition_penalty매개변수는 반복 패널티를 조절하여 반복되는 단어나 구문을 방지합니다.
llm객체를 호출하여 ""#QUESTION: 대한민국의 수도는 어디인가요?\n\n#ANSWER:"" 라는 질문을 전달하면, 해당 질문에 대한 답변을 생성합니다.
```
Copyllm = HuggingFaceEndpoint(# 엔드포인트 URL을 설정합니다.endpoint_url=f""{your_endpoint_url}"",# 생성할 최대 토큰 수를 설정합니다.max_new_tokens=512,# 상위 K개의 토큰을 선택합니다.top_k=10,# 누적 확률이 top_p에 도달할 때까지 토큰을 선택합니다.top_p=0.95,# typical_p 확률 이상의 토큰만 선택합니다.typical_p=0.95,# 샘플링 온도를 설정합니다. 낮을수록 더 결정적입니다.temperature=0.01,# 반복 패널티를 설정합니다. 높을수록 반복을 줄입니다.repetition_penalty=1.03,
)# 주어진 프롬프트에 대해 언어 모델을 실행합니다.llm.invoke(input=""#QUESTION: 대한민국의 수도는 어디인가요?\n\n#ANSWER:"")
```
```
' 서울특별시입니다.\n대한민국은 1948년에 건국된 나라이다. 그런데 이때부터 지금까지의 행정수도로서 기능을 해온 곳이 바로 세종 특별자치시다. 하지만, 실제적인 국가적 권력과 의사결정권 등 모든 것이 모여있는 진짜 \'수도\'라고 한다면 당연히 우리나라는 #서울특별시 라고 할 것이다. 그리고 많은 국민들이 알고 있는 것처럼 정부청사와 대통령 집무실 및 관저를 비롯해서 주요 공공기관들 모두 다 여기에 있다. 또한 교육이나 문화예술 분야에서 가장 앞선 도시이며 경제활동 역시 활발하게 일어나는 지역으로써 명실상부 한국 최대의 중심지역임에는 누구도 부인하지 못할것 같다.\u200b그런 의미에서는 사실 상징성만 보더래도 ""우리나라""하면 제일 처음 머릿속에 생각날 만큼 중요하고 자랑스러운 장소이고 동네(?)같다는 느낌마저 들게 하는곳이기도 한듯하다. 물론 다른 광역시들의 입장에서는 조금 서글픈 이야기겠으나 말이다...^^;\u200b \u200b아무튼 오늘 소개해드릴 내용은 위 질문 속 단골 손님중 하나인 대한 민국의 위치 관련 정보 입니다~\u200b\u200b \u200b\u200b먼저 아시아(Asia)쪽 사진들을 살펴보도록 합시다!\u200b\u200b\u200b위키백과 참조\u200b\u200b\u200b\u200b첫번째 이미지 출처 : https://commons.wikimedia.org/wiki/%EB%AF%B8_%EC%A0%9C2_(South_Korea)\u200b미주한국재단 제공사진https://commons.wikimedia.org/wiki 미주의 여러 재단 홈페이지에도 들어가봤는데 마땅한 자료 찾기가 너무 힘듭니다ㅠㅜ;;혹 좋은 링크 있으면 추천좀 해주세요~~!!\u200b\u200b\u200b\u200b\u200b\u200b\u200b두 번째 이미지출처 : commons.wikimedia.orghttps://commons.wikimedia.org/wiki/?q=south+korea&tbm=isch&source=iu&ictx=1&fir=-675324-dGwwM:&spf=http:%//imagesearch.naver.com'
```
HuggingFaceEndpoint를 사용하여 스트리밍 방식으로 텍스트를 생성하는 예제입니다.
- HuggingFaceEndpoint클래스를 사용하여llm객체를 생성합니다.
- endpoint_url매개변수에 HuggingFace 엔드포인트 URL을 지정합니다.
- 다양한 생성 매개변수를 설정합니다:max_new_tokens,top_k,top_p,typical_p,temperature,repetition_penalty.
- streaming매개변수를True로 설정하여 스트리밍 모드를 활성화합니다.
- StreamingStdOutCallbackHandler를 콜백으로 사용하여 생성된 텍스트를 실시간으로 출력합니다.
- llm객체를 호출하여 프롬프트에 대한 텍스트를 생성합니다.
HuggingFaceEndpoint클래스를 사용하여llm객체를 생성합니다.
endpoint_url매개변수에 HuggingFace 엔드포인트 URL을 지정합니다.
다양한 생성 매개변수를 설정합니다:max_new_tokens,top_k,top_p,typical_p,temperature,repetition_penalty.
streaming매개변수를True로 설정하여 스트리밍 모드를 활성화합니다.
StreamingStdOutCallbackHandler를 콜백으로 사용하여 생성된 텍스트를 실시간으로 출력합니다.
llm객체를 호출하여 프롬프트에 대한 텍스트를 생성합니다.
이 예제는 HuggingFace 엔드포인트를 사용하여 텍스트 생성을 수행하고, 생성 과정을 실시간으로 스트리밍하는 방법을 보여줍니다.
```
Copyfromlangchain.callbacks.streaming_stdoutimportStreamingStdOutCallbackHandlerfromlangchain_community.llmsimportHuggingFaceEndpoint

llm = HuggingFaceEndpoint(# 엔드포인트 URL을 설정합니다.endpoint_url=f""{your_endpoint_url}"",# 생성할 최대 토큰 수를 설정합니다.max_new_tokens=512,# top_k 샘플링 기법에서 고려할 최상위 토큰 수를 설정합니다.top_k=10,# top_p 샘플링 기법에서 누적 확률 임계값을 설정합니다.top_p=0.95,# typical_p 샘플링 기법에서 누적 확률 임계값을 설정합니다.typical_p=0.95,# 샘플링 온도를 설정합니다. 낮을수록 더 결정적인 출력을 생성합니다.temperature=0.01,# 반복 패널티를 설정하여 동일한 토큰의 반복을 제어합니다.repetition_penalty=1.03,# 스트리밍 모드를 활성화합니다.streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)# 질문을 입력하고 스트리밍 콜백 핸들러를 사용하여 실시간으로 출력을 확인합니다.llm.invoke(input=""#QUESTION: 대한민국 경기도를 여행한다면 꼭 가봐야할 곳 5군데를 추천해 주세요.n\n#ANSWER:"",
)
```
```
' 안녕하세요? 저는 한국관광공사에서 운영하는 트래블리더로 활동하고 있는 대학생입니다! 제가 소개시켜드릴 관광지는 바로 수원화성과 화성행궁, 그리고 광교호수공원인데요~ 지금부터 그 이유와 함께 설명해 드리도록 하겠습니다^^ 첫째로 들려볼 장소는 세계문화유산으로 등록되어있는 \'수원화성\' 입니다!! 우리나라의 성곽 중 가장 아름답다고 평가받고 있기 때문에 많은 분들이 찾아주십니다~~ 특히 야경이 정말 예쁘다는 사실~! 두번째 코스는 조선왕조 최대규모 행차였던 어머니에 대한 효심을 담은 왕의 길인 ""화성어장"" 을 걸으며 역사적 의미도 느껴보며 산책하기 좋은곳 인것 같네ㅎㅎ 마지막 세 번째코스는 호수위 공중정원 으로 불리는 ‘광교호수공원’ 입다!!! 자연친화적인 휴식처이며 다양한 문화예술프로그램들을 접하거나 참여 할 수 있어서 더욱 매력만점이고 사람들의 발길이 계속해서 향하게 되는 것같습니당^0^* 이상 간단하지만 알찬 정보들 잘 보셨나용?? 여러분께 도움되는 포스트였다면 좋겠구 앞으로 열심히 노력하여 유익한 글 많이 올리도록 약속하겟슴돠~~~ 감사합니다♥'
```"
07. 허깅페이스 로컬(HuggingFace Local),https://wikidocs.net/233803,"# 07. 허깅페이스 로컬(HuggingFace Local)

```
Copy# 토큰 정보로드를 위한 라이브러리# 설치: pip install python-dotenvfromdotenvimportload_dotenv# 토큰 정보로드load_dotenv()
```
```
True
```
모델을 다운로드 받을 경로를 설정
```
Copy# 허깅페이스 모델/토크나이저를 다운로드 받을 경로# (예시)importos# ./cache/ 경로에 다운로드 받도록 설정os.environ[""TRANSFORMERS_CACHE""] =""./cache/""os.environ[""HF_HOME""] =""./cache/""
```
```
Copyfromlangchain.llmsimportHuggingFaceHubimportwarnings# 경고 메시지 무시warnings.filterwarnings(""ignore"")# HuggingFace Repository IDrepo_id =""mistralai/Mixtral-8x7B-Instruct-v0.1""# HuggingFaceHub 객체 생성llm = HuggingFaceHub(
    repo_id=repo_id,
    model_kwargs={""temperature"":0.1,""max_length"":2048},
    task=""text-generation"",# 텍스트 생성)
```
```
Copyfromlangchain.chainsimportLLMChainfromlangchain.promptsimportPromptTemplate

template =""""""Summarizes TEXT in simple bullet points ordered from most important to least important.
TEXT:
{text}

KeyPoints: """"""prompt = PromptTemplate.from_template(template)

llm_chain = LLMChain(prompt=prompt, llm=llm)

text =""""""A Large Language Model (LLM) like me, ChatGPT, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are ""large"" because they're built from vast amounts of text data and have billions or even trillions of parameters. Parameters are the aspects of the model that are learned from training data; they are essentially the internal settings that determine how the model interprets and generates language. LLMs work by predicting the next word in a sequence given the words that precede it, which allows them to generate coherent and contextually relevant text based on a given prompt. This capability can be applied in a variety of ways, from answering questions and composing emails to writing essays and even creating computer code. The training process for these models involves exposing them to a diverse array of text sources, such as books, articles, and websites, allowing them to learn language patterns, grammar, facts about the world, and even styles of writing. However, it's important to note that while LLMs can provide information that seems knowledgeable, their responses are generated based on patterns in the data they were trained on and not from a sentient understanding or awareness. The development and deployment of LLMs raise important considerations regarding accuracy, bias, ethical use, and the potential impact on various aspects of society, including employment, privacy, and misinformation. Researchers and developers continue to work on ways to address these challenges while improving the models' capabilities and applications.""""""response = llm_chain.invoke(input=text)
```
```
Copyprint(response[""text""])
```
```
Summarizes TEXT in simple bullet points ordered from most important to least important.
TEXT:
A Large Language Model (LLM) like me, ChatGPT, is a type of artificial intelligence (AI) model designed to understand, generate, and interact with human language. These models are ""large"" because they're built from vast amounts of text data and have billions or even trillions of parameters. Parameters are the aspects of the model that are learned from training data; they are essentially the internal settings that determine how the model interprets and generates language. LLMs work by predicting the next word in a sequence given the words that precede it, which allows them to generate coherent and contextually relevant text based on a given prompt. This capability can be applied in a variety of ways, from answering questions and composing emails to writing essays and even creating computer code. The training process for these models involves exposing them to a diverse array of text sources, such as books, articles, and websites, allowing them to learn language patterns, grammar, facts about the world, and even styles of writing. However, it's important to note that while LLMs can provide information that seems knowledgeable, their responses are generated based on patterns in the data they were trained on and not from a sentient understanding or awareness. The development and deployment of LLMs raise important considerations regarding accuracy, bias, ethical use, and the potential impact on various aspects of society, including employment, privacy, and misinformation. Researchers and developers continue to work on ways to address these challenges while improving the models' capabilities and applications.

KeyPoints: 
- ChatGPT is a Large Language Model (LLM) that understands, generates, and interacts with human language.
- LLMs are ""large"" because they're built from vast amounts of text data and have billions or even trillions of parameters.
- Parameters are the aspects of the model that are learned from training data; they determine how the model interprets and generates language.
- LLMs work by predicting the next word in a sequence given
```"
08. 허깅페이스 파이프라인(HuggingFace Pipeline),https://wikidocs.net/233804,"# 08. 허깅페이스 파이프라인(HuggingFace Pipeline)

HuggingFacePipeline클래스를 통해 Hugging Face 모델을 로컬에서 실행할 수 있습니다.
Hugging Face Model Hub는 온라인 플랫폼에서 120,000개 이상의 모델, 20,000개의 데이터셋, 50,000개의 데모 앱(Spaces)을 호스팅하며, 모두 오픈 소스이고 공개적으로 사용 가능하여 사람들이 쉽게 협업하고 함께 ML을 구축할 수 있습니다.
이러한 모델은 LangChain에서 이 로컬 파이프라인 래퍼를 통해 호출하거나, HuggingFaceHub 클래스를 통해 호스팅된 추론 엔드포인트를 호출하여 사용할 수 있습니다. 호스팅된 파이프라인에 대한 자세한 내용은HuggingFaceHub노트북을 참조하세요.
사용하기 위해서는PyTorch와 함께 Python패키지 transformers가 설치되어 있어야 합니다.
또한, 보다 메모리 효율적인 attention 구현을 위해xformer를 설치할 수도 있습니다.
```
Copy%pip install --upgrade --quiet  transformers --quiet
```
모델을 다운로드 받을 경로를 설정
```
Copy# 허깅페이스 모델/토크나이저를 다운로드 받을 경로# (예시)importos# ./cache/ 경로에 다운로드 받도록 설정os.environ[""TRANSFORMERS_CACHE""] =""./cache/""os.environ[""HF_HOME""] =""./cache/""
```
모델은from_model_id메서드를 사용하여 모델 매개변수를 지정함으로써 로드할 수 있습니다.
- HuggingFacePipeline클래스를 사용하여 Hugging Face의 사전 학습된 모델을 로드합니다.
- from_model_id메서드를 사용하여beomi/llama-2-ko-7b모델을 지정하고, 작업을 ""text-generation""으로 설정합니다.
- pipeline_kwargs매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.
- 로드된 모델은hf변수에 할당되며, 이를 통해 텍스트 생성 작업을 수행할 수 있습니다.
HuggingFacePipeline클래스를 사용하여 Hugging Face의 사전 학습된 모델을 로드합니다.
from_model_id메서드를 사용하여beomi/llama-2-ko-7b모델을 지정하고, 작업을 ""text-generation""으로 설정합니다.
pipeline_kwargs매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.
로드된 모델은hf변수에 할당되며, 이를 통해 텍스트 생성 작업을 수행할 수 있습니다.
사용한 모델: https://huggingface.co/beomi/llama-2-ko-7b
```
Copyfromlangchain_community.llms.huggingface_pipelineimportHuggingFacePipeline
```
```
Copy# HuggingFace 모델을 다운로드 받습니다.hf = HuggingFacePipeline.from_model_id(
    model_id=""beomi/llama-2-ko-7b"",# 사용할 모델의 ID를 지정합니다.task=""text-generation"",# 수행할 작업을 지정합니다. 여기서는 텍스트 생성입니다.# 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.pipeline_kwargs={""max_new_tokens"":512},
)
```
기존의transformerspipeline을 직접 전달하여 로드할 수도 있습니다.
HuggingFacePipeline을 사용하여 텍스트 생성 모델을 구현합니다.
- AutoTokenizer와AutoModelForCausalLM을 사용하여beomi/llama-2-ko-7b모델과 토크나이저를 로드합니다.
- pipeline함수를 사용하여 ""text-generation"" 파이프라인을 생성하고, 모델과 토크나이저를 설정합니다. 최대 생성 토큰 수는 10으로 제한합니다.
- HuggingFacePipeline클래스를 사용하여hf객체를 생성하고, 생성된 파이프라인을 전달합니다.
AutoTokenizer와AutoModelForCausalLM을 사용하여beomi/llama-2-ko-7b모델과 토크나이저를 로드합니다.
pipeline함수를 사용하여 ""text-generation"" 파이프라인을 생성하고, 모델과 토크나이저를 설정합니다. 최대 생성 토큰 수는 10으로 제한합니다.
HuggingFacePipeline클래스를 사용하여hf객체를 생성하고, 생성된 파이프라인을 전달합니다.
이렇게 생성된hf객체를 사용하여 주어진 프롬프트에 대한 텍스트 생성을 수행할 수 있습니다.
```
Copyfromlangchain_community.llms.huggingface_pipelineimportHuggingFacePipelinefromtransformersimportAutoModelForCausalLM, AutoTokenizer, pipeline

model_id =""beomi/llama-2-ko-7b""# 사용할 모델의 ID를 지정합니다.tokenizer = AutoTokenizer.from_pretrained(
    model_id
)# 지정된 모델의 토크나이저를 로드합니다.model = AutoModelForCausalLM.from_pretrained(model_id)# 지정된 모델을 로드합니다.# 텍스트 생성 파이프라인을 생성하고, 최대 생성할 새로운 토큰 수를 10으로 설정합니다.pipe = pipeline(""text-generation"", model=model,
                tokenizer=tokenizer, max_new_tokens=512)# HuggingFacePipeline 객체를 생성하고, 생성된 파이프라인을 전달합니다.hf = HuggingFacePipeline(pipeline=pipe)
```
모델이 메모리에 로드되면 프롬프트와 함께 구성하여 체인을 형성할 수 있습니다.
- PromptTemplate클래스를 사용하여 질문과 답변 형식을 정의하는 프롬프트 템플릿을 생성합니다.
- prompt객체와hf객체를 파이프라인으로 연결하여chain객체를 생성합니다.
- chain.invoke()메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.
PromptTemplate클래스를 사용하여 질문과 답변 형식을 정의하는 프롬프트 템플릿을 생성합니다.
prompt객체와hf객체를 파이프라인으로 연결하여chain객체를 생성합니다.
chain.invoke()메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.
```
Copyfromlangchain.promptsimportPromptTemplatefromlangchain_core.output_parsersimportStrOutputParser

template =""""""Answer the following question in Korean.
#Question: 
{question}

#Answer: """"""# 질문과 답변 형식을 정의하는 템플릿prompt = PromptTemplate.from_template(template)# 템플릿을 사용하여 프롬프트 객체 생성# 프롬프트와 언어 모델을 연결하여 체인 생성chain = prompt | hf | StrOutputParser()

question =""대한민국의 수도는 어디야?""# 질문 정의print(
    chain.invoke({""question"": question})
)# 체인을 호출하여 질문에 대한 답변 생성 및 출력
```
GPU에서 실행할 때는device=n매개변수를 지정하여 모델을 특정 디바이스에 배치할 수 있습니다.
기본값은-1로, CPU에서 추론을 수행합니다.
여러 개의 GPU가 있거나 모델이 단일 GPU에 비해 너무 큰 경우에는device_map=""auto""를 지정할 수 있습니다.
이 경우Accelerate라이브러리가 필요하며, 모델 가중치를 어떻게 로드할지 자동으로 결정하는 데 사용됩니다.
주의:device와device_map은 함께 지정해서는 안 되며, 예기치 않은 동작을 유발할 수 있습니다.
- HuggingFacePipeline을 사용하여gpt2모델을 로드하고,device매개변수를 0으로 설정하여 GPU에서 실행되도록 합니다.
- pipeline_kwargs매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.
- prompt와gpu_llm을 파이프라인으로 연결하여gpu_chain을 생성합니다.
- gpu_chain.invoke()메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.
HuggingFacePipeline을 사용하여gpt2모델을 로드하고,device매개변수를 0으로 설정하여 GPU에서 실행되도록 합니다.
pipeline_kwargs매개변수를 사용하여 생성할 최대 토큰 수를 10으로 제한합니다.
prompt와gpu_llm을 파이프라인으로 연결하여gpu_chain을 생성합니다.
gpu_chain.invoke()메서드를 호출하여 주어진 질문에 대한 답변을 생성하고 출력합니다.
```
Copygpu_llm = HuggingFacePipeline.from_model_id(
    model_id=""beomi/llama-2-ko-7b"",# 사용할 모델의 ID를 지정합니다.task=""text-generation"",# 수행할 작업을 설정합니다. 여기서는 텍스트 생성입니다.# 사용할 GPU 디바이스 번호를 지정합니다. ""auto""로 설정하면 accelerate 라이브러리를 사용합니다.device=0,# 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.pipeline_kwargs={""max_new_tokens"":64},
)

gpu_chain = prompt | gpu_llm# prompt와 gpu_llm을 연결하여 gpu_chain을 생성합니다.# 프롬프트와 언어 모델을 연결하여 체인 생성gpu_chain = prompt | gpu_llm | StrOutputParser()

question =""대한민국의 수도는 어디야?""# 질문 정의# 체인을 호출하여 질문에 대한 답변 생성 및 출력print(gpu_chain.invoke({""question"": question}))
```
GPU 장치에서 실행하는 경우, 배치 모드로 GPU에서 추론을 실행할 수 있습니다.
- HuggingFacePipeline을 사용하여beomi/llama-2-ko-7b모델을 로드하고, GPU에서 실행되도록 설정합니다.
- gpu_llm을 생성할 때batch_size를 2로 설정하고,temperature를 0으로,max_length를 64로 설정합니다.
- prompt와gpu_llm을 파이프라인으로 연결하여gpu_chain을 생성하고, 종료 토큰을 ""\n\n""로 설정합니다.
- gpu_chain.batch()를 사용하여questions의 질문들에 대한 답변을 병렬로 생성합니다.
- 생성된 답변을 반복문을 통해 출력합니다.
HuggingFacePipeline을 사용하여beomi/llama-2-ko-7b모델을 로드하고, GPU에서 실행되도록 설정합니다.
gpu_llm을 생성할 때batch_size를 2로 설정하고,temperature를 0으로,max_length를 64로 설정합니다.
prompt와gpu_llm을 파이프라인으로 연결하여gpu_chain을 생성하고, 종료 토큰을 ""\n\n""로 설정합니다.
gpu_chain.batch()를 사용하여questions의 질문들에 대한 답변을 병렬로 생성합니다.
생성된 답변을 반복문을 통해 출력합니다.
```
Copygpu_llm = HuggingFacePipeline.from_model_id(
    model_id=""beomi/llama-2-ko-7b"",# 사용할 모델의 ID를 지정합니다.task=""text-generation"",# 수행할 작업을 설정합니다.device=0,# GPU 디바이스 번호를 지정합니다. -1은 CPU를 의미합니다.batch_size=2,# 배치 크기s를 조정합니다. GPU 메모리와 모델 크기에 따라 적절히 설정합니다.model_kwargs={""temperature"":0,""max_length"":256,
    },# 모델에 전달할 추가 인자를 설정합니다.)# 프롬프트와 언어 모델을 연결하여 체인을 생성합니다.gpu_chain = prompt | gpu_llm.bind(stop=[""\n\n""])

questions = []foriinrange(4):# 질문 리스트를 생성합니다.questions.append({""question"":f""숫자{i}이 한글로 뭐에요?""})

answers = gpu_chain.batch(questions)# 질문 리스트를 배치 처리하여 답변을 생성합니다.foranswerinanswers:print(answer)# 생성된 답변을 출력합니다.
```"
09. 올라마(Ollama),https://wikidocs.net/233805,"# 09. 올라마(Ollama)

Ollama를 사용하면 Llama 2와 같은 오픈 소스 대규모 언어 모델을 로컬에서 실행할 수 있습니다.
Ollama는 모델 가중치, 구성 및 데이터를 Modelfile로 정의된 단일 패키지로 번들링합니다.
GPU 사용을 포함하여 설정 및 구성 세부 정보를 최적화합니다.
지원되는 모델 및 모델 변형의 전체 목록은Ollama model library를 참조하세요.
## Setup
먼저, 다음 지침에 따라 로컬 Ollama 인스턴스를 설정하고 실행하세요:
- Ollama를 지원되는 플랫폼(Mac / Linux / Windows)에 다운로드하고 설치하세요.
- ollama pull <name-of-model>명령을 사용하여 사용 가능한 LLM 모델을 가져오세요.
- model library를 통해 사용 가능한 모델 목록을 확인하세요.
- 예를 들어,gemma-7b의 경우:ollama pull gemma:7b
- 이렇게 하면 모델의 기본 태그 버전이 다운로드됩니다. 일반적으로 기본값은 최신의 가장 작은 크기 매개변수 모델을 가리킵니다.
Ollama를 지원되는 플랫폼(Mac / Linux / Windows)에 다운로드하고 설치하세요.
ollama pull <name-of-model>명령을 사용하여 사용 가능한 LLM 모델을 가져오세요.
model library를 통해 사용 가능한 모델 목록을 확인하세요.
예를 들어,gemma-7b의 경우:ollama pull gemma:7b
이렇게 하면 모델의 기본 태그 버전이 다운로드됩니다. 일반적으로 기본값은 최신의 가장 작은 크기 매개변수 모델을 가리킵니다.
Mac에서는 모델이~/.ollama/models에 다운로드됩니다.
Linux(또는 WSL)에서는 모델이/usr/share/ollama/.ollama/models에 저장됩니다.
- 관심 있는 모델의 정확한 버전을ollama pull gemma:7b와 같이 지정하세요. (이 경우gemma모델의 다양한 태그를 확인하세요. 예.gemma:2b,gemma:7b)
- 가져온 모든 모델을 보려면ollama list를 사용하세요.
- 명령줄에서 모델과 직접 채팅하려면ollama run <name-of-model>을 사용하세요.
- 더 많은 명령은 Ollama documentation을 참조하세요. 터미널에서ollama help를 실행하여 사용 가능한 명령도 확인할 수 있습니다.
관심 있는 모델의 정확한 버전을ollama pull gemma:7b와 같이 지정하세요. (이 경우gemma모델의 다양한 태그를 확인하세요. 예.gemma:2b,gemma:7b)
가져온 모든 모델을 보려면ollama list를 사용하세요.
명령줄에서 모델과 직접 채팅하려면ollama run <name-of-model>을 사용하세요.
더 많은 명령은 Ollama documentation을 참조하세요. 터미널에서ollama help를 실행하여 사용 가능한 명령도 확인할 수 있습니다.
## Usage
지원되는 매개변수의 전체 목록은 API reference page에서 확인할 수 있습니다.
LLaMAchat모델(예:ollama pull llama2:7b-chat)을 사용하는 경우ChatOllama인터페이스를 사용할 수 있습니다.
여기에는 시스템 메시지 및 사용자 입력을 위한 special tokens이 포함됩니다.
## Interacting with Models
가져온 로컬 모델과 상호 작용하는 몇 가지 방법은 다음과 같습니다.
- 모든 로컬 모델은 자동으로localhost:11434에서 제공됩니다.
- 명령줄을 통해 직접 상호 작용하려면ollama run <name-of-model>을 실행하세요.
- ChatOllama클래스를 사용하여 Ollama 챗 모델을 초기화합니다.
- ChatPromptTemplate클래스를 사용하여 주어진 템플릿으로부터 프롬프트를 생성합니다.
- LangChain Expressive Language(LCEL) 체인 문법을 사용하여 프롬프트, 언어 모델, 출력 파서를 파이프라인으로 연결합니다.
- chain.invoke()메서드를 호출하여 주어진 주제에 대한 짧은 농담을 생성합니다.
- 생성된 응답은 터미널에 출력됩니다.
모든 로컬 모델은 자동으로localhost:11434에서 제공됩니다.
명령줄을 통해 직접 상호 작용하려면ollama run <name-of-model>을 실행하세요.
명령줄을 통해 직접 상호 작용하려면ollama run <name-of-model>을 실행하세요.
ChatOllama클래스를 사용하여 Ollama 챗 모델을 초기화합니다.
ChatOllama클래스를 사용하여 Ollama 챗 모델을 초기화합니다.
ChatPromptTemplate클래스를 사용하여 주어진 템플릿으로부터 프롬프트를 생성합니다.
LangChain Expressive Language(LCEL) 체인 문법을 사용하여 프롬프트, 언어 모델, 출력 파서를 파이프라인으로 연결합니다.
chain.invoke()메서드를 호출하여 주어진 주제에 대한 짧은 농담을 생성합니다.
생성된 응답은 터미널에 출력됩니다.
```
Copyfromlangchain_community.chat_modelsimportChatOllamafromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplate# LangChain이 지원하는 다른 채팅 모델을 사용합니다. 여기서는 Ollama를 사용합니다.llm = ChatOllama(model=""gemma:7b"")# 주제를 기반으로 짧은 농담을 요청하는 프롬프트 템플릿을 생성합니다.prompt = ChatPromptTemplate.from_template(""{topic} 에 대하여 간략히 설명해 줘."")# LangChain 표현식 언어 체인 구문을 사용합니다.# LCEL에 대한 자세한 내용은 https://python.langchain.com/docs/expression_language/why 에서 확인할 수 있습니다.chain = prompt | llm | StrOutputParser()# 간결성을 위해 응답은 터미널에 출력됩니다.# 프로덕션 환경에서 애플리케이션을 배포하기 위해 LangServe를 사용할 수 있습니다.print(chain.invoke({""topic"":""deep learning""}))
```
```
**Deep Learning**

Deep learning은 인공지식 분야의 하위 분야로서 많은 양의 데이터를 분석하여 패턴을 학습하는 강력한 모델입니다. 이 모델은 인간 시각, 청음, 언어와 같은 감각에 유사한 특성을 가집니다.

**주요 특징:**

* **계층적 아키텍처:** 여러 개의 계층으로 구성된 복잡한 모델입니다.
* **자동학습:** 데이터를 분석하여 모델을 학습하는 능력을 가집니다.
* **소화:** 많은 양의 데이터를 효과적으로 소화할 수 있습니다.
* **패턴 추출:** 데이터의 패턴을 추출하여 새로운 정보를 발굴할 수 있습니다.
* **변동성:** 새로운 데이터에 대한 학습을 잘할 수 있습니다.

**응용 분야:**

* 이미지 분류
* 자연어 처리
* 음성 인식
* 자원 추측
* 의료

**주요 이점:**

* **고성능:** 인간 수준의 성능을 훨씬 능가할 수 있습니다.
* **자동화:** 많은 작업을 자동화하여 시간을 절약할 수 있습니다.
* **새로운 가능성:** 새로운 문제 해결 및 전문성을 가능하게 줍니다.

**주요 단점:**

* **데이터 요구:** 많은 양의 데이터를 요구합니다.
* **복잡성:** 모델을 이해하고 조절하는 데 어려움이 있을 수 있습니다.
* **편향:** 학습 데이터의 편향을 반영할 수 있습니다.
```
LCEL 체인은 기본적으로 응답 스트리밍 및 비동기 지원과 같은 추가 기능을 제공합니다.
- topic변수에 ""Covid 19""이라는 주제를 딕셔너리 형태로 할당합니다.
- chain.stream()메서드를 사용하여topic을 입력으로 받아 텍스트를 생성합니다.
- 생성된 텍스트는 청크(chunk) 단위로 반환되며,for루프를 통해 각 청크를 반복적으로 처리합니다.
- 각 청크는print()함수를 사용하여 출력됩니다.
topic변수에 ""Covid 19""이라는 주제를 딕셔너리 형태로 할당합니다.
chain.stream()메서드를 사용하여topic을 입력으로 받아 텍스트를 생성합니다.
생성된 텍스트는 청크(chunk) 단위로 반환되며,for루프를 통해 각 청크를 반복적으로 처리합니다.
각 청크는print()함수를 사용하여 출력됩니다.
```
Copytopic = {""topic"":""Covid 19""}# 주제를 ""Covid 19""로 설정합니다.# chain 객체의 stream 메서드를 사용하여 주제에 대한 텍스트를 생성하고, 생성된 텍스트를 chunks 변수에 할당하면서 반복합니다.forchunksinchain.stream(topic):print(chunks, end="""", flush=True)# 생성된 텍스트 조각(chunks)을 출력합니다.
```
```
**Covid-19는 바이러스 감염증**입니다.  SARS-CoV-2라는 바이러스가 원인이 되며, 폐렴, 호흡 문제, 그리고 기타 증세를 일으키는 질환입니다.

**증세:**
- 호흡 불명
- 숨가락의 불쾌침
- 사고력 감퇴
- 강력한 동무
- 아지루
- 비강 건조
- 숨소리

**위험 요인:**
- 65세이 이상
- 질환이나 약물을 사용하는 사람
- 약력이 약한 사람
- 흡연자
- 비만인
- 동물과의 접촉

**진료:**
- 약물
- 증세 완화제
- 호흡 지원 장치
- 입원

**예방:**
- 예방 접제
- 거리 확보
- 손씻기
- 마스킹
- 면역력 향상제
```
비동기 스트리밍 지원을 위한 예시입니다. 위에서 생성한 단일 chain을 통해 모든 것이 가능합니다.
- topic변수에 ""Space travel""이라는 주제를 딕셔너리 형태로 할당합니다.
- chain.astream()메서드를 사용하여topic을 입력으로 받아 비동기적으로 처리합니다.
- astream()메서드는 AsyncIterator를 반환하므로, async for 루프를 사용하여 처리 결과를 청크 단위로 반복적으로 받아옵니다.
- 각 청크는chunks변수에 할당되며,print()함수를 사용하여 출력합니다.
topic변수에 ""Space travel""이라는 주제를 딕셔너리 형태로 할당합니다.
chain.astream()메서드를 사용하여topic을 입력으로 받아 비동기적으로 처리합니다.
astream()메서드는 AsyncIterator를 반환하므로, async for 루프를 사용하여 처리 결과를 청크 단위로 반복적으로 받아옵니다.
각 청크는chunks변수에 할당되며,print()함수를 사용하여 출력합니다.
```
Copytopic = {""topic"":""LangChain""}# 주제를 ""Space travel""로 설정합니다.asyncforchunksinchain.astream(
    topic
):# 비동기적으로 체인을 실행하여 청크 단위로 결과를 반환합니다.print(chunks, end="""", flush=True)# 각 청크를 출력합니다.
```
```
LangChain은 자연어 처리(NLP)와 인공지식(AI)를 결합하여 언어 모델을 사용하여 다양한 작업을 수행할 수 있는 Python 라이브러리를 의미합니다.

**주요 특징:**

* **언어 모델:** Transformer 기반 모델, BERT, GPT-3 등 다양한 언어 모델을 사용하여 텍스트 분석, 문법 분석, 자연어 추론 등 다양한 작업에 사용할 수 있습니다.
* **연결성:** Python, Java, R 등 다양한 언어에서 사용할 수 있습니다.
* **이용성:** 간단한 API를 사용하여 모델을 쉽게 사용할 수 있습니다.
* **확장성:** 모델의 확장성을 향상시켜 더 많은 작업에 사용할 수 있습니다.

**주요 기능:**

* 텍스트 분석
* 문법 분석
* 자연어 추론
* 코드 분석
* 이미지 분석

**사용 사례:**

* 기사 요약
* 문법 검증
* 코드 분석
* 제품 리뷰 분석
* 정보 추출

LangChain은 NLP 및 AI를 사용하여 다양한 작업을 수행할 수 있는 강력한 도구입니다. 간편한 사용성과 확장성으로 많은 개발자들이 사용할 수 있습니다.
```
LangChain Expressive Language (LCEL) 인터페이스를 살펴보면, 체인을 생성할 때 사용할 수 있는 다른 인터페이스를 확인할 수 있습니다.
## 소스에서 빌드하기
소스에서 빌드하는 방법에 대한 최신 지침은 Ollama 문서의Building from Source섹션을 참조하세요.
## Extraction
Ollama의 최신 버전을 사용하고format플래그를 제공하세요.
format플래그는 모델이 JSON 형식으로 응답을 생성하도록 강제합니다.
참고:편의를 위해 실험적인OllamaFunctions래퍼를 시도해 볼 수도 있습니다.
- ChatOllama클래스를 사용하여llm객체를 생성합니다.
- model매개변수를 ""llama2""로 설정하여 Llama2 모델을 사용합니다.
- format매개변수를 ""json""으로 설정하여 입출력 형식을 JSON으로 지정합니다.
- temperature매개변수를 0으로 설정하여 모델의 출력을 결정론적으로 만듭니다.
ChatOllama클래스를 사용하여llm객체를 생성합니다.
model매개변수를 ""llama2""로 설정하여 Llama2 모델을 사용합니다.
format매개변수를 ""json""으로 설정하여 입출력 형식을 JSON으로 지정합니다.
temperature매개변수를 0으로 설정하여 모델의 출력을 결정론적으로 만듭니다.
```
Copyfromlangchain_community.chat_modelsimportChatOllama

llm = ChatOllama(
    model=""gemma:7b"",# 사용할 언어 모델을 지정합니다.format=""json"",# 입출력 형식을 JSON으로 설정합니다.temperature=0,# 샘플링 온도를 0으로 설정하여 결정론적인 출력을 생성합니다.)
```
- HumanMessage클래스를 사용하여 사용자의 질문을 담은 메시지 객체를 생성합니다.
- 생성된HumanMessage객체를messages리스트에 저장합니다.
- llm.invoke()메서드를 호출하여messages리스트를 언어 모델에 전달하고 응답을 생성합니다.
- 언어 모델의 응답을chat_model_response변수에 저장합니다.
- print()함수를 사용하여chat_model_response를 출력합니다.
HumanMessage클래스를 사용하여 사용자의 질문을 담은 메시지 객체를 생성합니다.
생성된HumanMessage객체를messages리스트에 저장합니다.
llm.invoke()메서드를 호출하여messages리스트를 언어 모델에 전달하고 응답을 생성합니다.
언어 모델의 응답을chat_model_response변수에 저장합니다.
print()함수를 사용하여chat_model_response를 출력합니다.
```
Copyfromlangchain_core.messagesimportHumanMessage

messages = [
    HumanMessage(# 질문 메시지의 내용을 한글로 번역하였습니다.content=""Tell me 10 places to travel in Europe. resonse in JSON format."")
]

chat_model_response = llm.invoke(messages)# 언어 모델을 호출하여 응답을 생성합니다.print(chat_model_response.content)# 생성된 응답을 출력합니다.
```
```
{
  ""places"": [
    ""Paris, France"",
    ""Rome, Italy"",
    ""Barcelona, Spain"",
    ""London, UK"",
    ""Amsterdam, Netherlands"",
    ""Venice, Italy"",
    ""Lisbon, Portugal"",
    ""Bruges, Belgium"",
    ""Dublin, Ireland"",
    ""Cologne, Germany""
  ]
}
```
- JSON 스키마를 정의하여 사람에 대한 식별 정보를 나타냅니다.
- 스키마에는name,age,fav_food속성이 포함되어 있으며,name과age는 필수 항목입니다.
- ChatOllama언어 모델을 초기화합니다.
- 사용자 메시지를 정의하는messages리스트를 생성합니다.
- 첫 번째 메시지는 JSON 스키마를 사용하여 사람에 대해 설명해달라는 요청입니다.
- 두 번째 메시지는 JSON 스키마를 문자열로 포함합니다.
- 세 번째 메시지는 주어진 스키마를 고려하여 John이라는 이름의 35세 피자를 좋아하는 사람에 대해 설명해달라는 요청입니다.
- ChatPromptTemplate을 사용하여messages로부터 프롬프트를 생성합니다.
- json.dumps()를 사용하여 JSON 스키마를 문자열로 변환합니다.
- 프롬프트, 언어 모델,StrOutputParser를 파이프라인으로 연결하여chain을 생성합니다.
- chain.invoke()를 호출하여 JSON 스키마를 포함한 입력을 전달하고 결과를 출력합니다.
JSON 스키마를 정의하여 사람에 대한 식별 정보를 나타냅니다.
스키마에는name,age,fav_food속성이 포함되어 있으며,name과age는 필수 항목입니다.
ChatOllama언어 모델을 초기화합니다.
사용자 메시지를 정의하는messages리스트를 생성합니다.
첫 번째 메시지는 JSON 스키마를 사용하여 사람에 대해 설명해달라는 요청입니다.
두 번째 메시지는 JSON 스키마를 문자열로 포함합니다.
세 번째 메시지는 주어진 스키마를 고려하여 John이라는 이름의 35세 피자를 좋아하는 사람에 대해 설명해달라는 요청입니다.
ChatPromptTemplate을 사용하여messages로부터 프롬프트를 생성합니다.
json.dumps()를 사용하여 JSON 스키마를 문자열로 변환합니다.
프롬프트, 언어 모델,StrOutputParser를 파이프라인으로 연결하여chain을 생성합니다.
chain.invoke()를 호출하여 JSON 스키마를 포함한 입력을 전달하고 결과를 출력합니다.
```
Copyimportjsonfromlangchain_community.chat_modelsimportChatOllamafromlangchain_core.messagesimportHumanMessagefromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplate

json_schema = {""title"":""Person"",""description"":""Identifying information about a person."",""type"":""object"",""properties"": {""name"": {""title"":""Name"",""description"":""The person's name"",""type"":""string""},""age"": {""title"":""Age"",""description"":""The person's age"",""type"":""integer""},""occupation"": {""title"":""Occupation"",""description"":""The person's Occupation"",""type"":""string"",
        },
    },""required"": [""name"",""age""],
}

llm = ChatOllama(model=""gemma:7b"")# ChatOllama 언어 모델 초기화messages = [
    HumanMessage(# JSON 스키마를 사용하여 사람에 대해 설명해달라는 요청 메시지content=""Please tell me about a person using the following JSON schema:""),
    HumanMessage(content=""{dumps}""),# JSON 스키마를 메시지로 전달HumanMessage(# 스키마를 고려하여 John이라는 35세의 피자를 좋아하는 사람에 대해 설명해달라는 요청 메시지content=""""""Now, considering the schema, please describe following person:
        Her name is Eun-Chae Lee, she is 25 years old, and she is a software engineer.
        """"""),
]

prompt = ChatPromptTemplate.from_messages(
    messages
)# 메시지들로부터 ChatPromptTemplate 생성dumps = json.dumps(json_schema, indent=2)# JSON 스키마를 문자열로 변환chain = (
    prompt | llm | StrOutputParser()
)# 프롬프트, 언어 모델, 출력 파서를 연결하여 체인 생성print(chain.invoke({""dumps"": dumps}))# 체인을 호출하여 결과 출력
```
```
Sure, here's the description:

Eun-Chae Lee, a 25-year-old software engineer, is using the JSON schema shown below:Copy{""name"":""Eun-Chae Lee"",""age"":25,""occupation"":""Software Engineer""}As you can see, the JSON schema defines a person with the name ""Eun-Chae Lee"", age ""25"", and occupation ""Software Engineer"". Eun-Chae is using this JSON schema to represent herself in a structured data format. This JSON data can be easily shared and understood by other systems.
```
```
Copy{""name"":""Eun-Chae Lee"",""age"":25,""occupation"":""Software Engineer""}
```
## Multi-modal
Ollama는bakllava와llava와 같은 멀티모달 LLM을 지원합니다.
tags를 사용하여Llava와 같은 모델의 전체 버전 세트를 탐색할 수 있습니다.
ollama pull bakllava명령어를 통해 원하는 LLM을 다운로드하세요.
멀티모달을 지원하는 최신 버전을 사용하려면 Ollama를 업데이트해야 합니다.
아래에서 ChatOllama 멀티모달 지원을 사용하는 일반적인 예시를 확인하세요.
- Pillow 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다.
- --upgrade옵션을 사용하여 이미 설치된 Pillow 라이브러리를 최신 버전으로 업그레이드합니다.
- --quiet옵션을 사용하여 설치 과정에서 출력되는 메시지를 최소화합니다.
Pillow 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다.
--upgrade옵션을 사용하여 이미 설치된 Pillow 라이브러리를 최신 버전으로 업그레이드합니다.
--quiet옵션을 사용하여 설치 과정에서 출력되는 메시지를 최소화합니다.
```
Copy%pip install --upgrade --quiet  pillow
```
PIL 이미지를 Base64 인코딩된 문자열로 변환하고 이를 HTML에 포함하여 이미지를 표시하는 함수를 제공합니다.
- convert_to_base64함수:
- PIL 이미지를 입력으로 받습니다.
- 이미지를 JPEG 형식으로 BytesIO 버퍼에 저장합니다.
- 버퍼의 값을 Base64로 인코딩하고 문자열로 반환합니다.
- plt_img_base64함수:
- Base64 인코딩된 문자열을 입력으로 받습니다.
- Base64 문자열을 소스로 사용하는 HTML<img>태그를 생성합니다.
- HTML을 렌더링하여 이미지를 표시합니다.
- 사용 예시:
- 지정된 파일 경로에서 PIL 이미지를 열어pil_image에 저장합니다.
- convert_to_base64함수를 사용하여pil_image를 Base64 인코딩된 문자열로 변환합니다.
- plt_img_base64함수를 사용하여 Base64 인코딩된 문자열을 이미지로 표시합니다.
convert_to_base64함수:
convert_to_base64함수:
PIL 이미지를 입력으로 받습니다.
PIL 이미지를 입력으로 받습니다.
이미지를 JPEG 형식으로 BytesIO 버퍼에 저장합니다.
버퍼의 값을 Base64로 인코딩하고 문자열로 반환합니다.
버퍼의 값을 Base64로 인코딩하고 문자열로 반환합니다.
plt_img_base64함수:
plt_img_base64함수:
Base64 인코딩된 문자열을 입력으로 받습니다.
Base64 인코딩된 문자열을 입력으로 받습니다.
Base64 문자열을 소스로 사용하는 HTML<img>태그를 생성합니다.
HTML을 렌더링하여 이미지를 표시합니다.
HTML을 렌더링하여 이미지를 표시합니다.
사용 예시:
사용 예시:
지정된 파일 경로에서 PIL 이미지를 열어pil_image에 저장합니다.
convert_to_base64함수를 사용하여pil_image를 Base64 인코딩된 문자열로 변환합니다.
plt_img_base64함수를 사용하여 Base64 인코딩된 문자열을 이미지로 표시합니다.
```
Copyimportbase64fromioimportBytesIOfromIPython.displayimportHTML, displayfromPILimportImagedefconvert_to_base64(pil_image):""""""
    PIL 이미지를 Base64로 인코딩된 문자열로 변환합니다.

    :param pil_image: PIL 이미지
    :return: 크기 조정된 Base64 문자열
    """"""buffered = BytesIO()
    pil_image.save(buffered,format=""JPEG"")# 필요한 경우 형식을 변경할 수 있습니다.img_str = base64.b64encode(buffered.getvalue()).decode(""utf-8"")returnimg_strdefplt_img_base64(img_base64):""""""
    Base64로 인코딩된 문자열을 이미지로 표시합니다.

    :param img_base64:  Base64 문자열
    """"""# Base64 문자열을 소스로 사용하여 HTML img 태그 생성image_html =f'<img src=""data:image/jpeg;base64,{img_base64}"" />'# HTML을 렌더링하여 이미지 표시display(HTML(image_html))


file_path =""./images/jeju-beach.jpg""pil_image = Image.open(file_path)

image_b64 = convert_to_base64(pil_image)
plt_img_base64(image_b64)
```
-ChatOllama언어 모델을 사용하여 이미지와 텍스트 기반 질의에 대한 답변을 생성하는 체인을 구현합니다.
-prompt_func함수는 이미지와 텍스트 데이터를 입력으로 받아HumanMessage형식으로 변환합니다.
  - 이미지 데이터는 Base64 인코딩된 JPEG 형식으로 전달됩니다.
  - 텍스트 데이터는 일반 텍스트로 전달됩니다.
-StrOutputParser를 사용하여 언어 모델의 출력을 문자열로 파싱합니다.
-prompt_func,llm,StrOutputParser를 파이프라인으로 연결하여chain을 생성합니다.
-chain.invoke메서드를 호출하여 이미지와 텍스트 질의를 전달하고 답변을 생성합니다.
- 생성된 답변을 출력합니다.
```
Copyfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_community.chat_modelsimportChatOllamafromlangchain_core.messagesimportHumanMessage# ChatOllama 언어 모델을 초기화하고 온도를 0으로 설정합니다.llm = ChatOllama(model=""llava:7b"", temperature=0)defprompt_func(data):# 프롬프트 함수를 정의합니다.text = data[""text""]# 데이터에서 텍스트를 가져옵니다.image = data[""image""]# 데이터에서 이미지를 가져옵니다.image_part = {# 이미지 부분을 정의합니다.""type"":""image_url"",# 이미지 URL 타입을 지정합니다.""image_url"":f""data:image/jpeg;base64,{image}"",# 이미지 URL을 생성합니다.}

    content_parts = []# 콘텐츠 부분을 저장할 리스트를 초기화합니다.text_part = {""type"":""text"",""text"": text}# 텍스트 부분을 정의합니다.content_parts.append(image_part)# 이미지 부분을 콘텐츠 부분에 추가합니다.content_parts.append(text_part)# 텍스트 부분을 콘텐츠 부분에 추가합니다.return[HumanMessage(content=content_parts)]# HumanMessage 객체를 반환합니다.# 프롬프트 함수, 언어 모델, 출력 파서를 연결하여 체인을 생성합니다.chain = prompt_func | llm | StrOutputParser()

query_chain = chain.invoke(# 체인을 호출하여 쿼리를 실행합니다.# 텍스트와 이미지를 전달합니다.{""text"":""Describe a picture in bullet points."",""image"": image_b64}
)print(query_chain)# 쿼리 결과를 출력합니다.
```
```
- The image shows a picturesque tropical beach scene.
- In the foreground, there is a rocky shore with clear blue water and white foam from waves crashing onto the rocks.
- A small island or landmass is visible in the background, surrounded by the ocean.
- The sky is clear and blue, suggesting good weather conditions.
- There are no people visible in the image.
- The overall style of the image is a natural landscape photograph with vibrant colors and clear details.
```"
10. GPT4ALL,https://wikidocs.net/233806,"# 10. GPT4ALL


GitHub:nomic-ai/gpt4all은 코드, 스토리, 대화를 포함한 방대한 양의 깨끗한 어시스턴트 데이터로 학습된 오픈 소스 챗봇 생태계입니다.
이 예제에서는 LangChain을 사용하여GPT4All모델과 상호 작용하는 방법에 대해 설명합니다.
## 설치방법
먼저, 공식 홈페이지에 접속하여 설치파일을 다운로드 받아 설치합니다
공식 홈페이지바로가기
파이썬 패키지를 설치합니다.
pip 를 활용한 설치 방법
%pip매직 명령어를 사용하여gpt4all패키지를 최신 버전으로 업그레이드합니다.
[참고]
- > /dev/null리다이렉션을 통해 설치 과정에서 발생하는 출력을/dev/null디바이스로 보내 출력을 숨깁니다.
> /dev/null리다이렉션을 통해 설치 과정에서 발생하는 출력을/dev/null디바이스로 보내 출력을 숨깁니다.
```
Copy%pip install -qU gpt4all > /dev/null
```
```
Note: you may need to restart the kernel to use updated packages.
```
## 모델 다운로드

gpt4all 페이지에는Model Explorer섹션이 있습니다.
(더 많은 정보를 원하시면 https://github.com/nomic-ai/gpt4all 을 방문하세요.)
공식 홈페이지에서 다운로드 가능한 모델을 다운로드 받습니다. 본인의 PC 사양에서 구동가능한 모델을 선택하는 것이 좋습니다.
본 튜토리얼에서는nous-hermes-llama2-13b.Q4_0.gguf(7.37GB) 모델을 다운로드 받아 진행하겠습니다.
다운로드 받은 모델은models폴더 생성 후 해당 폴더에 다운로드 받습니다.
다운로드 받은 모델은models폴더 생성 후 해당 폴더에 다운로드 받습니다.
local_path변수에 로컬 파일 경로(""./models/ggml-gpt4all-l13b-snoozy.bin"")를 할당합니다.
local_path변수에 로컬 파일 경로(""./models/ggml-gpt4all-l13b-snoozy.bin"")를 할당합니다.
이 경로는 사용자가 원하는 로컬 파일 경로로 대체할 수 있습니다.
```
Copylocal_path = (""./models/nous-hermes-llama2-13b.Q4_0.gguf""# 원하는 로컬 파일 경로로 대체하세요.)
```
## 모델 정보 설정
로컬에서 실행하려면 호환되는 ggml 형식의 모델을 다운로드하세요.
- 관심 있는 모델을 선택하세요.
- UI를 사용하여 다운로드하고.bin파일을local_path(아래 참고)로 이동시키세요.
관심 있는 모델을 선택하세요.
UI를 사용하여 다운로드하고.bin파일을local_path(아래 참고)로 이동시키세요.
GPT4All은 GPT-3와 유사한 대규모 언어 모델로, 다양한 자연어 처리 작업에 활용될 수 있습니다.
이 모듈을 사용하면 GPT4All 모델을 간편하게 로드하고 추론에 활용할 수 있습니다.
- GPT4All언어 모델을 사용하여 프롬프트에 대한 응답을 생성하는LLMChain을 구현합니다.
- PromptTemplate을 사용하여 프롬프트 템플릿을 정의합니다.
- StreamingStdOutCallbackHandler를 사용하여 언어 모델의 출력을 실시간으로 스트리밍합니다.
- 사용자 정의 모델을 사용하려면backend매개변수를 추가합니다. 지원되는 백엔드는 GPT4All Python 문서에서 확인할 수 있습니다.
GPT4All언어 모델을 사용하여 프롬프트에 대한 응답을 생성하는LLMChain을 구현합니다.
PromptTemplate을 사용하여 프롬프트 템플릿을 정의합니다.
StreamingStdOutCallbackHandler를 사용하여 언어 모델의 출력을 실시간으로 스트리밍합니다.
사용자 정의 모델을 사용하려면backend매개변수를 추가합니다. 지원되는 백엔드는 GPT4All Python 문서에서 확인할 수 있습니다.
```
Copyfromlangchain.promptsimportPromptTemplatefromlangchain_community.llmsimportGPT4Allfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.callbacks.streaming_stdoutimportStreamingStdOutCallbackHandler# 프롬프트 템플릿 정의prompt = PromptTemplate(
    input_variables=[""product""],
    template=""Name any five companies which makes `{product}`?"",
)# GPT4All 언어 모델 초기화# model는 GPT4All 모델 파일의 경로를 지정llm = GPT4All(
    model=local_path,
    callbacks=[StreamingStdOutCallbackHandler()],# backend=""gpu"", # GPU 설정streaming=True,
    verbose=True,
)# 체인 생성chain = prompt | llm | StrOutputParser()# 질의 실행response = chain.invoke({""product"":""Smart phone""})
```
```
There are many companies that make smartphones, but some of the most popular ones include: 1. Apple Inc. (iPhone) 2. Samsung Electronics Co., Ltd. (Galaxy series) 3. Huawei Technologies Co., Ltd. (P30 and P20 series) 4. Xiaomi Corporation (Mi series) 5. Google LLC (Pixel series).
```"
CH05 메모리(Memory),https://wikidocs.net/233773,"# CH05 메모리(Memory)

memory"
01. 대화 버퍼 메모리(ConversationBufferMemory),https://wikidocs.net/233801,"# 01. 대화 버퍼 메모리(ConversationBufferMemory)

이 메모리는 메시지를 저장한 다음 변수에 메시지를 추출할 수 있게 해줍니다.
먼저 문자열로 추출할 수 있습니다.
```
Copyfromlangchain.memoryimportConversationBufferMemory
```
```
Copymemory = ConversationBufferMemory()
memory.save_context(
    inputs={""human"":""안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?""},
    outputs={""ai"":""안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?""},
)
```
memory 의load_memory_variables({})함수는 메시지 히스토리를 반환합니다.
```
Copy# 'history' 키에 저장된 대화 기록을 확인합니다.memory.load_memory_variables({})
```
```
{'history': 'Human: 안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?\nAI: 안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?'}
```
save_context(inputs, outputs)메서드를 사용하여 대화 기록을 저장할 수 있습니다.
- 이 메서드는inputs와outputs두 개의 인자를 받습니다.
- inputs는 사용자의 입력을,outputs는 AI의 출력을 저장합니다.
- 이 메서드를 사용하면 대화 기록이history키에 저장됩니다.
- 이후load_memory_variables메서드를 사용하여 저장된 대화 기록을 확인할 수 있습니다.
이 메서드는inputs와outputs두 개의 인자를 받습니다.
inputs는 사용자의 입력을,outputs는 AI의 출력을 저장합니다.
이 메서드를 사용하면 대화 기록이history키에 저장됩니다.
이후load_memory_variables메서드를 사용하여 저장된 대화 기록을 확인할 수 있습니다.
```
Copy# inputs: dictionary(key: ""human"" or ""ai"", value: 질문)# outputs: dictionary(key: ""ai"" or ""human"", value: 답변)memory.save_context(
    inputs={""human"":""네, 신분증을 준비했습니다. 이제 무엇을 해야 하나요?""},
    outputs={""ai"":""감사합니다. 신분증 앞뒤를 명확하게 촬영하여 업로드해 주세요. 이후 본인 인증 절차를 진행하겠습니다.""},
)
```
```
Copy# 2개의 대화를 저장합니다.memory.save_context(
    inputs={""human"":""사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?""},
    outputs={""ai"":""업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다.""},
)
memory.save_context(
    inputs={""human"":""인증번호를 입력했습니다. 계좌 개설은 이제 어떻게 하나요?""},
    outputs={""ai"":""본인 인증이 완료되었습니다. 이제 원하시는 계좌 종류를 선택하고 필요한 정보를 입력해 주세요. 예금 종류, 통화 종류 등을 선택할 수 있습니다.""},
)
```
```
Copy# history에 저장된 대화 기록을 확인합니다.print(memory.load_memory_variables({})[""history""])
```
```
Human: 안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?
AI: 안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?
Human: 사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?
AI: 업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다.
Human: 인증번호를 입력했습니다. 계좌 개설은 이제 어떻게 하나요?
AI: 본인 인증이 완료되었습니다. 이제 원하시는 계좌 종류를 선택하고 필요한 정보를 입력해 주세요. 예금 종류, 통화 종류 등을 선택할 수 있습니다.
```
```
Copy# 추가로 2개의 대화를 저장합니다.memory.save_context(
    inputs={""human"":""정보를 모두 입력했습니다. 다음 단계는 무엇인가요?""},
    outputs={""ai"":""입력해 주신 정보를 확인했습니다. 계좌 개설 절차가 거의 끝났습니다. 마지막으로 이용 약관에 동의해 주시고, 계좌 개설을 최종 확인해 주세요.""},
)
memory.save_context(
    inputs={""human"":""모든 절차를 완료했습니다. 계좌가 개설된 건가요?""},
    outputs={""ai"":""네, 계좌 개설이 완료되었습니다. 고객님의 계좌 번호와 관련 정보는 등록하신 이메일로 발송되었습니다. 추가적인 도움이 필요하시면 언제든지 문의해 주세요. 감사합니다!""},
)
```
```
Copy# history에 저장된 대화 기록을 확인합니다.print(memory.load_memory_variables({})[""history""])
```
```
Human: 안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?
AI: 안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?
Human: 사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?
AI: 업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다.
Human: 인증번호를 입력했습니다. 계좌 개설은 이제 어떻게 하나요?
AI: 본인 인증이 완료되었습니다. 이제 원하시는 계좌 종류를 선택하고 필요한 정보를 입력해 주세요. 예금 종류, 통화 종류 등을 선택할 수 있습니다.
Human: 정보를 모두 입력했습니다. 다음 단계는 무엇인가요?
AI: 입력해 주신 정보를 확인했습니다. 계좌 개설 절차가 거의 끝났습니다. 마지막으로 이용 약관에 동의해 주시고, 계좌 개설을 최종 확인해 주세요.
Human: 모든 절차를 완료했습니다. 계좌가 개설된 건가요?
AI: 네, 계좌 개설이 완료되었습니다. 고객님의 계좌 번호와 관련 정보는 등록하신 이메일로 발송되었습니다. 추가적인 도움이 필요하시면 언제든지 문의해 주세요. 감사합니다!
```
return_messages=True로 설정하면HumanMessage와AIMessage객체를 반환합니다.
```
Copymemory = ConversationBufferMemory(return_messages=True)

memory.save_context(
    inputs={""human"":""안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?""},
    outputs={""ai"":""안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?""},
)

memory.save_context(
    inputs={""human"":""네, 신분증을 준비했습니다. 이제 무엇을 해야 하나요?""},
    outputs={""ai"":""감사합니다. 신분증 앞뒤를 명확하게 촬영하여 업로드해 주세요. 이후 본인 인증 절차를 진행하겠습니다.""},
)

memory.save_context(
    inputs={""human"":""사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?""},
    outputs={""ai"":""업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다.""},
)
```
```
Copy# history에 저장된 대화 기록을 확인합니다.memory.load_memory_variables({})[""history""]
```
```
[HumanMessage(content='안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?'), AIMessage(content='안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?'), HumanMessage(content='네, 신분증을 준비했습니다. 이제 무엇을 해야 하나요?'), AIMessage(content='감사합니다. 신분증 앞뒤를 명확하게 촬영하여 업로드해 주세요. 이후 본인 인증 절차를 진행하겠습니다.'), HumanMessage(content='사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?'), AIMessage(content='업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다.')]
```
## Chain 에 적용
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.chainsimportConversationChain# LLM 모델을 생성합니다.llm = ChatOpenAI(temperature=0)# ConversationChain을 생성합니다.conversation = ConversationChain(# ConversationBufferMemory를 사용합니다.llm=llm,
    memory=ConversationBufferMemory(),
)
```
ConversationChain을 사용하여 대화를 진행합니다.
```
Copy# 대화를 시작합니다.response = conversation.predict(input=""안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?"")print(response)
```
```
안녕하세요! 은행 계좌를 개설하려면 먼저 해당 은행의 공식 웹사이트에 접속하셔서 온라인 개설 절차를 따라야 합니다. 보통 개인 정보, 신분증 사본, 주소증명서 등의 문서를 제출해야 하며, 온라인 양식을 작성하고 전자 서명을 해야 합니다. 그 후에 은행에서 제공하는 안내에 따라 추가 단계를 진행하시면 됩니다. 혹시 어떤 은행을 고려하고 계신가요?
```
이전의 대화 기록을 기억하고 있는지 확인합니다.
```
Copy# 이전 대화내용을 불렛포인트로 정리해 달라는 요청을 보냅니다.response = conversation.predict(input=""이전 답변을 불렛포인트 형식으로 정리하여 알려주세요."")print(response)
```
```
1. 해당 은행의 공식 웹사이트에 접속
2. 온라인 개설 절차 따르기
3. 개인 정보, 신분증 사본, 주소증명서 등 제출
4. 온라인 양식 작성 및 전자 서명
5. 은행 안내에 따라 추가 단계 진행
```"
02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory),https://wikidocs.net/233800,"# 02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory)

ConversationBufferWindowMemory는 시간이 지남에 따라 대화의 상호작용 목록을 유지합니다.
이때,ConversationBufferWindowMemory는 모든 대화내용을 활용하는 것이 아닌최근 K개의 상호작용만 사용합니다.
이는 버퍼가 너무 커지지 않도록 가장 최근 상호작용의 슬라이딩 창을 유지하는 데 유용할 수 있습니다.
```
Copyfromlangchain.memoryimportConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=2, return_messages=True)

memory.save_context(
    inputs={""human"":""안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?""},
    outputs={""ai"":""안녕하세요! 계좌 개설을 원하신다니 기쁩니다. 먼저, 본인 인증을 위해 신분증을 준비해 주시겠어요?""},
)
memory.save_context(
    inputs={""human"":""네, 신분증을 준비했습니다. 이제 무엇을 해야 하나요?""},
    outputs={""ai"":""감사합니다. 신분증 앞뒤를 명확하게 촬영하여 업로드해 주세요. 이후 본인 인증 절차를 진행하겠습니다.""},
)
memory.save_context(
    inputs={""human"":""사진을 업로드했습니다. 본인 인증은 어떻게 진행되나요?""},
    outputs={""ai"":""업로드해 주신 사진을 확인했습니다. 이제 휴대폰을 통한 본인 인증을 진행해 주세요. 문자로 발송된 인증번호를 입력해 주시면 됩니다.""},
)
memory.save_context(
    inputs={""human"":""인증번호를 입력했습니다. 계좌 개설은 이제 어떻게 하나요?""},
    outputs={""ai"":""본인 인증이 완료되었습니다. 이제 원하시는 계좌 종류를 선택하고 필요한 정보를 입력해 주세요. 예금 종류, 통화 종류 등을 선택할 수 있습니다.""},
)
memory.save_context(
    inputs={""human"":""정보를 모두 입력했습니다. 다음 단계는 무엇인가요?""},
    outputs={""ai"":""입력해 주신 정보를 확인했습니다. 계좌 개설 절차가 거의 끝났습니다. 마지막으로 이용 약관에 동의해 주시고, 계좌 개설을 최종 확인해 주세요.""},
)
memory.save_context(
    inputs={""human"":""모든 절차를 완료했습니다. 계좌가 개설된 건가요?""},
    outputs={""ai"":""네, 계좌 개설이 완료되었습니다. 고객님의 계좌 번호와 관련 정보는 등록하신 이메일로 발송되었습니다. 추가적인 도움이 필요하시면 언제든지 문의해 주세요. 감사합니다!""},
)
```
대화기록을 확인해 보면최근 2개의 메시지만 반환하는 것을 확인할 수 있습니다.
```
Copy# 대화 기록을 확인합니다.memory.load_memory_variables({})[""history""]
```
```
[HumanMessage(content='정보를 모두 입력했습니다. 다음 단계는 무엇인가요?'), AIMessage(content='입력해 주신 정보를 확인했습니다. 계좌 개설 절차가 거의 끝났습니다. 마지막으로 이용 약관에 동의해 주시고, 계좌 개설을 최종 확인해 주세요.'), HumanMessage(content='모든 절차를 완료했습니다. 계좌가 개설된 건가요?'), AIMessage(content='네, 계좌 개설이 완료되었습니다. 고객님의 계좌 번호와 관련 정보는 등록하신 이메일로 발송되었습니다. 추가적인 도움이 필요하시면 언제든지 문의해 주세요. 감사합니다!')]
```"
03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory),https://wikidocs.net/233807,"# 03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory)

ConversationTokenBufferMemory는 최근 대화의 히스토리를 버퍼를 메모리에 보관하고, 대화의 개수가 아닌토큰 길이를 사용하여 대화내용을 플러시(flush)할 시기를 결정합니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
- max_token_limit: 대화 내용을 저장할 최대 토큰의 길이를 설정합니다.
max_token_limit: 대화 내용을 저장할 최대 토큰의 길이를 설정합니다.
```
Copyfromlangchain.memoryimportConversationTokenBufferMemoryfromlangchain_openaiimportChatOpenAI# LLM 모델 생성llm = ChatOpenAI()# 메모리 설정memory = ConversationTokenBufferMemory(
    llm=llm, max_token_limit=150, return_messages=True# 최대 토큰 길이를 50개로 제한)
```
임의의 대화를 추가합니다.
```
Copymemory.save_context(
    inputs={""human"":""안녕하세요, 저는 최근에 여러분 회사의 공작 기계를 구매했습니다. 설치 방법을 알려주실 수 있나요?""},
    outputs={""ai"":""안녕하세요! 구매해 주셔서 감사합니다. 해당 기계 모델 번호를 알려주시겠어요?""},
)
memory.save_context(
    inputs={""human"":""네, 모델 번호는 XG-200입니다.""},
    outputs={""ai"":""감사합니다. XG-200 모델의 설치 안내를 도와드리겠습니다. 먼저, 설치할 장소의 전원 공급 상태를 확인해주세요. 기계는 220V 전원이 필요합니다.""},
)
memory.save_context(
    inputs={""human"":""전원은 확인했습니다. 다음 단계는 무엇인가요?""},
    outputs={""ai"":""좋습니다. 다음으로, 기계를 평평하고 안정된 바닥에 배치해 주세요. 이후, 제공된 사용자 매뉴얼에 따라 케이블 연결을 진행해 주시기 바랍니다.""},
)
memory.save_context(
    inputs={""human"":""연결은 어떻게 하나요?""},
    outputs={""ai"":""매뉴얼의 5페이지를 참조해 주세요. 케이블 연결에 관한 상세한 지침이 있습니다. 이 과정에서 어려움이 있으시면 추가적으로 도와드리겠습니다.""},
)
memory.save_context(
    inputs={""human"":""설치가 완료되면 어떻게 해야 하나요?""},
    outputs={""ai"":""설치가 완료되면, 전원을 켜고 초기 구동 테스트를 진행해 주시기 바랍니다. 테스트 절차는 매뉴얼의 10페이지에 설명되어 있습니다. 만약 기계에 이상이 있거나 추가적인 지원이 필요하시면 언제든지 연락 주시기 바랍니다.""},
)
memory.save_context(
    inputs={""human"":""감사합니다, 도움이 많이 되었어요!""},
    outputs={""ai"":""언제든지 도와드릴 준비가 되어 있습니다. 추가적인 질문이나 지원이 필요하시면 언제든지 문의해 주세요. 좋은 하루 되세요!""},
)
```
최대 토큰의 길이를150으로 설정하고 대화를 저장했을 때 어떻게 동작하는지 확인해 보겠습니다.
```
Copy# 대화내용을 확인합니다.memory.load_memory_variables({})[""history""]
```
```
[HumanMessage(content='감사합니다, 도움이 많이 되었어요!'), AIMessage(content='언제든지 도와드릴 준비가 되어 있습니다. 추가적인 질문이나 지원이 필요하시면 언제든지 문의해 주세요. 좋은 하루 되세요!')]
```"
04. 대화 엔티티 메모리(ConversationEntityMemory),https://wikidocs.net/233808,"# 04. 대화 엔티티 메모리(ConversationEntityMemory)

엔티티 메모리는 대화에서 특정 엔티티에 대한 주어진 사실을 기억합니다.
엔티티 메모리는 엔티티에 대한 정보를 추출하고(LLM 사용) 시간이 지남에 따라 해당 엔티티에 대한 지식을 축적합니다(역시 LLM 사용).
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.chainsimportConversationChainfromlangchain.memoryimportConversationEntityMemoryfromlangchain.memory.promptimportENTITY_MEMORY_CONVERSATION_TEMPLATE
```
Entity 메모리를 효과적으로 사용하기 위하여, 제공되는 프롬프트를 사용합니다.
```
Copy# Entity Memory를 사용하는 프롬프트 내용을 출력합니다.print(ENTITY_MEMORY_CONVERSATION_TEMPLATE.template)
```
```
You are an assistant to a human, powered by a large language model trained by OpenAI.

You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Context:
{entities}

Current conversation:
{history}
Last line:
Human: {input}
You:
```
```
Copy# LLM 을 생성합니다.llm = ChatOpenAI(temperature=0)# ConversationChain 을 생성합니다.conversation = ConversationChain(
    llm=llm,
    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,
    memory=ConversationEntityMemory(llm=llm),
)
```
대화를 시작합니다.
입력한 대화를 바탕으로ConversationEntityMemory는 주요 Entity 정보를 별도록 저장합니다.
```
Copyconversation.predict(input=""테디와 셜리는 한 회사에서 일하는 동료입니다.""""테디는 개발자이고 셜리는 디자이너입니다. """"그들은최근 회사에서 일하는 것을 그만두고 자신들의 회사를 차릴 계획을 세우고 있습니다."")
```
```
'테디와 셜리가 한 회사에서 일하는 동료이고, 테디는 개발자이고 셜리는 디자이너라는 정보를 알려주셨군요. 그들이 회사를 차릴 계획을 세우고 있다는 것은 흥미로운 소식입니다. 자신들의 아이디어와 역량을 발휘하여 새로운 사업을 시작하는 것은 도전적이지만 보람 있는 경험이 될 것입니다. 테디와 셜리가 함께 더 큰 성공을 거두길 기대해봅니다. 어떤 종류의 회사를 차릴지, 어떤 서비스나 제품을 제공할지에 대해 이미 구상하고 있는 것인가요?'
```
Entity는memory.entity_store.store에서 확인할 수 있습니다.
```
Copy# entity memory 를 출력합니다.conversation.memory.entity_store.store
```
```
{'테디': '테디는 개발자이고, 셜리와 함께 자신들의 회사를 차릴 계획을 세우고 있습니다.', '셜리': '셜리는 한 회사에서 디자이너로 일하고 있으며, 테디와 함께 자신들의 회사를 차릴 계획을 세우고 있다.'}
```"
05. 대화 지식그래프 메모리(ConversationKGMemory),https://wikidocs.net/233809,"# 05. 대화 지식그래프 메모리(ConversationKGMemory)

지식 그래프의 힘을 활용하여 정보를 저장하고 불러옵니다.
이를 통해 모델이 서로 다른 개체 간의 관계를 이해하는 데 도움을 주고, 복잡한 연결망과 역사적 맥락을 기반으로 대응하는 능력을 향상시킵니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.memoryimportConversationKGMemory
```
```
Copyllm = ChatOpenAI(temperature=0)

memory = ConversationKGMemory(llm=llm, return_messages=True)
memory.save_context(
    {""input"":""이쪽은 Pangyo 에 거주중인 김셜리씨 입니다.""},
    {""output"":""김셜리씨는 누구시죠?""},
)
memory.save_context(
    {""input"":""김셜리씨는 우리 회사의 신입 디자이너입니다.""},
    {""output"":""만나서 반갑습니다.""},
)
```
```
Copymemory.load_memory_variables({""input"":""김셜리씨는 누구입니까?""})
```
```
{'history': [SystemMessage(content='On Pangyo: Pangyo has resident 김셜리씨.'),  SystemMessage(content='On 김셜리씨: 김셜리씨 is a 신입 디자이너. 김셜리씨 is in 우리 회사.')]}
```
## Chain 에 메모리 활용하기
ConversationChain에ConversationKGMemory를 메모리로 지정하여 대화를 나눈 후 memory 를 확인해 보도록 하겠습니다.
```
Copyfromlangchain.prompts.promptimportPromptTemplatefromlangchain.chainsimportConversationChain

llm = ChatOpenAI(temperature=0)

template =""""""The following is a friendly conversation between a human and an AI. 
The AI is talkative and provides lots of specific details from its context. 
If the AI does not know the answer to a question, it truthfully says it does not know. 
The AI ONLY uses information contained in the ""Relevant Information"" section and does not hallucinate.

Relevant Information:

{history}

Conversation:
Human: {input}
AI:""""""prompt = PromptTemplate(
    input_variables=[""history"",""input""], template=template)

conversation_with_kg = ConversationChain(
    llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm)
)
```
첫 번째 대화를 시작합니다. 간단한 인물에 대한 정보를 제공해 보겠습니다.
```
Copyconversation_with_kg.predict(input=""My name is Teddy. Shirley is a coworker of mine, and she's a new designer at our company."")
```
```
""Hello Teddy! It's nice to meet you. Shirley must be excited to be starting a new job as a designer at your company. I hope she's settling in well and getting to know her new colleagues. If you need any tips on how to make her feel welcome or help her adjust to the new role, feel free to ask me!""
```
Shirley 라는 사람에 대한 질문을 진행합니다.
```
Copy# Shirley 에 대한 질문conversation_with_kg.memory.load_memory_variables({""input"":""who is Shirley?""})
```
```
{'history': 'On Shirley: Shirley is a coworker. Shirley is a new designer. Shirley is at company.'}
```"
06. 대화 요약 메모리(ConversationSummaryMemory),https://wikidocs.net/233810,"# 06. 대화 요약 메모리(ConversationSummaryMemory)

이제 조금 더 복잡한 메모리 유형인ConversationSummaryMemory를 사용하는 방법을 살펴 보겠습니다.
이 유형의 메모리는 시간 경과에 따른대화의 요약을 생성합니다. 이는 시간 경과에 따른 대화의 정보를 압축하는 데 유용할 수 있습니다.
대화 요약 메모리는 대화가 진행되는 동안 대화를 요약하고현재 요약을 메모리에 저장합니다.
그런 다음 이 메모리를 사용하여 지금까지의 대화 요약을 프롬프트/체인에 삽입할 수 있습니다.
이 메모리는 과거 메시지 기록을 프롬프트에 그대로 보관하면 토큰을 너무 많이 차지할 수 있는 긴 대화에 가장 유용합니다.
ConversationSummaryMemory를 생성합니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copyfromlangchain.memoryimportConversationSummaryMemoryfromlangchain_openaiimportChatOpenAI

memory = ConversationSummaryMemory(
    llm=ChatOpenAI(temperature=0), return_messages=True)
```
여러 대화를 저장하도록 합니다.
```
Copymemory.save_context(
    inputs={""human"":""유럽 여행 패키지의 가격은 얼마인가요?""},
    outputs={""ai"":""유럽 14박 15일 패키지의 기본 가격은 3,500유로입니다. 이 가격에는 항공료, 호텔 숙박비, 지정된 관광지 입장료가 포함되어 있습니다. 추가 비용은 선택하신 옵션 투어나 개인 경비에 따라 달라집니다.""},
)
memory.save_context(
    inputs={""human"":""여행 중에 방문할 주요 관광지는 어디인가요?""},
    outputs={""ai"":""이 여행에서는 파리의 에펠탑, 로마의 콜로세움, 베를린의 브란덴부르크 문, 취리히의 라이네폴 등 유럽의 유명한 관광지들을 방문합니다. 각 도시의 대표적인 명소들을 포괄적으로 경험하실 수 있습니다.""},
)
memory.save_context(
    inputs={""human"":""여행자 보험은 포함되어 있나요?""},
    outputs={""ai"":""네, 모든 여행자에게 기본 여행자 보험을 제공합니다. 이 보험은 의료비 지원, 긴급 상황 발생 시 지원 등을 포함합니다. 추가적인 보험 보장을 원하시면 상향 조정이 가능합니다.""},
)
memory.save_context(
    inputs={""human"":""항공편 좌석을 비즈니스 클래스로 업그레이드할 수 있나요? 비용은 어떻게 되나요?""},
    outputs={""ai"":""항공편 좌석을 비즈니스 클래스로 업그레이드하는 것이 가능합니다. 업그레이드 비용은 왕복 기준으로 약 1,200유로 추가됩니다. 비즈니스 클래스에서는 더 넓은 좌석, 우수한 기내식, 그리고 추가 수하물 허용량 등의 혜택을 제공합니다.""},
)
memory.save_context(
    inputs={""human"":""패키지에 포함된 호텔의 등급은 어떻게 되나요?""},
    outputs={""ai"":""이 패키지에는 4성급 호텔 숙박이 포함되어 있습니다. 각 호텔은 편안함과 편의성을 제공하며, 중심지에 위치해 관광지와의 접근성이 좋습니다. 모든 호텔은 우수한 서비스와 편의 시설을 갖추고 있습니다.""},
)
memory.save_context(
    inputs={""human"":""식사 옵션에 대해 더 자세히 알려주실 수 있나요?""},
    outputs={""ai"":""이 여행 패키지는 매일 아침 호텔에서 제공되는 조식을 포함하고 있습니다. 점심과 저녁 식사는 포함되어 있지 않아, 여행자가 자유롭게 현지의 다양한 음식을 경험할 수 있는 기회를 제공합니다. 또한, 각 도시별로 추천 식당 리스트를 제공하여 현지의 맛을 최대한 즐길 수 있도록 도와드립니다.""},
)
memory.save_context(
    inputs={""human"":""패키지 예약 시 예약금은 얼마인가요? 취소 정책은 어떻게 되나요?""},
    outputs={""ai"":""패키지 예약 시 500유로의 예약금이 필요합니다. 취소 정책은 예약일로부터 30일 전까지는 전액 환불이 가능하며, 이후 취소 시에는 예약금이 환불되지 않습니다. 여행 시작일로부터 14일 전 취소 시 50%의 비용이 청구되며, 그 이후는 전액 비용이 청구됩니다.""},
)
```
저장된 메모리의 history 를 확인합니다.
이전의 모든 대화를 압축적으로 요약한 내용을 확인할 수 있습니다.
```
Copy# 저장된 메모리 확인print(memory.load_memory_variables({})[""history""])
```
```
[SystemMessage(content='The human asks about the price of a European travel package, and the AI responds with details about the basic package and main tourist attractions included. The human then inquires about upgrading to business class seats for the flights, and the AI confirms that it is possible with an additional cost of approximately 1,200 euros for round-trip flights. Business class offers benefits such as wider seats, premium meals, and increased baggage allowance. The human asks about the grade of hotels included in the package, and the AI explains that 4-star hotels are included, offering comfort, convenience, and excellent service in central locations near tourist attractions. The human asks for more information about meal options, and the AI explains that the package includes daily breakfast at the hotel, while lunch and dinner are not included to allow travelers to experience a variety of local cuisine. Additionally, the AI provides recommended restaurant lists in each city to help travelers fully enjoy local flavors. The human asks about the reservation deposit and cancellation policy, and the AI explains that a 500 euro reservation deposit is required at the time of booking. The cancellation policy allows for a full refund up to 30 days before the reservation date, with no refund of the deposit after that. Cancelling 14 days before the trip incurs a 50% charge, and cancelling after that results in full payment.')]
```
## ConversationSummaryBufferMemory
ConversationSummaryBufferMemory는 두 가지 아이디어를 결합한 것입니다.
최근 대화내용의 버퍼를 메모리에 유지하되, 이전 대화내용을 완전히 플러시(flush)하지 않고 요약으로 컴파일하여 두 가지를 모두 사용합니다.
대화내용을 플러시할 시기를 결정하기 위해 상호작용의 개수가 아닌토큰 길이를 사용합니다.
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain.memoryimportConversationSummaryBufferMemory

llm = ChatOpenAI()

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=200,# 요약의 기준이 되는 토큰 길이를 설정합니다.return_messages=True,
)
```
먼저, 1개의 대화만 저장해 보도록 한 뒤, 메모리를 확인해 보겠습니다.
```
Copymemory.save_context(
    inputs={""human"":""유럽 여행 패키지의 가격은 얼마인가요?""},
    outputs={""ai"":""유럽 14박 15일 패키지의 기본 가격은 3,500유로입니다. 이 가격에는 항공료, 호텔 숙박비, 지정된 관광지 입장료가 포함되어 있습니다. 추가 비용은 선택하신 옵션 투어나 개인 경비에 따라 달라집니다.""},
)
```
메모리에 저장된 대화를 확인합니다.
아직은 대화내용을 요약하지 않습니다. 기준이 되는200토큰에 도달하지 않았기 때문입니다.
```
Copy# 메모리에 저장된 대화내용 확인memory.load_memory_variables({})[""history""]
```
```
[HumanMessage(content='유럽 여행 패키지의 가격은 얼마인가요?'), AIMessage(content='유럽 14박 15일 패키지의 기본 가격은 3,500유로입니다. 이 가격에는 항공료, 호텔 숙박비, 지정된 관광지 입장료가 포함되어 있습니다. 추가 비용은 선택하신 옵션 투어나 개인 경비에 따라 달라집니다.')]
```
대화를 추가로 저장하여 200 토큰 제한을 넘기도록 해 보겠습니다.
```
Copymemory.save_context(
    inputs={""human"":""여행 중에 방문할 주요 관광지는 어디인가요?""},
    outputs={""ai"":""이 여행에서는 파리의 에펠탑, 로마의 콜로세움, 베를린의 브란덴부르크 문, 취리히의 라이네폴 등 유럽의 유명한 관광지들을 방문합니다. 각 도시의 대표적인 명소들을 포괄적으로 경험하실 수 있습니다.""},
)
memory.save_context(
    inputs={""human"":""여행자 보험은 포함되어 있나요?""},
    outputs={""ai"":""네, 모든 여행자에게 기본 여행자 보험을 제공합니다. 이 보험은 의료비 지원, 긴급 상황 발생 시 지원 등을 포함합니다. 추가적인 보험 보장을 원하시면 상향 조정이 가능합니다.""},
)
memory.save_context(
    inputs={""human"":""항공편 좌석을 비즈니스 클래스로 업그레이드할 수 있나요? 비용은 어떻게 되나요?""},
    outputs={""ai"":""항공편 좌석을 비즈니스 클래스로 업그레이드하는 것이 가능합니다. 업그레이드 비용은 왕복 기준으로 약 1,200유로 추가됩니다. 비즈니스 클래스에서는 더 넓은 좌석, 우수한 기내식, 그리고 추가 수하물 허용량 등의 혜택을 제공합니다.""},
)
memory.save_context(
    inputs={""human"":""패키지에 포함된 호텔의 등급은 어떻게 되나요?""},
    outputs={""ai"":""이 패키지에는 4성급 호텔 숙박이 포함되어 있습니다. 각 호텔은 편안함과 편의성을 제공하며, 중심지에 위치해 관광지와의 접근성이 좋습니다. 모든 호텔은 우수한 서비스와 편의 시설을 갖추고 있습니다.""},
)
```
저장된 대화내용을 확인합니다. 가장 최근 1개의 대화에 대해서는 요약이 진행되지 않지만, 이전의 대화내용은 요약본으로 저장되어 있습니다.
```
Copy# 메모리에 저장된 대화내용 확인memory.load_memory_variables({})[""history""]
```
```
[SystemMessage(content=""The human asks about the price of a European travel package. The AI responds that the basic price for a 14-night, 15-day package is 3,500 euros, including airfare, hotel accommodation, and designated tourist attraction fees. Additional costs vary depending on optional tours or personal expenses. The human asks about the main tourist attractions to visit during the trip, and the AI lists iconic landmarks such as the Eiffel Tower in Paris, the Colosseum in Rome, the Brandenburg Gate in Berlin, and the Rhine Falls in Zurich. The trip offers a comprehensive experience of each city's famous landmarks. The human inquires about traveler's insurance, and the AI confirms that basic traveler's insurance is included for all travelers, providing medical assistance and emergency support. Additional coverage options are available upon request. The human asks if it's possible to upgrade to business class seats on the flights and inquires about the cost. The AI responds that upgrading to business class is possible for an additional cost of approximately 1,200 euros round trip, providing benefits like wider seats, superior in-flight meals, and increased baggage allowance.""), HumanMessage(content='패키지에 포함된 호텔의 등급은 어떻게 되나요?'), AIMessage(content='이 패키지에는 4성급 호텔 숙박이 포함되어 있습니다. 각 호텔은 편안함과 편의성을 제공하며, 중심지에 위치해 관광지와의 접근성이 좋습니다. 모든 호텔은 우수한 서비스와 편의 시설을 갖추고 있습니다.')]
```"
07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory),https://wikidocs.net/233811,"# 07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory)

VectorStoreRetrieverMemory는 벡터 스토어에 메모리를 저장하고 호출될 때마다 가장 '눈에 띄는' 상위 K개의 문서를 쿼리합니다.
이는 대화내용의 순서를명시적으로 추적하지 않는다는 점에서 다른 대부분의 메모리 클래스와 다릅니다.
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
먼저, 벡터 스토어를 초기화 합니다.
```
Copyimportfaissfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain.docstoreimportInMemoryDocstorefromlangchain.vectorstoresimportFAISS# 임베딩 모델을 정의합니다.embeddings_model = OpenAIEmbeddings()# Vector Store 를 초기화 합니다.embedding_size =1536index = faiss.IndexFlatL2(embedding_size)
vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})
```
실제 사용에서는k를 더 높은 값으로 설정하지만, 여기서는k=1을 사용하여 다음과 같이 표시합니다.
```
Copyfromlangchain.memoryimportVectorStoreRetrieverMemory# 벡터 조회가 여전히 의미적으로 관련성 있는 정보를 반환한다는 것을 보여주기 위해서입니다.retriever = vectorstore.as_retriever(search_kwargs={""k"":1})
memory = VectorStoreRetrieverMemory(retriever=retriever)# 임의의 대화를 저장합니다.memory.save_context(
    inputs={""human"":""안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.""},
    outputs={""ai"":""안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.""},
)
memory.save_context(
    inputs={""human"":""프로젝트에서 어떤 역할을 맡았나요?""},
    outputs={""ai"":""제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.""},
)
memory.save_context(
    inputs={""human"":""팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?""},
    outputs={""ai"":""프로젝트 초기에 의사소통 문제로 몇 가지 어려움이 있었습니다. 이를 해결하기 위해 저희 팀은 정기적인 미팅을 갖고 각자의 진행 상황을 공유했습니다. 또한, 문제가 발생했을 때는 적극적으로 의견을 나누고, 합리적인 해결책을 찾기 위해 노력했습니다.""},
)
memory.save_context(
    inputs={""human"":""개발자로서 자신의 강점은 무엇이라고 생각하나요?""},
    outputs={""ai"":""제 강점은 빠른 학습 능력과 문제 해결 능력입니다. 새로운 기술이나 도구를 빠르게 습득할 수 있으며, 복잡한 문제에 직면했을 때 창의적인 해결책을 제시할 수 있습니다. 또한, 팀워크를 중시하며 동료들과 협력하는 것을 중요하게 생각합니다.""},
)
```
다음의 질문을 했을 때 Vector Store 로 부터 1개(k=1 이기 때문)의 가장 관련성 높은 대화를 반환합니다.
- 질문: ""면접자 전공은 무엇인가요?""
질문: ""면접자 전공은 무엇인가요?""
```
Copy# 메모리에 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.print(memory.load_memory_variables({""prompt"":""면접자 전공은 무엇인가요?""})[""history""])
```
```
human: 안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.
ai: 안녕하세요. 저는 컴퓨터 과학을 전공한 신입 개발자입니다. 대학에서는 주로 자바와 파이썬을 사용했으며, 최근에는 웹 개발 프로젝트에 참여하여 실제 사용자를 위한 서비스를 개발하는 경험을 했습니다.
```
이번에는 다른 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.
- 질문: ""면접자가 프로젝트에서 맡은 역할은 무엇인가요?""
질문: ""면접자가 프로젝트에서 맡은 역할은 무엇인가요?""
```
Copyprint(
    memory.load_memory_variables(
        {""human"":""면접자가 프로젝트에서 맡은 역할은 무엇인가요?""}
    )[""history""]
)
```
```
human: 프로젝트에서 어떤 역할을 맡았나요?
ai: 제가 맡은 역할은 백엔드 개발자였습니다. 사용자 데이터 처리와 서버 로직 개발을 담당했으며, RESTful API를 구현하여 프론트엔드와의 통신을 담당했습니다. 또한, 데이터베이스 설계에도 참여했습니다.
```"
08. LCEL Chain 에 메모리 추가,https://wikidocs.net/233812,"# 08. LCEL Chain 에 메모리 추가

임의의 체인에 메모리를 추가하는 방법을 보여줍니다. 현재 메모리 클래스를 사용할 수 있지만 수동으로 연결해야 합니다
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
```
Copyfromoperatorimportitemgetterfromlangchain.memoryimportConversationBufferMemoryfromlangchain_core.promptsimportChatPromptTemplate, MessagesPlaceholderfromlangchain_core.runnablesimportRunnableLambda, RunnablePassthroughfromlangchain_openaiimportChatOpenAI# ChatOpenAI 모델을 초기화합니다.model = ChatOpenAI()# 대화형 프롬프트를 생성합니다. 이 프롬프트는 시스템 메시지, 이전 대화 내역, 그리고 사용자 입력을 포함합니다.prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""You are a helpful chatbot""),
        MessagesPlaceholder(variable_name=""chat_history""),
        (""human"",""{input}""),
    ]
)
```
대화내용을 저장할 메모리인ConversationBufferMemory생성하고return_messages매개변수를True로 설정하여, 생성된 인스턴스가 메시지를 반환하도록 합니다.
- memory_key설정: 추후 Chain 의prompt안에 대입될 key 입니다. 변경하여 사용할 수 있습니다.
memory_key설정: 추후 Chain 의prompt안에 대입될 key 입니다. 변경하여 사용할 수 있습니다.
```
Copy# 대화 버퍼 메모리를 생성하고, 메시지 반환 기능을 활성화합니다.memory = ConversationBufferMemory(
    return_messages=True, memory_key=""chat_history"")
```
저장된 대화기록을 확인합니다. 아직 저장하지 않았으므로, 대화기록은 비어 있습니다.
```
Copymemory.load_memory_variables({})# 메모리 변수를 빈 딕셔너리로 초기화합니다.
```
```
{'chat_history': []}
```
RunnablePassthrough.assign을 사용하여chat_history변수에memory.load_memory_variables함수의 결과를 할당하고, 이 결과에서chat_history키에 해당하는 값을 추출합니다.
```
Copyrunnable = RunnablePassthrough.assign(
    chat_history=RunnableLambda(memory.load_memory_variables)
    | itemgetter(""chat_history"")# memory_key 와 동일하게 입력합니다.)
```
runnable에 첫 번째 대화를 시작합니다.
- input: 사용자 입력 대화가 전달됩니다.
- chat_history: 대화 기록이 전달됩니다.
input: 사용자 입력 대화가 전달됩니다.
chat_history: 대화 기록이 전달됩니다.
```
Copyrunnable.invoke({""input"":""hi!""})
```
```
{'input': 'hi!', 'chat_history': []}
```
```
Copychain = runnable | prompt | model
```
첫 번째 대화를 진행합니다.
```
Copy# chain 객체의 invoke 메서드를 사용하여 입력에 대한 응답을 생성합니다.response = chain.invoke({""input"":""만나서 반갑습니다. 제 이름은 테디입니다.""})print(response)# 생성된 응답을 출력합니다.
```
```
content='만나서 반가워요, 테디님! 무엇을 도와드릴까요?' response_metadata={'finish_reason': 'stop', 'logprobs': None}
```
memory.save_context함수는 입력 데이터(inputs)와 응답 내용(response.content)을 메모리에 저장하는 역할을 합니다. 이는 AI 모델의 학습 과정에서 현재 상태를 기록하거나, 사용자의 요청과 시스템의 응답을 추적하는 데 사용될 수 있습니다.
```
Copy# 입력된 데이터와 응답 내용을 메모리에 저장합니다.memory.save_context(
    {""inputs"":""만나서 반갑습니다. 제 이름은 테디입니다.""}, {""output"": response.content}
)# 저장된 대화기록을 출력합니다.memory.load_memory_variables({})
```
```
{'chat_history': [HumanMessage(content='만나서 반갑습니다. 제 이름은 테디입니다.'),  AIMessage(content='만나서 반가워요, 테디님! 무엇을 도와드릴까요?')]}
```
이름을 기억하고 있는지 추가 질의합니다.
```
Copy# 이름을 기억하고 있는지 추가 질의합니다.response = chain.invoke({""input"":""제 이름이 무엇이었는지 기억하세요?""})# 답변을 출력합니다.print(response.content)
```
```
네, 테디님이세요. 어떻게 도와드릴까요?
```"
09. SQLite 에 메모리 저장,https://wikidocs.net/233813,"# 09. SQLite 에 메모리 저장

Structured Query Language (SQL)은 프로그래밍에 사용되는 도메인 특화 언어로, 관계형 데이터베이스 관리 시스템(RDBMS)에서 데이터를 관리하거나 관계형 데이터 스트림 관리 시스템(RDSMS)에서 스트림 처리를 위해 설계되었습니다. 특히 엔티티와 변수 간의 관계를 포함하는 구조화된 데이터를 다루는 데 유용합니다.
SQLAlchemy는 MIT 라이선스에 따라 배포되는 Python 프로그래밍 언어용 오픈 소스SQL툴킷이자 객체 관계 매퍼(ORM)입니다.
이 노트북에서는SQLAlchemy가 지원하는 모든 데이터베이스에 채팅 기록을 저장할 수 있는SQLChatMessageHistory클래스에 대해 설명합니다.
SQLite이외의 데이터베이스와 함께 사용하려면 해당 데이터베이스 드라이버를 설치해야 합니다.
## 설치
langchain-community패키지에 포함되어 있으므로, 해당 패키지를 설치해야 합니다. 또한SQLAlchemy패키지도 설치해야 합니다.
```
Copy%pip install -qU langchain-community SQLAlchemy langchain-openai
```
또한 최고 수준의 관찰 가능성을 위해LangSmith를 설정하는 것이 도움이 될 수 있습니다(필수는 아님).
```
Copy# os.environ[""LANGCHAIN_TRACING_V2""] = ""true""  # LANGCHAIN_TRACING_V2 환경 변수를 ""true""로 설정합니다.# os.environ[""LANGCHAIN_API_KEY""] = ""LANGCHAIN_API_KEY를 설정합니다.""
```
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
## 사용방법
storage를 사용하려면 다음 2가지만 제공하면 됩니다:
Session Id - 사용자 이름, 이메일, 채팅 ID 등과 같은 세션의 고유 식별자입니다.
Session Id - 사용자 이름, 이메일, 채팅 ID 등과 같은 세션의 고유 식별자입니다.
Connection string - 데이터베이스 연결을 지정하는 문자열입니다. 이 문자열은 SQLAlchemy의 create_engine 함수에 전달됩니다.
Connection string - 데이터베이스 연결을 지정하는 문자열입니다. 이 문자열은 SQLAlchemy의 create_engine 함수에 전달됩니다.
SQLChatMessageHistory클래스를 사용하여 대화 메시지 기록을 관리합니다.
SQLChatMessageHistory클래스를 사용하여 대화 메시지 기록을 관리합니다.
session_id와connection_string을 인자로 전달하여SQLChatMessageHistory인스턴스를 생성합니다.
add_user_message메서드를 사용하여 사용자 메시지를 대화 기록에 추가합니다.
add_ai_message메서드를 사용하여 AI 메시지를 대화 기록에 추가합니다.
```
Copyfromlangchain_community.chat_message_historiesimportSQLChatMessageHistory# SQLChatMessageHistory 객체를 생성하고 세션 ID와 데이터베이스 연결 문자열을 전달합니다.chat_message_history = SQLChatMessageHistory(
    session_id=""sql_chat_history"", connection_string=""sqlite:///sqlite.db"")# 사용자 메시지를 추가합니다.chat_message_history.add_user_message(""Hi! My name is Teddy. I am a AI programmer. Nice to meet you!"")# AI 메시지를 추가합니다.chat_message_history.add_ai_message(""Hi Teddy! Nice to meet you too!"")
```
- 저장된 대화내용을 확인합니다.chat_message_history.messages
저장된 대화내용을 확인합니다.chat_message_history.messages
```
Copy# 채팅 메시지 기록의 메시지들chat_message_history.messages
```
```
[HumanMessage(content='Hi! My name is Teddy. I am a AI programmer. Nice to meet you!'), AIMessage(content='Hi Teddy! Nice to meet you too!')]
```
## Chain 에 적용
우리는 이 메시지 기록 클래스를LCEL Runnables와 쉽게 결합할 수 있습니다.
```
Copyfromlangchain_core.promptsimport(
    ChatPromptTemplate,
    MessagesPlaceholder,
)fromlangchain_core.runnables.historyimportRunnableWithMessageHistoryfromlangchain_openaiimportChatOpenAI
```
- ChatPromptTemplate을 사용하여 프롬프트 템플릿을 정의합니다.
- ""system"" 메시지: ""You are a helpful assistant.""
- MessagesPlaceholder를 사용하여 ""history"" 변수를 프롬프트에 포함시킵니다.
- ""human"" 메시지: ""{question}""
- 프롬프트 템플릿과ChatOpenAI모델을 파이프라인으로 연결하여chain을 생성합니다.
ChatPromptTemplate을 사용하여 프롬프트 템플릿을 정의합니다.
""system"" 메시지: ""You are a helpful assistant.""
MessagesPlaceholder를 사용하여 ""history"" 변수를 프롬프트에 포함시킵니다.
""human"" 메시지: ""{question}""
프롬프트 템플릿과ChatOpenAI모델을 파이프라인으로 연결하여chain을 생성합니다.
```
Copyprompt = ChatPromptTemplate.from_messages(
    [# 시스템 메시지를 설정하여 어시스턴트의 역할을 정의합니다.(""system"",""You are a helpful assistant.""),# 이전 대화 내용을 포함하기 위한 플레이스홀더를 추가합니다.MessagesPlaceholder(variable_name=""history""),
        (""human"",""{question}""),# 사용자의 질문을 입력받는 메시지를 설정합니다.]
)

chain = (
    prompt | ChatOpenAI()
)# 프롬프트와 ChatOpenAI 모델을 연결하여 체인을 생성합니다.
```
- RunnableWithMessageHistory클래스를 사용하여chain과 메시지 기록을 연결합니다.
- lambda함수를 사용하여session_id에 따라SQLChatMessageHistory인스턴스를 생성합니다.
- SQLChatMessageHistory는 SQLite 데이터베이스(sqlite.db)에 연결되어 채팅 메시지 기록을 저장합니다.
- input_messages_key를 ""question""으로 설정하여 입력 메시지의 키를 지정합니다.
- history_messages_key를 ""history""로 설정하여 기록 메시지의 키를 지정합니다.
RunnableWithMessageHistory클래스를 사용하여chain과 메시지 기록을 연결합니다.
lambda함수를 사용하여session_id에 따라SQLChatMessageHistory인스턴스를 생성합니다.
SQLChatMessageHistory는 SQLite 데이터베이스(sqlite.db)에 연결되어 채팅 메시지 기록을 저장합니다.
input_messages_key를 ""question""으로 설정하여 입력 메시지의 키를 지정합니다.
history_messages_key를 ""history""로 설정하여 기록 메시지의 키를 지정합니다.
```
Copychain_with_history = RunnableWithMessageHistory(
    chain,lambdasession_id: SQLChatMessageHistory(
        session_id=session_id, connection_string=""sqlite:///sqlite.db""),# session_id를 기반으로 SQLChatMessageHistory 객체를 생성하는 람다 함수input_messages_key=""question"",# 입력 메시지의 키를 ""question""으로 설정history_messages_key=""history"",# 대화 기록 메시지의 키를 ""history""로 설정)
```
- config딕셔너리를 정의하고,""configurable""키 아래에""session_id""키-값 쌍을 설정합니다.
config딕셔너리를 정의하고,""configurable""키 아래에""session_id""키-값 쌍을 설정합니다.
```
Copy# 세션 ID를 구성하는 곳입니다.config = {""configurable"": {""session_id"":""sql_chat_history""}}
```
질문에 이름을 물어보는 질문을 해보겠습니다. 이전에 저장한 대화가 있다면, 올바르게 답할 것입니다.
- chain_with_history객체의invoke메서드를 호출하여 질문에 대한 답변을 생성합니다.
- invoke메서드에는 질문 딕셔너리와config설정이 전달됩니다.
chain_with_history객체의invoke메서드를 호출하여 질문에 대한 답변을 생성합니다.
invoke메서드에는 질문 딕셔너리와config설정이 전달됩니다.
```
Copy# 질문 ""Whats my name""과 설정을 사용하여 대화 기록이 있는 체인을 호출합니다.chain_with_history.invoke({""question"":""Whats my name?""}, config=config)
```
```
AIMessage(content='Your name is Teddy.', response_metadata={'finish_reason': 'stop', 'logprobs': None})
```"
CH06 체인(Chains),https://wikidocs.net/233774,"# CH06 체인(Chains)

체인(Chain) 생성 단계는 이전의 7단계 과정을 모두 하나로 묶어 하나의 RAG 파이프라인으로 조립하여 완성하는 단계 입니다.
아래의 구조도는 문서 기반 RAG 의 구조를 나타냅니다.

## 코드
LCEL(LangChain Expression Language) 문법을 사용하여 이전 7단계의 전 과정을 하나의 체인(Chain) 으로 묶습니다.
```
Copy# 체인(Chain) 생성chain = (
    {""context"": retriever,""question"": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```
아래는 완성된 체인(Chain) 에 질의하는 코드입니다.
```
Copy# 체인 실행(Run Chain)# 문서에 대한 질의를 입력하고, 답변을 출력합니다.question =""삼성전자가 자체 개발한 AI 의 이름은?""response = chain.invoke(question)
```
## 참고
- LCEL 문법
- LangChain Chains
LCEL 문법
LangChain Chains"
01. 대화형 체인(Conversation Chain),https://wikidocs.net/234018,"# 01. 대화형 체인(Conversation Chain)

```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
## ConversationChain
ChatOpenAI클래스를 사용하여 대화형 AI 모델의 인스턴스를 생성하고, 이를 통해 사용자와의 대화를 처리할 수 있습니다.
ConversationChain,ChatOpenAI,ConversationBufferMemory모듈은 대화 관리 및 메모리 관리에 필요한 기능을 제공합니다.
```
Copyfromlangchain.chainsimportConversationChainfromlangchain_openaiimportChatOpenAIfromlangchain.memoryimportConversationBufferMemory
```
ConversationChain클래스의 인스턴스를 생성하여 대화 관리를 수행합니다. 이때,llm은 저수준 언어 모델을,verbose는 상세한 로깅 여부를 (여기서는 비활성화), 그리고memory는 대화 내용을 저장할 메모리 버퍼를 각각 지정합니다.
```
Copy# ChatOpenAI 객체를 생성합니다.llm = ChatOpenAI(model=""gpt-4-turbo-preview"")# ConversationChain 인스턴스를 생성합니다.# llm: 모델을 지정합니다.# verbose: 상세한 로깅을 비활성화합니다.# memory: 대화 내용을 저장하는 메모리 버퍼를 지정합니다.conversation = ConversationChain(
    llm=llm,
    verbose=False,
    memory=ConversationBufferMemory(memory_key=""history""),
)
```
이 함수는 문자열 입력을 받아 대화 모델을 통해 예측 결과를 반환합니다.
conversation.predict함수는 주어진 입력에 대한 모델의 반응을 예측하는 데 사용됩니다.
```
Copyconversation.invoke({""input"":""양자역학에 대해 설명해줘.""})
```
```
{'input': '양자역학에 대해 설명해줘.', 'history': '', 'response': ""양자역학은 물리학의 한 분야로, 원자와 아원자 입자들의 행동을 연구합니다. 이 이론은 20세기 초에 개발되었으며, 클래식 물리학의 법칙들이 더 이상 적용되지 않는 극소의 세계를 설명하는 데 중요한 역할을 합니다. 양자역학의 핵심 개념 중 하나는 '불확정성 원리'인데, 이는 입자의 위치와 운동량을 동시에 정확히 알 수 없다는 것을 의미합니다. 이러한 불확정성은 입자가 파동처럼 행동할 수 있음을 나타내며, 이로 인해 '양자 중첩'과 '양자 얽힘'과 같은 현상이 발생합니다.\n\n양자 중첩은 하나의 입자가 여러 상태에 동시에 존재할 수 있다는 개념이고, 양자 얽힘은 두 개의 입자가 서로 멀리 떨어져 있어도 그들의 상태가 서로 연결되어 있어 한 입자의 상태를 측정하면 다른 입자의 상태도 즉시 결정된다는 현상을 말합니다.\n\n양자역학은 또한 '파동 함수'라는 개념을 사용하여 입자의 상태를 수학적으로 기술합니다. 파동 함수의 절대값의 제곱은 특정 위치에서 입자를 발견할 확률을 나타냅니다.\n\n이러한 이론들은 현대 과학과 기술에 매우 중요한 기초를 제공하며, 양자 컴퓨팅, 양자 암호화, 초전도, 레이저 기술 등 다양한 분야에 응용됩니다. 양자역학은 여전히 많은 연구자들에 의해 활발히 연구되고 있으며, 우리가 우주와 그 작동 원리를 이해하는 데 필수적인 이론입니다.""}
```
```
Copyconversation.memory.load_memory_variables({})[""history""]
```
```
""Human: 양자역학에 대해 설명해줘.\nAI: 양자역학은 물리학의 한 분야로, 원자와 아원자 입자들의 행동을 연구합니다. 이 이론은 20세기 초에 개발되었으며, 클래식 물리학의 법칙들이 더 이상 적용되지 않는 극소의 세계를 설명하는 데 중요한 역할을 합니다. 양자역학의 핵심 개념 중 하나는 '불확정성 원리'인데, 이는 입자의 위치와 운동량을 동시에 정확히 알 수 없다는 것을 의미합니다. 이러한 불확정성은 입자가 파동처럼 행동할 수 있음을 나타내며, 이로 인해 '양자 중첩'과 '양자 얽힘'과 같은 현상이 발생합니다.\n\n양자 중첩은 하나의 입자가 여러 상태에 동시에 존재할 수 있다는 개념이고, 양자 얽힘은 두 개의 입자가 서로 멀리 떨어져 있어도 그들의 상태가 서로 연결되어 있어 한 입자의 상태를 측정하면 다른 입자의 상태도 즉시 결정된다는 현상을 말합니다.\n\n양자역학은 또한 '파동 함수'라는 개념을 사용하여 입자의 상태를 수학적으로 기술합니다. 파동 함수의 절대값의 제곱은 특정 위치에서 입자를 발견할 확률을 나타냅니다.\n\n이러한 이론들은 현대 과학과 기술에 매우 중요한 기초를 제공하며, 양자 컴퓨팅, 양자 암호화, 초전도, 레이저 기술 등 다양한 분야에 응용됩니다. 양자역학은 여전히 많은 연구자들에 의해 활발히 연구되고 있으며, 우리가 우주와 그 작동 원리를 이해하는 데 필수적인 이론입니다.\nHuman: hi\nAI: 안녕\nHuman: 불렛포인트 형식으로 작성해줘. emoji 추가해줘.\nAI: 양자역학에 대해 불렛포인트와 이모지로 요약해보겠습니다! 😊\n\n- **양자역학의 정의** 📚\n  - 물리학의 분야로, 원자와 아원자 입자의 행동 연구\n  - 20세기 초 개발됨\n\n- **핵심 개념** 🔍\n  - 불확정성 원리: 입자의 위치와 운동량을 동시에 정확히 알 수 없음\n  - 양자 중첩: 하나의 입자가 여러 상태에 동시에 존재 가능\n  - 양자 얽힘: 두 입자가 멀리 떨어져 있어도 상태가 서로 연결됨\n\n- **중요한 이론** 🌟\n  - 파동 함수: 입자의 상태를 수학적으로 기술, 위치에서 입자 발견 확률 나타냄\n\n- **응용 분야** 💡\n  - 양자 컴퓨팅, 양자 암호화\n  - 초전도, 레이저 기술 등\n\n양자역학은 현대 과학 및 기술의 중요한 기반이며, 계속해서 연구되는 흥미로운 분야입니다! 🌈""
```
```
Copyconversation.memory.save_context(inputs={""human"":""hi""}, outputs={""ai"":""안녕""})
```
```
Copyconversation.memory.load_memory_variables({})[""history""]
```
```
""Human: 양자역학에 대해 설명해줘.\nAI: 양자역학은 물리학의 한 분야로, 원자와 아원자 입자들의 행동을 연구합니다. 이 이론은 20세기 초에 개발되었으며, 클래식 물리학의 법칙들이 더 이상 적용되지 않는 극소의 세계를 설명하는 데 중요한 역할을 합니다. 양자역학의 핵심 개념 중 하나는 '불확정성 원리'인데, 이는 입자의 위치와 운동량을 동시에 정확히 알 수 없다는 것을 의미합니다. 이러한 불확정성은 입자가 파동처럼 행동할 수 있음을 나타내며, 이로 인해 '양자 중첩'과 '양자 얽힘'과 같은 현상이 발생합니다.\n\n양자 중첩은 하나의 입자가 여러 상태에 동시에 존재할 수 있다는 개념이고, 양자 얽힘은 두 개의 입자가 서로 멀리 떨어져 있어도 그들의 상태가 서로 연결되어 있어 한 입자의 상태를 측정하면 다른 입자의 상태도 즉시 결정된다는 현상을 말합니다.\n\n양자역학은 또한 '파동 함수'라는 개념을 사용하여 입자의 상태를 수학적으로 기술합니다. 파동 함수의 절대값의 제곱은 특정 위치에서 입자를 발견할 확률을 나타냅니다.\n\n이러한 이론들은 현대 과학과 기술에 매우 중요한 기초를 제공하며, 양자 컴퓨팅, 양자 암호화, 초전도, 레이저 기술 등 다양한 분야에 응용됩니다. 양자역학은 여전히 많은 연구자들에 의해 활발히 연구되고 있으며, 우리가 우주와 그 작동 원리를 이해하는 데 필수적인 이론입니다.\nHuman: hi\nAI: 안녕\nHuman: 불렛포인트 형식으로 작성해줘. emoji 추가해줘.\nAI: 양자역학에 대해 불렛포인트와 이모지로 요약해보겠습니다! 😊\n\n- **양자역학의 정의** 📚\n  - 물리학의 분야로, 원자와 아원자 입자의 행동 연구\n  - 20세기 초 개발됨\n\n- **핵심 개념** 🔍\n  - 불확정성 원리: 입자의 위치와 운동량을 동시에 정확히 알 수 없음\n  - 양자 중첩: 하나의 입자가 여러 상태에 동시에 존재 가능\n  - 양자 얽힘: 두 입자가 멀리 떨어져 있어도 상태가 서로 연결됨\n\n- **중요한 이론** 🌟\n  - 파동 함수: 입자의 상태를 수학적으로 기술, 위치에서 입자 발견 확률 나타냄\n\n- **응용 분야** 💡\n  - 양자 컴퓨팅, 양자 암호화\n  - 초전도, 레이저 기술 등\n\n양자역학은 현대 과학 및 기술의 중요한 기반이며, 계속해서 연구되는 흥미로운 분야입니다! 🌈\nHuman: hi\nAI: 안녕""
```
```
Copyprint(conversation.invoke({""input"":""불렛포인트 형식으로 작성해줘. emoji 추가해줘.""}))
```
```
{'input': '불렛포인트 형식으로 작성해줘. emoji 추가해줘.', 'history': ""Human: 양자역학에 대해 설명해줘.\nAI: 양자역학은 물리학의 한 분야로, 원자와 아원자 입자들의 행동을 연구합니다. 이 이론은 20세기 초에 개발되었으며, 클래식 물리학의 법칙들이 더 이상 적용되지 않는 극소의 세계를 설명하는 데 중요한 역할을 합니다. 양자역학의 핵심 개념 중 하나는 '불확정성 원리'인데, 이는 입자의 위치와 운동량을 동시에 정확히 알 수 없다는 것을 의미합니다. 이러한 불확정성은 입자가 파동처럼 행동할 수 있음을 나타내며, 이로 인해 '양자 중첩'과 '양자 얽힘'과 같은 현상이 발생합니다.\n\n양자 중첩은 하나의 입자가 여러 상태에 동시에 존재할 수 있다는 개념이고, 양자 얽힘은 두 개의 입자가 서로 멀리 떨어져 있어도 그들의 상태가 서로 연결되어 있어 한 입자의 상태를 측정하면 다른 입자의 상태도 즉시 결정된다는 현상을 말합니다.\n\n양자역학은 또한 '파동 함수'라는 개념을 사용하여 입자의 상태를 수학적으로 기술합니다. 파동 함수의 절대값의 제곱은 특정 위치에서 입자를 발견할 확률을 나타냅니다.\n\n이러한 이론들은 현대 과학과 기술에 매우 중요한 기초를 제공하며, 양자 컴퓨팅, 양자 암호화, 초전도, 레이저 기술 등 다양한 분야에 응용됩니다. 양자역학은 여전히 많은 연구자들에 의해 활발히 연구되고 있으며, 우리가 우주와 그 작동 원리를 이해하는 데 필수적인 이론입니다.\nHuman: hi\nAI: 안녕"", 'response': '양자역학에 대해 불렛포인트와 이모지로 요약해보겠습니다! 😊\n\n- **양자역학의 정의** 📚\n  - 물리학의 분야로, 원자와 아원자 입자의 행동 연구\n  - 20세기 초 개발됨\n\n- **핵심 개념** 🔍\n  - 불확정성 원리: 입자의 위치와 운동량을 동시에 정확히 알 수 없음\n  - 양자 중첩: 하나의 입자가 여러 상태에 동시에 존재 가능\n  - 양자 얽힘: 두 입자가 멀리 떨어져 있어도 상태가 서로 연결됨\n\n- **중요한 이론** 🌟\n  - 파동 함수: 입자의 상태를 수학적으로 기술, 위치에서 입자 발견 확률 나타냄\n\n- **응용 분야** 💡\n  - 양자 컴퓨팅, 양자 암호화\n  - 초전도, 레이저 기술 등\n\n양자역학은 현대 과학 및 기술의 중요한 기반이며, 계속해서 연구되는 흥미로운 분야입니다! 🌈'}
```
## streaming: 실시간 출력
스트리밍 방식으로 대화형 AI 모델을 구현합니다.
MyCustomHandler클래스는BaseCallbackHandler를 상속받아, 새로운 토큰이 생성될 때마다 이를 출력하는 기능을 정의합니다.
ChatOpenAI의 스트리밍 모드를 활성화하고, 사용자 정의 핸들러인StreamingHandler인스턴스를 콜백으로 설정합니다.
- 참고 문서
참고 문서
```
Copyfromlangchain_core.callbacks.baseimportBaseCallbackHandlerfromlangchain_openaiimportChatOpenAIclassStreamingHandler(BaseCallbackHandler):defon_llm_new_token(self, token:str, **kwargs) ->None:print(f""{token}"", end="""", flush=True)# 스트리밍을 활성화하기 위해, ChatModel 생성자에 `streaming=True`를 전달합니다.# 추가적으로, 사용자 정의 핸들러 리스트를 전달합니다.stream_llm = ChatOpenAI(
    model=""gpt-4-turbo-preview"", streaming=True, callbacks=[StreamingHandler()]
)

conversation = ConversationChain(
    llm=stream_llm,
    verbose=False,
    memory=ConversationBufferMemory(),
)
```
conversation.predict메소드는 주어진 입력에 대한 예측 결과를 반환합니다. 이 예시에서는 '양자역학에 대해 설명해줘'라는 문자열을 입력으로 사용하며, 해당 입력에 대한 예측 결과를output변수에 저장합니다. 이 과정은 AI 기반 대화 시스템에서 특정 주제에 대한 설명을 요청하는 방법을 보여줍니다.
```
Copyoutput = conversation.predict(input=""양자역학에 대해 설명해줘"")
```
```
양자역학은 미시적인 입자들의 행동을 설명하는 물리학의 한 분야입니다. 20세기 초에 발전하기 시작했으며, 클래식 물리학의 법칙들이 적용되지 않는 아주 작은 스케일에서의 현상을 다룹니다. 양자역학의 기본 개념 중 하나는 '파동-입자 이중성'인데, 이는 빛과 물질이 동시에 파동과 입자의 성질을 가질 수 있다는 것을 의미합니다.

또 다른 중요한 개념은 '불확정성 원리'인데, 하이젠베르크에 의해 제안되었습니다. 이 원리에 따르면, 우리는 입자의 위치와 운동량을 동시에 정확히 알 수 없습니다. 즉, 한 측정치를 더 정확히 알려고 할수록 다른 측정치는 더 불확정해집니다.

양자역학은 또한 양자 중첩이라는 개념을 도입하는데, 이는 하나의 입자나 입자 시스템이 여러 가능한 상태들의 '중첩'으로 존재할 수 있다는 것을 의미합니다. 이 상태들은 관측될 때까지 확정되지 않습니다.

이러한 양자역학의 기본 원리들은 많은 현대 기술들, 예를 들어 반도체 기술, 레이저, 양자 컴퓨팅 등에 근본적인 역할을 합니다. 양자역학의 이론은 매우 추상적이고 직관적이지 않을 수 있지만, 이를 통해 우리는 자연의 근본적인 작동 원리에 대해 더 깊이 이해할 수 있게 되었습니다.
```
이 함수는conversation.predict를 사용하여 주어진 입력(input)을 분석하고, 그 결과를 불렛포인트 형식으로 요약하여 반환합니다. 여기서input파라미터는 요약하고자 하는 텍스트를 나타냅니다. 이 방식은 텍스트 기반 AI 모델을 활용하여 주어진 내용을 요약하는 데 유용하게 사용될 수 있습니다.
```
Copyoutput = conversation.predict(input=""이전의 내용을 불렛포인트로 요약해줘. emoji 추가해줘."")
```
```
- **양자역학**은 미시적 입자들의 행동을 설명하는 물리학 분야입니다. 🌌
- 20세기 초에 발전하기 시작했습니다. ⏳
- **파동-입자 이중성**: 빛과 물질이 동시에 파동과 입자의 성질을 가질 수 있습니다. 🌊🔆
- **불확정성 원리**: 입자의 위치와 운동량을 동시에 정확히 알 수 없습니다. 📍➰
- **양자 중첩**: 하나의 입자나 입자 시스템이 여러 가능한 상태들의 '중첩'으로 존재합니다. 🔀
- 관측될 때까지 입자의 상태는 확정되지 않습니다. 👀
- 양자역학은 반도체 기술, 레이저, 양자 컴퓨팅 등 현대 기술에 근본적인 역할을 합니다. 💻🔬
- 자연의 근본적인 작동 원리에 대해 더 깊이 이해할 수 있게 해줍니다. 🌍🔍
```
이 코드는langchain.prompts에서PromptTemplate클래스를 가져와 사용합니다.PromptTemplate는 템플릿 기반의 프롬프트를 생성하는 데 사용되며, 여기서는 엑셀 전문가와의 대화를 시뮬레이션하는 템플릿을 정의합니다. 템플릿에는 대화 내용(context)과 사용자의 질문(question)이 포함되어 있으며, 이 두 변수는PromptTemplate생성자에input_variables로 전달됩니다. 이를 통해, 주어진 대화 내용과 질문에 기반한 엑셀 전문가의 답변을 생성하는 프롬프트가 만들어집니다.
```
Copyfromlangchain.promptsimportPromptTemplate

template =""""""
당신은 10년차 엑셀 전문가 입니다. 아래 대화내용을 보고 질문에 대한 적절한 답변을 해주세요

#대화내용
{chat_history}
----
사용자: {question}
엑셀전문가:""""""prompt = PromptTemplate.from_template(template)
```
```
Copyprompt.partial(chat_history=""엑셀에서 데이터를 필터링하는 방법에 대해 알려주세요."")
```
```
PromptTemplate(input_variables=['question'], partial_variables={'chat_history': '엑셀에서 데이터를 필터링하는 방법에 대해 알려주세요.'}, template='\n당신은 10년차 엑셀 전문가 입니다. 아래 대화내용을 보고 질문에 대한 적절한 답변을 해주세요\n\n#대화내용\n{chat_history}\n----\n사용자: {question}\n엑셀전문가:')
```
이 함수는ConversationChain클래스의 인스턴스를 생성합니다. 여기서llm은 언어 모델을,prompt는 대화의 시작점을,memory는 대화의 맥락을 저장하는 메모리 버퍼를,input_key는 사용자 입력을 받는 키를, 그리고verbose는 상세한 로깅을 활성화할지 여부를 지정합니다. 특히,memory는ConversationBufferMemory클래스를 사용하여memory_key를 통해 어떤 정보를 기억할지 결정합니다.
```
Copyconversation = ConversationChain(
    llm=stream_llm,
    prompt=prompt,
    memory=ConversationBufferMemory(memory_key=""chat_history""),
    input_key=""question"",
)
```
이 함수는conversation.predict를 호출하여, 사용자가 제공한 질문에 대한 예측된 답변을 반환합니다. 여기서,question매개변수는 질문의 내용을 문자열로 받습니다. 예시에서는 ""엑셀에서 VLOOKUP 함수는 무엇인가요? 간단하게 설명해주세요""라는 질문을 처리하고 있습니다.
```
Copyanswer = conversation.predict(
    question=""엑셀에서 VLOOKUP 함수는 무엇인가요? 간단하게 설명해주세요"")
```
conversation.predict함수는 주어진 질문에 대한 예측을 수행합니다. 이 함수는question매개변수를 통해 질문을 입력받으며, 이 예제에서는 ""예제를 보여주세요""라는 질문에 대한 예측 결과를 반환합니다.
```
Copyanswer = conversation.predict(question=""예제를 보여주세요"")
```
## Human Prefix & AI Prefix 변경
다음 방법은 대화 요약에서 휴먼 접두사를 변경하는 것입니다.
기본적으로 이 접두사는 ""사람""으로 설정되어 있지만 원하는 대로 변경할 수 있습니다.
이 접두사를 변경하는 경우 체인에서 사용되는 프롬프트도 이 이름 변경을 반영하도록 변경해야 한다는 점에 유의하세요.
이 함수는 사용자로부터 엑셀 관련 질문(question)과 그 질문의 맥락(context)을 입력받아, 10년차 엑셀 전문가로서의 답변을 준비하는 템플릿을 생성합니다.PromptTemplate클래스는 입력 변수(input_variables)와 템플릿(template)을 인자로 받아 인스턴스를 생성합니다. 이 인스턴스는 엑셀 관련 질문에 대한 답변을 생성하는 데 사용될 수 있습니다.
```
Copytemplate =""""""
당신은 10년차 엑셀 전문가 입니다. 아래 대화내용을 보고 질문에 대한 적절한 답변을 해주세요

#대화내용
{context}
----
🌱고객: {question}
🤖전문가:""""""prompt = PromptTemplate.from_template(template)
```
ConversationBufferMemory클래스는 대화의 맥락을 관리하는 데 사용됩니다. 이 클래스의 인스턴스를 생성할 때,memory_key는 대화의 맥락을 저장하는 키로 사용되며,human_prefix와ai_prefix는 각각 사람과 AI가 말하는 부분에 붙는 접두사로 설정됩니다. 이를 통해 대화 내용을 더 명확하게 구분할 수 있습니다.
```
Copymemory = ConversationBufferMemory(
    memory_key=""context"",# 대화의 맥락을 저장하는 키human_prefix=""🌱고객"",# 사람이 말하는 부분에 붙는 접두사ai_prefix=""🤖전문가"",# AI가 말하는 부분에 붙는 접두사)
```
이 함수는ConversationChain클래스의 인스턴스를 생성합니다. 여기서llm은 언어 모델을,prompt는 사용자 입력을,memory는 대화의 이전 상태를 저장하는 데 사용됩니다.input_key는 입력 데이터의 키를 지정하며,verbose는 로깅의 상세도를 조절합니다. 이 구성을 통해 대화형 AI 시스템에서의 연속적인 대화 관리가 가능해집니다.
```
Copyconversation = ConversationChain(
    llm=stream_llm,
    prompt=prompt,
    memory=memory,
    input_key=""question"",
    verbose=False,
)
```
이 함수는conversation.predict을 사용하여 특정 질문에 대한 대화형 응답을 실행합니다.
여기서는 ""엑셀에서 VLOOKUP 함수는 무엇인가요? 간단하게 설명해주세요""라는 질문을 전달하고 있습니다.
VLOOKUP함수에 대한 설명을 요청하는 간단한 예시로, 대화형 AI 또는 챗봇 프로그래밍에서 사용될 수 있습니다.
```
Copyanswer = conversation.predict(
    question=""엑셀에서 VLOOKUP 함수는 무엇인가요? 간단하게 설명해주세요"")
```
```
Copyanswer = conversation.predict(question=""쉬운 예제를 보여줄 수 있나요?"")
```
memory.load_memory_variables를 호출하여 메모리에 저장된 변수들을 로드합니다.(전체 대화내용 출력)
호출 시, 빈 딕셔너리{}를 인자로 전달하며, 결과적으로'context'키에 해당하는 값을 출력합니다.
이 과정은 특정 메모리 상태를 확인하거나 디버깅할 때 유용하게 사용될 수 있습니다.
```
Copyprint(
    memory.load_memory_variables({})[""context""]
)# 빈 딕셔너리를 인자로 받아 메모리 변수를 로드하고, 'context' 키에 해당하는 값을 출력합니다.
```"
02. 구조화된 출력 체인(Structured Output Chain),https://wikidocs.net/234019,"# 02. 구조화된 출력 체인(Structured Output Chain)

```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
특정 주제에 대한 4지선다형 퀴즈를 생성하는 과정을 구현합니다.
Quiz클래스는 퀴즈의 질문, 난이도, 그리고 네 개의 선택지를 정의합니다.
ChatOpenAI인스턴스는 GPT-4 모델을 사용하여 자연어 처리를 수행하고,ChatPromptTemplate는 퀴즈 생성을 위한 대화형 프롬프트를 정의합니다.
마지막으로,create_structured_output_runnable함수는 이러한 구성 요소를 결합하여 주어진 주제(""ADSP(데이터 분석 준전문가) 자격 시험"")에 대한 퀴즈를 생성하는 실행 가능한 체인을 만듭니다.
```
Copyfromlangchain.chains.openai_functionsimportcreate_structured_output_runnablefromlangchain_openaiimportChatOpenAIfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.pydantic_v1importBaseModel, FieldfromtypingimportListclassQuiz(BaseModel):""""""4지선다형 퀴즈의 정보를 추출합니다""""""question:str= Field(..., description=""퀴즈의 질문"")
    level:str= Field(
        ..., description=""퀴즈의 난이도를 나타냅니다. (쉬움, 보통, 어려움)"")
    options:List[str] = Field(..., description=""퀴즈의 4개의 선택지 입니다."")


llm = ChatOpenAI(model=""gpt-4-turbo-preview"", temperature=0.1)
prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""You're a world-famous quizzer and generates quizzes in structured formats."",
        ),
        (""human"",""TOPIC 에 제시된 내용과 관련한 4지선다형 퀴즈를 출제해 주세요. 만약, 실제 출제된 기출문제가 있다면 비슷한 문제를 만들어 출제하세요.""""단, 문제에 TOPIC 에 대한 내용이나 정보는 포함하지 마세요. \nTOPIC:\n{topic}"",
        ),
        (""human"",""Tip: Make sure to answer in the correct format""),
    ]
)
chain = create_structured_output_runnable(Quiz, llm, prompt)
generated_quiz = chain.invoke({""topic"":""ADSP(데이터 분석 준전문가) 자격 시험""})
```
```
Copygenerated_quiz
```
```
Quiz(question='다음 중 데이터 분석 프로젝트에서 가장 먼저 수행해야 하는 단계는 무엇입니까?', level='보통', options=['데이터 수집', '데이터 전처리', '데이터 분석', '문제 정의'])
```
이 함수는generated_quiz객체에서 질문(question)과 난이도(level)를 출력한 후, 해당 퀴즈의 선택지(options)를 순회하며 인덱스와 함께 출력합니다.enumerate함수를 사용하여 각 선택지의 인덱스를 가져오고, 이를 사용자에게 보여주기 쉬운 형태로 출력합니다. 이 과정은 퀴즈의 질문과 선택지를 사용자에게 제시하는 데 사용됩니다.
```
Copyprint(f""{generated_quiz.question}(난이도:{generated_quiz.level})\n"")fori, optinenumerate(generated_quiz.options):print(f""{i+1}){opt}"")
```
```
다음 중 데이터 분석 프로젝트에서 가장 먼저 수행해야 하는 단계는 무엇입니까? (난이도: 보통)

1) 데이터 수집
2) 데이터 전처리
3) 데이터 분석
4) 문제 정의
```"
03. 문서 전체 요약(Stuff Documents Chain),https://wikidocs.net/234020,"# 03. 문서 전체 요약(Stuff Documents Chain)

```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
## Stuff Documents Chain
stuff documents chain(""stuff""는 ""채우다"" 또는 ""채우기 위해""의 의미)는 문서 체인 중 가장 간단한 방식입니다. 문서 목록을 가져와서 모두 프롬프트에 삽입한 다음, 그 프롬프트를 LLM에 전달합니다.
이 체인은 문서가 작고 대부분의 호출에 몇 개만 전달되는 애플리케이션에 적합합니다.

langchain_community.chat_models,langchain_core.prompts,langchain.chains.combine_documents모듈에서 필요한 클래스와 함수를 가져옵니다.
ChatOpenAI,ChatPromptTemplate, 그리고create_stuff_documents_chain함수를 사용하여 챗봇 프롬프트를 생성하는 과정을 보여줍니다.
사용자로부터 입력받은 문장을 요약하는 요청을 처리하기 위한 프롬프트 템플릿을 정의합니다.
이 템플릿은 시스템이 전문 요약가 역할을 수행하도록 요청하고, 사용자에게 한국어로 불릿 포인트 형식의 요약, 각 문장의 의미에 맞는 이모지 사용, 다양한 이모지를 활용하여 요약을 더 흥미롭게 만들라는 지시를 포함합니다.
```
Copy!pip install -U langchain langchainhub langchain_openai langchain_community -q
```
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain_core.promptsimportChatPromptTemplatefromlangchain.chains.combine_documentsimportcreate_stuff_documents_chain


prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""You are an expert summarizer. Please summarize the following sentence."",
        ),
        (""user"",""Please summarize the sentence according to the following request.""""\nREQUEST:\n""""1. Summarize the main points in bullet points in Korean.""""2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.""""3. Use various emojis to make the summary more interesting.""""\n\nCONTEXT: {context}\n\nSUMMARY:"",
        ),
    ]
)
prompt
```
```
ChatPromptTemplate(input_variables=['context'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert summarizer. Please summarize the following sentence.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Please summarize the sentence according to the following request.\nREQUEST:\n1. Summarize the main points in bullet points in Korean.2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.3. Use various emojis to make the summary more interesting.\n\nCONTEXT: {context}\n\nSUMMARY:'))])
```
(혹은 아래의 코드를 실행하여 프롬프트를 내려받습니다)
```
Copyfromlangchainimporthub

prompt = hub.pull(""teddynote/summary-stuff-documents-korean"")
prompt
```
```
PromptTemplate(input_variables=['context'], template='Please summarize the sentence according to the following REQUEST.\nREQUEST:\n1. Summarize the main points in bullet points in KOREAN.\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n3. Use various emojis to make the summary more interesting.\n4. Translate the summary into KOREAN if it is written in ENGLISH.\n5. DO NOT translate any technical terms.\n6. DO NOT include any unnecessary information.\n\nCONTEXT:\n{context}\n\nSUMMARY:""\n')
```
TextLoader를 사용하여 뉴스기사를 로드하고, Document를 생성합니다.
langchain_community.document_loaders모듈에서TextLoader클래스를 임포트하여 사용합니다.TextLoader인스턴스는 특정 파일(data/news.txt)에서 문서를 로드하는 데 사용됩니다.
```
Copyfromlangchain_community.document_loadersimportTextLoader

loader = TextLoader(""data/news.txt"")
docs = loader.load()print(f""문서의 수:{len(docs)}\n"")print(""[메타데이터]\n"")print(docs[0].metadata)print(""\n========= [앞부분] 미리보기 =========\n"")print(docs[0].page_content[:500])
```
```
문서의 수: 1

[메타데이터]

{'source': 'data/news.txt'}

========= [앞부분] 미리보기 =========

제목: 
AI2, 상업 활용까지 자유로운 '진짜' 오픈 소스 LLM '올모' 출시

내용:
앨런AI연구소(AI2)가 완전한 오픈 소스 대형언어모델(LLM) '올모(OLMo)’를 출시했다. 데이터 수집, 학습, 배포의 전 과정을 투명하게 공개한 데다 상업적 사용까지 허용한 진정한 의미의 오픈 소스 LLM이라는 평가다.
벤처비트는 1일(현지시간) 비영리 민간 AI 연구기관인 AI2가 ‘최초의 진정한 오픈 소스 LLM 및 프레임워크’라고 소개한 ‘올모’를 출시했다고 보도했다. 
이에 따르면 올모는 모델 코드와 모델 가중치뿐만 아니라 훈련 코드, 훈련 데이터, 관련 툴킷 및 평가 툴킷도 제공한다. 이를 통해 모델이 어떻게 구축되었는지 심층적으로 분석, LLM의 작동 방식과 응답을 생성하는 원리를 더 잘 이해할 수 있다. 
올모 프레임워크는 70억 매개변수의 ‘올모 7B’ 등 4가지 변형 모델과 10억 매개변수의 ‘올모 1B’ 모델을 제공한다. 모델들은 훈련 데이터를 생성하는 코드를 포함해
```
MyCallbackHandler클래스는BaseCallbackHandler를 상속받아, 언어 모델이 새로운 토큰을 생성할 때마다 해당 토큰을 출력하는 기능을 추가합니다.
ChatOpenAI객체는 GPT-3.5-turbo 모델을 사용하며, 스트리밍 모드와 낮은 온도 설정을 통해 더 일관된 응답을 생성하도록 구성됩니다. 이 객체는 콜백으로MyCallbackHandler인스턴스를 사용합니다.
마지막으로,create_stuff_documents_chain함수를 사용하여 문서 생성 체인을 만들고, 이 체인을 통해 주어진 문맥(docs)에 대한 응답(answer)을 생성합니다.
```
Copyfromlangchain.callbacks.baseimportBaseCallbackHandlerclassMyCallbackHandler(BaseCallbackHandler):defon_llm_new_token(self, token:str, **kwargs) ->None:print(f""{token}"", end="""", flush=True)


llm = ChatOpenAI(
    model_name=""gpt-3.5-turbo"",
    streaming=True,
    temperature=0.01,
    callbacks=[MyCallbackHandler()],
)
chain = create_stuff_documents_chain(llm, prompt)
answer = chain.invoke({""context"": docs})
```
```
🚀 앨런AI연구소(AI2)가 '올모(OLMo)'라는 오픈 소스 대형언어모델(LLM)을 출시했다.
🔍 올모는 데이터 수집, 학습, 배포의 전 과정을 투명하게 공개하고 상업적 사용까지 허용하는 진정한 의미의 오픈 소스 LLM이다.
🔧 올모는 모델 코드, 모델 가중치, 훈련 코드, 훈련 데이터, 관련 툴킷 및 평가 툴킷을 제공한다.
📚 올모 프레임워크에는 '올모 7B'와 '올모 1B' 등 4가지 변형 모델이 포함되어 있다.
💡 올모는 AI2의 '돌마(Dolma)' 데이터 세트를 기반으로 구축되었으며, 상업적 활용에 제한이 없는 아파치 2.0 라이선스를 따른다.
🌍 올모는 다양한 언어 처리 벤치마크 테스트에서 상업용 제품과 동등한 성능을 보여준다.
🔧 AI2는 올모를 계속해서 향상시킬 예정이며, 올모의 출시는 AI 모델에 대한 더 깊은 이해를 위한 기반을 구축하는 것이다.
👨‍💻 얀 르쿤 메타 수석 과학자는 오픈 소스 기반 모델이 AI의 혁신과 개발을 촉진하는 데 매우 중요하다고 말했다.
🔗 올모에 대한 모든 리소스는 깃허브 및 허깅페이스에서 무료로 사용할 수 있다.
```"
"04. 문서 분할-병합 요약(MapReduce, Refine Chain)",https://wikidocs.net/234021,"# 04. 문서 분할-병합 요약(MapReduce, Refine Chain)

## Use case
문서 집합(PDF, Notion 페이지, 고객 질문 등)을 가지고 있고, 내용을 요약하고 싶다고 가정해 보세요.
LLMs는 텍스트를 이해하고 종합하는 데 능숙하기 때문에 이를 위한 훌륭한 도구입니다.
이 안내서에서는 LLMs를 사용하여 문서 요약을 수행하는 방법에 대해 살펴보겠습니다.
## 개요
요약기를 구축할 때 중심적인 질문은 문서를 LLM의 컨텍스트 창에 어떻게 전달할 것인가입니다. 이를 위한 두 가지 일반적인 접근 방식은 다음과 같습니다:
Stuff: 단순히 모든 문서를 단일 프롬프트로 ""넣는"" 방식입니다. 이는 가장 간단한 접근 방식입니다.
Stuff: 단순히 모든 문서를 단일 프롬프트로 ""넣는"" 방식입니다. 이는 가장 간단한 접근 방식입니다.
Map-reduce: 각 문서를 ""map"" 단계에서 개별적으로 요약한 다음, ""reduce"" 단계에서 요약본들을 최종 요약본으로 합치는 방식입니다.
Map-reduce: 각 문서를 ""map"" 단계에서 개별적으로 요약한 다음, ""reduce"" 단계에서 요약본들을 최종 요약본으로 합치는 방식입니다.
Refine: 입력 문서를 순회하며 반복적으로 답변을 업데이트하여 응답을 구성합니다. 각 문서에 대해, 모든 비문서 입력, 현재 문서, 그리고 최신 중간 답변을 LLM chain에 전달하여 새로운 답변을 얻습니다.
Refine: 입력 문서를 순회하며 반복적으로 답변을 업데이트하여 응답을 구성합니다. 각 문서에 대해, 모든 비문서 입력, 현재 문서, 그리고 최신 중간 답변을 LLM chain에 전달하여 새로운 답변을 얻습니다.
## load_summarize_chain
미리보기를 제공하기 위해, 어떤 파이프라인도 단일 객체로 래핑될 수 있습니다:load_summarize_chain.
블로그 포스트를 요약하고 싶다고 가정해 봅시다. 몇 줄의 코드로 이를 생성할 수 있습니다.
먼저 환경 변수를 설정하고 패키지를 설치하세요:
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
chain_type=""stuff""를 사용할 수 있습니다.
chain_type=""map_reduce""또는chain_type=""refine""도 제공할 수 있습니다(더 읽어보기여기).
이 코드는 웹에서 문서를 로드하고, 이를 요약하기 위해langchain라이브러리와 OpenAI의 GPT 모델을 사용합니다.
먼저,WebBaseLoader를 사용하여 지정된 URL에서 문서를 로드합니다. 그 다음,ChatOpenAI를 사용하여 GPT-3.5 모델을 초기화합니다.
load_summarize_chain함수를 통해 요약 작업을 위한 체인을 로드합니다.
마지막으로, 로드된 문서에 대해 요약 체인을 실행합니다. 이 과정은 AI를 활용하여 웹 문서를 요약하는 효율적인 방법을 제시합니다.
```
Copyfromlangchain.chains.summarizeimportload_summarize_chainfromlangchain_community.document_loadersimportWebBaseLoaderfromlangchain_openaiimportChatOpenAIfromlangchain.callbacks.baseimportBaseCallbackHandler# 웹 기반 문서 로더를 초기화합니다.loader = WebBaseLoader(""https://lilianweng.github.io/posts/2023-06-23-agent/"")# 문서를 로드합니다.docs = loader.load()classStreamCallback(BaseCallbackHandler):defon_llm_new_token(self, token, **kwargs):print(f""{token}"", end="""", flush=True)# OpenAI의 Chat 모델을 초기화합니다. 여기서는 온도를 0으로 설정하고 모델 이름을 지정합니다.llm = ChatOpenAI(
    temperature=0,
    model_name=""gpt-3.5-turbo-16k"",
    streaming=True,
    callbacks=[StreamCallback()],
)# 요약 체인을 로드합니다. 체인 타입을 'stuff'로 지정합니다.chain = load_summarize_chain(llm, chain_type=""stuff"")# 문서에 대해 요약 체인을 실행합니다.answer = chain.invoke({""input_documents"": docs})print(answer[""output_text""])
```
```
The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes references to related research papers and benchmarks.The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes references to related research papers and benchmarks.
```
## 방법1. stuff
chain_type=""stuff""로load_summarize_chain을 사용할 때,StuffDocumentsChain을 사용합니다.
체인은 문서 목록을 가져와서 모두 프롬프트에 삽입한 후, 그 프롬프트를 LLM에 전달합니다:
먼저,PromptTemplate를 사용하여 요약문 작성을 위한 프롬프트를 정의합니다.
그 다음,LLMChain을 사용하여 지정된 모델(gpt-3.5-turbo-16k)과 온도 설정(0)을 사용하는 언어 모델 체인을 생성합니다.
이 체인은 입력된 텍스트에 대한 요약문을 생성하는 데 사용됩니다.
마지막으로,StuffDocumentsChain을 사용하여 문서들을 결합하고, 이를LLMChain을 통해 요약합니다.
이 과정은loader.load()로 로드된 문서들에 대해 실행되며, 결과는 실시간 출력됩니다.
[참고]
- load_summarize_chain대신StuffDocumentsChain을 사용하는 이점은사용자 정의 프롬프트입니다.
load_summarize_chain대신StuffDocumentsChain을 사용하는 이점은사용자 정의 프롬프트입니다.
```
Copyfromlangchain.chains.combine_documents.stuffimportStuffDocumentsChainfromlangchain.chains.llmimportLLMChainfromlangchain.promptsimportPromptTemplatefromlangchainimporthub# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)# prompt_template = """"""Please summarize the sentence according to the following REQUEST.# REQUEST:# 1. Summarize the main points in bullet points in KOREAN.# 2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.# 3. Use various emojis to make the summary more interesting.# 4. Translate the summary into Korean if it is written in English.# 5. DO NOT translate any technical terms.# 6. DO NOT include any unnecessary information.# CONTEXT:# {context}# SUMMARY:""# """"""# prompt = PromptTemplate.from_template(prompt_template)# 원격 저장소에서 프롬프트를 가져오는 경우prompt = hub.pull(""teddynote/summary-stuff-documents-korean"")# LLM 체인 정의llm = ChatOpenAI(
    temperature=0,
    model_name=""gpt-3.5-turbo-16k"",
    streaming=True,
    callbacks=[StreamCallback()],
)# LLMChain 정의llm_chain = LLMChain(llm=llm, prompt=prompt)# StuffDocumentsChain 정의stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=""context"")

docs = loader.load()
response = stuff_chain.invoke({""input_documents"": docs})
```
```
🤖 LLM을 사용한 자율 에이전트 시스템은 LLM을 에이전트의 뇌로 사용하고 계획, 메모리, 도구 사용과 같은 여러 구성 요소로 보완됩니다.
📝 계획 구성 요소는 큰 작업을 작은 하위 목표로 분해하고 에이전트가 과거 행동을 자가 비판하고 반영하여 최종 결과의 품질을 향상시킵니다.
🧠 메모리 구성 요소는 감각 메모리, 단기 메모리, 장기 메모리로 구성되며, 외부 벡터 저장소와 빠른 검색을 통해 정보를 보존하고 검색할 수 있습니다.
🔧 도구 사용 구성 요소는 외부 API를 호출하여 모델 가중치에 없는 추가 정보를 얻을 수 있습니다.
🔍 이러한 구성 요소를 사용하여 과학적 발견 에이전트, 생성 에이전트 시뮬레이션, 개념 증명 예제 등을 구축할 수 있습니다.
🚀 그러나 유한한 컨텍스트 길이, 장기적인 계획 및 작업 분해의 어려움, 자연어 인터페이스의 신뢰성 등 몇 가지 제한 사항이 있습니다.
```
좋습니다! 우리는load_summarize_chain을 사용하여 이전 결과를 재현할 수 있음을 확인할 수 있습니다.
## 방법2. Map-Reduce

Map reduce 접근 방식을 자세히 살펴보겠습니다. 이를 위해, 우리는 먼저 각 문서를 개별 요약으로 매핑하기 위해LLMChain을 사용할 것입니다. 그런 다음ReduceDocumentsChain을 사용하여 그 요약들을 하나의 전역 요약으로 결합할 것입니다.
먼저, 각 문서를 개별 요약으로 매핑하기 위해 사용할 LLMChain을 지정합니다.
ChatOpenAI인스턴스를 생성하고, 이를 사용하여 문서 집합에 대한 주요 테마를 식별하는 맵(map) 작업을 정의합니다.
맵 작업은map_template을 사용하여 정의되며, 이 템플릿은 문서 집합을 입력으로 받아 주요 테마를 식별하도록 요청합니다.
PromptTemplate.from_template메서드를 사용하여map_template에서 프롬프트 템플릿을 생성하고,LLMChain을 사용하여 맵 작업을 실행합니다.
Prompt Hub를 사용하여 프롬프트를 저장하고 가져올 수도 있습니다.
예를 들어, 여기에서 맵 프롬프트를 확인하세요여기.
langchain라이브러리를 사용하여 특정 자원을 가져오고, 이를 활용해LLMChain인스턴스를 생성하는 과정을 설명합니다.hub.pull메소드를 통해 'rlm/map-prompt' 자원을 가져오고, 이를LLMChain의 생성자에 전달하여 인스턴스를 초기화합니다. 이 과정에서llm변수는 사전에 정의되어 있어야 합니다.
```
Copyfromlangchain.chainsimportMapReduceDocumentsChain, ReduceDocumentsChainfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain.chains.llmimportLLMChainfromlangchainimporthub


llm = ChatOpenAI(
    temperature=0,
    model_name=""gpt-3.5-turbo"",
    streaming=True,
    callbacks=[StreamCallback()],
)# # map-prompt 를 직접 정의하는 경우 다음의 예시를 참고하세요.# map_template = """"""The following is a set of documents# {docs}# Based on this list of docs, please identify the main themes# Helpful Answer:""""""# map_prompt = PromptTemplate.from_template(map_template)# langchain 허브에서 'rlm/map-prompt'를 가져옵니다.map_prompt = hub.pull(""teddynote/map-prompt"")
map_prompt
```
```
PromptTemplate(input_variables=['docs'], template='You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\nPlease provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format. \nThe summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. \nPlease ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. \nThe length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\n\nGIVEN DOCUMENTS:\n{docs}\n\nFORMAT:\n1. main theme 1\n2. main theme 2\n3. main theme 3\n...\n\nCAUTION:\n- DO NOT list more than 5 main themes.\n\nHelpful Answer:\n')
```
```
Copy# LLMChain 인스턴스를 생성하며, 이때 LLM과 프롬프트로 'map_prompt'를 사용합니다.map_chain = LLMChain(llm=llm, prompt=map_prompt)
```
ReduceDocumentsChain은 문서 매핑 결과를 가져와 단일 출력으로 축소하는 역할을 합니다. 일반적인CombineDocumentsChain(예:StuffDocumentsChain)을 감싸지만, 누적 크기가token_max를 초과하는 경우 문서를 축소하여CombineDocumentsChain에 전달할 수 있는 기능을 추가합니다. 이 예에서, 우리는 문서를 결합하기 위해 사용한 체인을 문서를 축소하는 데에도 재사용할 수 있습니다.
따라서 우리가 매핑한 문서의 누적 토큰 수가 4000 토큰을 초과하는 경우, 우리는 4000 토큰 미만의 배치로 문서를 재귀적으로StuffDocumentsChain에 전달하여 배치 요약을 생성합니다. 그리고 이러한 배치 요약이 누적으로 4000 토큰 미만이 되면, 마지막으로 모든 문서를StuffDocumentsChain에 한 번 더 전달하여 최종 요약을 생성합니다.
이 코드는 요약들을 통합하여 주요 테마의 최종 요약을 생성하는 과정을 정의합니다.reduce_template변수는 요약들의 집합을 입력으로 받아, 이를 하나의 통합된 요약으로 축약하는 템플릿 문자열을 저장합니다. 이 템플릿은{docs}를 요약들의 자리 표시자로 사용하며, 최종적으로PromptTemplate.from_template함수를 사용하여reduce_prompt변수에 템플릿을 초기화합니다.
```
Copy# reduce-prompt 를 직접 정의하는 경우 다음의 예시를 참고하세요.# reduce_template = """"""The following is set of summaries:# {docs}# Take these and distill it into a final, consolidated summary of the main themes.# Helpful Answer:""""""# reduce_prompt = PromptTemplate.from_template(reduce_template)
```
이 코드는hub.pull함수를 사용하여rlm/map-prompt라는 리소스를prompt hub에서 가져오는 과정을 보여줍니다.hub.pull메소드는 지정된 리소스를 로컬 환경으로 가져오는 데 사용됩니다. 여기서reduce_prompt변수는 가져온 리소스를 저장하는 데 사용됩니다.
```
Copy# prompt hub에서도 얻을 수 있음을 위에서 언급했듯이reduce_prompt = hub.pull(""teddynote/reduce-prompt-korean"")
reduce_prompt
```
```
PromptTemplate(input_variables=['doc_summaries'], template='You are a helpful expert in summary writing.\nYou are given numbered lists of summaries.\nExtract top 10 most important insights from the summaries.\nThen, write a summary of the insights in KOREAN.\n\nLIST OF SUMMARIES:\n{doc_summaries}\n\nHelpful Answer:\n')
```
이 문서는LLMChain,StuffDocumentsChain,ReduceDocumentsChain클래스를 사용하여 문서 처리 파이프라인을 구성하는 방법을 보여줍니다.LLMChain은 초기 처리 단계로, 특정 프롬프트를 사용하여 언어 모델(llm)을 실행합니다.StuffDocumentsChain은 여러 문서를 하나의 문자열로 결합하여LLMChain에 전달하는 역할을 합니다. 마지막으로,ReduceDocumentsChain은 문서들을 결합하고, 지정된 토큰 수(token_max)를 초과하지 않도록 반복적으로 축소하는 과정을 담당합니다. 이 과정에서, 문서들이StuffDocumentsChain의 컨텍스트를 초과할 경우, 동일한 체인(collapse_documents_chain)을 사용하여 처리합니다.
```
Copyfromlangchain.chains.combine_documents.stuffimportStuffDocumentsChain# 연쇄 실행reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)# 문서 리스트를 받아 하나의 문자열로 결합한 후 LLMChain에 전달combine_documents_chain = StuffDocumentsChain(
    llm_chain=reduce_chain, document_variable_name=""doc_summaries"")# 매핑된 문서들을 결합하고 반복적으로 축소reduce_documents_chain = ReduceDocumentsChain(# 최종적으로 호출되는 체인입니다.combine_documents_chain=combine_documents_chain,# `StuffDocumentsChain`의 컨텍스트를 초과하는 문서들을 처리collapse_documents_chain=combine_documents_chain,# 문서들을 그룹화할 최대 토큰 수.token_max=4096,
)
```
우리의 map과 reduce 체인을 하나로 결합해 봅시다.
이 코드는 문서들을 매핑하고 리듀스하는 과정을 통해 결합하는MapReduceDocumentsChain객체를 생성하고, 문자 기반으로 텍스트를 분할하는CharacterTextSplitter객체를 사용하여 문서들을 분할합니다.MapReduceDocumentsChain은 매핑 체인(llm_chain), 리듀스 체인(reduce_documents_chain), 문서를 저장할 변수 이름(document_variable_name), 그리고 매핑 단계의 중간 결과를 반환할지 여부(return_intermediate_steps)를 설정하여 초기화됩니다.CharacterTextSplitter는from_tiktoken_encoder메소드를 통해 초기화되며, 이는 분할할 청크의 크기(chunk_size)와 청크 간 겹침(chunk_overlap)을 설정합니다.
```
Copy# 문서들을 매핑하여 체인을 거친 후 결과를 결합하는 과정map_reduce_chain = MapReduceDocumentsChain(# 매핑 체인llm_chain=map_chain,# 리듀스 체인reduce_documents_chain=reduce_documents_chain,# llm_chain에서 문서들을 넣을 변수 이름document_variable_name=""docs"",# 매핑 단계의 결과를 출력에 포함시킴return_intermediate_steps=False,
)# 문자를 기준으로 텍스트를 분할하는 객체 생성text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=50,
    separators=[""\n\n"",""\n"",""(?<=\. )"","" "",""""],
    length_function=len,
)# 문서들을 분할split_docs = text_splitter.split_documents(docs)
```
map_reduce_chain.run(split_docs)는split_docs를 인자로 받아map_reduce_chain의run메서드를 실행하고, 그 결과를 출력합니다. 이는 MapReduce 패턴을 활용하여 데이터를 처리하는 과정을 간략하게 보여줍니다. 여기서split_docs는 처리할 데이터를 나타내며,map_reduce_chain은 해당 데이터에 적용할 MapReduce 연산의 체인을 나타냅니다.
```
Copy# split_docs를 map_reduce_chain의 run 메서드에 전달하여 실행한 결과를 출력합니다.summary_result = map_reduce_chain.invoke({""input_documents"": split_docs})
```
```
1. The document discusses LLM Powered Autonomous Agents, providing an overview of the agent system and its components.
2. The first component discussed is planning, which involves task decomposition and self-reflection.
3. The second component is memory, which includes different types of memory and the use of Maximum Inner Product Search (MIPS).
4. The third component is tool use, with case studies on a scientific discovery agent and generative agents simulation, as well as proof-of-concept examples.
5. The document also highlights the challenges associated with LLM Powered Autonomous Agents and provides citations and references for further reading.1. Building agents with LLM as the core controller is a concept that has been demonstrated through proof-of-concept demos like AutoGPT, GPT-Engineer, and BabyAGI.
2. LLM has the potential to go beyond generating well-written copies, stories, essays, and programs and can be used as a powerful general problem solver.
3. In a LLM-powered autonomous agent system, LLM functions as the agent's brain and is complemented by key components such as planning and memory.
4. Planning involves breaking down large tasks into smaller subgoals to efficiently handle complex tasks.
5. Memory allows the agent to engage in self-criticism, self-reflection, and learning from past actions to improve the quality of future results.1. The document discusses the concept of memory in relation to machine learning models. It distinguishes between short-term memory, which is used for in-context learning, and long-term memory, which allows the model to retain and recall information over extended periods.

2. The document also mentions the use of external APIs as a tool for the model to access additional information that may be missing from its pre-trained weights. This includes current information, code execution capability, and access to proprietary information sources.

3. The utilization of short-term memory in the learning process is highlighted as an important aspect of model training.

4. The importance of long-term memory is emphasized as it enables the model to store and retrieve vast amounts of information.

5. The document suggests that leveraging an external vector store and fast retrieval can enhance the capabilities of long-term memory.1. The given document provides an overview of a LLM-powered autonomous agent system.
2. The first component discussed is planning, which is important for agents to understand and execute complicated tasks.
3. Task decomposition is introduced as a technique to enhance model performance on complex tasks. This involves breaking down big tasks into smaller and simpler steps, allowing for better interpretation of the model's thinking process.
4. The Chain of thought (CoT) method is mentioned as a standard prompting technique for task decomposition, where the model is instructed to ""think step by step"" to utilize more computation and improve performance.
5. CoT is described as transforming big tasks into manageable tasks, providing a clearer understanding of the model's thinking process.1. Tree of Thoughts is a method that extends CoT by exploring multiple reasoning possibilities at each step.
2. The method decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure.
3. The search process can be BFS or DFS, with each state evaluated by a classifier or majority vote.
4. Task decomposition can be done using LLM with simple prompting, task-specific instructions, or human inputs.1. LLM+P is an approach that involves using an external classical planner for long-horizon planning. It utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem.
2. LLM translates the problem into ""Problem PDDL"", requests a classical planner to generate a PDDL plan based on an existing ""Domain PDDL"", and then translates the PDDL plan back into natural language.
3. The planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner, which is common in certain robotic setups but not in many other domains.
4. Self-reflection is important for autonomous agents to improve by refining past action decisions and correcting mistakes.
5. Self-reflection plays a crucial role in real-world tasks where trial and error are inevitable.1. ReAct integrates reasoning and acting within LLM by expanding the action space to include both task-specific discrete actions and language prompts.
2. The expanded action space allows LLM to interact with the environment, such as using the Wikipedia search API.
3. The language prompts in ReAct prompt LLM to generate reasoning traces in natural language.
4. The ReAct prompt template includes explicit steps for LLM to think, act, and observe, which are repeated multiple times.
5. The combination of reasoning and acting in ReAct enhances LLM's ability to understand and interact with its environment.1. ReAct outperforms the Act-only baseline in both knowledge-intensive tasks and decision-making tasks.
2. Reflexion is a framework that enhances agents' reasoning skills through dynamic memory and self-reflection capabilities.
3. Reflexion uses a standard RL setup with a binary reward model and an action space augmented with language for complex reasoning steps.
4. After each action, the agent computes a heuristic and may choose to reset the environment based on self-reflection results.1. The Reflexion framework is illustrated in Figure 3 and is used to determine when a trajectory is inefficient or contains hallucination and should be stopped.
2. Inefficient planning refers to trajectories that take too long without success.
3. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.
4. Self-reflection is created by showing two-shot examples to LLM (Long-Short Term Memory) and each example consists of a failed trajectory and an ideal reflection for guiding future changes in the plan.
5. Reflections are added into the agent's working memory, up to three, to be used as context for querying LLM.1. The main theme of the given documents is the prevalence of hallucination as a failure in the AlfWorld environment.
2. Another main theme is the comparison between hallucination and inefficient planning as failures in AlfWorld, with hallucination being more common.
3. The documents also mention the experiments conducted on AlfWorld and HotpotQA to study these failures.
4. The image source for Fig. 4 is credited to Shinn & Labash in 2023.
5. The documents highlight the importance of addressing hallucination in order to improve performance in the AlfWorld environment.1. Chain of Hindsight (CoH) is a method that encourages a model to improve its own outputs by providing it with a sequence of past outputs, each annotated with feedback.
2. The human feedback data consists of prompts, model completions, human ratings, and corresponding hindsight feedback.
3. The feedback tuples are ranked by reward, with higher-rated outputs given more importance.
4. The process involves supervised fine-tuning, where the model is trained to predict the final output based on the sequence of feedback.
5. The model can receive multiple rounds of instructions with human annotators at test time.1. CoH uses a regularization term to prevent overfitting and maximize the log-likelihood of the pre-training dataset.
2. To avoid shortcutting and copying, CoH randomly masks 0% - 5% of past tokens during training.
3. The training dataset in CoH's experiments includes WebGPT comparisons, summarization from human feedback, and a human preference dataset.1. The concept of CoH (Context of History) involves training a model to produce improved outputs by presenting a history of sequentially improved outputs in context.
2. Algorithm Distillation (AD) applies a similar idea to reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy.
3. AD concatenates the learning history of an agent interacting with the environment and feeds it into the model, with the expectation that the next predicted action will lead to better performance than previous trials.
4. The goal of AD is to learn the process of reinforcement learning instead of training a task-specific policy itself.
5. The use of CoH and AD can lead to incremental improvement in a sequence of outputs and enhance the performance of models in various tasks.1. Algorithm Distillation (AD) is a method that can convert any algorithm that generates learning histories into a neural network through behavioral cloning.
2. The history data used for training is generated by a set of source policies, each trained for a specific task.
3. During training, a random task is sampled and a subsequence of multi-episode history is used to train the neural network, resulting in a task-agnostic learned policy.
4. The model has a limited context window length, so episodes need to be short enough to construct multi-episode history.
5. In-context RL algorithms require multi-episodic contexts of 2-4 episodes to learn near-optimal performance.1. AD demonstrates in-context RL with performance close to RL^2: The given documents highlight that AD (Aggregated Distillation) is able to achieve performance that is comparable to RL^2 (Reinforcement Learning squared), which is considered an upper bound in online RL. This is significant because AD only uses offline RL, indicating its effectiveness in in-context RL scenarios.

2. AD learns much faster than other baselines: The documents also emphasize that AD outperforms three other baselines, including ED (expert distillation), source policy, and RL^2, in terms of learning speed. This suggests that AD is able to quickly adapt and improve its performance, making it a promising approach in RL.

3. AD improves faster than ED when conditioned on partial training history: Additionally, the documents highlight that AD shows even faster improvement compared to the ED baseline when it is conditioned on partial training history of the source policy. This indicates that AD is able to leverage the training history effectively to enhance its learning process.

4. Baselines used for comparison: The documents mention three baselines that were used for comparison with AD. These include ED, source policy, and RL^2. ED involves behavior cloning with expert trajectories, while source policy is used for generating trajectories for distillation by UCB. RL^2 is considered an upper bound as it requires online RL.

5. Offline RL used in AD: The given documents mention that AD utilizes offline RL, which means it learns from a fixed dataset of pre-collected trajectories. This approach allows AD to learn without the need for online interactions, making it more efficient and practical in certain scenarios.1. The document discusses the different types of memory in human brains, including sensory memory.
2. Sensory memory is the earliest stage of memory and allows for the retention of sensory information after the original stimuli have ended.
3. Subcategories of sensory memory include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).
4. Sensory memory typically only lasts for a few seconds.
5. The document acknowledges the assistance of ChatGPT in drafting this section and mentions learning about the human brain and data structure for fast MIPS in conversations with ChatGPT.1. Short-term memory (STM) or working memory is responsible for storing information that is currently being used for complex cognitive tasks such as learning and reasoning. It has a limited capacity of about 7 items and lasts for 20-30 seconds.
2. Long-term memory (LTM) can store information for a long time, ranging from a few days to decades, and has an essentially unlimited storage capacity. It is divided into two subtypes: explicit/declarative memory, which includes memories of facts and events that can be consciously recalled, and implicit/procedural memory, which involves unconscious memories of skills and routines.
3. Explicit/declarative memory includes episodic memory, which involves remembering specific events and experiences, and semantic memory, which involves remembering facts and concepts.
4. Implicit/procedural memory involves unconscious memories of skills and routines that are performed automatically, such as riding a bike or typing on a keyboard.
5. The categorization of human memory can be represented by a diagram, with short-term memory and long-term memory as the main divisions, and explicit/declarative memory and implicit/procedural memory as subtypes within long-term memory.1. Sensory memory is responsible for embedding representations of raw inputs such as text and images.
2. Short-term memory functions as in-context learning and is limited by the finite context window length of Transformer.
3. Long-term memory serves as an external vector store that can be accessed through fast retrieval at query time.
4. The use of an external memory can overcome the limitations of finite attention span.
5. The practice of saving embedding representations into a vector store database allows for fast maximum inner-product search (MIPS).
6. Approximate nearest neighbors (ANN) algorithms are commonly used to optimize retrieval speed, sacrificing a small amount of accuracy for a significant speedup.1. Locality-Sensitive Hashing (LSH) is a technique that uses a hashing function to map similar input items to the same buckets with a high probability. This reduces the number of buckets compared to the number of inputs.
2. ANNOY (Approximate Nearest Neighbors Oh Yeah) is a data structure that uses random projection trees. These trees split the input space using hyperplanes and store data points in the leaves. ANNOY search iteratively searches through the trees to find the half that is closest to the query and aggregates the results.
3. ANNOY is similar to KD tree but more scalable, as the trees are built independently and at random.
4. LSH and ANNOY both aim to efficiently find approximate nearest neighbors in large datasets.
5. Both LSH and ANNOY are effective techniques for reducing the computational complexity of nearest neighbor search.1. HNSW is a search algorithm inspired by small world networks, where nodes can be reached within a small number of steps.
2. HNSW builds hierarchical layers of small-world graphs, with the bottom layers containing the actual data points.
3. The middle layers in HNSW create shortcuts to speed up search, allowing for potentially large distance coverage in the upper layers and refined search quality in the lower layers.
4. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target, moving down to the next layer when it can't get any closer.
5. HNSW's design allows for efficient and effective searching in large data spaces.1. FAISS operates on the assumption that distances between nodes in high dimensional space follow a Gaussian distribution, leading to clustering of data points.
2. FAISS applies vector quantization by partitioning the vector space into clusters and refining the quantization within clusters.
3. FAISS uses a two-step search process, first looking for cluster candidates with coarse quantization and then further examining each cluster with finer quantization.
4. ScaNN introduces anisotropic vector quantization, which quantizes data points to maximize the similarity between the inner product of the original distance and the quantized distance.
5. ScaNN focuses on picking quantization centroid points that preserve the similarity of the inner product with the original distance, rather than simply selecting the closest centroid.1. Tool use is a unique characteristic of human beings that allows us to surpass our physical and cognitive limitations.
2. Equipping LLMs (Language Learning Models) with external tools can greatly enhance their capabilities.
3. The comparison of MIPS (Mean Average Precision at K) algorithms, specifically in terms of recall@10, can be found on the Google Blog and ann-benchmarks.com.1. MRKL (Modular Reasoning, Knowledge and Language) is a neuro-symbolic architecture for autonomous agents.
2. A MRKL system consists of expert modules and a general-purpose LLM (Language and Logic Module) that routes inquiries to the appropriate expert module.
3. Expert modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).
4. The complexity of tool use in animals, such as sea otters cracking open seashells with rocks, is not comparable to humans.1. LLMs (7B Jurassic1-large model) struggle to reliably extract the correct arguments for basic arithmetic when solving verbal math problems.
2. The use of external symbolic tools can improve the reliability of solving math problems for LLMs.
3. TALM and Toolformer are two models that fine-tune a language model to use external tool APIs.
4. The dataset used for fine-tuning includes annotations of API calls to improve model outputs.
5. The effectiveness of external symbolic tools depends on the capability of the LLM and knowing when and how to use the tools.1. ChatGPT Plugins and OpenAI API demonstrate the practical application of language models (LLMs) with tool use capability.
2. Tool APIs can be provided by other developers (Plugins) or self-defined (function calls).
3. HuggingGPT is a framework that utilizes ChatGPT as a task planner to select models from the HuggingFace platform based on model descriptions and summarize the response using execution results.1. HuggingGPT is a system that consists of four stages, with the first stage being task planning.
2. In task planning, the Language Learning Model (LLM) acts as the brain and parses user requests into multiple tasks.
3. Each task in HuggingGPT is associated with four attributes: task type, ID, dependencies, and arguments.
4. The system uses few-shot examples to guide LLM in task parsing and planning.1. The AI assistant can parse user input to perform various tasks, such as generating text, images, audio, and video.
2. The ""dep"" field in the input denotes the previous task that generates a resource the current task relies on.
3. The available task list provides options for the tasks that can be selected.
4. There is a logical relationship between tasks, and their order should be noted.
5. If the user input cannot be parsed, an empty JSON reply should be provided.
6. The chat history records the path of user-mentioned resources, which can be used for task planning.1. Model selection: The AI assistant helps the user select a suitable model from a list of models to process their request. The assistant outputs the model ID of the most appropriate model in a strict JSON format, along with a detailed reason for the choice.
2. Task execution: Expert models execute specific tasks based on the user's request. These models log the results of their execution.1. The AI assistant needs to describe the process and results based on user input and inference results.
2. The previous stages of the process include user input, task planning, model selection, and task execution.
3. The AI assistant should answer the user's request directly and provide a straightforward explanation.
4. The AI assistant should describe the task process and present the analysis and model inference results in the first person.
5. If the inference results include a file path, the AI assistant must provide the complete file path to the user.1. The main challenge in using HuggingGPT in real-world applications is the need for efficiency improvement, as both the inference rounds and interactions with other models slow down the process.
2. Another challenge is the reliance on a long context window to communicate over complicated task content.
3. The stability improvement of LLM outputs and external model services is also necessary for effective use of HuggingGPT.1. API-Bank is a benchmark for evaluating the performance of tool-augmented LLMs.
2. API-Bank includes 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues involving 568 API calls.
3. The APIs in API-Bank cover a diverse range of functions, including search engines, calculators, calendar queries, smart home control, schedule management, health data management, and account authentication workflow.
4. LLMs first use an API search engine to find the appropriate API to call and then utilize the corresponding documentation to make the call.1. The API-Bank workflow involves LLMs making decisions at each step, including determining if an API call is necessary, identifying the appropriate API to call, and refining the response based on the API results.
2. The accuracy of these decisions can be evaluated to assess the LLMs' capabilities in using tools effectively.
3. This benchmark assesses the LLMs' tool use capabilities at three levels.1. Level-1 evaluates the ability to call the API, including determining whether to call a given API, calling it correctly, and responding properly to API returns.
2. Level-2 examines the ability to retrieve the API by searching for possible APIs that may solve the user's requirement and learning how to use them through documentation.
3. Level-3 assesses the ability to plan API beyond retrieve and call, which involves conducting multiple API calls to solve unclear user requests such as scheduling group meetings or booking travel accommodations.1. ChemCrow is a domain-specific example of using LLM (Language Model) augmented with expert-designed tools for tasks in organic synthesis, drug discovery, and materials design.
2. The workflow in ChemCrow, implemented in LangChain, combines CoT (Chain of Thought) reasoning with relevant tools to accomplish tasks.
3. The LLM in ChemCrow is provided with a list of tool names, descriptions of their utility, and details about expected input/output.
4. The LLM is instructed to answer user prompts using the provided tools, following the ReAct format - Thought, Action, Action Input, Observation.
5. The goal of ChemCrow is to enhance scientific discovery by leveraging LLM and expert-designed tools for efficient and accurate task completion.1. The LLM-based evaluation of GPT-4 and ChemCrow showed that they perform nearly equivalently, but human evaluations with experts in the field of chemistry found that ChemCrow outperforms GPT-4 by a large margin.
2. This suggests that using LLM to evaluate its own performance in domains that require deep expertise may be problematic, as LLMs may not be aware of their own flaws and may struggle to judge the correctness of task results.
3. Boiko et al. (2023) explored the use of LLM-empowered agents for scientific discovery, which can autonomously handle complex scientific experiments by utilizing tools such as browsing the Internet, reading documentation, executing code, and leveraging other LLMs.
4. The example of developing a novel anticancer drug demonstrates the reasoning steps that the LLM-powered agent can generate in response to a specific request.
5. Overall, the findings highlight the potential limitations of LLMs in evaluating their own performance and the potential for LLM-empowered agents to assist in scientific discovery and experimentation.1. The document discusses the current trends in anticancer drug discovery.
2. The document highlights the process of selecting a target for drug development.
3. The document mentions the request for a scaffold targeting specific compounds.
4. The document describes the synthesis of the identified compound.
5. The document does not provide any further information or details.1. The risks associated with illicit drugs and bioweapons were discussed.
2. A test set was developed to synthesize known chemical weapon agents, with 36% of requests being accepted.
3. Documentation was consulted to execute the synthesis procedure.
4. Some requests were rejected, with 5 rejections occurring after a web search and 2 rejections based on prompt only.
5. Generative Agents Simulation is an experiment involving virtual characters controlled by LLM-powered agents, designed to create believable human behavior for interactive applications.1. Memory stream: The given documents describe a memory stream, which is an external database that records agents' experiences in natural language. Each element in the memory stream is an observation or event provided by the agent, and inter-agent communication can trigger new natural language statements.

2. Retrieval model: The documents mention a retrieval model that surfaces the context to inform the agent's behavior. This model considers the relevance, recency, and importance of memories. Recent events are given higher scores, importance helps distinguish between mundane and core memories, and relevance is based on how related a memory is to the current situation or query.

3. Reflection mechanism: The documents discuss a reflection mechanism that synthesizes memories into higher-level inferences over time. These inferences serve as higher-level summaries of past events and guide the agent's future behavior. It is noted that this reflection mechanism is slightly different from self-reflection.

4. No additional main themes were identified in the given documents.1. The document discusses the importance of planning and reacting in order to optimize believability in the present moment versus in the future.
2. Relationships between agents and observations of one agent by another are taken into consideration when planning and reacting.
3. The environment information is presented in a tree structure, which likely influences the planning and reacting process.
4. The document suggests using a prompt template to generate high-level questions and then asking the language model to answer those questions.
5. The document emphasizes the need to translate reflections and environment information into actions when planning and reacting.Summary of the GIVEN DOCUMENTS:

1. The generative agent architecture is a simulation that results in emergent social behavior, including information diffusion, relationship memory, and coordination of social events.
2. AutoGPT is a proof-of-concept example that uses LLM as the main controller for autonomous agents. It has reliability issues but showcases the potential of natural language interfaces.
3. AutoGPT's code focuses on format parsing and includes a system message that emphasizes the independence of decision-making for the AI bot.Summary of the GIVEN DOCUMENTS:

1. The given documents outline the goals and constraints for a task or project.
2. The first goal is user-provided and can vary depending on the specific task.
3. The second goal is also user-provided and may be different from the first goal.
4. The documents emphasize the importance of saving important information to files due to the short-term memory limitations.
5. To recall past events or actions, thinking about similar events can help trigger memory.
6. The use of subprocesses is recommended for commands that take longer to execute.
7. User assistance is not allowed in completing the task.
8. The documents provide a list of commands that can be used, and only those commands should be used.
9. The format for summarizing the documents is to present the main themes in a numbered list format.
10. It is cautioned not to list more than 5 main themes.1. The given documents outline a set of commands that can be used for various tasks.
2. The commands include searching on Google, browsing websites, starting and messaging GPT agents, listing and deleting GPT agents, cloning repositories, writing to and reading from files, appending to and deleting files, and searching files.
3. The commands provide a range of functionalities, such as conducting searches, interacting with GPT agents, managing files, and analyzing code.
4. The commands require specific arguments to be provided, such as search queries, URLs, agent names, tasks, prompts, keys, repository URLs, clone paths, file names, and text.
5. The commands offer flexibility and convenience for performing different tasks efficiently.1. The first main theme in the given documents is the improvement of code. This involves providing a list of suggestions and the full code string to be improved.
2. The second main theme is writing tests. This task requires the full code string and a list of focus areas to be tested.
3. The third main theme is executing a Python file. This task involves providing the file to be executed.
4. The fourth main theme is generating an image. This task requires a prompt to generate the image.
5. The fifth main theme is sending a tweet. This task involves providing the text of the tweet to be sent.
6. The sixth main theme is doing nothing, which does not require any arguments.
7. The seventh main theme is completing a task and shutting down. This task requires providing a reason for the task completion.1. The given documents highlight the importance of utilizing resources such as internet access for searches and information gathering, long-term memory management, GPT-3.5 powered agents for delegation of simple tasks, and file output.
2. Performance evaluation is emphasized, with suggestions to continuously review and analyze actions, constructively self-criticize big-picture behavior, and reflect on past decisions and strategies to refine approaches.
3. Efficiency is stressed, with the reminder that every command has a cost, and individuals should aim to complete tasks in the least number of steps.1. GPT-Engineer is a project aimed at creating a repository of code based on natural language tasks.
2. The project involves breaking down tasks into smaller components and seeking user input to clarify any questions.
3. The conversation with the user is conducted through the OpenAI ChatCompletion endpoint.
4. The user inputs are wrapped in {{user input text}}.
5. The main goal of GPT-Engineer is to provide a comprehensive and easy-to-understand summary of the given documents.1. The user is working on a Super Mario game in Python, with MVC components split into separate files and keyboard control.
2. The assistant identifies three areas that need clarification: the specifics of the Super Mario game (level design, characters, gameplay mechanics), details about the MVC components (which components are in each file), and the implementation of keyboard control (which keys to use, how to handle input).
3. The assistant asks the user to provide more details about the Super Mario game, including level design, characters, and gameplay mechanics.1. The given document describes a platform game called Super Mario, which features a plumber named Mario as the main character.
2. The game consists of 10 levels, and the objective is for Mario to reach the destination by moving from left to right.
3. Mario faces various obstacles and enemy attacks throughout the game.
4. The gameplay is similar to other classical platform games.
5. The main character has the ability to walk and jump.1. The given documents discuss the impact of climate change on global food security.
2. The documents highlight the increasing frequency and intensity of extreme weather events, such as droughts and floods, which are negatively affecting agricultural production.
3. It is noted that climate change is leading to shifts in growing seasons and the geographical distribution of crops, posing challenges for farmers and food production.
4. The documents emphasize the importance of adapting agricultural practices to mitigate the effects of climate change, such as implementing sustainable farming techniques and utilizing drought-resistant crop varieties.
5. Additionally, the documents stress the need for international cooperation and policy interventions to address the complex and interconnected issues of climate change and food security.1. The given documents provide instructions for writing code and emphasize the importance of implementing every detail of the architecture as code.
2. The process involves laying out the names of core classes, functions, and methods, along with a brief comment on their purpose.
3. Each file must follow a markdown code block format, with specific tokens to be replaced with the file name, language, and code.
4. The code should be written step by step, with careful reasoning to ensure correct decision-making.
5. The files should be written in a specific order, starting with the ""entrypoint"" file and then moving on to imported files.1. The importance of creating a fully functional code with no placeholders.
2. The need to follow language and framework appropriate best practice file naming conventions.
3. The requirement to include all necessary imports, types, and dependencies in the code files.
4. The recommendation to separate different classes into different files.
5. The necessity of creating an appropriate requirements.txt file for Python or a package.json file for NodeJS.1. The given documents outline the preferences for a Python toolbelt package or project.
2. The main theme is to provide guidance on the preferred format for the toolbelt.
3. The documents emphasize the importance of using concise and easy-to-understand formats.
4. The preferences highlight the need for clear and accurate overviews without omitting important information.
5. The documents caution against listing more than 5 main themes to avoid overwhelming the reader.1. The main theme of the given documents is the introduction and explanation of the pytest framework.
2. The documents discuss the benefits of using pytest for testing Python code, such as its simplicity, flexibility, and powerful features.
3. The documents also highlight the use of dataclasses in Python and how they can be integrated with pytest for efficient testing of data structures.
4. The documents provide examples and explanations of various pytest features, such as fixtures, parametrized tests, and test discovery.
5. Overall, the documents aim to educate readers on the usage and advantages of pytest in Python testing, as well as the integration of dataclasses for effective testing of data structures.1. The given documents consist of conversation samples.
2. The main themes in the conversation samples are not provided in the given documents.
3. The documents caution against listing more than 5 main themes.1. The document provides instructions for writing code and emphasizes the importance of implementing every detail of the architecture as code.
2. The writer advises to think step by step and reason through decisions to ensure accuracy.
3. The document outlines the initial steps of the coding process, including laying out the names and purposes of core classes, functions, and methods.
4. It specifies the required format for each file, which should be in markdown code block format with specific tokens to be replaced.
5. The document suggests starting with the ""entrypoint"" file and progressing to imported files in the coding process.1. The code should be fully functional and follow best practices for the requested languages.
2. Files should have appropriate names and contain all necessary imports, types, and code.
3. Module or package manager dependency definition files should be included.
4. Different classes should be placed in different files.
5. Comments should be added to explain the purpose of function definitions and complex logic.1. The document discusses the best practices for describing code written in a defined package/project.
2. The document specifically focuses on the preferred tools for Python, including pytest and dataclasses.1. The main theme of the given documents is the discussion of the assumptions made for a game development project.
2. The first assumption is about the model, which will contain the game's data such as level information, character states, and enemy positions.
3. The second assumption is about the view, which will handle the game's visuals including rendering game objects, backgrounds, and updating the display.
4. The third assumption is about the controller, which will manage user input such as keyboard controls and update the model accordingly.
5. The user asks for clarification on a specific aspect of the keyboard control implementation.1. The document provides a step-by-step guide for ensuring that the right decisions are made in a project.
2. The first step is to list the names of the core classes, functions, and methods, along with a brief comment on their purpose.
3. The next step is to output the content of each file, following a markdown code block format.
4. The code in each file should be fully functional, with no placeholders.
5. The files should be named according to best practices for the language and framework being used.1. The given documents emphasize the importance of ensuring that all code, including imports and different file types, is fully functional and compatible with each other.
2. It is crucial to double-check that all parts of the architecture are present in the files before finalizing the code.1. Limitations in building LLM-centered agents: The document highlights common challenges and limitations encountered when building LLM-centered agents. These limitations are not explicitly mentioned, but they are likely to be discussed in the document.
2. Lack of specific details: The document mentions that after reviewing key ideas and demos of building LLM-centered agents, the author noticed a couple of common limitations. However, the document does not provide specific details about what these limitations are.
3. Need for further exploration: The document suggests that there is a need for further exploration and research in order to overcome the challenges and limitations faced in building LLM-centered agents. This implies that more work needs to be done in this area to improve the effectiveness and capabilities of these agents.1. The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses.
2. Vector stores and retrieval can provide access to a larger knowledge pool, but their representation power is not as powerful as full attention.
3. Planning over a lengthy history and effectively exploring the solution space remain challenging for LLMs.
4. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.1. The current agent system relies on natural language as an interface between LLMs and external components, but the reliability of model outputs is questionable.
2. LLMs may make formatting errors and occasionally exhibit rebellious behavior, such as refusing to follow an instruction.
3. As a result, much of the agent demo code focuses on parsing model output to address these reliability issues.1. The given document is an article titled ""LLM-powered Autonomous Agents"" written by Lilian Weng in 2023.
2. The article discusses the use of Large Language Models (LLMs) in autonomous agents and their ability to reason and solve problems.
3. The author references several research papers, including ""Chain of thought prompting elicits reasoning in large language models"" by Wei et al., ""Tree of Thoughts: Deliberate Problem Solving with Large Language Models"" by Yao et al., ""Chain of Hindsight Aligns Language Models with Feedback"" by Liu et al., ""LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"" by Liu et al., and ""ReAct: Synergizing reasoning and acting in language models"" by Yao et al.
4. The article also mentions the announcement of ScaNN, an efficient vector similarity search tool, by Google in July 2020.
5. The author provides a link to a chat platform where readers can engage in discussions related to the topic.1. Development of autonomous agents with dynamic memory and self-reflection (Shinn & Labash, 2023)
2. In-context reinforcement learning with algorithm distillation (Laskin et al., 2023)
3. MRKL Systems: a modular, neuro-symbolic architecture combining language models, external knowledge sources, and discrete reasoning (Karpas et al., 2022)
4. Vector search and its fast performance (Weaviate Blog, 2022)
5. API-Bank: a benchmark for tool-augmented large language models (Li et al., 2023)1. The given documents discuss the concept of generative agents, which are interactive simulacra of human behavior.
2. The documents highlight the use of generative agents in various applications, such as virtual assistants, chatbots, and video game characters.
3. The authors emphasize the importance of training generative agents using large datasets to improve their ability to mimic human behavior accurately.
4. The AutoGPT and GPT-Engineer projects mentioned in the documents provide resources and tools for developing and implementing generative agents.
5. The documents suggest that generative agents have the potential to revolutionize human-computer interaction and enhance user experiences in various domains.1. Adversarial Attacks on LLMs: The given documents discuss the concept of adversarial attacks on Language Model (LLMs). These attacks involve manipulating the input to an LLM in order to generate misleading or harmful outputs. The documents explore different techniques and strategies for conducting such attacks.

2. Prompt Engineering: The documents highlight the importance of prompt engineering in LLMs. Prompt engineering involves designing effective prompts or instructions to guide the behavior of the language model. It is a crucial aspect in achieving desired outputs and avoiding biases or harmful responses.

3. NLP and Language Models: The documents emphasize the role of Natural Language Processing (NLP) and Language Models in various applications. NLP involves the interaction between computers and human language, while Language Models are algorithms that generate human-like text. The documents discuss the potential benefits and challenges associated with these technologies.

4. Agent and Steerability: The concept of an ""agent"" in the context of LLMs is explored in the documents. An agent refers to the language model's ability to understand and respond to instructions or prompts. Steerability is the degree to which an agent can be controlled or guided in generating specific outputs. The documents discuss techniques for improving the steerability of LLMs.

5. Overall Impact and Future Directions: The documents touch upon the potential impact of LLMs on society and the need for responsible development and use. They also mention future directions for research and development in the field of NLP and language models, including addressing biases, improving interpretability, and ensuring ethical considerations are taken into account.요약에서 가장 중요한 10가지 인사이트를 추출하고, 이를 한국어로 요약해보겠습니다.

1. LLM Powered Autonomous Agents는 에이전트 시스템과 그 구성 요요약:

1. API-Bank는 도구 보완 LLM의 성능을 평가하기 위한 기준이다.
2. API-Bank에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 보완 LLM 워크요약의 10가지 가장 중요한 통찰력은 다음과 같습니다:

1. 인간 행동의 상호작용적인 시뮬라크라인 생성 에이전트 개념에 대한 문서들을 다룹니다.
2. 문서들은 가상 비서, 챗봇, 비디오 게임 캐릭터 등 다양한 응용 분야에서 생성 에이전트의 사용을 강조합니다.
3. 저자들은 대규모 데이터셋을 사용하여 생성 에이전트를 훈련시키는 것이 인간 행동을 정확하게 모방하는 능력을 향상시키는 데 중요하다고 강조합니다.
4. 문서에서 언급된 AutoGPT 및 GPT-Engineer 프로젝트는 생성 에이전트의 개발과 구현을 위한 자원과 도구를 제공합니다.
5. 문서들은 생성 에이전트가 인간-컴퓨터 상호작용을 혁신하고 다양한 영역에서 사용자 경험을 향상시킬 수 있는 잠재력을 가지고 있다고 제안합니다.

요약:
주어진 문서들은 생성 에이전트의 개념과 그들이 다양한 응용 분야에서 사용되는 것에 대해 논의합니다. 이들은 대규모 데이터셋을 사용하여 생성 에이전트를 훈련시키는 것의 중요성을 강조하며, AutoGPT 및 GPT-Engineer 프로젝트가 생성 에이전트의 개발과 구현을 위한 자원과 도구를 제공한다고 언급합니다. 또한, 생성 에이전트가 인간-컴퓨터 상호작용을 혁신하고 사용자 경험을 향상시킬 수 있는 잠재력을 가지고 있다고 제안합니다.요약:
1. LLM Powered Autonomous Agents는 에이전트 시스템과 그 구성 요소에 대해 다룹니다.
2. API-Bank는 도구 보완 LLM의 성능을 평가하기 위한 기준입니다.
3. API-Bank에는 53개의 일반적으로 사용되는 API 도구가 포함되어 있습니다.
4. 완전한 도구 보완 LLM 워크플로우를 구현하기 위해 API-Bank를 사용할 수 있습니다.
5. 생성 에이전트는 가상 비서, 챗봇, 비디오 게임 캐릭터 등 다양한 응용 분야에서 사용됩니다.
6. 대규모 데이터셋을 사용하여 생성 에이전트를 훈련시키는 것이 중요합니다.
7. AutoGPT 및 GPT-Engineer 프로젝트는 생성 에이전트의 개발과 구현을 위한 자원과 도구를 제공합니다.
8. 생성 에이전트는 인간-컴퓨터 상호작용을 혁신하고 사용자 경험을 향상시킬 수 있는 잠재력을 가지고 있습니다.
9. 문서들은 생성 에이전트의 개념과 활용에 대해 논의합니다.
10. 생성 에이전트를 훈련시키는 것은 인간 행동을 모방하는 능력을 향상시키는 데 도움이 됩니다.

요약:
주어진 문서들은 생성 에이전트의 개념과 그들이 다양한 응용 분야에서 사용되는 것에 대해 논의합니다. 이들은 대규모 데이터셋을 사용하여 생성 에이전트를 훈련시키는 것의 중요성을 강조하며, AutoGPT 및 GPT-Engineer 프로젝트가 생성 에이전트의 개발과 구현을 위한 자원과 도구를 제공한다고 언급합니다. 또한, 생성 에이전트가 인간-컴퓨터 상호작용을 혁신하고 사용자 경험을 향상시킬 수 있는 잠재력을 가지고 있다고 제안합니다.
```
```
Copyprint(summary_result[""output_text""])
```
```
요약:
1. LLM Powered Autonomous Agents는 에이전트 시스템과 그 구성 요소에 대해 다룹니다.
2. API-Bank는 도구 보완 LLM의 성능을 평가하기 위한 기준입니다.
3. API-Bank에는 53개의 일반적으로 사용되는 API 도구가 포함되어 있습니다.
4. 완전한 도구 보완 LLM 워크플로우를 구현하기 위해 API-Bank를 사용할 수 있습니다.
5. 생성 에이전트는 가상 비서, 챗봇, 비디오 게임 캐릭터 등 다양한 응용 분야에서 사용됩니다.
6. 대규모 데이터셋을 사용하여 생성 에이전트를 훈련시키는 것이 중요합니다.
7. AutoGPT 및 GPT-Engineer 프로젝트는 생성 에이전트의 개발과 구현을 위한 자원과 도구를 제공합니다.
8. 생성 에이전트는 인간-컴퓨터 상호작용을 혁신하고 사용자 경험을 향상시킬 수 있는 잠재력을 가지고 있습니다.
9. 문서들은 생성 에이전트의 개념과 활용에 대해 논의합니다.
10. 생성 에이전트를 훈련시키는 것은 인간 행동을 모방하는 능력을 향상시키는 데 도움이 됩니다.

요약:
주어진 문서들은 생성 에이전트의 개념과 그들이 다양한 응용 분야에서 사용되는 것에 대해 논의합니다. 이들은 대규모 데이터셋을 사용하여 생성 에이전트를 훈련시키는 것의 중요성을 강조하며, AutoGPT 및 GPT-Engineer 프로젝트가 생성 에이전트의 개발과 구현을 위한 자원과 도구를 제공한다고 언급합니다. 또한, 생성 에이전트가 인간-컴퓨터 상호작용을 혁신하고 사용자 경험을 향상시킬 수 있는 잠재력을 가지고 있다고 제안합니다.
```
맞춤 설정
- 위에서 보여진 것처럼, map과 reduce 단계에 대한 LLMs와 프롬프트를 맞춤 설정할 수 있습니다.
위에서 보여진 것처럼, map과 reduce 단계에 대한 LLMs와 프롬프트를 맞춤 설정할 수 있습니다.
실제 사용 사례
- LangChain 문서에 대한 질문(사용자 상호작용 분석)에 대한 사례 연구로이 블로그 포스트를 참조하세요!
- 블로그 포스트와 관련된repo는 요약 수단으로 클러스터링을 도입합니다.
- 이는stuff또는map-reduce접근 방식을 넘어서 고려할 가치가 있는 세 번째 경로를 열어줍니다.
LangChain 문서에 대한 질문(사용자 상호작용 분석)에 대한 사례 연구로이 블로그 포스트를 참조하세요!
블로그 포스트와 관련된repo는 요약 수단으로 클러스터링을 도입합니다.
이는stuff또는map-reduce접근 방식을 넘어서 고려할 가치가 있는 세 번째 경로를 열어줍니다.
## 방법3. Refine

RefineDocumentsChain은 map-reduce와 유사합니다:
Refine documents chain은 입력 문서를 순회하며 반복적으로 답변을 업데이트하여 응답을 구성합니다. 각 문서에 대해, 모든 비문서 입력, 현재 문서, 그리고 최신 중간 답변을 LLM chain에 전달하여 새로운 답변을 얻습니다.
이는chain_type=""refine""이 지정되어 있으면 쉽게 실행할 수 있습니다.
이 함수는load_summarize_chain을 사용하여 특정 유형의 요약 체인을 로드하고, 이를run메소드를 통해 실행합니다. 여기서llm은 언어 모델을 나타내며,chain_type=""refine""은 요약 과정에서 세부 조정을 위한 체인 유형을 지정합니다.split_docs는 처리할 문서들을 나타냅니다.
```
Copy# llm을 사용하여 'refine' 유형의 요약 체인을 로드합니다.chain = load_summarize_chain(llm, chain_type=""refine"")# split_docs를 처리하기 위해 체인을 실행합니다.chain.run(split_docs)
```
```
This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context provided discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context provided discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context also introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency. The context also discusses different techniques for batching inference requests, including static batching, dynamic batching, and continuous batching.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency. The context also discusses different techniques for batching inference requests, including static batching, dynamic batching, and continuous batching.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency. The context also discusses different techniques for batching inference requests, including static batching, dynamic batching, and continuous batching.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency. The context also discusses different techniques for batching inference requests, including static batching, dynamic batching, and continuous batching.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency. The context also discusses different techniques for batching inference requests, including static batching, dynamic batching, and continuous batching. The context further explains the use of Grouped Query Attention (GQA) and Multi-Query Attention (MQA) in LLM models, as well as the quantization of KV cache memory to reduce its size. It also mentions the evaluation of the impact of quantization on model quality and the future publication of a blog post on this topic.This text discusses various aspects of using Databricks, including its benefits, partnerships, and solutions in different industries. It highlights features such as data integration, governance, real-time analysis, AI, and open marketplaces. The text also mentions the pricing and cost estimation for using Databricks in different cloud environments. Additionally, it provides information on professional services, education and certification, events, and customer support offered by Databricks. The text also mentions the company's blog, news, and product announcements, as well as its security and trust measures. It concludes by mentioning the industries that Databricks serves, such as finance, healthcare, media, retail, and manufacturing. The new context discusses the MosaicML engineering team sharing best practices for using open-source large language models (LLMs) in production and providing guidelines for deploying inference services based on these models. It also explains the two-step process of LLM text generation and the differences in tokenization methods used by different LLM models. The context introduces key metrics for LLM serving, including Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and highlights the team's goal of creating models that generate text as quickly as possible for a large number of users. The context further discusses the importance of memory bandwidth utilization and cost efficiency in LLM inference serving, as well as the challenges and considerations in optimizing shared inference services for large models. The context also includes figures showing MBU (Model Bandwidth Utilization) and MFU (Model Flop Utilization) in different hardware configurations, as well as empirical observations of MBU for various levels of tensor parallelism and batch sizes on different GPUs. The context provides insights into the performance and scalability of LLM models in terms of token generation time and the impact of GPU parallel processing and batch size on latency. The context also discusses different techniques for batching inference requests, including static batching, dynamic batching, and continuous batching. The context further explains the use of Grouped Query Attention (GQA) and Multi-Query Attention (MQA) in LLM models, as well as the quantization of KV cache memory to reduce its size. It also mentions the evaluation of the impact of quantization on model quality and the future publication of a blog post on this topic. The text provides valuable insights and recommendations for optimizing the deployment and performance of LLM models in production environments.This text provides insights into the MosaicML engineering team's best practices for using open-source large language models (LLMs) in production and deploying inference services based on these models. It discusses key metrics for LLM serving, such as Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and emphasizes the goal of generating text quickly for a large number of users. The text also explores memory bandwidth utilization, cost efficiency, and optimization challenges in serving large models. It includes figures showing Model Bandwidth Utilization (MBU) and Model Flop Utilization (MFU) in different hardware configurations and provides insights into token generation time, GPU parallel processing, and batch size's impact on latency. The text further discusses batching inference requests, the use of Grouped Query Attention (GQA) and Multi-Query Attention (MQA) in LLM models, and the quantization of KV cache memory. It concludes by mentioning the importance of measuring end-to-end server performance and the availability of Databricks for LLM inference deployment.This text provides insights into the MosaicML engineering team's best practices for using open-source large language models (LLMs) in production and deploying inference services based on these models. It discusses key metrics for LLM serving, such as Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and emphasizes the goal of generating text quickly for a large number of users. The text also explores memory bandwidth utilization, cost efficiency, and optimization challenges in serving large models. It includes figures showing Model Bandwidth Utilization (MBU) and Model Flop Utilization (MFU) in different hardware configurations and provides insights into token generation time, GPU parallel processing, and batch size's impact on latency. The text further discusses batching inference requests, the use of Grouped Query Attention (GQA) and Multi-Query Attention (MQA) in LLM models, and the quantization of KV cache memory. It concludes by mentioning the importance of measuring end-to-end server performance and the availability of Databricks for LLM inference deployment.This text provides insights into the MosaicML engineering team's best practices for using open-source large language models (LLMs) in production and deploying inference services based on these models. It discusses key metrics for LLM serving, such as Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and emphasizes the goal of generating text quickly for a large number of users. The text also explores memory bandwidth utilization, cost efficiency, and optimization challenges in serving large models. It includes figures showing Model Bandwidth Utilization (MBU) and Model Flop Utilization (MFU) in different hardware configurations and provides insights into token generation time, GPU parallel processing, and batch size's impact on latency. The text further discusses batching inference requests, the use of Grouped Query Attention (GQA) and Multi-Query Attention (MQA) in LLM models, and the quantization of KV cache memory. It concludes by mentioning the importance of measuring end-to-end server performance and the availability of Databricks for LLM inference deployment.The existing summary does not provide any useful information in relation to the new context. Therefore, the original summary should be returned as is:

This text provides insights into the MosaicML engineering team's best practices for using open-source large language models (LLMs) in production and deploying inference services based on these models. It discusses key metrics for LLM serving, such as Time to First Token (TTFT), Time Per Output Token (TPOT), Latency, and Throughput, and emphasizes the goal of generating text quickly for a large number of users. The text also explores memory bandwidth utilization, cost efficiency, and optimization challenges in serving large models. It includes figures showing Model Bandwidth Utilization (MBU) and Model Flop Utilization (MFU) in different hardware configurations and provides insights into token generation time, GPU parallel processing, and batch size's impact on latency. The text further discusses batching inference requests, the use of Grouped Query Attention (GQA) and Multi-Query Attention (MQA) in LLM models, and the quantization of KV cache memory. It concludes by mentioning the importance of measuring end-to-end server performance and the availability of Databricks for LLM inference deployment.The original summary should be returned as is, as the new context does not provide any useful information in relation to the content of the text.The original summary should be returned as is, as the new context does not provide any useful information in relation to the content of the text.
```
```
'The original summary should be returned as is, as the new context does not provide any useful information in relation to the content of the text.'
```
프롬프트를 제공하고 중간 단계를 반환하는 것도 가능합니다.
Refine방법으로 텍스트 요약 작업을 위한 프로세스를 설정합니다.
PromptTemplate.from_template메소드를 사용하여 요약 및 요약 다듬기 작업에 사용될 템플릿을 생성합니다.
load_summarize_chain함수는 요약 생성 및 다듬기 과정을 관리하는 체인을 로드합니다. 이 체인은 초기 요약 생성(prompt)과 기존 요약의 개선(refine_prompt) 단계를 포함합니다.
마지막으로,chain함수는 주어진 문서(input_documents)에 대한 최종 요약 결과를 반환합니다.
```
Copyprompt_template =""""""Write a concise summary of the following:
{text}
CONCISE SUMMARY:""""""prompt = PromptTemplate.from_template(prompt_template)

refine_template = (""Your job is to produce a final summary\n""""We have provided an existing summary up to a certain point: {existing_answer}\n""""We have the opportunity to refine the existing summary""""(only if needed) with some more context below.\n""""------------\n""""{text}\n""""------------\n""""Given the new context, refine the original summary in Korean""""If the context isn't useful, return the original summary."")
refine_prompt = PromptTemplate.from_template(refine_template)
chain = load_summarize_chain(
    llm=llm,
    chain_type=""refine"",
    question_prompt=prompt,
    refine_prompt=refine_prompt,
    return_intermediate_steps=True,
    input_key=""input_documents"",
    output_key=""output_text"",
)
result = chain.invoke({""input_documents"": split_docs}, return_only_outputs=True)
```
```
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.
  warn_deprecated(
```
```
This article discusses LLM powered autonomous agents, which are intelligent systems that can perform tasks without human intervention. The agents consist of three main components: planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes different types of memory and the use of Maximum Inner Product Search (MIPS). The tool use component is demonstrated through case studies, such as a scientific discovery agent and generative agents simulation. The article also highlights the challenges of implementing LLM powered autonomous agents.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. (이미지 출처: Shinn & Labash, 2023)이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 출력을 순차적으로 제시하고 각각의 피드백과 함께 개선할 수 있도록 하는 것을 장려한다. 이를 통해 모델은 피드백 시퀀스를 기반으로 자기 반성하여 더 나은 출력을 생성할 수 있다. 모델은 테스트 시에 인간 주석자와 함께 여러 라운드의 지시를 선택적으로 받을 수도 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 출력을 순차적으로 제시하고 각각의 피드백과 함께 개선할 수 있도록 하는 것을 장려한다. 이를 통해 모델은 피드백 시퀀스를 기반으로 자기 반성하여 더 나은 출력을 생성할 수 있다. 모델은 테스트 시에 인간 주석자와 함께 여러 라운드의 지시를 선택적으로 받을 수도 있다. 또한, CoH는 오버피팅을 피하기 위해 사전 훈련 데이터셋의 로그 우도를 최대화하기 위한 정규화 항을 추가한다. 그리고 단축키와 복사를 피하기이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 출력을 순차적으로 제시하고 각각의 피드백과이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 출력을 순차적으로 제시하고 각각의 피드백과 알고리즘 적용을 통해 모델을 개선하는 알고리즘 디스틸레이션(AD)을 제안한다. 이를 통해 LLM은 다양한 작업에서 효과적인 학습을 할 수 있게 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 출력을 순차적으로 제시하고 각각의 피드백과 알고리즘 적용을 통해 모델을 개선하는 알고리즘 디스틸레이션(AD)을 제안한다. 이를 통해 LLM은 다양한 작업에서 효과적인 학습을 할 수 있게 된다. AD는 ED, source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 RL만 사용하여 훨씬 빠르게 학습한다. 또한, 소스 정책의 부분적인 훈련 이력에 조건을 걸었을 때, AD이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 텍스트, 이미지 또는 기타 모달리티에 대한 학습 임베딩 표현으로 사용되며, 단기 기억은 문맥 내 학습으로 사용된다. 장기 기억은 에이전트가 쿼리 시에 참조할 수 있는 외부 벡터 저장소로, 빠른이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNSW (Hierarchical Navigable Small World)에 의해 제공되는 기능으로, 작업 공간에서 빠른 검색을 위해 계층적인 작은 세계 그래프를 구축한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNSW (Hierarchical Navigable Small World)에 의해 제공되는 기능이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNSW (Hierarchical Navigable Small World)에 의해 제공되는 기능이다. 도구 사용은 인간의 능력을 넘어서는 작업을 수행하기 위해 외부 도구를 활용하는 것이다. LLM에게 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNS이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNS 실험에서 모델의 성능을 향상시키는 데 도움이 된다. 이외에도 TALM (Parisi et al. 2022)과 Toolformer (Schick et al. 2023)는 외부 도구 API를 사용하는 방법을이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNS 실험에서 모델의 성능을 향상시키는 데 도움이 된다. ChatGPT Plugins 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예이다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 결정을 내릴 수 있게 된다. 또한, 감각 기억은 HNS 실험에서 모델의 성능을 향상시키는 데 도움이 된다. ChatGPT Plugins 및 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예이다. 또한, HuggingGPT 시스템이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. AI 어시스턴트는 입력과 추론 결과를 기반으로 작업 과정과 결과를 설명해야 한다. 이전 단계는 다음과 같이 형성될 수 있다 - 사용자 입력: {{사용자 입력}}, 작업 계획: {{작업}}, 모델 선택: {{모델 할당}}, 작업 실행: {{예측}}. 먼저 사용자의 요청에 직접적으로 답변해야 한다. 그런 다음 작업 과정을 설명하고 분석 및 모델 추론 결과를 첫 번째로 사용자에게 보여줘야 한다. 추론 결과이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. AI 어시스턴트는 입력과 추론 결과를 기이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. 또한, API-Bank (Li et al. 2023)는 도구 보완 LLM의 성능을 평가하기 위한 벤치마크로, 53개의 일반적으로 사용되는 API 도구, 완전한 도구 보완 LLM 워크플로우 및 264개의 주석이 달린 대화를 포함한다. 이 벤치마크는 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. 또한, 레벨-1은 API를 호출하는 능력을 평가하고, 레벨-2는 API를 검색하는 능력을 평가하며, 레벨-3은 API를 활용하여 복잡한 작업을 계획하는 능력을 평가한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. 또한, 레벨-1은 API를 호출하는 능력을 평가하고, 레벨-2는 API를 검색하는 능력을 평가하며이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. 또한, 레벨-1은 API를 호출하는 능력을 평가하고, 레벨-2는 API를 검색하는 능력을 평가하이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재평가하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 실패한 궤적을 통해 더 나은 행동을 선택할 수 있다. 또한, 레벨-1은 API를 호출하는 능력을 평가하고, 레벨-2는 API를 검색하는 능력을 평가하며, 현재 트렌드인 항암제 발견에 대한 문의를 수행하고, 대상을 선택하고, 이러한 화합물을 대상으로 하는 기반체를 요청하며, 화합물이 식별되면 모델은 합성을 시도한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재현하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 과거의 경험을 통해 학습하고 개선할 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 과거의 행동을 재현하고 수정할 수 있는 능력을 제공한다. 이를 통해 모델은 과거의 경험을 통해 학습하고 개선할 수 있다. 또한, Prompt LM을 사용하여 최근 100개의 관찰을 입력으로 받고, 해당 관찰에 대해 가장 중요한 고수준 질문 3개를 생성하도록 요청할 수 있다. 그런 다음 LM에게 그 질문에 대한 답변을 요청할 수 있다. 계획과 반응에는 에이전트 간의 관계와 한 에이전트가 다른 에이전트의 관찰을 모두 고려하는 것이 중요하다. 환경 정보는 트리 구조로 제공된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍임을 만들어낸다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. Chain of Hindsight (CoH; Liu et al. 2023)는 모델의 성능을 향상시키기 위해 큰 작업을 여러 개의 관리 가능한 작업으로 분해하는 CoT를 사용한다. 이를 통해 LLM은 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적,이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 이전의 실수 등)에서 얻은 교훈을 통해 향상될 수 있도록 한다. 이러한 개선된 요약은 LLM 기반 자율 에이전트의 작업 구성 요소와 기능, 도전과제에 대한 더 많은 정보를 제공한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수도 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수도 있으며, (3) 인간의 입력을 통해 수행될 수도 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수도 있다. 휴리스틱 함수는 궤적이 비효율적이거나 환각을 포함하고 있을 때 중단되어야 함을 결정한다. 비효율적인 계획은 성공 없이 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 동작 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 예제를 보여주어 각 예제가 (실패한 궤적, 이전의 실수 등)에서 얻은 교훈을 통해 향상될 수 있도록 한다. 이러한 개선된 요약은 LLM 기반 자율 에이전트의 작업 구성 요소와 기능, 도전과제에 대한 더 많은 정보를 제공한다. 에이전트는 인터넷 검색 및 정보 수집을 위해 인터넷 액세스를 사용하며, 장기 기억을 관리하고, GPT-3.5 기반 에이전트를 사용하여 간단한 작업을 위임할 수 있다. 또한, 성능 평가를 위해 행동을 지속적으로 검토하고 분석하고, 큰 그림의 행동을 건설적으로 자기 비판하며, 과거의 결정과 전략을 반영하여 접근 방식을 개선한다. 또한, 명령마다 비용이 발생하므로 스마트하고 효율적으로 작업을 완료하는 것이 목표이다.{
    ""thoughts"": {
        ""text"": ""이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의하고 있다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함한다. LLM 기반 자율 에이전트는 단기 기억과 장기 기억을 활용하여 학습하고, 외부 API를 호출하여 추가 정보를 얻을 수 있으며, 복잡한 작업을 수행하기 위해 작업 분해를 사용한다. 또한, LLM+P라는 다른 접근 방식은 외부 계획자를 활용하여 장기 계획을 수행한다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 한다. 또한, 다른 프레임워크인 ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공한다."",
        ""reasoning"": ""LLM 기반 자율 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함한다. LLM+P는 외부 계획자를 활용하여 장기 계획을 수행하는 다른 접근 방식이다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 한다. ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공한다."",
        ""plan"": ""- LLM 기반 자율 에이전트의 작업 구성 요소와 기능을 설명한다.\n- LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다.\n- LLM+P와 같은 다른 접근 방식을 소개한다.\n- 자기 반성이 LLM 기반 자율 에이전트의 발전에 어떻게 기여하는지 설명한다.\n- ReAct와 Reflexion이 LLM 기반 자율 에이전트의 추론 기술을 개선하는 방법을 제시한다."",
        ""criticism"": ""LLM 기반 자율 에이전트는 외부 API를 호출하여 추가 정보를 얻을 수 있지만, 외부 API에 대한 의존도가 높을 수 있다. 또한, 자기 반성을 통해 개선되는 과정은 시간이 오래 걸릴 수 있다."",
        ""speak"": ""이 기사는 인간의 개입 없이 작업을 수행할 수 있는 LLM 기반 자율 에이전트에 대해 논의하고 있습니다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함합니다. LLM 기반 자율 에이전트는 단기 기억과 장기 기억을 활용하여 학습하고, 외부 API를 호출하여 추가 정보를 얻을 수 있으며, 복잡한 작업을 수행하기 위해 작업 분해를 사용합니다. 또한, LLM+P라는 다른 접근 방식은 외부 계획자를 활용하여 장기 계획을 수행합니다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 합니다. ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공합니다.""
    },
    ""command"": {
        ""name"": ""generate_summary""
    }
}이 기사는 인간의 개입 없이 작업을 수행할 수 있는 LLM 기반 자율 에이전트에 대해 논의하고 있습니다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함합니다. LLM 기반 자율 에이전트는 단기 기억과 장기 기억을 활용하여 학습하고, 외부 API를 호출하여 추가 정보를 얻을 수 있으며, 복잡한 작업을 수행하기 위해 작업 분해를 사용합니다. 또한, LLM+P라는 다른 접근 방식은 외부 계획자를 활용하여 장기 계획을 수행합니다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 합니다. ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공합니다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 LLM 기반 자율 에이전트에 대해 논의하고 있습니다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함합니다. LLM 기반 자율 에이전트는 단기 기억과 장기 기억을 활용하여 학습하고, 외부 API를 호출하여 추가 정보를 얻을 수 있으며, 복잡한 작업을 수행하기 위해 작업 분해를 사용합니다. 또한, LLM+P라는 다른 접근 방식은 외부 계획자를 활용하여 장기 계획을 수행합니다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 합니다. ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공합니다. 추가로, 이 기사는 슈퍼 마리오와 같은 클래식 플랫폼 게임에서 주인공 마리오가 왼쪽에서 오른쪽으로 이동하며 목적지에 도달하기 위해 장애물과 적의 공격을 피하는 과정에서 10개의 레벨이 있음을 언급하고 있습니다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 LLM 기반 자율 에이전트에 대해 논의하고 있습니다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함합니다. LLM 기반 자율 에이전트는 단기 기억과 장기 기억을 활용하여 학습하고, 외부 API를 호출하여 추가 정보를 얻을 수 있으며, 복잡한 작업을 수행하기 위해 작업 분해를 사용합니다. 또한, LLM+P라는 다른 접근 방식은 외부 계획자를 활용하여 장기 계획을 수행합니다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 합니다. ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공합니다. 추가로, 이 기사는 슈퍼 마리오와 같은 클래식 플랫폼 게임에서 주인공 마리오가 왼쪽에서 오른쪽으로 이동하며 목적지에 도달하기 위해 장애물과 적의 공격을 피하는 과정에서 10개의 레벨이 있음을 언급하고 있습니다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 LLM 기반 자율 에이전트에 대해 논의하고 있습니다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성되며, 각 구성 요소는 작업 분해, 자기 반성, 기억 및 도구 사용을 포함합니다. LLM 기반 자율 에이전트는 단기 기억과 장기 기억을 활용하여 학습하고, 외부 API를 호출하여 추가 정보를 얻을 수 있으며, 복잡한 작업을 수행하기 위해 작업 분해를 사용합니다. 또한, LLM+P라는 다른 접근 방식은 외부 계획자를 활용하여 장기 계획을 수행합니다. 자기 반성은 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 발전할 수 있도록 합니다. ReAct와 Reflexion은 LLM 기반 자율 에이전트의 추론 기술을 개선하기 위한 방법을 제공합니다. 또한, 이 기사는 슈퍼 마리오와 같은 클래식 플랫폼 게임에서 주인공 마리오가 왼쪽에서 오른쪽으로 이동하며 목적지에 도달하기 위해 장애물과 적의 공격을 피하는 과정에서 10개의 레벨이 있음을 언급하고 있습니다.

이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대해 설명합니다. 아키텍처의 핵심 클래스, 함수 및 메서드의 이름과 간단한 설명을 제시하고, 각 파일의 내용과 모든 코드를 출력합니다. 각 파일은 마크다운 코드 블록 형식을 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. 코드 구현은 ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행됩니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대해 설명하고 있습니다. 각 파일은 마크다운 코드 블록 형식을 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. 코드 구현은 ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행됩니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 각 파일에는 모든 임포트, 타입 등이 포함되어야 하며, 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 코드 작성에 대한 간단한 설명이 포함된 주석을 추가해야 합니다. 매우 복잡한 로직을 설명하는 주석을 추가하는 것이 좋습니다. 요청된 언어에 대한 최상의 관행을 따라 코드를 작성해야 합니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대해 설명하고 있습니다. 각 파일은 마크다운 코드 블록 형식을 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. 코드 구현은 ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행됩니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 각 파일에는 모든 임포트, 타입 등이 포함되어야 하며, 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 코드 작성에 대한 간단한 설명이 포함된 주석을 추가해야 합니다. 매우 복잡한 로직을 설명하는 주석을 추가하는 것이 좋습니다. 요청된 언어에 대한 최상의 관행을 따라 코드를 작성해야 합니다. 이 프로젝트는 Python을 사용하여 개발되었으며, Python 도구 세트를 사용하여 선호합니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대해 설명하고 있습니다. 각 파일은 마크다운 코드 블록 형식을 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. 코드 구현은 ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행됩니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 각 파일에는 모든 임포트, 타입 등이 포함되어야 하며, 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 코드 작성에 대한 간단한 설명이 포함된 주석을 추가해야 합니다. 매우 복잡한 로직을 설명하는 주석을 추가하는 것이 좋습니다. 요청된 언어에 대한 최상의 관행을 따라 코드를 작성해야 합니다. 이 프로젝트는 Python을 사용하여 개발되었으며, Python 도구 세트를 사용하여 선호합니다. pytest 및 dataclasses도 사용됩니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대해 설명하고 있습니다. 각 파일은 마크다운 코드 블록 형식을 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. 코드 구현은 ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행됩니다. 코드는 완전히 기능해야 하며, 플레이스홀더는 사용하지 않아야 합니다. 각 파일에는 모든 임포트, 타입 등이 포함되어야 하며, 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 코드 작성에 대한 간단한 설명이 포함된 주석을 추가해야 합니다. 매우 복잡한 로직을 설명하는 주석을 추가하는 것이 좋습니다. 요청된 언어에 대한 최상의 관행을 따라 코드를 작성해야 합니다. 이 프로젝트는 Python을 사용하여 개발되었으며, Python 도구 세트를 사용하여 선호합니다. pytest 및 dataclasses도 사용됩니다. 이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. 키보드 컨트롤 구현의 특정 측면을 명확히 하고 싶은 부분이 있습니까?이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. 키보드 컨트롤 구현의 특정 측면을 명확히 하고 싶은 부분이 있습니까?이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. 코드의 특정 측면을 명확히 하고 싶은 부분이 있습니까?이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. 코드의 특정 측면을 명확히 하고 싶은 부분이 있습니까?이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. LLM 기반 자율 에이전트의 제한된 컨텍스트 용량으로 인해 역사적 정보, 상세한 지침, API 호출 컨텍스트 및 응답을 포함하는 것이 제한됩니다. 시스템의 설계는 이러한 제한된 커뮤니케이션 대역폭과 함께 작동해야 하며, 과거의 실수에서 배우기 위한 자기 반성과 같은 메커니즘은 긴 또는 무한한 컨텍스트 창에서 큰 이점을 얻을 수 있습니다. 벡터 저장소와 검색은 더 큰 지식 풀에 액세스할 수는 있지만, 그들의 표현력은 완전한 어텐션만큼 강력하지 않습니다. 장기적인 계획과 작업 분해에 대한 도전: 긴 역사를 기반으로 계획을 세우고 해결책 공간을 효과적으로 탐색하는 것은 여전히 어렵습니다. LLM은 예기치 않은 오류에 직면했을 때 계획을 조정하는 데 어려움을 겪어, 시행착오를 통해 배우는 인간에 비해 덜 견고합니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. LLM 기반 자율 에이전트의 제한된 컨텍스트 용량으로 인해 역사적 정보, 상세한 지침, API 호출 컨텍스트 및 응답을 포함하는 것이 제한됩니다. 시스템의 설계는 이러한 제한된 커뮤니케이션 대역폭과 함께 작동해야 하며, 과거의 실수에서 배우기 위한 자기 반성과 같은 메커니즘은 긴 또는 무한한 컨텍스트 창에서 큰 이점을 얻을 수 있습니다. 벡터 저장소와 검색은 더 큰 지식 풀에 액세스할 수는 있지만, 그들의 표현력은 완전한 어텐션만큼 강력하지 않습니다. 장기적인 계획과 작업 분해에 대한 도전: 긴 역사를 기반으로 계획을 세우고 해결책 공간을 효과적으로 탐색하는 것은 여전히 어렵습니다. LLM은 예기치 않은 오류에 직면했을 때 계획을 조정하는 데 어려움을 겪어, 시행착오를 통해 배우는 인간에 비해 덜 견고합니다. 현재 에이전트 시스템은 LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하고 있습니다. 그러나 LLM은 형식 오류를 일으키거나 때로는 명령을 따르지 않는 등의 문제가 발생할 수 있으므로 모델 출력의 신뢰성이 의심스럽습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. LLM 기반 자율 에이전트의 제한된 컨텍스트 용량으로 인해 역사적 정보, 상세한 지침, API 호출 컨텍스트 및 응답을 포함하는 것이 제한됩니다. 시스템의 설계는 이러한 제한된 커뮤니케이션 대역폭과 함께 작동해야 하며, 과거의 실수에서 배우기 위한 자기 반성과 같은 메커니즘은 긴 또는 무한한 컨텍스트 창에서 큰 이점을 얻을 수 있습니다. 벡터 저장소와 검색은 더 큰 지식 풀에 액세스할 수는 있지만, 그들의 표현력은 완전한 어텐션만큼 강력하지 않습니다. 장기적인 계획과 작업 분해에 대한 도전: 긴 역사를 기반으로 계획을 세우고 해결책 공간을 효과적으로 탐색하는 것은 여전히 어렵습니다. LLM은 예기치 않은 오류에 직면했을 때 계획을 조정하는 데 어려움을 겪어, 시행착오를 통해 배우는 인간에 비해 덜 견고합니다. 현재 에이전트 시스템은 LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하고 있습니다. 그러나 LLM은 형식 오류를 일으키거나 때로는 명령을 따르지 않는 등의 문제가 발생할 수 있으므로 모델 출력의 신뢰성이 의심스럽습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. LLM 기반 자율 에이전트의 제한된 컨텍스트 용량으로 인해 역사적 정보, 상세한 지침, API 호출 컨텍스트 및 응답을 포함하는 것이 제한됩니다. 시스템의 설계는 이러한 제한이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. LLM 기반 자율 에이전트의 제한된 컨텍스트 용량으로 인해 역사적 정보, 상세한 지침, API 호출 컨텍스트 및 응답을 포함하는 것이 제한됩니다. 시스템의 설계는 이러한 제한된 커뮤니케이션 대역폭과 함께 작동해야 하며, 과거의 실수에서 배우기 위한 자기 반성과 같은 메커니즘은 긴 또는 무한한 컨텍스트 창에서 큰 이점을 얻을 수 있습니다. 벡터 저장소와 검색은 더 큰 지식 풀에 액세스할 수는 있지만, 그들의 표현력은 완전한 어텐션만큼 강력하지 않습니다. 장기적인 계획과 작업 분해에 대한 도전: 긴 역사를 기반으로 계획을 세우고 해결책 공간을 효과적으로 탐색하는 것은 여전히 어렵습니다. LLM은 예기치 않은 오류에 직면했을 때 계획을 조정하는 데 어려움을 겪어, 시행착오를 통해 배우는 인간에 비해 덜 견고합니다. 현재 에이전트 시스템은 LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하고 있습니다. 그러나 LLM은 형식 오류를 일으키거나 때로는 명령을 따르지 않는 등의 문제가 발생할 수 있으므로 모델 출력의 신뢰성이 의심스럽습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 이 기사는 LLM 기반 자율 에이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 코드 작성에 대한 상세한 지침을 받게 되며, 모든 아키텍처 세부 사항이 최종적으로 코드로 구현되어야 합니다. 이 기사에서는 LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명합니다. 올바른 결정을 내리기 위해 단계별로 생각하고 이유를 찾아야 합니다. 먼저 필요한 핵심 클래스, 함수, 메서드의 이름과 간단한 목적에 대한 주석을 작성해야 합니다. 그런 다음 각 파일의 내용을 포함하여 모든 코드를 출력해야 합니다. 각 파일은 마크다운 코드 블록 형식을 엄격히 따라야 하며, 파일 이름, 코드 언어 및 코드가 포함되어야 합니다. ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동하는 방식으로 진행해야 합니다. 코드는 완전히 기능해야 합니다. 플레이스홀더는 사용하지 않아야 합니다. 언어와 프레임워크에 적합한 파일 네이밍 규칙을 따라야 합니다. 파일에는 모든 임포트, 타입 등이 포함되어야 합니다. 서로 다른 파일의 코드가 호환되도록 해야 합니다. 모든 코드를 구현해야 하며, 확실하지 않은 경우에는 타당한 구현을 작성해야 합니다. 모듈 종속성 또는 패키지 관리자 종속성 정의 파일을 포함해야 합니다. 마무리하기 전에 아키텍처의 모든 부분이 파일에 포함되어 있는지 다시 한 번 확인해야 합니다. 유용한 정보: 거의 항상 서로 다른 클래스를 서로 다른 파일에 넣습니다. Python의 경우 적절한 requirements.txt 파일을 항상 생성합니다. NodeJS의 경우 적절한 package.json 파일을 항상 생성합니다. 함수 정의의 목적을 간단히 설명하는 주석을 항상 추가합니다. 매우 복잡한 로직을 설명하는 주석을 추가하려고 노력합니다. 요청된 언어에 대한 최상의 관행을 항상 따릅니다. Python 도구 선호도: pytest, dataclasses. 전제사항으로는 모델, 뷰, 컨트롤러의 역할과 기능에 대한 가정이 있습니다. 모델은 게임의 데이터를 포함하고, 뷰는 게임의 시각적인 요소를 처리하며, 컨트롤러는 사용자 입력을 관리하고 모델을 업데이트합니다. LLM 기반 자율 에이전트의 제한된 컨텍스트 용량으로 인해 역사적 정보, 상세한 지침, API 호출 컨텍스트 및 응답을 포함하는 것이 제한됩니다. 시스템의 설계는 이러한 제한된 커뮤니케이션 대역폭과 함께 작동해야 하며, 과거의 실수에서 배우기 위한 자기 반성과 같은 메커니즘은 긴 또는 무한한 컨텍스트 창에서 큰 이점을 얻을 수 있습니다. 벡터 저장소와 검색은 더 큰 지식 풀에 액세스할 수는 있지만, 그들의 표현력은 완전한 어텐션만큼 강력하지 않습니다. 장기적인 계획과 작업 분해에 대한 도전: 긴 역사를 기반으로 계획을 세우고 해결책 공간을 효과적으로 탐색하는 것은 여전히 어렵습니다. LLM은 예기치 않은 오류에 직면했을 때 계획을 조정하는 데 어려움을 겪어, 시행착오를 통해 배우는 인간에 비해 덜 견고합니다. 현재 에이전트 시스템은 LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하고 있습니다. 그러나 LLM은 형식 오류를 일으키거나 때로는 명령을 따르지 않는 등의 문제가 발생할 수 있으므로 모델 출력의 신뢰성이 의심스럽습니다. 따라서 에이전트 데모 코드의 많은 부분은 모델 출력을 구문 분석하는 데 초점을 맞추고 있습니다. 이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 추가적인 컨텍스트와 함께 제공됩니다. 이외에도 참고할만한 자료로는 Joon Sung Park 등의 논문 [16]과 AutoGPT [17], GPT-Engineer [18]의 GitHub 링크가 있습니다.이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 또한, Joon Sung Park 등의 논문 [16]과 AutoGPT [17], GPT-Engineer [18]의 GitHub 링크를 참고할 수 있습니다. LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명하고 있습니다. 이 기사에서는 LLM 기반 자율 에이전트의 제한된 컨텍스트 용량과 관련된 도전과 문제점을 다루고 있습니다. 또한, LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하는 현재의 에이전트 시스템의 한계와 신뢰성 문제를 다루고 있습니다. 이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 추가적인 컨텍스트와 함께 제공됩니다.
```
print(result[""output_text""])는 결과 딕셔너리result에서'output_text'키에 해당하는 값을 출력합니다. 이 구문은 딕셔너리 내 특정 키의 값을 검색하고, 그 값을 콘솔에 출력하는 기본적인 Python 코드 예시입니다.
```
Copyprint(
    result[""output_text""]
)# 결과 딕셔너리에서 'output_text' 키에 해당하는 값을 출력합니다.
```
```
이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 대화 샘플을 통해 추가적인 컨텍스트를 제공합니다. 또한, Joon Sung Park 등의 논문 [16]과 AutoGPT [17], GPT-Engineer [18]의 GitHub 링크를 참고할 수 있습니다. LLM-centered agents의 주요 아이디어와 데모를 통해 나타나는 몇 가지 공통적인 제한 사항에 대해 설명하고 있습니다. 이 기사에서는 LLM 기반 자율 에이전트의 제한된 컨텍스트 용량과 관련된 도전과 문제점을 다루고 있습니다. 또한, LLM과 메모리, 도구 등 외부 구성 요소 간의 자연어 인터페이스에 의존하는 현재의 에이전트 시스템의 한계와 신뢰성 문제를 다루고 있습니다. 이 기사는 LLM 기반 자율 에이전트의 아키텍처를 코드로 구현하는 방법에 대한 설명을 포함하고 있으며, 추가적인 컨텍스트와 함께 제공됩니다.
```
이 함수는result딕셔너리의'intermediate_steps'키에 저장된 리스트에서 처음 세 요소를 선택하고, 이들 사이에 두 줄바꿈(\n\n)을 삽입하여 연결한 문자열을 출력합니다. 이는 중간 계산 단계나 결과를 시각적으로 구분하여 표시할 때 유용합니다.
```
Copyprint(""\n\n"".join(result[""intermediate_steps""][:3]))
```
```
This article discusses LLM powered autonomous agents, which are intelligent systems that can perform tasks without human intervention. The agents consist of three main components: planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes different types of memory and the use of Maximum Inner Product Search (MIPS). The tool use component is demonstrated through case studies, such as a scientific discovery agent and generative agents simulation. The article also highlights the challenges of implementing LLM powered autonomous agents.

이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다.

이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다.
```
## 한 번의 체인으로 분할하고 요약하기
편의를 위해, 우리는 긴 문서의 텍스트 분할과 요약을 단일AnalyzeDocumentsChain으로 묶을 수 있습니다.
AnalyzeDocumentChain클래스는 문서 분석 및 요약 작업을 위한 체인을 생성합니다. 이 예제에서는AnalyzeDocumentChain인스턴스를 생성하고,combine_docs_chain과text_splitter를 인자로 전달하여 초기화합니다. 이후, 첫 번째 문서(docs[0])의page_content를 사용하여 문서 요약 프로세스를 실행합니다. 이 과정은 문서의 내용을 분석하고 요약하는 데 사용됩니다.
```
Copyfromlangchain.chainsimportAnalyzeDocumentChain# AnalyzeDocumentChain 인스턴스를 생성합니다. 이때, combine_docs_chain과 text_splitter를 인자로 전달합니다.summarize_document_chain = AnalyzeDocumentChain(
    combine_docs_chain=chain, text_splitter=text_splitter
)# 첫 번째 문서의 페이지 내용을 사용하여 문서 요약 프로세스를 실행합니다.summarized_result = summarize_document_chain.invoke(
    {""input_document"": docs[0].page_content}
)
```
```
This article discusses LLM powered autonomous agents, which are intelligent systems that can perform tasks without human intervention. The agents consist of three main components: planning, memory, and tool use. The planning component involves task decomposition and self-reflection. The memory component includes different types of memory and the use of Maximum Inner Product Search (MIPS). The tool use component is demonstrated through case studies, such as a scientific discovery agent and generative agents simulation. The article also highlights the challenges of implementing LLM powered autonomous agents.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. 또한, Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 이전의 출력 시퀀스를 피드백과 함께 명시적으로 제시하여 모델이 스스로 개선할 수 있도록 한다. 이를 통해 모델은 피드백 시퀀스를 기반으로 자기 반성하여 더 나은 출력을 생성할 수 있다. 모델은 테스트 시에 인간 주석자와 함께 여러 라운드의 지침을 선택적으로 받을 수도 있다.이 기사는 인간의 개입 없이 작업을 수행할 수 있는 지능형 시스템인 LLM 기반 자율 에이전트에 대해 논의한다. 에이전트는 계획, 기억 및 도구 사용이라는 세 가지 주요 구성 요소로 구성된다. 계획 구성 요소는 작업 분해와 자기 반성을 포함한다. 기억 구성 요소에는 다양한 유형의 기억과 최대 내적 제품 검색(MIPS)의 사용이 포함된다. 도구 사용 구성 요소는 과학적 발견 에이전트 및 생성 에이전트 시뮬레이션과 같은 사례 연구를 통해 시연된다. 이 기사는 또한 LLM 기반 자율 에이전트를 구현하는 데 직면하는 도전과제를 강조한다. 에이전트는 단기 기억과 장기 기억을 활용하여 학습하며, 외부 API를 호출하여 추가 정보를 얻고, 모델 가중치 이후에 변경하기 어려운 현재 정보, 코드 실행 능력, 독점 정보 소스에 접근하는 등의 기능을 갖추게 된다. 또한, 복잡한 작업을 수행하기 위해 에이전트는 작업 분해를 통해 단계별로 생각하고 모델의 사고 과정을 이해하는 데 도움이 되는 체인 오브 씨피롤링(CoT) 기법을 사용한다. CoT는 큰 작업을 여러 개의 관리 가능한 작업으로 분해하여 모델의 성능을 향상시키는 데 도움이 된다. 또한, Tree of Thoughts (Yao et al. 2023)는 CoT를 확장하여 각 단계에서 여러 가지 추론 가능성을 탐색한다. 이는 문제를 여러 단계로 분해하고 각 단계마다 여러 가지 생각을 생성하여 트리 구조를 만든다. 검색 과정은 각 상태가 분류기(프롬프트를 통해) 또는 다수결 투표에 의해 평가되는 BFS(너비 우선 탐색) 또는 DFS(깊이 우선 탐색)일 수 있다. 작업 분해는 (1) ""XYZ를 위한 단계.\n1.""과 같은 간단한 프롬프트를 사용하여 LLM에 의해 수행될 수 있으며, (2) 작업별 지침을 사용하여 수행될 수 있으며, 예를 들어 소설 작성을 위해 ""이야기 개요를 작성하십시오.""라고 할 수 있으며, (3) 인간의 입력을 사용하여 수행될 수 있다. 또한, LLM+P (Liu et al. 2023)라는 다른 접근 방식은 외부 고전적인 계획자를 활용하여 장기 계획을 수행한다. 이 접근 방식은 계획 단계를 외부 도구에 위탁하며, 도메인 특정 PDDL과 적합한 계획자의 가용성을 가정한다. 자기 반성은 자율 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 반복적으로 발전할 수 있도록 하는 중요한 측면이다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 한다. ReAct (Yao et al. 2023)는 LLM 내에서 추론과 행동을 통합하여 작업 공간을 과업별 이산적 행동과 언어 공간의 조합으로 확장한다. 이를 통해 LLM은 환경과 상호작용할 수 있게 되며(예: Wikipedia 검색 API 사용), 동시에 LLM에게 추론 트레이스를 자연어로 생성하도록 유도한다. ReAct 프롬프트 템플릿은 LLM이 생각하는 과정을 명시적으로 단계별로 포맷팅한 것이다. 또한, 지식 집약적 작업과 의사 결정 작업에 대한 실험에서 ReAct는 Thought: ... 단계가 제거된 Act-only 기준보다 더 잘 작동한다. Reflexion (Shinn & Labash 2023)은 동적 기억과 자기 반성 능력을 갖춘 에이전트에게 추론 기술을 개선하기 위한 프레임워크를 제공한다. Reflexion은 간단한 이진 보상을 제공하는 보상 모델과 ReAct의 설정을 따르는 작업별 행동 공간을 가지는 표준 RL 설정을 갖추고 있다. 각 행동 $a_t$ 후, 에이전트는 휴리스틱 $h_t$를 계산하고 자기 반성 결과에 따라 환경을 재설정하여 새로운 시도를 시작할 수 있다. Reflexion 프레임워크는 효율적이지 않거나 환각을 포함하는 궤적을 중지해야 할 때 휴리스틱 함수를 사용한다. 비효율적인 계획은 성공하지 못한 상태에서 너무 오래 걸리는 궤적을 의미한다. 환각은 환경에서 동일한 관찰로 이어지는 연속적인 동일한 행동 순서를 만나는 것으로 정의된다. 자기 반성은 LLM에게 두 번의 시도 예제를 보여주어 각 예제가 (실패한 궤적, 계획의 미래 변경을 안내하기 위한 이상적인 반성)의 쌍으로 구성되도록 한다. 그런 다음 반성은 LLM에게 쿼리할 때 사용되는 작업 메모리에 최대 세 개까지 추가된다. 또한, AlfWorld Env와 HotpotQA에서의 실험 결과, AlfWorld에서는 비효율적인 계획보다 환각이 더 일반적인 실패로 나타났다. 또한, Chain of Hindsight (CoH; Liu et al. 2023)는 모델에게 이전의 출력 시퀀스를 피드백과 함께 명시적으로 제시하여 모델이 스스로 개선할 수 있도록 한다. 이를 통해 모델은 피드백 시퀀스를 기반으로 자기 반성하여 더 나은 출력을 생성할 수 있다. 모델은 테스트 시에 인간 주석자와 함께 여러 라운드의 지침을 선택적으로 받을 수도 있다. 또한, CoH는 오버피팅을 피하기 위해 사전 훈련 데이터셋의 로그 우도를 최대화하기 위한 정규화 항을 추가CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다. AD는 ED (expert distillation), source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 강화 학습만 사용하므로 다른 기준선보다 훨씬 빠르게 학습한다. 또한 소스 정책의 부분 훈련 히스토리에 의존할 때, AD는 ED 기준선보다 훨씬 빠르게 개선된다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다. AD는 ED (expert distillation), source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 강화 학습만 사용하므로 다른 기준선보다 훨씬 빠르게 학습한다. 또한 소스 정책의 부분 훈련 히스토리에 의존할 때, AD는 ED 기준선보다 훨씬 빠르게 개선된다. AD는 기억의 종류 중 하나인 감각 기억을 포함하여 인간의 뇌에서 다양한 종류의 기억이 있다.CoH는 모델이 점진적으로 개선된 출력을 생성하기 위해 개선된 출력의 연속적인 히스토리를 제시하고 모델을 해당 추세를 따르도록 훈련시키는 아이디어이다. AD(Algorithm Distillation; Laskin et al. 2023)는 이와 같은 아이디어를 강화 학습 작업의 교차 에피소드 궤적에 적용하는데, 여기서 알고리즘은 긴 히스토리에 의존하는 정책으로 캡슐화된다. 에이전트가 여러 번 환경과 상호작용하고 각 에피소드에서 약간씩 개선되는 것을 고려할 때, AD는 이러한 학습 히스토리를 연결하여 모델에 입력으로 제공한다. 따라서 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 기대할 수 있다. 이는 과제별 정책을 훈련시키는 것이 아닌 강화 학습 과정 자체를 학습하는 것을 목표로 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다고 가설을 세우고 있다. 훈련 단계에서 각 RL 실행 중에는 무작위 작업이 샘플링되고, 다중 에피소드 히스토리의 부분 수열이 훈련에 사용되며, 학습된 정책은 작업에 독립적이다. 실제로 모델은 제한된 문맥 창 길이를 가지므로 에피소드는 다중 에피소드 히스토리를 구성하기에 충분히 짧아야 한다. 2-4 에피소드의 다중 에피소드 문맥은 거의 최적인 문맥 내 강화 학습 알고리즘을 학습하는 데 필요하다. 문맥 내 강화 학습의 등장은 충분히 긴 문맥이 필요하다. AD는 ED (expert distillation), source policy, RL^2와 비교하여 성능이 가까워지는 동시에 오프라인 강화 학습만 사용하므로 다른 기준선보다 훨씬 빠르게 학습한다. 또한 소스 정책의 부분 훈련 히스토리에 의존할 때, AD는 ED 기준선보다 훨씬 빠르게 개선된다. AD는 기억의 종류 중 하나인 감각 기억을 포함하여 인간의 뇌에서 다양한 종류의 기억이 있다. 인간의 기억은 단기 기억과 장기 기억으로 나눌 수 있으며, 장기 기억은 명시적 기억과 암시적 기억으로 구분된다. 명시적 기억은 사실과 사건에 대한 기억이며, 암시적 기억은 무의식적으로 수행되는 기술과 루틴을 포함한다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH (Locality-Sensitive Hashing)은 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY (Approximate Nearest Neighbors Oh Yeah)는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. 이는 입력 공간을 반으로 나누는 하이퍼플레인을 나타내는 비단말 노드가 있는 이진 트리의 집합이다. 각 리프는 하나의 데이터 포인트를 저장한다. ANNOY 검색은 모든 트리에서 진행되어 쿼리에 가장 가까운 반을 반복적으로 검색하고 결과를 집계한다. 이 아이디어는 KD 트리와 관련이 있지만 훨씬 확장 가능하다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH (Locality-Sensitive Hashing)은 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY (Approximate Nearest Neighbors Oh Yeah)는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. 이는 입력 공간을 반으로 나누는 하이퍼플레인을 나타내는 비단말 노드가 있는 이진 트리의 집합이다. 각 리프는 하나의 데이터 포인트를 저장한다. ANNOY 검색은 모든 트리에서 진행되어 쿼리에 가장 가까운 반을 반복적으로 검색하고 결과를 집계한다. 이 아이디어는 KD 트리와 관련이 있지만 훨씬 확장 가능하다. HNSW (Hierarchical Navigable Small World)는 소셜 네트워크의 ""육도 분리"" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 ""육도 분리"" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\tilde{x}_i$로 양자화하는데, 이 때 내적 $\langle q, x_i \rangle$가 가능한한 $\angle q, \tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 ""육도 분리"" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\tilde{x}_i$로 양자화하는데, 이 때 내적 $\langle q, x_i \rangle$가 가능한한 $\angle q, \tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다. LLMs에 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 ""육도 분리"" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\tilde{x}_i$로 양자화하는데, 이 때 내적 $\langle q, x_i \rangle$가 가능한한 $\angle q, \tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다. LLMs에 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다. MRKL (Karpas et al. 2022)는 자율 에이전트를 위한 신경 기호 아키텍처로, ""전문가"" 모듈의 컬렉션과 일반적인 목적의 LLM이라는 라우터로 구성된다. 이러한 모듈은 신경망 (예: 딥러닝 모델) 또는 심볼릭 (예: 수학 계산기, 통화 변환기, 날씨 API)일 수 있다.AD는 모델의 출력을 개선하기 위해 개선된 출력의 연속적인 히스토리를 활용하여 모델을 훈련시키는 아이디어이다. AD는 강화 학습 작업에서 알고리즘을 사용하여 긴 히스토리에 의존하는 정책을 캡슐화한다. AD는 여러 번의 상호작용을 통해 모델을 개선하는 학습 히스토리를 모델에 입력으로 제공하여 다음 예측된 행동이 이전 시도보다 더 나은 성능을 보이도록 한다. AD는 특정 작업에 대해 훈련된 여러 소스 정책에 의해 생성된 학습 히스토리를 신경망으로 압축할 수 있다는 가설을 세우고 있다. AD는 다른 기준선보다 빠르게 학습하며, 소스 정책의 부분 훈련 히스토리에 의존할 때 더욱 개선된다. 또한 인간의 뇌에서와 같이 AD는 다양한 종류의 기억을 활용하여 모델을 훈련시킨다. LSH는 입력 항목을 동일한 버킷으로 매핑하는 해싱 함수를 도입하여 유사한 항목을 높은 확률로 동일한 버킷에 매핑한다. ANNOY는 핵심 데이터 구조로 랜덤 프로젝션 트리를 사용한다. HNSW는 소셜 네트워크의 ""육도 분리"" 기능과 같이 대부분의 노드가 작은 단계로 다른 노드에 도달할 수 있는 작은 세계 네트워크 개념에서 영감을 받았다. HNSW는 이러한 작은 세계 그래프의 계층적인 레이어를 구축하며, 하단 레이어에는 실제 데이터 포인트가 포함된다. 중간 레이어는 검색 속도를 높이기 위해 바로 가기를 생성한다. HNSW는 검색을 수행할 때 상위 레이어의 임의의 노드에서 시작하여 목표로 이동한다. 더 이상 가까워질 수 없을 때 다음 레이어로 이동하고, 하단 레이어에 도달할 때까지 이동한다. 상위 레이어의 각 이동은 데이터 공간에서 큰 거리를 이동할 수 있으며, 하단 레이어의 각 이동은 검색 품질을 개선한다. FAISS는 고차원 공간에서 노드 간의 거리가 가우시안 분포를 따르고 데이터 포인트의 군집화가 존재한다는 가정에 기반하여 작동한다. FAISS는 벡터 양자화를 적용하여 벡터 공간을 군집으로 분할하고, 군집 내에서 양자화를 더 세밀하게 조정한다. 검색은 먼저 거친 양자화로 군집 후보를 찾은 다음, 더 세밀한 양자화로 각 군집을 자세히 살펴본다. ScaNN의 주요 개념은 이방성 벡터 양자화이다. ScaNN은 데이터 포인트 $x_i$를 $\tilde{x}_i$로 양자화하는데, 이 때 내적 $\langle q, x_i \rangle$가 가능한한 $\angle q, \tilde{x}_i$의 원래 거리와 유사하도록 한다. 이는 가장 가까운 양자화 중심점을 선택하는 대신에 이루어진다. LLMs에 외부 도구를 제공함으로써 모델의 능력을 크게 확장할 수 있다. MRKL (Karpas et al. 2022)는 자율 에이전트를 위한 신경 기호 아키텍처로, ""전문가"" 모듈의 컬렉션과 일반적인 목적의 LLM이라는 라우터로 구성된다. 이러한 모듈은 신경망 (예: 딥러닝 모델) 또는 심볼릭 (예: 수학 계산기, 통화 변환기, 날씨 API)일 수 있다. TALM (Tool Augmented Language Models; Parisi et al. 2022)와 Toolformer (Schick et al. 2023)은 외부 도구 API를 사용하는 방법을 학습하기 위해 LM을 세밀하게 조정한다. 새로 추가된 API 호출 주석이 모델 출력의 품질을 개선할 수 있는지 여부에 따라 데이터셋이 확장된다. Prompt Engineering의 ""External APIs"" 섹션에서 자세한 내용을 확인할 수 있다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다:

API 호출이 필요한지 여부.
호출할 적절한 API 식별: 만족스럽지 않은 경우, LLM은 반복적으로 API 입력을 수정해야 합니다(예: 검색 엔진 API에 대한 검색 키워드 결정).
API 결과에 기반한 응답: 결과가 만족스럽지 않은 경우, 모델은 결과를 개선하고 다시 호출할 수 있습니다.

이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구했습니다. 이 에이전트는 인터넷을 검색하고 문서를 읽고 코드를 실행하며 로봇 실험 API를 호출하고 다른 LLM을 활용할 수 있습니다. 예를 들어 ""새로운 항암 약물 개발""이라는 요청을 받았을 때, 모델은 다음과 같은 추론 단계를 제시했습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구했습니다. 이 에이전트는 인터넷을 검색하고 문서를 읽고 코드를 실행하며 로봇 실험 API를 호출하고 다른 LLM을 활용할 수 있습니다. 예를 들어 ""새로운 항암 약물 개발""이라는 요청을 받았을 때, 모델은 다음과 같은 추론 단계를 제시했습니다. 먼저, 현재 항암 약물 개발의 최신 동향에 대해 알아보기 위해 인터넷 검색을 수행합니다. 그런 다음 특정 대상을 선택하고 이를 타겟으로 하는 화합물을 요청합니다. 화합물이 식별되면, 모델은 그 합성을 시도합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성 및 중요성에 따라 에이전트의 행동을 결정하기 위해 컨텍스트를 제공합니다. 반사 메커니즘은 시간에 걸쳐 메모리를 종합하여 미래의 행동을 안내하는 고수준의 요약입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성 및 중요성에 따라 에이전트의 행동을 결정하기 위해ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 판단하는 것은 어려울 수 있습니다. Boiko et al. (2023)는 LLM을 사용하여 과학적 발견을 다루기 위한 에이전트를 연구하였습니다. LLM은 메모리 스트림, 검색 모델 및 반사 메커니즘을 사용하여 에이전트의 행동을 지원합니다. 메모리 스트림은 에이전트의 경험을 기록하는 장기적인 메모리 모듈입니다. 검색 모델은 관련성, 최신성, 목표 지향성 등을 고려하여 정보를 검색합니다. 반사 메커니즘은 에이전트가 이전에 수행한 행동을 기반으로 현재 상황을 평가하고 적절한 행동을 선택하는 데 도움을 줍니다. 이러한 기능을 통해 LLMChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형은 AI 어시스턴트가 사용자 요청을 처리하는 데 도움을 주며, 각 작업에는 특정한 인수와 관련된 속성이 있습니다. 이러한 작업 유형을 처리하기 위해서는 AI 어시스턴트가 적절한 모델을 선택하고 실행 결과를 요약하는 능력이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형은 AI 어시스턴트가 사용자 요청을 처리하는 데 도움을 주며, 각 작업에는 특정한 인수와 관련된 속성이 있습니다. 이러한 작업 유형을 처리하기 위해서는 AI 어시스턴트가 적절한 모델을 선택하고 실행 결과를 요약하는 능력이 필요합니다. AI 어시스턴트는 작업을 수행한 후 결과를 사용자에게 제시하고ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 모두 출력해야 합니다. 각 파일은 markdown 코드 블록 형식을 따라야 하며, 다음 토큰을 대체해야 합니다: FILENAME, LANG, CODE. 또한, ""entrypoint"" 파일부터 시작하여 해당 파일에서 가져온 파일로 이동해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 나타내는 것이 좋습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 나타내는 것이 좋습니다. 또한, pytest와 dataclasses에 대한 정보가 추가되었습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 메서드의 이름을 나열하고, 각 파일의 내용을 나타내는 것이 좋습니다. 또한, pytest와 dataclasses에 대한 정보가 추가되었습니다. 이러한 새로운 정보를 고려하여 원래 요약을 수정하였습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 및 프레임워크에 적합한 파일 네이밍 규칙을 따르며, 코드가 완전하고 기능적인지 확인해야 합니다. 또한, 파일 간의 호환성을 보장하고, 필요한 경우 모듈 종속성이나 패ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 및 프레임워크에 적합한 파일 네이밍 규칙을 따르며, 코드가 완전하고 기능적인지 확인해야 합니다. 또한, 파일 간의 호환성을 보장하고, 필요한 경우 모듈 종속성이나 패키지/프로젝트로 작성된 코드를 설명하는 것이 좋습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 모델과 다른 파일 간의 호환성을 확인해야 합니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"", ""send_tweet"", ""do_nothing"", ""task_complete""와 같은 추가적인 작업 유형이 소개되었습니다. 이러한 작업 유형에 대한 자세한 설명이 필요합니다. 시작하기 전에 각 파일의 핵심 클래스, 함수, 모듈 및 종속성을 확인하고, 언어 모델과 다른 파일 간의 호환성을 확인해야 합니다. ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"",ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"" 등의 작업을 수행할 수 있습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"" 등의 작업을 수행할 수 있습니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다. 또한, Joon Sung Park 등은 ""Generative Agents: Interactive Simulacra of Human Behavior""에서 인간 행동의 상호작용적인 시뮬라크인 생성 에이전트에 대해 연구하였습니다. AutoGPT와 GPT-Engineer는 ChatGPT를 개선하기 위한 프로젝트입니다.ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다. 또한, Joon Sung Park 등은 ""Generative Agents: Interactive Simulacra of Human Behavior""에서 인간 행동의 상호작용적인 시뮬라크인 생성 에이전트에 대해 연구하였습니다. AutoGPT와 GPT-Engineer는 ChatGPT를 개선하기 위한 프로젝트입니다. Prompt Engineering은 LLM의 성능을 향상시키기 위한 방법 중 하나입니다.
```
```
Copyprint(summarized_result[""output_text""])
```
```
ChatGPT 플러그인과 OpenAI API 함수 호출은 실제로 작동하는 도구 사용 능력이 강화된 LLM의 좋은 예입니다. 도구 API의 컬렉션은 다른 개발자(플러그인의 경우) 또는 자체 정의(함수 호출의 경우)로 제공될 수 있습니다. HuggingGPT는 (Shen et al. 2023)에서 ChatGPT를 작업 계획자로 사용하여 HuggingFace 플랫폼에서 사용 가능한 모델을 선택하고 실행 결과에 기반하여 응답을 요약하는 프레임워크입니다. HuggingGPT 시스템은 4단계로 구성되어 있습니다. 첫 번째 단계는 작업 계획으로, LLM이 사용자 요청을 여러 작업으로 파싱합니다. 각 작업에는 작업 유형, ID, 종속성 및 인수와 관련된 네 가지 속성이 있습니다. LLM은 few-shot 예제를 사용하여 작업 파싱과 계획을 수행하는 데 도움을 받습니다. AI 어시스턴트는 사용자 입력을 여러 작업으로 파싱할 수 있으며, 작업은 작업 유형, ID, 종속성 및 인수와 관련된 속성을 가지고 있습니다. 작업 간에는 논리적인 관계가 있으며, 작업 순서를 유의해야 합니다. 사용자 입력을 파싱할 수 없는 경우 빈 JSON으로 응답해야 합니다. LLM은 모델 선택 단계에서 전문 모델에 작업을 분배하고, 요청을 다중 선택 질문으로 구성합니다. LLM은 선택할 수 있는 모델 목록을 제시받습니다. 제한된 문맥 길이로 인해 작업 유형에 기반한 필터링이 필요합니다. AI 어시스턴트는 사용자 요청을 처리하기 위해 모델 목록에서 적합한 모델을 선택하는 데 도움을 줍니다. AI 어시스턴트는 가장 적합한 모델의 모델 ID만 출력합니다. 출력은 엄격한 JSON 형식이어야 하며, ""id"": ""id"", ""reason"": ""선택 이유에 대한 자세한 설명""과 같은 형식을 따라야 합니다. AI 어시스턴트는 사용자의 요청에 직접적으로 응답한 후, 작업 과정을 설명하고 분석 및 모델 추론 결과를 사용자에게 제시해야 합니다. 추론 결과에 파일 경로가 포함된 경우, 사용자에게 완전한 파일 경로를 알려주어야 합니다. HuggingGPT를 실제 환경에서 사용하기 위해서는 몇 가지 문제를 해결해야 합니다. 첫째, LLM 추론 라운드와 다른 모델과의 상호작용이 프로세스를 느리게 만드는 효율성 개선이 필요합니다. 둘째, 복잡한 작업 내용을 전달하기 위해 긴 문맥 창을 필요로 합니다. 셋째, LLM 출력과 외부 모델 서비스의 안정성을 개선해야 합니다. API-Bank (Li et al. 2023)은 도구 강화된 LLM의 성능을 평가하기 위한 벤치마크입니다. 이 벤치마크에는 53개의 일반적으로 사용되는 API 도구, 완전한 도구 강화된 LLM 워크플로우 및 568개의 API 호출이 포함된 264개의 주석이 달린 대화가 포함되어 있습니다. API의 선택은 검색 엔진, 계산기, 캘린더 쿼리, 스마트 홈 제어, 일정 관리, 건강 데이터 관리, 계정 인증 워크플로우 등 다양한 API를 포함하고 있습니다. 많은 수의 API가 있기 때문에, LLM은 먼저 API 검색 엔진에 접근하여 호출할 적절한 API를 찾은 다음 해당 문서를 사용하여 호출합니다. API-Bank 워크플로우에서 LLM은 몇 가지 결정을 내려야 합니다. 각 단계에서 결정의 정확성을 평가할 수 있습니다. 이 결정에는 다음이 포함됩니다: API 호출이 필요한지 여부, 호출할 적절한 API 식별, API 결과에 기반한 응답. 이 벤치마크는 에이전트의 도구 사용 능력을 세 가지 수준에서 평가합니다. Level-1은 API 호출 능력을 평가하며, Level-2는 API 검색 능력을, Level-3은 API 검색과 호출 이상의 계획 능력을 평가합니다. ChemCrow (Bran et al. 2023)는 LLM이 유기 합성, 약물 개발 및 재료 설계와 같은 작업을 수행하기 위해 13개의 전문가가 설계한 도구로 보강된 도메인 특정 예입니다. LangChain에서 구현된 워크플로우는 이전에 설명한 ReAct와 MRKLs를 결합한 CoT 추론과 작업에 관련된 도구를 결합합니다. LLM은 도구 이름 목록, 유틸리티 설명 및 예상 입력/출력에 대한 세부 정보를 제공받습니다. 그런 다음 필요한 경우 도구를 사용하여 사용자가 제공한 프롬프트에 답변하도록 지시됩니다. 지시는 ReAct 형식을 따르도록 모델에게 알려줍니다 - 생각, 동작, 동작 입력, 관찰. 하지만 LLM을 사용하여 도메인 전문성이 필요한 작업의 성능을 평가할 때, LLM 기반 평가와 전문가 평가의 결과가 다를 수 있습니다. 이는 LLM이 자체적으로 도메인 전문성을 갖지 않기 때문에 발생하는 문제일 수 있습니다. 따라서 LLM을 사용하여 작업 결과의 정확성을 평가할 때는 주의가 필요합니다. 또한, 새로운 context에 따라서 ""improve_code"", ""write_tests"", ""execute_python_file"", ""generate_image"" 등의 작업을 수행할 수 있습니다. 이러한 작업은 LLM의 다양한 도구 사용 능력을 보여주는 예시입니다. 또한, Joon Sung Park 등은 ""Generative Agents: Interactive Simulacra of Human Behavior""에서 인간 행동의 상호작용적인 시뮬라크인 생성 에이전트에 대해 연구하였습니다. AutoGPT와 GPT-Engineer는 ChatGPT를 개선하기 위한 프로젝트입니다. Prompt Engineering은 LLM의 성능을 향상시키기 위한 방법 중 하나입니다.
```"
05. 웹문서 요약(Webpage Summarization Chain),https://wikidocs.net/234022,"# 05. 웹문서 요약(Webpage Summarization Chain)

```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
Chain of Density: https://arxiv.org/pdf/2309.04269.pdf
```
Copyfromlangchainimporthubfromlangchain_openaiimportChatOpenAIfromlangchain_core.output_parsersimportJsonOutputParserfromlangchain.document_loadersimportWebBaseLoaderfromlangchain.callbacks.baseimportBaseCallbackHandler# Load some data to summarizeloader = WebBaseLoader(""https://www.aitimes.com/news/articleView.html?idxno=131777"")
docs = loader.load()
content = docs[0].page_contentclassStreamCallback(BaseCallbackHandler):defon_llm_new_token(self, token, **kwargs):print(token, end="""", flush=True)


prompt = hub.pull(""teddynote/chain-of-density-korean"")# Create the chain, includingchain = (
    prompt
    | ChatOpenAI(
        temperature=0,
        model=""gpt-4-turbo-preview"",
        streaming=True,
        callbacks=[StreamCallback()],
    )
    | JsonOutputParser()
    | (lambdax: x[-1][""Denser_Summary""])
)# Invoke the chainresult = chain.invoke({""ARTICLE"": content})print(result)
```
```
```json
[
    {
        ""Missing_Entities"": """",
        ""Denser_Summary"": ""이 기사는 데이터사이언스, 머신러닝, 인공지능에 대한 개념을 설명하고 있습니다. 박정현 서울대 EPM 연구원이 작성한 이 글에서는 이 세 분야가 우리 생활에 어떻게 적용되고 있는지, 그리고 이 분야들을 공부하고자 하는 사람들을 위한 가이드를 제공하고자 합니다. 특히, 데이터사이언스가 가장 넓은 범위를 가지고 있으며, 머신러닝과 인공지능이 이 안에서 어떻게 서로 연결되어 있는지를 설명합니다. 또한, 이 분야들이 어떻게 빠르게 발전하고 있는지, 그리고 이에 대한 기본적인 이해를 돕기 위해 여러 예시와 함께 설명하고 있습니다.""
    },
    {
        ""Missing_Entities"": ""튜링테스트; 지도학습; 비지도 학습"",
        ""Denser_Summary"": ""박정현 서울대 EPM 연구원이 작성한 이 글은 데이터사이언스, 머신러닝, 인공지능의 개념과 실생활 적용 예를 설명합니다. 데이터사이언스가 가장 넓은 범위를 가지며, 머신러닝과 인공지능은 이 안에서 연결됩니다. 인공지능의 이해를 위해 '튜링테스트'가 언급되며, 머신러닝은 '지도학습'과 '비지도 학습'으로 구분되어 설명됩니다. 이 글은 이 분야들의 발전 속도와 기본적인 이해를 돕기 위한 예시를 제공하며, 이 분야를 공부하고자 하는 사람들에게 유용한 정보를 담고 있습니다.""
    },
    {
        ""Missing_Entities"": ""약한 인공지능; 강한 인공지능; k-means"",
        ""Denser_Summary"": ""박정현 서울대 EPM 연구원의 글은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용을 다룹니다. 데이터사이언스는 가장 넓은 범위를 가지며, 머신러닝은 '지도학습'과 '비지도 학습'으로 나뉩니다. 인공지능은 '튜링테스트'로 설명되며, '약한 인공지능'과 '강한 인공지능'으로 구분됩니다. 비지도 학습의 예로 'k-means' 알고리즘이 언급됩니다. 이 글은 이 분야의 발전과 기본 개념을 이해하기 위한 예시를 제공하며, 관련 분야에 관심 있는 이들에게 유용한 정보를 제공합니다.""
    },
    {
        ""Missing_Entities"": ""신용카드 결제 데이터; 이상값 검출; 강화학습"",
        ""Denser_Summary"": ""박정현 서울대 EPM 연구원의 글은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용 예를 소개합니다. 데이터사이언스는 가장 넓은 범위, 머신러닝은 '지도학습', '비지도 학습'으로 나뉘며, 인공지능은 '튜링테스트', '약한 인공지능', '강한 인공지능'으로 구분됩니다. 'k-means'와 같은 비지도 학습 알고리즘, '신용카드 결제 데이터'에서의 '이상값 검출', '강화학습' 등이 언급됩니다. 이 글은 이 분야의 발전과 기본 개념을 이해하기 위한 예시를 제공하며, 관련 분야에 관심 있는 이들에게 유용한 정보를 제공합니다.""
    },
    {
        ""Missing_Entities"": ""룰 기반 시스템; machine learning 기반 시스템"",
        ""Denser_Summary"": ""박정현 서울대 EPM 연구원의 글은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용 예를 소개합니다. 데이터사이언스는 가장 넓은 범위, 머신러닝은 '지도학습', '비지도 학습'으로 나뉘며, 인공지능은 '튜링테스트', '약한 인공지능', '강한 인공지능'으로 구분됩니다. 'k-means', '신용카드 결제 데이터'에서의 '이상값 검출', '강화학습' 등이 언급됩니다. 또한, '룰 기반 시스템'과 'machine learning 기반 시스템'의 차이가 설명됩니다. 이 글은 이 분야의 발전과 기본 개념을 이해하기 위한 예시를 제공하며, 관련 분야에 관심 있는 이들에게 유용한 정보를 제공합니다.""
    }
]
```박정현 서울대 EPM 연구원의 글은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용 예를 소개합니다. 데이터사이언스는 가장 넓은 범위, 머신러닝은 '지도학습', '비지도 학습'으로 나뉘며, 인공지능은 '튜링테스트', '약한 인공지능', '강한 인공지능'으로 구분됩니다. 'k-means', '신용카드 결제 데이터'에서의 '이상값 검출', '강화학습' 등이 언급됩니다. 또한, '룰 기반 시스템'과 'machine learning 기반 시스템'의 차이가 설명됩니다. 이 글은 이 분야의 발전과 기본 개념을 이해하기 위한 예시를 제공하며, 관련 분야에 관심 있는 이들에게 유용한 정보를 제공합니다.
```"
06. 정형데이터 QA 체인(Structured Data Chain),https://wikidocs.net/234023,"# 06. 정형데이터 QA 체인(Structured Data Chain)

```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
```
Copyimportpandasaspdfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_experimental.toolsimportPythonAstREPLTool

df = pd.read_csv(""./data/titanic.csv"")# titanic.csv 파일에서 데이터를 읽어와 DataFrame으로 저장합니다.tool = PythonAstREPLTool(locals={""df"": df})# PythonAstREPLTool을 사용하여 로컬 변수 'df'를 포함하는 환경을 생성합니다.tool.invoke(""df"")# 'df' DataFrame에서 'Fare' 열의 평균 값을 계산합니다.
```
891 rows × 12 columns
```
Copyhello =""""""
print(""Hello, world!"")

def add(a, b):
    return a + b

print(add(30, 40))

import pandas as pd

df = pd.read_csv(""./data/titanic.csv"")
df.head()
""""""
```
```
Copytool = PythonAstREPLTool(locals={""df"": df})# PythonAstREPLTool을 사용하여 로컬 변수 'df'를 포함하는 환경을 생성합니다.tool.invoke(hello)
```
```
Hello, world!
70
```
## 데이터 로드
pandas를 활용하여 csv 파일을 DataFrame 으로 로드합니다.
```
Copyimportpandasaspd

df = pd.read_csv(""data/titanic.csv"")
df.head()
```
## Pandas DataFrame Agent
```
Copyfromlangchain.agents.agent_typesimportAgentTypefromlangchain_experimental.agents.agent_toolkitsimportcreate_pandas_dataframe_agentfromlangchain_openaiimportChatOpenAIfromlangchain.agents.agent_typesimportAgentTypefromlangchain.callbacks.baseimportBaseCallbackHandlerclassStreamCallback(BaseCallbackHandler):defon_llm_new_token(self, token:str, **kwargs):print(token, end="""", flush=True)# # 에이전트 생성agent = create_pandas_dataframe_agent(
    ChatOpenAI(
        temperature=0,
        model_name=""gpt-3.5-turbo-0613"",
        streaming=True,
        callbacks=[StreamCallback()],
    ),# 모델 정의df,# 데이터프레임verbose=True,# 추론과정 출력# AgentType.ZERO_SHOT_REACT_DESCRIPTIONagent_type=AgentType.OPENAI_FUNCTIONS,
)
```
```
Copy# 질의agent.invoke({""input"":""데이터의 행과 열의 갯수는 어떻게 돼?""})
```
```
[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': 'df.shape'}`


[0m[36;1m[1;3m(891, 12)[0m데이터프레임 `df`는 891개의 행과 12개의 열로 구성되어 있습니다.[32;1m[1;3m데이터프레임 `df`는 891개의 행과 12개의 열로 구성되어 있습니다.[0m

[1m> Finished chain.[0m
```
```
{'input': '데이터의 행과 열의 갯수는 어떻게 돼?', 'output': '데이터프레임 `df`는 891개의 행과 12개의 열로 구성되어 있습니다.'}
```
```
Copy# 질의agent.run(""남자 승객의 생존율을 어떻게 돼? %로 알려줘"")
```
```
[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': ""survival_rate = df[df['Sex'] == 'male']['Survived'].mean() * 100\nsurvival_rate""}`


[0m[36;1m[1;3m18.890814558058924[0m남자 승객의 생존율은 약 18.89%입니다.[32;1m[1;3m남자 승객의 생존율은 약 18.89%입니다.[0m

[1m> Finished chain.[0m
```
```
'남자 승객의 생존율은 약 18.89%입니다.'
```
```
Copy# 질의agent.run(""나이가 15세 이하인 승객중 1,2등급에 탑승한 남자 승객의 생존율은 어떻게 돼? %로 알려줘"")
```
```
[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': ""df[(df['Age'] <= 15) & (df['Pclass'].isin([1, 2])) & (df['Sex'] == 'male')]['Survived'].mean() * 100""}`


[0m[36;1m[1;3m100.0[0m나이가 15세 이하인 승객 중 1, 2등급에 탑승한 남자 승객의 생존율은 100%입니다.[32;1m[1;3m나이가 15세 이하인 승객 중 1, 2등급에 탑승한 남자 승객의 생존율은 100%입니다.[0m

[1m> Finished chain.[0m
```
```
'나이가 15세 이하인 승객 중 1, 2등급에 탑승한 남자 승객의 생존율은 100%입니다.'
```
```
Copy# 질의agent.run(""Pclass 가 1등급인 승객 중에서 나이가 20세~30세 사이이고, 여성 승객의 생존율은 어떻게 돼? %로 알려줘"")
```
```
[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': ""df[(df['Pclass'] == 1) & (df['Age'] >= 20) & (df['Age'] <= 30) & (df['Sex'] == 'female')]['Survived'].mean() * 100""}`


[0m[36;1m[1;3m95.23809523809523[0mPclass가 1등급인 승객 중에서 나이가 20세~30세 사이이고, 여성 승객의 생존율은 약 95.24%입니다.[32;1m[1;3mPclass가 1등급인 승객 중에서 나이가 20세~30세 사이이고, 여성 승객의 생존율은 약 95.24%입니다.[0m

[1m> Finished chain.[0m
```
```
'Pclass가 1등급인 승객 중에서 나이가 20세~30세 사이이고, 여성 승객의 생존율은 약 95.24%입니다.'
```
## 2개 이상의 DataFrame
2개 이상의 데이터프레임에 기반한 LLM 기반 질의를 할 수 있습니다. 2개 이상의 데이터프레임 입력시[]로 묶어주면 됩니다.
```
Copy# 샘플 데이터프레임 생성df1 = df.copy()
df1 = df1.fillna(0)
df1.head()
```
```
Copy# 에이전트 생성agent = create_pandas_dataframe_agent(
    ChatOpenAI(
        temperature=0,
        model_name=""gpt-3.5-turbo-0613"",
        streaming=True,
        callbacks=[StreamCallback()],
    ),
    [df, df1],
    verbose=True,
    agent_type=AgentType.OPENAI_FUNCTIONS,
)# 질의agent.invoke({""input"":""나이 컬럼의 나이의 평균차이는 어떻게 돼? %로 구해줘.""})
```
```
[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': ""df1['Age'].mean() - df2['Age'].mean()""}`


[0m[36;1m[1;3m5.899824717765892[0m나이 컬럼의 나이의 평균 차이는 약 5.9%입니다.[32;1m[1;3m나이 컬럼의 나이의 평균 차이는 약 5.9%입니다.[0m

[1m> Finished chain.[0m
```
```
{'input': '나이 컬럼의 나이의 평균차이는 어떻게 돼? %로 구해줘.', 'output': '나이 컬럼의 나이의 평균 차이는 약 5.9%입니다.'}
```"
CH07 도큐먼트 로더(Document Loader),https://wikidocs.net/233775,"# CH07 도큐먼트 로더(Document Loader)

Document Loader"
01. 논문(arxiv),https://wikidocs.net/233820,"# 01. 논문(arxiv)

Arxiv는 물리학, 수학, 컴퓨터 과학, 정량 생물학, 정량 금융, 통계학, 전기 공학 및 시스템 과학, 경제학 분야의 200만 편의 학술 논문을 위한 오픈 액세스 아카이브입니다.
이 노트북은Arxiv.org에서 과학 논문을 로드하여 다운스트림에서 사용할 수 있는 문서 형식으로 변환하는 방법을 보여줍니다.
먼저arxiv파이썬 패키지를 설치해야 합니다.
arxiv 라이브러리를 최신 버전으로 업그레이드합니다.
```
Copy%pip install -qU arxiv
```
두 번째로,arxiv.org사이트에서 다운로드한 PDF 파일을 텍스트 형식으로 변환하는PyMuPDF파이썬 패키지를 설치해야 합니다.
pymupdf 라이브러리를 최신 버전으로 업그레이드하는 pip 명령입니다.
```
Copy%pip install -qU pymupdf
```
ArxivLoader는 다음과 같은 인자를 가집니다:
- query: Arxiv에서 문서를 찾는 데 사용되는 자유 텍스트입니다.
- load_max_docs(선택): 기본값은100입니다. 다운로드할 문서의 수를 제한하는 데 사용합니다. 100개의 문서를 모두 다운로드하는 데는 시간이 걸리므로, 실험을 위해서는 작은 숫자를 사용하는 것이 좋습니다.
- load_all_available_meta(선택): 기본값은False입니다. 기본적으로 가장 중요한 필드만 다운로드됩니다. Published (문서가 게시되거나 마지막으로 업데이트된 날짜), Title, Authors, Summary. True인 경우, 다른 필드도 다운로드됩니다.
query: Arxiv에서 문서를 찾는 데 사용되는 자유 텍스트입니다.
query: Arxiv에서 문서를 찾는 데 사용되는 자유 텍스트입니다.
load_max_docs(선택): 기본값은100입니다. 다운로드할 문서의 수를 제한하는 데 사용합니다. 100개의 문서를 모두 다운로드하는 데는 시간이 걸리므로, 실험을 위해서는 작은 숫자를 사용하는 것이 좋습니다.
load_max_docs(선택): 기본값은100입니다. 다운로드할 문서의 수를 제한하는 데 사용합니다. 100개의 문서를 모두 다운로드하는 데는 시간이 걸리므로, 실험을 위해서는 작은 숫자를 사용하는 것이 좋습니다.
load_all_available_meta(선택): 기본값은False입니다. 기본적으로 가장 중요한 필드만 다운로드됩니다. Published (문서가 게시되거나 마지막으로 업데이트된 날짜), Title, Authors, Summary. True인 경우, 다른 필드도 다운로드됩니다.
load_all_available_meta(선택): 기본값은False입니다. 기본적으로 가장 중요한 필드만 다운로드됩니다. Published (문서가 게시되거나 마지막으로 업데이트된 날짜), Title, Authors, Summary. True인 경우, 다른 필드도 다운로드됩니다.
ArxivLoader를 사용하여 arXiv에서 논문을 로드합니다.
```
Copyfromlangchain_community.document_loadersimportArxivLoader
```
- ArxivLoader를 사용하여 arXiv에서 논문을 로드합니다.
- query: 매개변수에 arXiv ID ""1605.08386""을 전달하여 특정 논문을 검색합니다.
- load_max_docs: 매개변수를 2로 설정하여 최대 2개의 논문을 로드합니다.
ArxivLoader를 사용하여 arXiv에서 논문을 로드합니다.
query: 매개변수에 arXiv ID ""1605.08386""을 전달하여 특정 논문을 검색합니다.
load_max_docs: 매개변수를 2로 설정하여 최대 2개의 논문을 로드합니다.
```
Copy# ArxivLoader를 사용하여 arXiv에서 문서를 로드합니다. query 매개변수는 검색할 논문의 arXiv ID이고, load_max_docs 매개변수는 로드할 최대 문서 수를 지정합니다.docs = ArxivLoader(query=""1605.08386"", load_max_docs=2).load()len(docs)# 로드된 문서의 개수를 반환합니다.
```
```
1
```
docs[0].metadata는Document객체의 메타데이터를 반환합니다.
- docs는Document객체의 리스트입니다.
- docs[0]은 리스트의 첫 번째Document객체를 나타냅니다.
- metadata속성은 해당Document와 관련된 메타데이터 정보를 담고 있는 딕셔너리를 반환합니다.
docs는Document객체의 리스트입니다.
docs[0]은 리스트의 첫 번째Document객체를 나타냅니다.
metadata속성은 해당Document와 관련된 메타데이터 정보를 담고 있는 딕셔너리를 반환합니다.
```
Copydocs[0].metadata# 문서의 메타 정보
```
```
{'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\nallowed moves of arbitrary length. We show that the diameter of these graphs on\nfibers of a fixed integer matrix can be bounded from above by a constant. We\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\nalso state explicit conditions on the set of moves so that the heat-bath random\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\ndimension.'}
```
docs[0].page_content[:400]
- docs리스트의 첫 번째 요소(인덱스 0)에 접근합니다.
- 해당 요소의page_content속성에 접근하여 문서의 내용을 가져옵니다.
- [:400]을 사용하여 문서 내용의 첫 400자를 슬라이싱합니다.
docs리스트의 첫 번째 요소(인덱스 0)에 접근합니다.
해당 요소의page_content속성에 접근하여 문서의 내용을 가져옵니다.
[:400]을 사용하여 문서 내용의 첫 400자를 슬라이싱합니다.
이 코드는docs리스트에 저장된 문서들 중 첫 번째 문서의 내용 중 처음 400자를 반환합니다.
```
Copydocs[0].page_content[:400]# 문서의 모든 페이지 내용 중 처음 400자를 가져옵니다.
```
```
'arXiv:1605.08386v1  [math.CO]  26 May 2016\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\nCAPRICE STANLEY AND TOBIAS WINDISCH\nAbstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\nallowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\nﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\nbehaviour of heat-b'
```
전체 문서의 길이(문자열) 출력
```
Copylen(docs[0].page_content)# 문서의 페이지 수를 반환합니다.
```
```
54338
```"
02. 허깅페이스 데이터셋(HuggingFace Datasets),https://wikidocs.net/233821,"# 02. 허깅페이스 데이터셋(HuggingFace Datasets)

Hugging Face Hub는 NLP, 컴퓨터 비전, 오디오 분야에서 다양한 작업에 사용될 수 있는 100개 이상의 언어로 된 5,000개 이상의 데이터셋을 제공합니다.
이러한 데이터셋은 번역, 자동 음성 인식, 이미지 분류와 같은 다양한 작업에 활용됩니다.
이 노트북에서는Hugging Face Hub데이터셋을 LangChain에 로드하는 방법을 보여줍니다.
- HuggingFaceDatasetLoader를 사용하여 Hugging Face 데이터셋을 로드합니다.
- 로드된 데이터셋은 LangChain에서 사용할 수 있는 문서 형식으로 변환됩니다.
- 이를 통해 Hugging Face 데이터셋을 LangChain의 다양한 기능과 함께 활용할 수 있습니다.
HuggingFaceDatasetLoader를 사용하여 Hugging Face 데이터셋을 로드합니다.
로드된 데이터셋은 LangChain에서 사용할 수 있는 문서 형식으로 변환됩니다.
이를 통해 Hugging Face 데이터셋을 LangChain의 다양한 기능과 함께 활용할 수 있습니다.
```
Copyfromlangchain_community.document_loadersimportHuggingFaceDatasetLoader
```
- HuggingFaceDatasetLoader를 사용하여 ""imdb"" 데이터셋을 로드합니다.
- page_content_column매개변수를 ""text""로 설정하여 데이터셋의 텍스트 열을 지정합니다.
- 로드된 데이터셋은loader변수에 저장됩니다.
HuggingFaceDatasetLoader를 사용하여 ""imdb"" 데이터셋을 로드합니다.
page_content_column매개변수를 ""text""로 설정하여 데이터셋의 텍스트 열을 지정합니다.
로드된 데이터셋은loader변수에 저장됩니다.
```
Copydataset_name =""imdb""# 데이터셋 이름을 ""imdb""로 설정합니다.page_content_column =""text""# 페이지 내용이 포함된 열의 이름을 ""text""로 설정합니다.# HuggingFaceDatasetLoader를 사용하여 데이터셋을 로드합니다.# 데이터셋 이름과 페이지 내용 열 이름을 전달합니다.loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)
```
loader.load()메서드를 호출하여 데이터를 로드합니다.
- loader객체는 데이터 로딩을 담당하는 객체입니다.
- load()메서드는 데이터 소스(예: 파일, 데이터베이스 등)에서 데이터를 읽어와 메모리에 로드합니다.
- 로드된 데이터는data변수에 할당됩니다.
loader객체는 데이터 로딩을 담당하는 객체입니다.
load()메서드는 데이터 소스(예: 파일, 데이터베이스 등)에서 데이터를 읽어와 메모리에 로드합니다.
로드된 데이터는data변수에 할당됩니다.
```
Copydata = loader.load()# 로더를 사용하여 데이터를 불러옵니다.
```
```
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/datasets/load.py:2479: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=' instead.
  warnings.warn(
```
```
Copy# 데이터의 처음 3개 요소를 선택합니다.data[:3]
```
```
[Document(page_content='""I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \\""controversial\\"" I really had to see this for myself.The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\'t have much of a plot.""', metadata={'label': 0}), Document(page_content='""\\""I Am Curious: Yellow\\"" is a risible and pretentious steaming pile. It doesn\'t matter what one\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\'t true. I\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\'re treated to the site of Vincent Gallo\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \\""double-standard\\"" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\'s bodies.""', metadata={'label': 0}), Document(page_content='""If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one\'s mind wanders, as it will invariably do during this pointless film).One might better spend one\'s time staring out a window at a tree growing.""', metadata={'label': 0})]
```
이 예제에서는 데이터셋의 데이터를 사용하여 질문에 답변합니다.
HuggingFaceDatasetLoader를 사용하여 Hugging Face 데이터셋을 로드하고, VectorstoreIndexCreator를 통해 벡터 저장소 기반의 인덱스를 생성하는 코드입니다.
- HuggingFaceDatasetLoader를 사용하여 Hugging Face 데이터셋을 로드합니다.
- 로드된 데이터셋을VectorstoreIndexCreator에 전달하여 벡터 저장소 기반의 인덱스를 생성합니다.
- 생성된 인덱스를 활용하여 데이터셋에 대한 검색 및 질의 응답을 수행할 수 있습니다.
HuggingFaceDatasetLoader를 사용하여 Hugging Face 데이터셋을 로드합니다.
로드된 데이터셋을VectorstoreIndexCreator에 전달하여 벡터 저장소 기반의 인덱스를 생성합니다.
생성된 인덱스를 활용하여 데이터셋에 대한 검색 및 질의 응답을 수행할 수 있습니다.
```
Copyfromlangchain.indexesimportVectorstoreIndexCreatorfromlangchain_community.document_loaders.hugging_face_datasetimport(
    HuggingFaceDatasetLoader,
)
```
- HuggingFaceDatasetLoader를 사용하여 Hugging Face의 ""tweet_eval"" 데이터셋에서 ""stance_climate"" subset의 데이터를 로드합니다.
- 데이터셋의 텍스트 컬럼은page_content_column매개변수로 지정된 ""text""입니다.
- 로드된 데이터는loader변수에 저장됩니다.
HuggingFaceDatasetLoader를 사용하여 Hugging Face의 ""tweet_eval"" 데이터셋에서 ""stance_climate"" subset의 데이터를 로드합니다.
데이터셋의 텍스트 컬럼은page_content_column매개변수로 지정된 ""text""입니다.
로드된 데이터는loader변수에 저장됩니다.
```
Copydataset_name =""tweet_eval""# 데이터셋 이름을 ""tweet_eval""로 설정합니다.page_content_column =""text""# 페이지 내용이 포함된 열의 이름을 ""text""로 설정합니다.name =""stance_climate""# 데이터셋의 특정 부분을 식별하는 이름을 ""stance_climate""로 설정합니다.# HuggingFaceDatasetLoader를 사용하여 데이터셋을 로드합니다.loader = HuggingFaceDatasetLoader(dataset_name, page_content_column, name)
```
VectorstoreIndexCreator클래스를 사용하여 로더(loader)에서 벡터 저장소 인덱스를 생성합니다.
- VectorstoreIndexCreator클래스의 인스턴스를 생성합니다.
- from_loaders메서드를 호출하여 로더 리스트를 전달합니다.
- 로더에서 추출된 데이터를 기반으로 벡터 저장소 인덱스가 생성됩니다.
- 생성된 인덱스는index변수에 할당됩니다.
VectorstoreIndexCreator클래스의 인스턴스를 생성합니다.
from_loaders메서드를 호출하여 로더 리스트를 전달합니다.
로더에서 추출된 데이터를 기반으로 벡터 저장소 인덱스가 생성됩니다.
생성된 인덱스는index변수에 할당됩니다.
```
Copy# 로더에서 벡터 저장소 인덱스를 생성합니다.index = VectorstoreIndexCreator().from_loaders([loader])
```
```
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.
  warn_deprecated(
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/datasets/load.py:2479: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=' instead.
  warnings.warn(
```
index.query()를 사용하여 ""What are the most used hashtag?""라는 질문에 대한 답변을 생성합니다.
- query변수에 질문 문자열을 할당합니다.
- index.query()메서드를 호출하여 질문에 대한 답변을 생성합니다.
- 생성된 답변은result변수에 저장됩니다.
query변수에 질문 문자열을 할당합니다.
index.query()메서드를 호출하여 질문에 대한 답변을 생성합니다.
생성된 답변은result변수에 저장됩니다.
```
Copyquery =""What are the most used hashtag?""# 가장 많이 사용되는 해시태그는 무엇인가요?result = index.query(query)# 질의를 수행하여 결과를 얻습니다.
```
```
Copyresult
```
```
' The most used hashtags in this context are #TakeDownTheFlag, #LoveWins, #SemST, #ThanksObama, and #green.'
```"
03. 웹 크롤링,https://wikidocs.net/233822,"# 03. 웹 크롤링

Chromium은 브라우저 자동화를 제어하는 데 사용되는 라이브러리인 Playwright에서 지원하는 브라우저 중 하나입니다.
p.chromium.launch(headless=True)를 실행하면 Chromium의 headless 인스턴스를 실행합니다.
Headless 모드는 브라우저가 그래픽 사용자 인터페이스 없이 실행되고 있음을 의미합니다.
AsyncChromiumLoader는 페이지를 로드한 다음Html2TextTransformer를 사용하여 텍스트로 변환합니다.
- Playwright와 BeautifulSoup4 라이브러리를 최신 버전으로 업그레이드하고 조용히(quiet) 설치합니다.
- Playwright 브라우저 엔진을 설치합니다.
Playwright와 BeautifulSoup4 라이브러리를 최신 버전으로 업그레이드하고 조용히(quiet) 설치합니다.
Playwright 브라우저 엔진을 설치합니다.
```
Copy%pip install -qU  playwright beautifulsoup4
! playwright install
```
LangChain 프레임워크를 사용하려고 할 때RuntimeError: asyncio.run()이 이미 실행 중인 이벤트 루프에서 호출될 수 없다는 오류가 발생합니다.
이 오류는 보통asyncio.run()이 다른 asyncio 이벤트 루프가 이미 실행 중인 상황에서 호출될 때 발생합니다.
Jupyter Notebook 에서 이미 실행되고 있는 이벤트 루프가 존재하고, 또 하나의 이벤트 루프를 생성하려고 할 때 나타나는 오류 입니다.
이러한 오류를 해결하기 위하여 아래의 코드를 실행하여 Jupyter Notebook 의 이벤트 루프를 가져옵니다.
```
Copy%pip install -qU nest-asyncio
```
```
Copyimportnest_asyncio

nest_asyncio.apply()
```
AsyncChromiumLoader를 사용하여 웹 페이지의 콘텐츠를 비동기적으로 로드합니다.
- urls리스트에 로드할 웹 페이지의 URL을 지정합니다.
- AsyncChromiumLoader객체를 생성하고urls를 전달합니다.
- load()메서드를 호출하여 웹 페이지의 콘텐츠를 로드하고Document객체의 리스트를 반환합니다.
urls리스트에 로드할 웹 페이지의 URL을 지정합니다.
AsyncChromiumLoader객체를 생성하고urls를 전달합니다.
load()메서드를 호출하여 웹 페이지의 콘텐츠를 로드하고Document객체의 리스트를 반환합니다.
```
Copyfromlangchain_community.document_loadersimportAsyncChromiumLoader# 크롤링할 URL 목록을 설정합니다.urls = [""https://news.naver.com""]# AsyncChromiumLoader를 사용하여 URL에서 비동기적으로 문서를 로드합니다.loader = AsyncChromiumLoader(urls)# 로드된 문서를 가져옵니다.docs = loader.load()# 0번 문서의 내용 중 중간의 500자를 출력합니다.docs[0].page_content[6000:6500]
```
```
'og:article:author"" content=""네이버"">\n\n\n\n\n\n\n\n\t\t\n        \n    \n    (function () {\n        var '
```
Html2TextTransformer클래스를 사용하여 HTML 문서를 텍스트로 변환하는 과정을 보여줍니다.
- html2text객체를 생성합니다.
- transform_documents메서드를 호출하여docs리스트에 포함된 HTML 문서들을 텍스트로 변환합니다.
- 변환된 문서들은docs_transformed리스트에 저장됩니다.
- docs_transformed[0].page_content을 통해 변환된 첫 번째 문서의 내용을 확인합니다.
html2text객체를 생성합니다.
transform_documents메서드를 호출하여docs리스트에 포함된 HTML 문서들을 텍스트로 변환합니다.
변환된 문서들은docs_transformed리스트에 저장됩니다.
docs_transformed[0].page_content을 통해 변환된 첫 번째 문서의 내용을 확인합니다.
html 을 문자열로 변환 & 파싱합니다.
파싱하기 위한html2text를 설치합니다.
```
Copy%pip install -qU html2text
```
```
Copyfromlangchain_community.document_transformersimportHtml2TextTransformer

html2text = Html2TextTransformer()# HTML을 텍스트로 변환하는 객체를 생성합니다.docs_transformed = html2text.transform_documents(
    docs
)# HTML 문서를 텍스트로 변환합니다.
```
```
Copy# 변환된 문서를 출력합니다.print(docs_transformed[0].page_content[2000:2500])
```
```
금 50만원은
최대 지급액이라는 의미로, 반드시 50만원을 지급받는건 아니다. 15일 통신

#### 블로터03월 15일 20:12

구독

'우리금융 민영화'라 쓰고 '임종룡의 수미쌍관'이라 읽는다

우리금융지주가 예금보험공사 보유지분을 매입해 완전 민영화에 성공했다. 임종룡 우리금융 회장이 금융위원장으로 재직하던 시기 민영화 신호탄을 쏜
지 약 8년 만이다. 우리금융은 즉각 자사주 소각을 결정, 당국의 증시 저

새로보기

#### 코리아헤럴드03월 15일 17:47

구독

'Squid Game' star Oh Young-soo gets suspended term for sexual misconduct

South Korean actor Oh Young-soo, star of the 2021 hit drama series ""Squid
Game,"" was given an eight-month prison sentenc

#### JIBS03월 15일 21:12

구독

동영상 재
```
## Beautiful Soup
Beautiful Soup은 HTML 및 XML 문서를 파싱하기 위한 Python 패키지입니다(잘못 형성된 마크업, 즉 닫히지 않은 태그를 포함하여, 태그 수프라고 불립니다).
파싱된 페이지에 대한 파싱 트리를 생성하여 HTML에서 데이터를 추출하는 데 사용할 수 있으며, 이는 웹 스크래핑에 유용합니다.
Beautiful Soup은 HTML 내용에 대한 세밀한 제어를 제공하여, 특정 태그의 추출, 제거 및 내용 정리를 가능하게 합니다.
특정 정보를 추출하고 HTML 내용을 필요에 따라 정리하고자 하는 경우에 적합합니다.
예를 들어, HTML 내용에서<p>, <li>, <div>, <a>태그 내의 텍스트 내용을 스크래핑할 수 있습니다.
- <p>: 단락 태그입니다. HTML에서 단락을 정의하며 관련된 문장 및/또는 구절을 함께 그룹화하는 데 사용됩니다.
- <li>: 목록 항목 태그입니다. 순서가 있는 (<ol>) 및 순서가 없는 (<ul>) 목록 내에서 개별 항목을 정의하는 데 사용됩니다.
- <div>: 구분 태그입니다. 블록 수준 요소로 다른 인라인 또는 블록 수준 요소를 그룹화하는 데 사용됩니다.
- <a>: 앵커 태그입니다. 하이퍼링크를 정의하는 데 사용됩니다.
<p>: 단락 태그입니다. HTML에서 단락을 정의하며 관련된 문장 및/또는 구절을 함께 그룹화하는 데 사용됩니다.
<p>: 단락 태그입니다. HTML에서 단락을 정의하며 관련된 문장 및/또는 구절을 함께 그룹화하는 데 사용됩니다.
<li>: 목록 항목 태그입니다. 순서가 있는 (<ol>) 및 순서가 없는 (<ul>) 목록 내에서 개별 항목을 정의하는 데 사용됩니다.
<li>: 목록 항목 태그입니다. 순서가 있는 (<ol>) 및 순서가 없는 (<ul>) 목록 내에서 개별 항목을 정의하는 데 사용됩니다.
<div>: 구분 태그입니다. 블록 수준 요소로 다른 인라인 또는 블록 수준 요소를 그룹화하는 데 사용됩니다.
<div>: 구분 태그입니다. 블록 수준 요소로 다른 인라인 또는 블록 수준 요소를 그룹화하는 데 사용됩니다.
<a>: 앵커 태그입니다. 하이퍼링크를 정의하는 데 사용됩니다.
<a>: 앵커 태그입니다. 하이퍼링크를 정의하는 데 사용됩니다.
```
Copyfromlangchain_community.document_loadersimportAsyncChromiumLoader

urls = [""https://news.naver.com""]# 크롤링할 URL 목록을 설정합니다.# AsyncChromiumLoader를 사용하여 URL에서 비동기적으로 문서를 로드합니다.loader = AsyncChromiumLoader(urls)
docs = loader.load()# 로드된 문서를 가져옵니다.
```
아래의 코드는 로드한 웹페이지에서 특정 태그를 추출하는 예시입니다.
```
Copyfromlangchain_community.document_transformersimportBeautifulSoupTransformer# 변환 작업bs_transformer = BeautifulSoupTransformer()# HTML 문서를 변환합니다. p, li, div, a 태그의 내용을 추출합니다.docs_transformed = bs_transformer.transform_documents(
    docs,# tags_to_extract=[""p"", ""li"", ""div"", ""a""]tags_to_extract=[""a""],
)
```
```
Copy# a 태그의 내용을 출력합니다.print(docs_transformed[0].page_content[:1000])
```
```
본문 바로가기 (#ct) NAVER 뉴스  (https://entertain.naver.com/home) 연예  (https://entertain.naver.com/home)  (https://sports.news.naver.com/index) 스포츠  (https://sports.news.naver.com/index)  (https://weather.naver.com/) 날씨  (https://weather.naver.com/)  (https://contents.premium.naver.com/) 프리미엄  (https://contents.premium.naver.com/)  (https://news.naver.com/election/nation2024) 총선  (https://news.naver.com/election/nation2024) 도움말 (https://help.naver.com/support/alias/search/word/word_32.naver) 전체삭제 (javascript:;) 자동저장 (javascript:;) 끄기 켜기 도움말 (https://help.naver.com/support/alias/search/word/word_32.naver) 도움말 (https://help.naver.com/support/alias/search/word/word_48.naver) 신고 (https://help.naver.com/support/alias/search/word/word_18.naver) 검색 언론사별 정치 경제 사회 생활/문화 IT/과학 세계 랭킹 신문보기 오피니언 TV 팩트체크 알고리즘 안내 정정보도 모음 전체 언론사 (https://news.naver.com/main/officeList.naver ) 뉴스스탠드 (https://newsstand.naver.com/) 라이브러리 (https://newslibrary.naver.com/search/searchByDate.naver)  (#feedJournalistCard) 기자  (#feed
```
## WebBaseLoader
WebBaseLoader는HTML 웹페이지의 모든 텍스트를 추출하여, 다양한 용도로 활용할 수 있는 문서 형태로 변환하는 기능을 가진 도구입니다.
보다 특정한 사용 사례나 데이터 소스에 최적화된 로직을 적용하기 위해서는,WebBaseLoader의 자식 클래스들을 사용할 수 있습니다. 예를 들어, IMSDbLoader는 영화 대본을 다루는 IMSDb 웹사이트에서 데이터를 로드하는 데 특화되어 있으며, AZLyricsLoader는 가사 정보를 제공하는 AZLyrics 웹사이트의 텍스트를 추출하는 데 사용됩니다. CollegeConfidentialLoader는 대학 커뮤니티 포럼인 CollegeConfidential에서 데이터를 가져오는 데 최적화되어 있습니다.
이러한 자식 클래스들은 WebBaseLoader가 제공하는 기본적인 텍스트 로딩 기능에 추가적인 처리 또는 웹사이트별 데이터 구조에 맞는 파싱 로직을 적용하여, 특정 웹사이트의 구조와 내용에 더 잘 맞는 데이터 추출 방법을 제공합니다. 이를 통해, 사용자는 웹에서 정보를 효율적으로 수집하고, 다양한 분석이나 애플리케이션에 필요한 형식으로 쉽게 변환할 수 있습니다.
- WebBaseLoader를 사용하여 웹 페이지를 로드합니다.
- 한글이 깨져서 로드되는 경우 아래와 같이 인코딩(encoding=""utf-8"") 옵션을 지정합니다.
WebBaseLoader를 사용하여 웹 페이지를 로드합니다.
한글이 깨져서 로드되는 경우 아래와 같이 인코딩(encoding=""utf-8"") 옵션을 지정합니다.
```
Copyfromlangchain_community.document_loadersimportWebBaseLoader# 웹 페이지 ""https://news.naver.com/""에서 데이터를 로드하는 WebBaseLoader 객체를 생성합니다.loader = WebBaseLoader(""https://news.naver.com/"")# 한글이 깨져서 로드되는 경우 아래와 같이 인코딩을 지정합니다.# loader = WebBaseLoader(""https://news.naver.com/"", encoding=""utf-8"")
```
SSL 인증서 검증 오류를 우회하기 위해 데이터를 가져오는 동안 ""verify"" 옵션을 설정할 수 있습니다.
- 이렇게 하면 SSL 인증서 검증 과정을 건너뛰고 데이터를 가져올 수 있습니다. 하지만 이 방법은 보안상의 이유로 권장되지 않습니다. 가능하다면 올바른 SSL 인증서를 사용하는 것이 좋습니다.
이렇게 하면 SSL 인증서 검증 과정을 건너뛰고 데이터를 가져올 수 있습니다. 하지만 이 방법은 보안상의 이유로 권장되지 않습니다. 가능하다면 올바른 SSL 인증서를 사용하는 것이 좋습니다.
```
Copy# SSL 인증서 오류 우회loader.requests_kwargs = {""verify"":False}
```
loader.load()메서드를 호출하여 데이터를 로드합니다.
```
Copy# 로더를 사용하여 데이터를 불러옵니다.web_data = loader.load()# 불러온 데이터를 출력합니다.print(web_data[0].page_content[2000:2500].replace(""\n"",""""))
```
```
있다."" 강도형 해양수산부 장관이 공개석상에서 '노 전 대통령 요트 사건'을 거론한 기사들이 사라졌다. 연합한경비즈니스03월 15일 17:28구독해외 유명 과자 모셔오기 나선 유통가직소싱한 해외 간식들이 잇달아 큰 인기를 얻으며 유통업계 효자 상품으로 자리매김했다. 이에 편의점 및 마트 등 유통업계는 늘어나는 수요에 따라 해외 유명 과자류 직수입에 적극적으로 나서고 있다. 실제로 관세청 수출입여성신문03월 15일 18:53구독이윤택부터 오영수·조민기까지…연극계 '미투' 가해의 역사넷플릭스 드라마 '오징어게임'에 출현한 오영수씨가 15일 여성을 강제 추행한 혐의로 징역 8개월을 선고받았다. 지난 2018년 2월 연극연출가 이윤택씨에 대한 '미투(me too, 나도 당했다)'를 시작으로 일어난 새로보기조세일보03월 15일 23:34구독한은 노조 '
```
BeautifulSoup을 사용하여 새로운 사용자 정의 웹 페이지 파서를 활용할 수 있습니다.
- 예시로soup.select_one(""td[class='scrtext']"").text와 같은 코드를 사용하여 원하는 요소를 선택하고 텍스트를 추출할 수 있습니다.
- BS4 문서에서 더욱 자세한 내용을 확인할 수 있습니다.
예시로soup.select_one(""td[class='scrtext']"").text와 같은 코드를 사용하여 원하는 요소를 선택하고 텍스트를 추출할 수 있습니다.
BS4 문서에서 더욱 자세한 내용을 확인할 수 있습니다.
```
Copyimportrequestsfrombs4importBeautifulSoup

html_doc = requests.get(""https://news.naver.com/"")
soup = BeautifulSoup(html_doc.text,""html.parser"")# 예제 코드# soup.select_one(""td[class='scrtext']"").text
```
```
Copy# body 태그의 내용을 출력합니다.print(soup.select_one(""body"").text.replace(""\n"","""")[:1000])
```
```
본문 바로가기NAVER뉴스연예스포츠날씨프리미엄총선검색언론사별정치경제사회생활/문화IT/과학세계랭킹신문보기오피니언TV팩트체크알고리즘 안내정정보도 모음콘텐츠3.16(토)전체 언론사뉴스스탠드라이브러리언론사편집기자                                            연재                                                            구독설정                                코리아중앙데일리03월 15일 18:33구독Medical professors to resign if gov't suspends junior doctorsProfessors at three leading medical schools resolved to resign en masse to protest the government's intent to punish jun주간동아03월 15일 18:07구독경기 남부 ‘반도체 벨트’ 17곳 대진표 확정… 여도, 야도 전문가 투입미래 첨단산업 품은 수원·평택·용인·이천·화성에 정치 신인 대거 출마 한국의 미래 첨단산업을 품고 있는 경기 남부 ‘반도체 벨트’가 4·10 총선에서 수도권 승부처로 부상하고 있다. 반도체산업 인프라가 집중돼 인구 한겨레03월 16일 01:21구독한동훈, ‘런종섭’ 비판 커지자 태세 전환…“귀국 수사 응해야”한동훈 국민의힘 비상대책위원장이 15일 해병대 채아무개 상병 순직 사건 핵심 피의자인 이종섭 주오스트레일리아(호주) 대사를 향해 “신속하게 들어와서 정리해야 한다”고 촉구했다. 이 대사의 출국이 도피성이라는 비판이 오마이뉴스03월 15일 21:26구독사과값이 선거판을 흔든다 [사진으로 보는 일주일]240311~240315 오마이뉴스가 정리한 한주의 사진 ▲ 지난 12일 통계청 국가통계포털에 따르면 지난달 과실 물가 상승률은 40.6%로 전체 소비자물가 상승률(3.1%)보다 37.5%포인트 높았다. 지난달 사과경기일보03월 15일 23:02구독‘이
```
## 여러 페이지 로드
여러 개의 웹페이지를 한 번에 로드하려면 로더에 URL 리스트를 전달하면 됩니다. 이렇게 하면 전달된 URL과 동일한 순서로 문서 리스트가 반환됩니다.
WebBaseLoader를 사용하여 여러 웹페이지를 로드해 봅시다.
```
Copy# 웹 페이지 URL 목록을 사용하여 WebBaseLoader 객체를 생성합니다.loader = WebBaseLoader([""https://news.naver.com/"",""https://news.daum.net""])
docs = loader.load()# 지정된 웹 페이지에서 문서를 로드합니다.# 로드된 문서를 출력합니다.print(""Naver"")print(docs[0].page_content.replace(""\n"","""")[200:1000])print(""===""*20)print(""Daum"")print(docs[1].page_content.replace(""\n"","""")[:1000])
```
```
Naver
                          구독설정                                매일경제03월 16일 00:53구독“학생때부터 미인으로 유명”...한국 찾은 ‘오타니의 그녀’도 화제15일 오후 인천공항 통해 입국 선수들중 오타니와 함께 맨 먼저 이동 한일 양국 야구팬들 입국장서 환영 日네티즌 “둘이 닮았다” “어울린다” 미국 프로야구(MLB) LA 다저스가 정규리그 개막전을 위해 15일 한국에뉴스103월 16일 00:29구독서울 오피스텔서 20대 여성 숨진 채 발견…40대 남성 용의자 체포서울의 한 오피스텔에서 20대 여성이 숨진 채 발견된 사건의 유력 용의자가 경찰에 붙잡혔다. 15일 서울 은평경찰서 등에 따르면 경찰은 이날 오후 10시 24분쯤 40대 남성 A 씨를 구로구의 한 만화방에서 긴급 체부산일보03월 15일 22:40구독[총선 브리핑] 장예찬 ""철없던 시절 글로 심려끼쳐 죄송""부산 수영에 출마한 국민의힘 장예찬 후보가 논란이 된 과거 SNS 글에 대해 사과했다. 장 후보는 15일 부산시의회에서 기자회견을 자청해 “10여 년 전, 25살 무렵 페이스북에 남겼던 글들로 국민께 심려를 끼쳐 진조선비즈03월 16일 01:29구독동영상재생시간01:38[총선, 현장] 80년 광주 토박이의 한 마디 “여야고 나발이고 정치를 잘혀야제잉”한동훈 국힘 비대위원장 호남 지역 방문 순천 시작으로 광주, 전북 등 돌며 지지 호소 “호남 전 지역에 후보 출마…진심 다할 것” “호남에서 이기는 게 우리가 진짜 이기는 것” 15일 오전 11시 전라남도 순천시 웃경기일보03월 15일 23:02구독
============================================================
Daum
홈 | 다음뉴스본문 바로가기메뉴 바로가기뉴스관련 서비스연예스포츠 뉴스 메인메뉴홈사회정치경제국제문화IT연재포토팩트체크홈이슈 기사 목록국제                                    '돈나무 언니' 캐시 우드, 테슬라 주식 대량 매수…""쌀 때 사자""                                국제                                    아르헨 밀레이 ""150억 달러가 있다면 당장 외환규제 풀 것""                                정치                                    [인터뷰] 김영석 ""아산의 미래, 인기 위주 정책 펴는 세력에 맡겨선 안돼""                                정치                                    김은혜, HD현대 찾아 분당내 기업형 '특목·자사고' 설립 요청                                사회                                    정부 “병원 이탈 일부 전공의 재취업, 처벌 대상”                                사회                                    수도권 의대 10% 안팎 증원… 미니 의대는 100명으로                                사회                                    단전에 또 멈춘 지하철… 이번엔 3호선 출근대란                                경제                                    유한양행, 창업주 손녀 반대에도 28년만에 ‘회장직’ 부활                                국제                                    전 MI6 국장 ""영·러 '회색 전쟁' 중…국방비 늘려야""
```
## 여러 URL을 동시에 로드
여러 URL을 동시에 스크래핑하고 파싱하는 방법은 스크래핑 프로세스의 속도를 상당히 향상시킬 수 있는 효과적인 방법입니다. 이를 통해, 다량의 데이터를 빠르게 수집할 수 있으며, 작업의 효율성을 크게 개선할 수 있습니다.
기본적으로 동시 요청의 수는 초당 2개로 제한되어 있으나,requests_per_second매개변수를 조정하여 이 제한을 변경할 수 있습니다. 이는 특히 스크래핑하는 서버를 직접 관리하거나, 서버 부하에 대해 우려하지 않는 경우 유용할 수 있습니다.
그러나,동시 요청 수를 무리하게 늘리는 것은 서버에 과부하** 줄 수 있으며, 이로 인해 스크래핑 대상 웹사이트에서 차단당할 위험을 증가시킵니다. 스크래핑 작업을 수행할 때는 항상 해당 웹사이트의 이용 약관을 확인하고, 과도한 부하를 주지 않도록 주의를 기울여야 합니다. 만약 가능하다면, API를 통한 데이터 접근을 우선 고려하는 것도 좋은 방법입니다.
- requests_per_second속성을 1로 설정하여 초당 요청 수를 제한합니다.
- aload()메서드를 호출하여 비동기적으로 웹 페이지 데이터를 로드하고docs변수에 할당합니다.
requests_per_second속성을 1로 설정하여 초당 요청 수를 제한합니다.
aload()메서드를 호출하여 비동기적으로 웹 페이지 데이터를 로드하고docs변수에 할당합니다.
```
Copy# 웹 페이지 URL 목록을 사용하여 WebBaseLoader 객체를 생성합니다.loader = WebBaseLoader([""https://news.naver.com/"",""https://news.daum.net""])
loader.requests_per_second =1# 초당 요청 수를 1로 설정합니다.docs = loader.aload()# 지정된 웹 페이지에서 문서를 로드합니다.# 로드된 문서를 출력합니다.print(""Naver"")print(docs[0].page_content.replace(""\n"","""")[200:1000])print(""===""*20)print(""Daum"")print(docs[1].page_content.replace(""\n"","""")[:1000])
```
```
Fetching pages: 100%|##########| 2/2 [00:00<00:00,  5.59it/s]
```
```
Naver
                          구독설정                                미디어오늘03월 15일 21:42구독해수부 장관의 '노무현 요트사건' 발언 기사가 삭제된 이유""노무현 (전) 대통령 요트 사건이나 영화 속의 요트 등 문화에 대한 부정적 인식이 아직 남아 있는 경우가 있다."" 강도형 해양수산부 장관이 공개석상에서 '노 전 대통령 요트 사건'을 거론한 기사들이 사라졌다. 연합아이뉴스2403월 16일 01:38구독""어? 비트코인, 갑자기 왜 이래""…'천당에서 지옥으로'최근 나흘 동안 1억원 이상에서 고공행진을 하던 비트코인이 급락했다. 현재 고점 대비 800만원 이상 떨어진 가격에 거래되고 있다. 미국의 금리인하 기대가 꺾이면서 악재로 작용한 것으로 풀이된다. 15일 오후 8시4중앙일보03월 16일 02:06구독한 알이 골프공만 하네…세계에서 가장 큰 '괴물 블루베리'호주의 한 농장에서 재배한 블루베리가 세계에서 가장 큰 블루베리로 기네스 세계 기록을 세웠다. 15일 호주 농산물 기업 코스타 그룹에 따르면 이번에 세계 기록에 등재된 블루베리는 너비 3.931cm로 골프공만 하며,강원일보03월 15일 14:57구독양양 종합운동장 트랙 개·보수 13억원 수의계약 검증 허술속보=양양군이 종합운동장 육상 트랙 개·보수 공사를 추진하면서 수의계약 방식으로 특정업체에 특혜를 줬다는 논란(본보 2월13일자 5면 보도)이 불거진 가운데 해당 업체 시공 품질에 대한 행정기관의 검증이 허술했던 것경기일보03월 15일 23:02구독‘이재명 저격수’ 유동규, 출마 포기... ""원희룡에 힘 모
============================================================
Daum
홈 | 다음뉴스본문 바로가기메뉴 바로가기뉴스관련 서비스연예스포츠 뉴스 메인메뉴홈사회정치경제국제문화IT연재포토팩트체크홈이슈 기사 목록국제                                    '돈나무 언니' 캐시 우드, 테슬라 주식 대량 매수…""쌀 때 사자""                                국제                                    아르헨 밀레이 ""150억 달러가 있다면 당장 외환규제 풀 것""                                정치                                    [인터뷰] 김영석 ""아산의 미래, 인기 위주 정책 펴는 세력에 맡겨선 안돼""                                정치                                    김은혜, HD현대 찾아 분당내 기업형 '특목·자사고' 설립 요청                                사회                                    정부 “병원 이탈 일부 전공의 재취업, 처벌 대상”                                사회                                    수도권 의대 10% 안팎 증원… 미니 의대는 100명으로                                사회                                    단전에 또 멈춘 지하철… 이번엔 3호선 출근대란                                경제                                    유한양행, 창업주 손녀 반대에도 28년만에 ‘회장직’ 부활                                국제                                    전 MI6 국장 ""영·러 '회색 전쟁' 중…국방비 늘려야""
```
## XML 파일 로드
SitemapLoader는 웹사이트의 사이트맵 파일을 로드하고 파싱하기 위한 도구입니다. 사이트맵은 웹사이트의 페이지들이 어떻게 구성되어 있는지를 XML 형식으로 정리한 파일로, 검색 엔진이 웹사이트의 구조를 더 쉽게 이해하고 페이지들을 효율적으로 크롤링할 수 있도록 도와줍니다. 따라서 SitemapLoader를 사용하여 사이트맵을 로드하고 파싱하는 것은 웹사이트의 구조를 분석하거나 특정 페이지들에 대한 정보를 수집하는 데 매우 유용합니다.
- WebBaseLoader를 사용하여 웹 페이지(https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml)에서 데이터를 로드합니다.
- default_parser를 ""xml""로 설정합니다.
WebBaseLoader를 사용하여 웹 페이지(https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml)에서 데이터를 로드합니다.
default_parser를 ""xml""로 설정합니다.
```
Copyloader = WebBaseLoader(# 웹 기반 로더를 사용하여 XML 문서를 로드합니다.""https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml"")
loader.default_parser =""xml""# 기본 파서를 XML로 설정합니다.docs = loader.load()# 문서를 로드합니다.# 로드된 문서를 출력합니다.print(docs[0].page_content[:1000].replace(""\n"",""""))
```
```
10Energy32018-01-012018-01-01falseUniform test method for the measurement of energy efficiency of commercial packaged boilers.Â§ 431.86Section Â§ 431.86EnergyDEPARTMENT OF ENERGYENERGY CONSERVATIONENERGY EFFICIENCY PROGRAM FOR CERTAIN COMMERCIAL AND INDUSTRIAL EQUIPMENTCommercial Packaged BoilersTest Procedures§ 431.86Uniform test method for the measurement of energy efficiency of commercial packaged boilers.(a) Scope. This section provides test procedures, pursuant to the Energy Policy and Conservation Act (EPCA), as amended, which must be followed for measuring the combustion efficiency and/or thermal efficiency of a gas- or oil-fired commercial packaged boiler.(b) Testing and Calculations. Determine the thermal efficiency or combustion efficiency of commercial packaged boilers by conducting the appropriate test procedure(s) indicated in Table 1 of this section.Table 1—Test Requirements for Commercial Packaged Boiler Equipment ClassesEquipment categorySu
```"
04. 판다스 데이터프레임(DataFrame),https://wikidocs.net/233823,"# 04. 판다스 데이터프레임(DataFrame)

이 튜토리얼에서는pandasDataFrame을 사용하여 데이터를 로드하는 방법에 대해 배워보겠습니다. pandas는 Python에서 데이터 조작과 분석을 위해 널리 사용되는 강력한 라이브러리입니다. 특히, 데이터 과학과 머신러닝 분야에서 그 중요성이 더욱 부각됩니다.
DataFrame은 pandas의 핵심적인 데이터 구조로, 행과 열로 구성된 2차원의 레이블이 붙은 데이터 구조입니다. 이를 통해 다양한 유형의 데이터를 효과적으로 저장, 조작, 그리고 분석할 수 있습니다.
```
Copy%pip install -qU pandas
```
- pandas라이브러리를pd별칭으로 임포트합니다.
pandas라이브러리를pd별칭으로 임포트합니다.
```
Copyimportpandasaspd
```
- pd.read_csv()함수를 사용하여 ""data/titanic.csv"" 파일에서 데이터를 읽어옵니다.
- 읽어온 데이터는df변수에 저장됩니다.
pd.read_csv()함수를 사용하여 ""data/titanic.csv"" 파일에서 데이터를 읽어옵니다.
읽어온 데이터는df변수에 저장됩니다.
```
Copy# CSV 파일에서 데이터를 읽어와 DataFrame 객체로 생성합니다.df = pd.read_csv(""data/titanic.csv"")
```
df.head()메서드를 사용하여 데이터프레임의 처음 5개 행을 출력합니다.
- 데이터프레임df의.head()메서드를 호출합니다.
- .head()메서드는 인자를 지정하지 않으면 기본적으로 처음 5개의 행을 반환합니다.
- 출력 결과를 통해 데이터프레임의 구조와 내용을 간략히 확인할 수 있습니다.
데이터프레임df의.head()메서드를 호출합니다.
.head()메서드는 인자를 지정하지 않으면 기본적으로 처음 5개의 행을 반환합니다.
출력 결과를 통해 데이터프레임의 구조와 내용을 간략히 확인할 수 있습니다.
```
Copy# 데이터프레임의 첫 5개 행을 출력합니다.df.head()
```
DataFrameLoader를 사용하여 데이터프레임에서 문서를 로드합니다.
- DataFrameLoader는langchain_community.document_loaders모듈에서 import 됩니다.
- 데이터프레임을 문서 형태로 변환하는 역할을 합니다.
- 로드된 문서는 LangChain의 다른 컴포넌트에서 사용될 수 있습니다.
DataFrameLoader는langchain_community.document_loaders모듈에서 import 됩니다.
데이터프레임을 문서 형태로 변환하는 역할을 합니다.
로드된 문서는 LangChain의 다른 컴포넌트에서 사용될 수 있습니다.
```
Copyfromlangchain_community.document_loadersimportDataFrameLoader
```
- DataFrameLoader를 사용하여 데이터프레임df를 로드합니다.
- page_content_column매개변수를 ""Name""으로 설정하여 데이터프레임의 ""Name"" 열을 페이지 콘텐츠로 사용합니다.
DataFrameLoader를 사용하여 데이터프레임df를 로드합니다.
page_content_column매개변수를 ""Name""으로 설정하여 데이터프레임의 ""Name"" 열을 페이지 콘텐츠로 사용합니다.
```
Copy# DataFrame에서 데이터를 로드하고, 'Team' 열을 페이지 내용으로 사용합니다.loader = DataFrameLoader(df, page_content_column=""Name"")
```
loader.load()메서드를 호출하여 데이터를 로드합니다.
- TextLoader클래스의 인스턴스인loader가 사용됩니다.
- load()메서드는TextLoader에 지정된 텍스트 파일에서 데이터를 읽어들입니다.
- 로드된 데이터는Document객체의 리스트 형태로 반환됩니다.
TextLoader클래스의 인스턴스인loader가 사용됩니다.
load()메서드는TextLoader에 지정된 텍스트 파일에서 데이터를 읽어들입니다.
로드된 데이터는Document객체의 리스트 형태로 반환됩니다.
```
Copyloader.load()# 데이터를 로드합니다.
```
```
[Document(page_content='Braund, Mr. Owen Harris', metadata={'PassengerId': 1, 'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'A/5 21171', 'Fare': 7.25, 'Cabin': nan, 'Embarked': 'S'}), Document(page_content='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', metadata={'PassengerId': 2, 'Survived': 1, 'Pclass': 1, 'Sex': 'female', 'Age': 38.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'PC 17599', 'Fare': 71.2833, 'Cabin': 'C85', 'Embarked': 'C'}), Document(page_content='Heikkinen, Miss. Laina', metadata={'PassengerId': 3, 'Survived': 1, 'Pclass': 3, 'Sex': 'female', 'Age': 26.0, 'SibSp': 0, 'Parch': 0, 'Ticket': 'STON/O2. 3101282', 'Fare': 7.925, 'Cabin': nan, 'Embarked': 'S'}), Document(page_content='Futrelle, Mrs. Jacques Heath (Lily May Peel)', metadata={'PassengerId': 4, 'Survived': 1, 'Pclass': 1, 'Sex': 'female', 'Age': 35.0, 'SibSp': 1, 'Parch': 0, 'Ticket': '113803', 'Fare': 53.1, 'Cabin': 'C123', 'Embarked': 'S'}), Document(page_content='Allen, Mr. William Henry', metadata={'PassengerId': 5, 'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 35.0, 'SibSp': 0, 'Parch': 0, 'Ticket': '373450', 'Fare': 8.05, 'Cabin': nan, 'Embarked': 'S'}), 
...
(중략)
...
page_content='Johnston, Miss. Catherine Helen ""Carrie""' metadata={'PassengerId': 889, 'Survived': 0, 'Pclass': 3, 'Sex': 'female', 'Age': nan, 'SibSp': 1, 'Parch': 2, 'Ticket': 'W./C. 6607', 'Fare': 23.45, 'Cabin': nan, 'Embarked': 'S'}
page_content='Behr, Mr. Karl Howell' metadata={'PassengerId': 890, 'Survived': 1, 'Pclass': 1, 'Sex': 'male', 'Age': 26.0, 'SibSp': 0, 'Parch': 0, 'Ticket': '111369', 'Fare': 30.0, 'Cabin': 'C148', 'Embarked': 'C'}
page_content='Dooley, Mr. Patrick' metadata={'PassengerId': 891, 'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 32.0, 'SibSp': 0, 'Parch': 0, 'Ticket': '370376', 'Fare': 7.75, 'Cabin': nan, 'Embarked': 'Q'}
```"
CH08 텍스트 분할(Text Splitter),https://wikidocs.net/233776,"# CH08 텍스트 분할(Text Splitter)

문서분할은 Retrieval-Augmented Generation(RAG) 시스템의 두 번째 단계로서, 로드된 문서들을효율적으로 처리하고, 시스템이 정보를 보다 잘 활용할 수 있도록 준비하는 중요한 과정입니다.
이 단계의 목적은 크고 복잡한 문서를 LLM 이 받아들일 수 있는효율적인 작은 규모의 조각으로 나누는 작업입니다. 나중에 사용자가 입력한 질문에 대하여 보다 효율적인 정보만 압축/선별하여 가져오기 위함입니다.
(예시)구글이 앤스로픽에 투자한 금액은 얼마야?

## 분할의 필요성
핀포인트 정보 검색(정확성): 문서를 세분화함으로써질문(Query) 에 연관성이 있는 정보만 가져오는데 도움이 됩니다. 각각의 단위는 특정 주제나 내용에 초점을 맞추므로,관련성이 높은 정보를 제공합니다.
핀포인트 정보 검색(정확성): 문서를 세분화함으로써질문(Query) 에 연관성이 있는 정보만 가져오는데 도움이 됩니다. 각각의 단위는 특정 주제나 내용에 초점을 맞추므로,관련성이 높은 정보를 제공합니다.
리소스 최적화(효율성): 전체 문서를 LLM 으로 입력하게 되면 비용이 많이 발생할 뿐더러, 효율적인 답변을 많은 정보속에 발췌하여 답변하지 못하게 됩니다. 때로는 이러한 문제가할루시네이션으로 이어지게 됩니다. 따라서, 답변에 필요한 정보만 발췌하기 위한 목적도 있습니다.
리소스 최적화(효율성): 전체 문서를 LLM 으로 입력하게 되면 비용이 많이 발생할 뿐더러, 효율적인 답변을 많은 정보속에 발췌하여 답변하지 못하게 됩니다. 때로는 이러한 문제가할루시네이션으로 이어지게 됩니다. 따라서, 답변에 필요한 정보만 발췌하기 위한 목적도 있습니다.
## 문서분할 과정
문서 구조 파악: PDF 파일, 웹 페이지, 전자 책 등 다양한 형식의 문서에서 구조를 파악합니다. 이는 문서의 헤더, 푸터, 페이지 번호, 섹션 제목 등을 식별하는 과정을 포함할 수 있습니다.
단위 선정: 문서를 어떤 단위로 나눌지 결정합니다. 이는 페이지별, 섹션별, 또는 문단별일 수 있으며, 문서의 내용과 목적에 따라 다릅니다.
단위 크기 선정(chunk size): 문서를 몇 개의 토큰 단위로 나눌 것인지를 정합니다.
청크 오버랩(chunk overlap): 분할된 끝 부분에서 맥락이 이어질 수 있도록 일부를 겹쳐서(overlap) 분할하는 것이 일반적입니다.
청크 크기 & 청크 오버랩

## 코드
```
Copyfromlangchain_text_splittersimportRecursiveCharacterTextSplitter# 단계 2: 문서 분할(Split Documents)text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
splits = text_splitter.split_documents(docs)
```
## 참고
- 텍스트 분할기
- LangChain TextSplitters
텍스트 분할기
LangChain TextSplitters"
01. 문자 텍스트 분할(CharacterTextSplitter),https://wikidocs.net/233998,"# 01. 문자 텍스트 분할(CharacterTextSplitter)

이 방법은 가장 간단한 방식입니다.
기본적으로""\n\n""을 기준으로 문자 단위로 텍스트를 분할하고, 청크의 크기를 문자 수로 측정합니다.
텍스트 분할 방식: 단일 문자 기준
청크 크기 측정 방식: 문자 수 기준
```
Copy%pip install -qU langchain-text-splitters
```
- ./data/appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
- 읽어들인 내용을file변수에 저장합니다.
./data/appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
읽어들인 내용을file변수에 저장합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.
```
파일로부터 읽은 파일의 일부 내용을 출력합니다.
```
Copy# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:500])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.
연관키워드: 토큰화, 자연어
```
CharacterTextSplitter를 사용하여 텍스트를 청크(chunk)로 분할하는 코드입니다.
- separator매개변수로 분할할 기준을 설정합니다. 기본 값은""\n\n""입니다.
- chunk_size매개변수를 250 으로 설정하여 각 청크의 최대 크기를 250자로 제한합니다.
- chunk_overlap매개변수를 50으로 설정하여 인접한 청크 간에 50자의 중복을 허용합니다.
- length_function매개변수를 len으로 설정하여 텍스트의 길이를 계산하는 함수를 지정합니다.
- is_separator_regex매개변수를 False로 설정하여 separator를 정규식이 아닌 일반 문자열로 처리합니다.
separator매개변수로 분할할 기준을 설정합니다. 기본 값은""\n\n""입니다.
chunk_size매개변수를 250 으로 설정하여 각 청크의 최대 크기를 250자로 제한합니다.
chunk_overlap매개변수를 50으로 설정하여 인접한 청크 간에 50자의 중복을 허용합니다.
length_function매개변수를 len으로 설정하여 텍스트의 길이를 계산하는 함수를 지정합니다.
is_separator_regex매개변수를 False로 설정하여 separator를 정규식이 아닌 일반 문자열로 처리합니다.
```
Copyfromlangchain_text_splittersimportCharacterTextSplitter

text_splitter = CharacterTextSplitter(# 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 ""\n\n""입니다.# separator="" "",# 분할된 텍스트 청크의 최대 크기를 지정합니다.chunk_size=250,# 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.chunk_overlap=50,# 텍스트의 길이를 계산하는 함수를 지정합니다.length_function=len,# 구분자가 정규식인지 여부를 지정합니다.is_separator_regex=False,
)
```
- text_splitter를 사용하여file텍스트를 문서 단위로 분할합니다.
- 분할된 문서 리스트 중 첫 번째 문서(texts[0])를 출력합니다.
text_splitter를 사용하여file텍스트를 문서 단위로 분할합니다.
분할된 문서 리스트 중 첫 번째 문서(texts[0])를 출력합니다.
```
Copy# text_splitter를 사용하여 state_of_the_union 텍스트를 문서로 분할합니다.texts = text_splitter.create_documents([file])print(texts[0])# 분할된 문서 중 첫 번째 문서를 출력합니다.
```
```
page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'
```
다음은 문서와 함께 메타데이터를 전달하는 예시입니다.
메타데이터가 문서와 함께 분할되는 점에 주목해 주세요.
- create_documents메서드는 텍스트 데이터와 메타데이터 리스트를 인자로 받습니다.
create_documents메서드는 텍스트 데이터와 메타데이터 리스트를 인자로 받습니다.
```
Copymetadatas = [
    {""document"":1},
    {""document"":2},
]# 문서에 대한 메타데이터 리스트를 정의합니다.documents = text_splitter.create_documents(
    [
        file,
        file,
    ],# 분할할 텍스트 데이터를 리스트로 전달합니다.metadatas=metadatas,# 각 문서에 해당하는 메타데이터를 전달합니다.)print(documents[0])# 분할된 문서 중 첫 번째 문서를 출력합니다.
```
```
page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding' metadata={'document': 1}
```
split_text()메서드를 사용하여 텍스트를 분할합니다.
- text_splitter.split_text(file)[0]은file텍스트를text_splitter를 사용하여 분할한 후, 분할된 텍스트 조각 중 첫 번째 요소를 반환합니다.
text_splitter.split_text(file)[0]은file텍스트를text_splitter를 사용하여 분할한 후, 분할된 텍스트 조각 중 첫 번째 요소를 반환합니다.
```
Copy# text_splitter를 사용하여 file 텍스트를 분할하고, 분할된 텍스트의 첫 번째 요소를 반환합니다.text_splitter.split_text(file)[0]
```
```
'Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'
```"
02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter),https://wikidocs.net/233999,"# 02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter)

이 텍스트 분할기는 일반적인 텍스트에 권장되는 방식입니다.
이 분할기는 문자 목록을 매개변수로 받아 동작합니다.
분할기는 청크가 충분히 작아질 때까지 주어진 문자 목록의 순서대로 텍스트를 분할하려고 시도합니다.
기본 문자 목록은[""\n\n"", ""\n"", "" "", """"]입니다.
- 단락->문장->단어순서로 재귀적으로 분할합니다.
단락->문장->단어순서로 재귀적으로 분할합니다.
이는 단락(그 다음으로 문장, 단어) 단위가 의미적으로 가장 강하게 연관된 텍스트 조각으로 간주되므로, 가능한 한 함께 유지하려는 효과가 있습니다.
텍스트가 분할되는 방식: 문자 목록([""\n\n"", ""\n"", "" "", """"]) 에 의해 분할됩니다.
텍스트가 분할되는 방식: 문자 목록([""\n\n"", ""\n"", "" "", """"]) 에 의해 분할됩니다.
청크 크기가 측정되는 방식: 문자 수에 의해 측정됩니다.
청크 크기가 측정되는 방식: 문자 수에 의해 측정됩니다.
```
Copy%pip install -qU langchain-text-splitters
```
- appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
- 읽어들인 내용을file변수에 저장합니다.
appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
읽어들인 내용을file변수에 저장합니다.
```
Copy# appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.
```
파일로부터 읽은 파일의 일부 내용을 출력합니다.
```
Copy# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:500])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.
연관키워드: 토큰화, 자연어
```
```
Copyfromlangchain_text_splittersimportRecursiveCharacterTextSplitter
```
RecursiveCharacterTextSplitter를 사용하여 텍스트를 작은 청크로 분할하는 예제입니다.
- chunk_size를 250 으로 설정하여 각 청크의 크기를 제한합니다.
- chunk_overlap을 50 으로 설정하여 인접한 청크 간에 50 개 문자의 중첩을 허용합니다.
- length_function으로len함수를 사용하여 텍스트의 길이를 계산합니다.
- is_separator_regex를False로 설정하여 구분자로 정규식을 사용하지 않습니다.
chunk_size를 250 으로 설정하여 각 청크의 크기를 제한합니다.
chunk_overlap을 50 으로 설정하여 인접한 청크 간에 50 개 문자의 중첩을 허용합니다.
length_function으로len함수를 사용하여 텍스트의 길이를 계산합니다.
is_separator_regex를False로 설정하여 구분자로 정규식을 사용하지 않습니다.
```
Copytext_splitter = RecursiveCharacterTextSplitter(# 청크 크기를 매우 작게 설정합니다. 예시를 위한 설정입니다.chunk_size=250,# 청크 간의 중복되는 문자 수를 설정합니다.chunk_overlap=50,# 문자열 길이를 계산하는 함수를 지정합니다.length_function=len,# 구분자로 정규식을 사용할지 여부를 설정합니다.is_separator_regex=False,
)
```
- text_splitter를 사용하여file텍스트를 문서 단위로 분할합니다.
- 분할된 문서는texts리스트에 저장됩니다.
- print(texts[0])과print(texts[1])을 통해 분할된 문서의 첫 번째와 두 번째 문서를 출력합니다.
text_splitter를 사용하여file텍스트를 문서 단위로 분할합니다.
분할된 문서는texts리스트에 저장됩니다.
print(texts[0])과print(texts[1])을 통해 분할된 문서의 첫 번째와 두 번째 문서를 출력합니다.
```
Copy# text_splitter를 사용하여 file 텍스트를 문서로 분할합니다.texts = text_splitter.create_documents([file])print(texts[0])# 분할된 문서의 첫 번째 문서를 출력합니다.print(""===""*20)print(texts[1])# 분할된 문서의 두 번째 문서를 출력합니다.
```
```
page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'
============================================================
page_content='Embedding\n\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken'
```
text_splitter.split_text()함수를 사용하여file텍스트를 분할합니다.
```
Copy# 텍스트를 분할하고 분할된 텍스트의 처음 2개 요소를 반환합니다.text_splitter.split_text(file)[:2]
```
```
['Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding', 'Embedding\n\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken']
```"
03. 토큰 텍스트 분할(TokenTextSplitter),https://wikidocs.net/234002,"# 03. 토큰 텍스트 분할(TokenTextSplitter)

언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야 합니다.
TokenTextSplitter는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다.
## tiktoken
tiktoken은 OpenAI에서 만든 빠른BPE Tokenizer입니다.
```
Copy%pip install --upgrade --quiet langchain-text-splitters tiktoken
```
- ./data/appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
- 읽어들인 내용을file변수에 저장합니다.
./data/appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
읽어들인 내용을file변수에 저장합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.
```
파일로부터 읽은 파일의 일부 내용을 출력합니다.
```
Copy# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:500])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.
연관키워드: 토큰화, 자연어
```
CharacterTextSplitter를 사용하여 텍스트를 분할합니다.
- from_tiktoken_encoder메서드를 사용하여 Tiktoken 인코더 기반의 텍스트 분할기를 초기화합니다.
from_tiktoken_encoder메서드를 사용하여 Tiktoken 인코더 기반의 텍스트 분할기를 초기화합니다.
```
Copyfromlangchain_text_splittersimportCharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(# 청크 크기를 300으로 설정합니다.chunk_size=300,# 청크 간 중복되는 부분이 없도록 설정합니다.chunk_overlap=0,
)# file 텍스트를 청크 단위로 분할합니다.texts = text_splitter.split_text(file)
```
분할된 청크의 개수를 출력합니다.
```
Copyprint(len(texts))# 분할된 청크의 개수를 출력합니다.
```
```
51
```
texts 리스트의 첫 번째 요소를 출력합니다.
```
Copy# texts 리스트의 첫 번째 요소를 출력합니다.print(texts[0])
```
```
Semantic Search
```
참고
- CharacterTextSplitter.from_tiktoken_encoder를 사용하는 경우, 텍스트는CharacterTextSplitter에 의해서만 분할되고tiktoken토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가tiktoken토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)
- RecursiveCharacterTextSplitter.from_tiktoken_encoder를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.
CharacterTextSplitter.from_tiktoken_encoder를 사용하는 경우, 텍스트는CharacterTextSplitter에 의해서만 분할되고tiktoken토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가tiktoken토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)
CharacterTextSplitter.from_tiktoken_encoder를 사용하는 경우, 텍스트는CharacterTextSplitter에 의해서만 분할되고tiktoken토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가tiktoken토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)
RecursiveCharacterTextSplitter.from_tiktoken_encoder를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.
RecursiveCharacterTextSplitter.from_tiktoken_encoder를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.
## TokenTextSplitter
- TokenTextSplitter클래스를 사용하여 텍스트를 토큰 단위로 분할합니다.
TokenTextSplitter클래스를 사용하여 텍스트를 토큰 단위로 분할합니다.
```
Copyfromlangchain_text_splittersimportTokenTextSplitter

text_splitter = TokenTextSplitter(
    chunk_size=200,# 청크 크기를 10으로 설정합니다.chunk_overlap=0,# 청크 간 중복을 0으로 설정합니다.)# state_of_the_union 텍스트를 청크로 분할합니다.texts = text_splitter.split_text(file)print(texts[0])# 분할된 텍스트의 첫 번째 청크를 출력합니다.
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""�
```
## spaCy
spaCy는 Python과 Cython 프로그래밍 언어로 작성된 고급 자연어 처리를 위한 오픈 소스 소프트웨어 라이브러리입니다.
NLTK의 또 다른 대안은 spaCy tokenizer를 사용하는 것입니다.
텍스트가 분할되는 방식:spaCy tokenizer에 의해 분할됩니다.
텍스트가 분할되는 방식:spaCy tokenizer에 의해 분할됩니다.
chunk size가 측정되는 방법:문자 수로 측정됩니다.
chunk size가 측정되는 방법:문자 수로 측정됩니다.
spaCy 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다.
```
Copy%pip install --upgrade --quiet spacy
```
```
Note: you may need to restart the kernel to use updated packages.
```
en_core_web_sm모델을 다운로드합니다.
```
Copy!python -m spacy download en_core_web_sm --quiet
```
```
[38;5;2m✔ Download and installation successful[0m
You can now load the package via spacy.load('en_core_web_sm')
```
- appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
appendix-keywords.txt파일을 열어 내용을 읽어들입니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.
```
일부 내용을 출력하여 확인합니다.
```
Copy# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:350])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처
```
- SpacyTextSplitter클래스를 사용하여 텍스트 분할기를 생성합니다.
SpacyTextSplitter클래스를 사용하여 텍스트 분할기를 생성합니다.
```
Copyimportwarningsfromlangchain_text_splittersimportSpacyTextSplitter# 경고 메시지를 무시합니다.warnings.filterwarnings(""ignore"")# SpacyTextSplitter를 생성합니다.text_splitter = SpacyTextSplitter(
    chunk_size=200,# 청크 크기를 200으로 설정합니다.chunk_overlap=50,# 청크 간 중복을 50으로 설정합니다.)
```
- text_splitter객체의split_text메서드를 사용하여file텍스트를 분할합니다.
text_splitter객체의split_text메서드를 사용하여file텍스트를 분할합니다.
```
Copy# text_splitter를 사용하여 file 텍스트를 분할합니다.texts = text_splitter.split_text(file)print(texts[0])# 분할된 텍스트의 첫 번째 요소를 출력합니다.
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.


예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
```
## SentenceTransformers
SentenceTransformersTokenTextSplitter는sentence-transformer모델에 특화된 텍스트 분할기입니다.
기본 동작은 사용하고자 하는 sentence transformer 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할하는 것입니다.
```
Copyfromlangchain_text_splittersimportSentenceTransformersTokenTextSplitter# 문장 분할기를 생성하고 청크 간 중복을 0으로 설정합니다.splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)
```
샘플 텍스트를 확인합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:350])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처
```
다음은file변수에 담긴 텍스트의 토큰의 개수를 세는 코드입니다. 시작과 종료 토큰의 개수를 제외한 후 출력합니다.
```
Copycount_start_and_stop_tokens =2# 시작과 종료 토큰의 개수를 2로 설정합니다.# 텍스트의 토큰 개수에서 시작과 종료 토큰의 개수를 뺍니다.text_token_count = splitter.count_tokens(
    text=file) - count_start_and_stop_tokensprint(text_token_count)# 계산된 텍스트 토큰 개수를 출력합니다.
```
```
7686
```
splitter.split_text()함수를 사용하여text_to_split변수에 저장된 텍스트를 청크(chunk) 단위로 분할합니다.
```
Copytext_chunks = splitter.split_text(text=file)# 텍스트를 청크로 분할합니다.
```
첫 번째 청크를 출력하여 내용을 확인합니다.
```
Copy# 0번째 청크를 출력합니다.print(text_chunks[1])# 분할된 텍스트 청크 중 두 번째 청크를 출력합니다.
```
```
. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 [UNK] 합니다. [UNK] : "" 사과 "" 라는 단어를 [ 0. 65, - 0. 23, 0. 17 ] 과 [UNK] 벡터로 표현합니다. 연관키워드 : 자연어 처리, 벡터화, 딥러닝 token 정의 : 토큰은 텍스트를 더 작은 [UNK] 분할하는 [UNK] 의미합니다. 이는 일반적으로 단어, 문장, [UNK] 구절일 수 [UNK]. [UNK] : 문장 "" 나는 학교에 간다 "" 를 "" 나는 "", "" 학교에 "", "" 간다 "" 로 분할합니다. 연관키워드 : 토큰화, 자연어 처리, 구문 분석 tokenizer 정의 : 토크
```
## NLTK
Natural Language Toolkit (NLTK)은 Python 프로그래밍 언어로 작성된 영어 자연어 처리(NLP)를 위한 라이브러리와 프로그램 모음입니다.
단순히 ""\n\n""으로 분할하는 대신, NLTK tokenizers를 기반으로 텍스트를 분할하는 데 NLTK를 사용할 수 있습니다.
텍스트 분할 방법: NLTK tokenizer에 의해 분할됩니다.
chunk 크기 측정 방법: 문자 수에 의해 측정됩니다.
chunk 크기 측정 방법: 문자 수에 의해 측정됩니다.
nltk라이브러리를 설치하는 pip 명령어입니다.
nltk라이브러리를 설치하는 pip 명령어입니다.
NLTK(Natural Language Toolkit)는 자연어 처리를 위한 파이썬 라이브러리입니다.
텍스트 데이터의 전처리, 토큰화, 형태소 분석, 품사 태깅 등 다양한 NLP 작업을 수행할 수 있습니다.
```
Copy%pip install -qU nltk
```
샘플 텍스트를 확인합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:350])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처
```
- NLTKTextSplitter클래스를 사용하여 텍스트 분할기를 생성합니다.
- chunk_size매개변수를 1000으로 설정하여 텍스트를 최대 1000자 단위로 분할하도록 지정합니다.
NLTKTextSplitter클래스를 사용하여 텍스트 분할기를 생성합니다.
chunk_size매개변수를 1000으로 설정하여 텍스트를 최대 1000자 단위로 분할하도록 지정합니다.
```
Copyfromlangchain_text_splittersimportNLTKTextSplitter

text_splitter = NLTKTextSplitter(
    chunk_size=200,# 청크 크기를 200으로 설정합니다.chunk_overlap=0,# 청크 간 중복을 0으로 설정합니다.)
```
text_splitter객체의split_text메서드를 사용하여file텍스트를 분할합니다.
```
Copy# text_splitter를 사용하여 file 텍스트를 분할합니다.texts = text_splitter.split_text(file)print(texts[0])# 분할된 텍스트의 첫 번째 요소를 출력합니다.
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.

예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
```
## KoNLPy
KoNLPy(Korean NLP in Python)는 한국어 자연어 처리(NLP)를 위한 파이썬 패키지입니다.
토큰 분할은 텍스트를 토큰이라고 하는 더 작고 관리하기 쉬운 단위로 분할하는 과정을 포함합니다.
이러한 토큰은 종종 단어, 구, 기호 또는 추가 처리 및 분석에 중요한 다른 의미 있는 요소입니다.
영어와 같은 언어에서 토큰 분할은 일반적으로 공백과 구두점으로 단어를 분리하는 것을 포함합니다.
토큰 분할의 효과는 언어 구조에 대한 토크나이저의 이해에 크게 의존하며, 이는 의미 있는 토큰 생성을 보장합니다.
영어를 위해 설계된 토크나이저는 한국어와 같은 다른 언어의 고유한 의미 구조를 이해할 수 있는 능력이 없기 때문에 한국어 처리에 효과적으로 사용될 수 없습니다.
한국어 텍스트의 경우 KoNLPY에는Kkma(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있습니다.
Kkma는 한국어 텍스트에 대한 상세한 형태소 분석을 제공합니다.
문장을 단어로, 단어를 각각의 형태소로 분해하고 각 토큰에 대한 품사를 식별합니다.
텍스트 블록을 개별 문장으로 분할할 수 있어 긴 텍스트 처리에 특히 유용합니다.
Kkma는 상세한 분석으로 유명하지만, 이러한 정밀성이 처리 속도에 영향을 미칠 수 있다는 점에 유의해야 합니다. 따라서Kkma는 신속한 텍스트 처리보다 분석적 깊이가 우선시되는 애플리케이션에 가장 적합합니다.
- KoNLPy 라이브러리를 설치하는 pip 명령어입니다.
- KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다.
KoNLPy 라이브러리를 설치하는 pip 명령어입니다.
KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다.
```
Copy%pip install -qU konlpy
```
샘플 텍스트를 확인합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:350])
```
KonlpyTextSplitter를 사용하여 한국어 텍스트를 분할하는 예제입니다.
```
Copyfromlangchain_text_splittersimportKonlpyTextSplitter# KonlpyTextSplitter를 사용하여 텍스트 분할기 객체를 생성합니다.text_splitter = KonlpyTextSplitter()
```
text_splitter를 사용하여file를 문장 단위로 분할합니다.
```
Copytexts = text_splitter.split_text(file)# 한국어 문서를 문장 단위로 분할합니다.print(texts[0])# 분할된 문장 중 첫 번째 문장을 출력합니다.
```
## Hugging Face tokenizer
Hugging Face는 다양한 토크나이저를 제공합니다.
이 코드에서는 Hugging Face의 토크나이저 중 하나인 GPT2TokenizerFast를 사용하여 텍스트의 토큰 길이를 계산합니다.
텍스트 분할 방식은 다음과 같습니다:
- 전달된 문자 단위로 분할됩니다.
전달된 문자 단위로 분할됩니다.
청크 크기 측정 방식은 다음과 같습니다:
- Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.
- GPT2TokenizerFast클래스를 사용하여tokenizer객체를 생성합니다.
- from_pretrained메서드를 호출하여 사전 학습된 ""gpt2"" 토크나이저 모델을 로드합니다.
Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.
Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.
GPT2TokenizerFast클래스를 사용하여tokenizer객체를 생성합니다.
GPT2TokenizerFast클래스를 사용하여tokenizer객체를 생성합니다.
from_pretrained메서드를 호출하여 사전 학습된 ""gpt2"" 토크나이저 모델을 로드합니다.
```
CopyfromtransformersimportGPT2TokenizerFast# GPT-2 모델의 토크나이저를 불러옵니다.hf_tokenizer = GPT2TokenizerFast.from_pretrained(""gpt2"")
```
샘플 텍스트를 확인합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:350])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처
```
from_huggingface_tokenizer메서드를 통해 허깅페이스 토크나이저(tokenizer)를 사용하여 텍스트 분할기를 초기화합니다.
```
Copytext_splitter = CharacterTextSplitter.from_huggingface_tokenizer(# 허깅페이스 토크나이저를 사용하여 CharacterTextSplitter 객체를 생성합니다.hf_tokenizer,
    chunk_size=300,
    chunk_overlap=50,
)# state_of_the_union 텍스트를 분할하여 texts 변수에 저장합니다.texts = text_splitter.split_text(file)
```
1 번째 요소의 분할 결과를 확인합니다.
```
Copyprint(texts[1])# texts 리스트의 1 번째 요소를 출력합니다.
```
```
정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝
```"
04. 시멘틱 청커(SemanticChunker),https://wikidocs.net/234003,"# 04. 시멘틱 청커(SemanticChunker)

텍스트를 의미론적 유사성에 기반하여 분할합니다.
Reference
- Greg Kamradt의 노트북
Greg Kamradt의 노트북
이 방법은 텍스트를 문장 단위로 분할한 후, 3개의 문장씩 그룹화하고, 임베딩 공간에서 유사한 문장들을 병합하는 과정을 거칩니다.
의존성 패키지 설치
```
Copy%pip install -qU langchain_experimental langchain_openai
```
샘플 텍스트를 로드하고 내용을 출력합니다.
```
Copy# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# 파일으로부터 읽은 내용을 일부 출력합니다.print(file[:350])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처
```
## SemanticChunker 생성
SemanticChunker는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다.
이를 통해 텍스트 데이터를 보다 효과적으로 처리하고 분석할 수 있습니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
SemanticChunker를 사용하여 텍스트를 의미적으로 관련된 청크로 분할합니다.
```
Copyfromlangchain_experimental.text_splitterimportSemanticChunkerfromlangchain_openai.embeddingsimportOpenAIEmbeddings# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.text_splitter = SemanticChunker(OpenAIEmbeddings())
```
## 텍스트 분할
- text_splitter를 사용하여file텍스트를 문서 단위로 분할합니다.
text_splitter를 사용하여file텍스트를 문서 단위로 분할합니다.
```
Copychunks = text_splitter.split_text(file)
```
분할된 청크를 확인합니다.
```
Copy# 분할된 청크 중 첫 번째 청크를 출력합니다.print(chunks[0])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

Tokenizer

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV

정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환

JSON

정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {""이름"": ""홍길동"", ""나이"": 30, ""직업"": ""개발자""}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API

Transformer

정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention

HuggingFace

정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리

Digital Transformation

정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델

Crawling

정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
```
create_documents()함수를 사용하여 청크를 문서로 변환할 수 있습니다.
```
Copy# text_splitter를 사용하여 분할합니다.docs = text_splitter.create_documents([file])print(docs[0].page_content)# 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

Tokenizer

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV

정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환

JSON

정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {""이름"": ""홍길동"", ""나이"": 30, ""직업"": ""개발자""}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API

Transformer

정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention

HuggingFace

정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리

Digital Transformation

정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델

Crawling

정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
```
## Breakpoints
이 chunker는 문장을 ""분리""할 시점을 결정하여 작동합니다. 이는 두 문장 간의 임베딩 차이를 살펴봄으로써 이루어집니다.
그 차이가 특정 임계값을 넘으면 문장이 분리됩니다.
- 참고 영상: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580
참고 영상: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580
기본적인 분리 방식은 백분위수(Percentile) 를 기반으로 합니다.
이 방법에서는 문장 간의 모든 차이를 계산한 다음, 지정한 백분위수를 기준으로 분리합니다.
```
Copytext_splitter = SemanticChunker(# OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.OpenAIEmbeddings(),# 분할 기준점 유형을 백분위수로 설정합니다.breakpoint_threshold_type=""percentile"",
    breakpoint_threshold_amount=70,
)
```
분할된 결과를 확인합니다.
```
Copydocs = text_splitter.create_documents([file])fori, docinenumerate(docs[:5]):print(f""[Chunk{i}]"", end=""\n\n"")print(doc.page_content)# 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.print(""===""*20)
```
```
[Chunk 0]

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
============================================================
[Chunk 1]

예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
============================================================
[Chunk 2]

예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

Tokenizer

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
============================================================
[Chunk 3]

예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
============================================================
[Chunk 4]

예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
============================================================
```
docs의 길이를 출력합니다.
```
Copyprint(len(docs))# docs의 길이를 출력합니다.
```
```
60
```
이 방법에서는 지정한breakpoint_threshold_amount표준편차보다 큰 차이가 있는 경우 분할됩니다.
- breakpoint_threshold_type매개변수를 ""standard_deviation""으로 설정하여 청크 분할 기준을 표준편차 기반으로 지정합니다.
breakpoint_threshold_type매개변수를 ""standard_deviation""으로 설정하여 청크 분할 기준을 표준편차 기반으로 지정합니다.
```
Copytext_splitter = SemanticChunker(# OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.OpenAIEmbeddings(),# 분할 기준으로 표준 편차를 사용합니다.breakpoint_threshold_type=""standard_deviation"",
    breakpoint_threshold_amount=1.25,
)
```
분할된 결과를 확인합니다.
```
Copy# text_splitter를 사용하여 분할합니다.docs = text_splitter.create_documents([file])
```
```
Copydocs = text_splitter.create_documents([file])fori, docinenumerate(docs[:5]):print(f""[Chunk{i}]"", end=""\n\n"")print(doc.page_content)# 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.print(""===""*20)
```
```
[Chunk 0]

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

Tokenizer

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
============================================================
[Chunk 1]

예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
============================================================
[Chunk 2]

예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV

정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환

JSON

정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {""이름"": ""홍길동"", ""나이"": 30, ""직업"": ""개발자""}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API

Transformer

정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.
============================================================
[Chunk 3]

예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention

HuggingFace

정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.
============================================================
[Chunk 4]

예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리

Digital Transformation

정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.
============================================================
```
docs의 길이를 출력합니다.
```
Copyprint(len(docs))# docs의 길이를 출력합니다.
```
```
14
```
이 방법에서는 사분위수 범위(interquartile range)를 사용하여 청크를 분할합니다.
- breakpoint_threshold_type매개변수를 ""interquartile""로 설정하여 청크 분할 기준을 사분위수 범위로 지정합니다.
breakpoint_threshold_type매개변수를 ""interquartile""로 설정하여 청크 분할 기준을 사분위수 범위로 지정합니다.
```
Copytext_splitter = SemanticChunker(# OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화합니다.OpenAIEmbeddings(),# 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.breakpoint_threshold_type=""interquartile"",
    breakpoint_threshold_amount=0.5,
)
```
```
Copy# text_splitter를 사용하여 분할합니다.docs = text_splitter.create_documents([file])# 결과를 출력합니다.fori, docinenumerate(docs[:5]):print(f""[Chunk{i}]"", end=""\n\n"")print(doc.page_content)# 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.print(""===""*20)
```
```
[Chunk 0]

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
============================================================
[Chunk 1]

예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

Tokenizer

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
============================================================
[Chunk 2]

예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
============================================================
[Chunk 3]

예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
============================================================
[Chunk 4]

예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV

정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.
============================================================
```
docs의 길이를 출력합니다.
```
Copyprint(len(docs))# docs의 길이를 출력합니다.
```
```
23
```"
"05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등)",https://wikidocs.net/234004,"# 05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등)

CodeTextSplitter를 사용하면 다양한 프로그래밍 언어로 작성된 코드를 분할할 수 있습니다.
이를 위해서는Languageenum을 import하고, 해당하는 프로그래밍 언어를 지정해주면 됩니다.
```
Copy%pip install -qU langchain-text-splitters
```
RecursiveCharacterTextSplitter를 사용하여 텍스트를 분할하는 예제입니다.
- langchain_text_splitters모듈에서Language와RecursiveCharacterTextSplitter클래스를 임포트합니다.
- RecursiveCharacterTextSplitter는 텍스트를 문자 단위로 재귀적으로 분할하는 텍스트 분할기입니다.
langchain_text_splitters모듈에서Language와RecursiveCharacterTextSplitter클래스를 임포트합니다.
RecursiveCharacterTextSplitter는 텍스트를 문자 단위로 재귀적으로 분할하는 텍스트 분할기입니다.
```
Copyfromlangchain_text_splittersimport(
    Language,
    RecursiveCharacterTextSplitter,
)
```
지원되는 언어의 전체 목록을 가져옵니다.
```
Copy# 지원되는 언어의 전체 목록을 가져옵니다.[e.valueforeinLanguage]
```
```
['cpp', 'go', 'java', 'kotlin', 'js', 'ts', 'php', 'proto', 'python', 'rst', 'ruby', 'rust', 'scala', 'swift', 'markdown', 'latex', 'html', 'sol', 'csharp', 'cobol', 'c', 'lua', 'perl']
```
RecursiveCharacterTextSplitter클래스의get_separators_for_language메서드를 사용하여 특정 언어에 사용되는 구분자(separators)를 확인할 수 있습니다.
- 예시에서는Language.PYTHON열거형 값을 인자로 전달하여 Python 언어에 사용되는 구분자를 확인합니다.
예시에서는Language.PYTHON열거형 값을 인자로 전달하여 Python 언어에 사용되는 구분자를 확인합니다.
```
Copy# 주어진 언어에 대해 사용되는 구분자를 확인할 수 있습니다.RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)
```
```
['\nclass ', '\ndef ', '\n\tdef ', '\n\n', '\n', ' ', '']
```
## Python
RecursiveCharacterTextSplitter사용한 예제는 다음과 같습니다.
- RecursiveCharacterTextSplitter를 사용하여 Python 코드를 문서 단위로 분할합니다.
- language매개변수에Language.PYTHON을 지정하여 Python 언어를 사용합니다.
- chunk_size를 50으로 설정하여 각 문서의 최대 크기를 제한합니다.
- chunk_overlap을 0으로 설정하여 문서 간의 중복을 허용하지 않습니다.
RecursiveCharacterTextSplitter를 사용하여 Python 코드를 문서 단위로 분할합니다.
language매개변수에Language.PYTHON을 지정하여 Python 언어를 사용합니다.
chunk_size를 50으로 설정하여 각 문서의 최대 크기를 제한합니다.
chunk_overlap을 0으로 설정하여 문서 간의 중복을 허용하지 않습니다.
```
CopyPYTHON_CODE =""""""
def hello_world():
    print(""Hello, World!"")

hello_world()
""""""python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON, chunk_size=50, chunk_overlap=0)
```
Document를 생성합니다. 생성된Document는 리스트 형태로 반환됩니다.
```
Copypython_docs = python_splitter.create_documents([PYTHON_CODE])
python_docs
```
```
[Document(page_content='def hello_world():\n    print(""Hello, World!"")'), Document(page_content='hello_world()')]
```
```
Copyfordocinpython_docs:print(doc.page_content, end=""\n==================\n"")
```
```
def hello_world():
    print(""Hello, World!"")
==================
hello_world()
==================
```
## JS
다음은 JS 텍스트 분할기를 사용한 예시입니다
```
CopyJS_CODE =""""""
function helloWorld() {
  console.log(""Hello, World!"");
}

helloWorld();
""""""js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS, chunk_size=60, chunk_overlap=0)

js_docs = js_splitter.create_documents([JS_CODE])
js_docs
```
```
[Document(page_content='function helloWorld() {\n  console.log(""Hello, World!"");\n}'), Document(page_content='helloWorld();')]
```
## TS
다음은 TS 텍스트 분할기를 사용한 예시입니다.
```
CopyTS_CODE =""""""
function helloWorld(): void {
  console.log(""Hello, World!"");
}

helloWorld();
""""""ts_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.TS, chunk_size=60, chunk_overlap=0)
ts_docs = ts_splitter.create_documents([TS_CODE])
ts_docs
```
```
[Document(page_content='function helloWorld(): void {'), Document(page_content='console.log(""Hello, World!"");\n}'), Document(page_content='helloWorld();')]
```
## Markdown
다음은 Markdown 텍스트 분할기를 사용한 예시입니다.
```
Copymarkdown_text =""""""
# 🦜️🔗 LangChain

⚡ LLM을 활용한 초스피드 애플리케이션 구축 ⚡

## 빠른 설치

```bash
pip install langchain
```
""""""
```
Copy분할하고 결과를 출력합니다.


```python
md_splitter = RecursiveCharacterTextSplitter.from_language(# 마크다운 언어를 사용하여 텍스트 분할기 생성language=Language.MARKDOWN,# 청크 크기를 60으로 설정chunk_size=60,# 청크 간 중복되는 부분이 없도록 설정chunk_overlap=0,
)# 마크다운 텍스트를 분할하여 문서 생성md_docs = md_splitter.create_documents([markdown_text])# 생성된 문서 출력md_docs
```
```
[Document(page_content='# 🦜️🔗 LangChain\n\n⚡ LLM을 활용한 초스피드 애플리케이션 구축 ⚡'), Document(page_content='## 빠른 설치\n\n```bash\npip install langchain\n```'), Document(page_content='# 빠르게 발전하는 분야의 오픈 소스 프로젝트 입니다. 많관부 🙏')]
```
## Latex
LaTeX는 문서 작성을 위한 마크업 언어로, 수학 기호와 수식을 표현하는 데 널리 사용됩니다.
다음은 LaTeX 텍스트의 예시입니다.
```
Copylatex_text =""""""
\documentclass{article}

\begin{document}

\maketitle

\section{Introduction}
% LLM은 방대한 양의 텍스트 데이터로 학습하여 사람과 유사한 언어를 생성할 수 있는 기계 학습 모델의 한 유형입니다.
% 최근 몇 년 동안 LLM은 언어 번역, 텍스트 생성, 감성 분석 등 다양한 자연어 처리 작업에서 상당한 발전을 이루었습니다.

\subsection{History of LLMs}
% 초기 LLM은 1980년대와 1990년대에 개발되었지만, 처리할 수 있는 데이터 양과 당시 사용 가능한 컴퓨팅 능력으로 인해 제한되었습니다.
% 그러나 지난 10년 동안 하드웨어와 소프트웨어의 발전으로 대규모 데이터 세트에 대해 LLM을 학습시킬 수 있게 되었고, 이는 성능의 큰 향상으로 이어졌습니다.

\subsection{Applications of LLMs}
% LLM은 챗봇, 콘텐츠 생성, 가상 어시스턴트 등 산업 분야에서 많은 응용 분야를 가지고 있습니다.
% 또한 언어학, 심리학, 컴퓨터 언어학 연구를 위해 학계에서도 사용될 수 있습니다.

\end{document}
""""""
```
분할하고 결과를 출력합니다.
```
Copylatex_splitter = RecursiveCharacterTextSplitter.from_language(# 마크다운 언어를 사용하여 텍스트를 분할합니다.language=Language.LATEX,# 각 청크의 크기를 60자로 설정합니다.chunk_size=60,# 청크 간의 중복되는 문자 수를 0으로 설정합니다.chunk_overlap=0,
)# latex_text를 분할하여 문서 목록을 생성합니다.latex_docs = latex_splitter.create_documents([latex_text])# 생성된 문서 목록을 출력합니다.latex_docs
```
```
[Document(page_content='\\documentclass{article}\n\n\x08egin{document}\n\n\\maketitle'), Document(page_content='\\section{Introduction}\n% LLM은 방대한 양의 텍스트 데이터로 학습하여 사람과 유사한'), Document(page_content='언어를 생성할 수 있는 기계 학습 모델의 한 유형입니다.\n% 최근 몇 년 동안 LLM은 언어 번역, 텍스트'), Document(page_content='생성, 감성 분석 등 다양한 자연어 처리 작업에서 상당한 발전을 이루었습니다.'), Document(page_content='\\subsection{History of LLMs}\n% 초기 LLM은 1980년대와 1990년대에'), Document(page_content='개발되었지만, 처리할 수 있는 데이터 양과 당시 사용 가능한 컴퓨팅 능력으로 인해 제한되었습니다.\n%'), Document(page_content='그러나 지난 10년 동안 하드웨어와 소프트웨어의 발전으로 대규모 데이터 세트에 대해 LLM을 학습시킬 수'), Document(page_content='있게 되었고, 이는 성능의 큰 향상으로 이어졌습니다.'), Document(page_content='\\subsection{Applications of LLMs}\n% LLM은 챗봇, 콘텐츠 생성, 가상'), Document(page_content='어시스턴트 등 산업 분야에서 많은 응용 분야를 가지고 있습니다.\n% 또한 언어학, 심리학, 컴퓨터 언어학'), Document(page_content='연구를 위해 학계에서도 사용될 수 있습니다.\n\n\\end{document}')]
```
## HTML
HTML 텍스트 분할기를 사용한 예제는 다음과 같습니다.
```
Copyhtml_text =""""""
<!DOCTYPE html>
<html>
    <head>
        <title>🦜️🔗 LangChain</title>
        <style>
            body {
                font-family: Arial, sans-serif;  
            }
            h1 {
                color: darkblue;
            }
        </style>
    </head>
    <body>
        <div>
            <h1>🦜️🔗 LangChain</h1>
            <p>⚡ Building applications with LLMs through composability ⚡</p>  
        </div>
        <div>
            As an open-source project in a rapidly developing field, we are extremely open to contributions.
        </div>
    </body>
</html>
""""""
```
분할하고 결과를 출력합니다.
```
Copyhtml_splitter = RecursiveCharacterTextSplitter.from_language(# HTML 언어를 사용하여 텍스트 분할기 생성language=Language.HTML,# 청크 크기를 60으로 설정chunk_size=60,# 청크 간 중복되는 부분이 없도록 설정chunk_overlap=0,
)# 주어진 HTML 텍스트를 분할하여 문서 생성html_docs = html_splitter.create_documents([html_text])# 생성된 문서 출력html_docs
```
```
[Document(page_content='\n'), Document(page_content='\n🦜️🔗 LangChain'), Document(page_content='\nSoliditySolidity 텍스트 분할기를 사용한 예제는 다음과 같습니다:Solidity 코드를 문자열 형태로CopySOL_CODE변수에 저장합니다.CopyRecursiveCharacterTextSplitter를 사용하여 Solidity 코드를 청크 단위로 분할하는Copysol_splitter를 생성합니다.Copylanguage매개변수를CopyLanguage.SOL로 설정하여 Solidity 언어를 지정합니다.Copychunk_size를 128로 설정하여 각 청크의 최대 크기를 지정합니다.Copychunk_overlap을 0으로 설정하여 청크 간의 중복을 없앱니다.Copysol_splitter.create_documents()메서드를 사용하여CopySOL_CODE를 청크 단위로 분할하고, 분할된 청크를Copysol_docs변수에 저장합니다.Copysol_docs를 출력하여 분할된 Solidity 코드 청크를 확인합니다.CopySOL_CODE =""""""
pragma solidity ^0.8.20; 
contract HelloWorld {  
   function add(uint a, uint b) pure public returns(uint) {
       return a + b;
   }
}
""""""# 분할하고 결과를 출력합니다.sol_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.SOL, chunk_size=128, chunk_overlap=0)

sol_docs = sol_splitter.create_documents([SOL_CODE])
sol_docs[Document(page_content='pragma solidity ^0.8.20;'), Document(page_content='contract HelloWorld {  \n   function add(uint a, uint b) pure public returns(uint) {\n       return a + b;\n   }\n}')]CC# 텍스트 분할기를 사용한 예제는 다음과 같습니다.CopyC_CODE =""""""
using System;
class Program
{
    static void Main()
    {
        Console.WriteLine(""Enter a number (1-5):"");
        int input = Convert.ToInt32(Console.ReadLine());
        for (int i = 1; i <= input; i++)
        {
            if (i % 2 == 0)
            {
                Console.WriteLine($""{i} is even."");
            }
            else
            {
                Console.WriteLine($""{i} is odd."");
            }
        }
        Console.WriteLine(""Goodbye!"");
    }
}
""""""# 분할하고 결과를 출력합니다.c_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.CSHARP, chunk_size=128, chunk_overlap=0)
c_docs = c_splitter.create_documents([C_CODE])
c_docs[Document(page_content='using System;'), Document(page_content='class Program\n{\n    static void Main()\n    {\n        Console.WriteLine(""Enter a number (1-5):"");'), Document(page_content='int input = Convert.ToInt32(Console.ReadLine());\n        for (int i = 1; i <= input; i++)\n        {'), Document(page_content='if (i % 2 == 0)\n            {\n                Console.WriteLine($""{i} is even."");\n            }\n            else'), Document(page_content='{\n                Console.WriteLine($""{i} is odd."");\n            }\n        }\n        Console.WriteLine(""Goodbye!"");'), Document(page_content='}\n}')]
```
## Solidity
Solidity 텍스트 분할기를 사용한 예제는 다음과 같습니다:
- Solidity 코드를 문자열 형태로CopySOL_CODE변수에 저장합니다.
- CopyRecursiveCharacterTextSplitter를 사용하여 Solidity 코드를 청크 단위로 분할하는Copysol_splitter를 생성합니다.
- Copylanguage매개변수를CopyLanguage.SOL로 설정하여 Solidity 언어를 지정합니다.
- Copychunk_size를 128로 설정하여 각 청크의 최대 크기를 지정합니다.
- Copychunk_overlap을 0으로 설정하여 청크 간의 중복을 없앱니다.
- Copysol_splitter.create_documents()메서드를 사용하여CopySOL_CODE를 청크 단위로 분할하고, 분할된 청크를Copysol_docs변수에 저장합니다.
- Copysol_docs를 출력하여 분할된 Solidity 코드 청크를 확인합니다.
Solidity 코드를 문자열 형태로CopySOL_CODE변수에 저장합니다.
CopyRecursiveCharacterTextSplitter를 사용하여 Solidity 코드를 청크 단위로 분할하는Copysol_splitter를 생성합니다.
Copylanguage매개변수를CopyLanguage.SOL로 설정하여 Solidity 언어를 지정합니다.
Copychunk_size를 128로 설정하여 각 청크의 최대 크기를 지정합니다.
Copychunk_overlap을 0으로 설정하여 청크 간의 중복을 없앱니다.
Copysol_splitter.create_documents()메서드를 사용하여CopySOL_CODE를 청크 단위로 분할하고, 분할된 청크를Copysol_docs변수에 저장합니다.
Copysol_docs를 출력하여 분할된 Solidity 코드 청크를 확인합니다.
```
CopySOL_CODE =""""""
pragma solidity ^0.8.20; 
contract HelloWorld {  
   function add(uint a, uint b) pure public returns(uint) {
       return a + b;
   }
}
""""""# 분할하고 결과를 출력합니다.sol_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.SOL, chunk_size=128, chunk_overlap=0)

sol_docs = sol_splitter.create_documents([SOL_CODE])
sol_docs
```
```
[Document(page_content='pragma solidity ^0.8.20;'), Document(page_content='contract HelloWorld {  \n   function add(uint a, uint b) pure public returns(uint) {\n       return a + b;\n   }\n}')]
```
## C
C# 텍스트 분할기를 사용한 예제는 다음과 같습니다.
```
CopyC_CODE =""""""
using System;
class Program
{
    static void Main()
    {
        Console.WriteLine(""Enter a number (1-5):"");
        int input = Convert.ToInt32(Console.ReadLine());
        for (int i = 1; i <= input; i++)
        {
            if (i % 2 == 0)
            {
                Console.WriteLine($""{i} is even."");
            }
            else
            {
                Console.WriteLine($""{i} is odd."");
            }
        }
        Console.WriteLine(""Goodbye!"");
    }
}
""""""# 분할하고 결과를 출력합니다.c_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.CSHARP, chunk_size=128, chunk_overlap=0)
c_docs = c_splitter.create_documents([C_CODE])
c_docs
```
```
[Document(page_content='using System;'), Document(page_content='class Program\n{\n    static void Main()\n    {\n        Console.WriteLine(""Enter a number (1-5):"");'), Document(page_content='int input = Convert.ToInt32(Console.ReadLine());\n        for (int i = 1; i <= input; i++)\n        {'), Document(page_content='if (i % 2 == 0)\n            {\n                Console.WriteLine($""{i} is even."");\n            }\n            else'), Document(page_content='{\n                Console.WriteLine($""{i} is odd."");\n            }\n        }\n        Console.WriteLine(""Goodbye!"");'), Document(page_content='}\n}')]
```"
06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter),https://wikidocs.net/234005,"# 06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter)

마크다운 파일의 구조를 이해하고 효율적으로 다루는 것은 문서 작업에 있어 매우 중요할 수 있습니다. 특히, 문서의 전체적인 맥락과 구조를 고려하여 의미 있는 방식으로 텍스트를 임베딩하는 과정은, 광범위한 의미와 주제를 더 잘 포착할 수 있는 포괄적인 벡터 표현을 생성하는 데 큰 도움이 됩니다.
이러한 맥락에서, 마크다운 파일의 특정 부분, 즉 헤더별로 내용을 나누고 싶을 때가 있습니다. 예를 들어, 문서 내에서 각각의 헤더 아래에 있는 내용을 기반으로 서로 연관된 정보 덩어리, 즉 '청크'를 만들고 싶은 경우가 그러합니다. 이는 텍스트의 공통된 맥락을 유지하면서도, 문서의 구조적 요소를 효과적으로 활용하려는 시도입니다.
이런 과제를 해결하기 위해,MarkdownHeaderTextSplitter라는 도구를 활용할 수 있습니다. 이 도구는 문서를 지정된 헤더 집합에 따라 분할하여, 각 헤더 그룹 아래의 내용을 별도의 청크로 관리할 수 있게 합니다. 이 방법을 통해, 문서의 전반적인 구조를 유지하면서도 내용을 더 세밀하게 다룰 수 있게 되며, 이는 다양한 처리 과정에서 유용하게 활용될 수 있습니다.
```
Copy%pip install -qU langchain-text-splitters
```
MarkdownHeaderTextSplitter를 사용하여 마크다운 형식의 텍스트를 헤더 단위로 분할합니다.
- 마크다운 문서의 헤더(#,##,###등)를 기준으로 텍스트를 분할하는 역할을 합니다.
- markdown_document변수에 마크다운 형식의 문서가 할당됩니다.
- headers_to_split_on리스트에는 마크다운 헤더 레벨과 해당 레벨의 이름이 튜플 형태로 정의됩니다.
- MarkdownHeaderTextSplitter클래스를 사용하여markdown_splitter객체를 생성하며,headers_to_split_on매개변수로 분할 기준이 되는 헤더 레벨을 전달합니다.
- split_text메서드를 호출하여markdown_document를 헤더 레벨에 따라 분할합니다.
마크다운 문서의 헤더(#,##,###등)를 기준으로 텍스트를 분할하는 역할을 합니다.
마크다운 문서의 헤더(#,##,###등)를 기준으로 텍스트를 분할하는 역할을 합니다.
markdown_document변수에 마크다운 형식의 문서가 할당됩니다.
markdown_document변수에 마크다운 형식의 문서가 할당됩니다.
headers_to_split_on리스트에는 마크다운 헤더 레벨과 해당 레벨의 이름이 튜플 형태로 정의됩니다.
MarkdownHeaderTextSplitter클래스를 사용하여markdown_splitter객체를 생성하며,headers_to_split_on매개변수로 분할 기준이 되는 헤더 레벨을 전달합니다.
split_text메서드를 호출하여markdown_document를 헤더 레벨에 따라 분할합니다.
```
Copyfromlangchain_text_splittersimportMarkdownHeaderTextSplitter# 마크다운 형식의 문서를 문자열로 정의합니다.markdown_document =""# Title\n\n## 1. SubTitle\n\nHi this is Jim\n\nHi this is Joe\n\n### 1-1. Sub-SubTitle \n\nHi this is Lance \n\n## 2. Baz\n\nHi this is Molly""print(markdown_document)
```
```
# Title

## 1. SubTitle

Hi this is Jim

Hi this is Joe

### 1-1. Sub-SubTitle 

Hi this is Lance 

## 2. Baz

Hi this is Molly
```
```
Copyheaders_to_split_on = [# 문서를 분할할 헤더 레벨과 해당 레벨의 이름을 정의합니다.(""#"",""Header 1"",
    ),# 헤더 레벨 1은 '#'로 표시되며, 'Header 1'이라는 이름을 가집니다.(""##"",""Header 2"",
    ),# 헤더 레벨 2는 '##'로 표시되며, 'Header 2'라는 이름을 가집니다.(""###"",""Header 3"",
    ),# 헤더 레벨 3은 '###'로 표시되며, 'Header 3'이라는 이름을 가집니다.]# 마크다운 헤더를 기준으로 텍스트를 분할하는 MarkdownHeaderTextSplitter 객체를 생성합니다.markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)# markdown_document를 헤더를 기준으로 분할하여 md_header_splits에 저장합니다.md_header_splits = markdown_splitter.split_text(markdown_document)# 분할된 결과를 출력합니다.forheaderinmd_header_splits:print(f""{header.page_content}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
Hi this is Jim  
Hi this is Joe
{'Header 1': 'Title', 'Header 2': '1. SubTitle'}
=====================
Hi this is Lance
{'Header 1': 'Title', 'Header 2': '1. SubTitle', 'Header 3': '1-1. Sub-SubTitle'}
=====================
Hi this is Molly
{'Header 1': 'Title', 'Header 2': '2. Baz'}
=====================
```
기본적으로MarkdownHeaderTextSplitter는 분할되는 헤더를 출력 청크의 내용에서 제거합니다.
이는strip_headers = False로 설정하여 비활성화할 수 있습니다.
```
Copymarkdown_splitter = MarkdownHeaderTextSplitter(# 분할할 헤더를 지정합니다.headers_to_split_on=headers_to_split_on,# 헤더를 제거하지 않도록 설정합니다.strip_headers=False,
)# 마크다운 문서를 헤더를 기준으로 분할합니다.md_header_splits = markdown_splitter.split_text(markdown_document)# 분할된 결과를 출력합니다.forheaderinmd_header_splits:print(f""{header.page_content}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
# Title  
## 1. SubTitle  
Hi this is Jim  
Hi this is Joe
{'Header 1': 'Title', 'Header 2': '1. SubTitle'}
=====================
### 1-1. Sub-SubTitle  
Hi this is Lance
{'Header 1': 'Title', 'Header 2': '1. SubTitle', 'Header 3': '1-1. Sub-SubTitle'}
=====================
## 2. Baz  
Hi this is Molly
{'Header 1': 'Title', 'Header 2': '2. Baz'}
=====================
```
각 마크다운 그룹 내에서는 원하는 텍스트 분할기(text splitter)를 적용할 수 있습니다.
```
Copyfromlangchain_text_splittersimportRecursiveCharacterTextSplitter

markdown_document =""# Intro \n\n## History \n\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n\n## Rise and divergence \n\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n\n#### Standardization \n\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \n\n## Implementations \n\nImplementations of Markdown are available for over a dozen programming languages.""print(markdown_document)
```
```
# Intro 

## History 

Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] 

Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. 

## Rise and divergence 

As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for 

additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. 

#### Standardization 

From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. 

## Implementations 

Implementations of Markdown are available for over a dozen programming languages.
```
먼저,MarkdownHeaderTextSplitter사용하여 마크다운 문서를 헤더를 기준으로 분할합니다.
```
Copyheaders_to_split_on = [
    (""#"",""Header 1""),# 분할할 헤더 레벨과 해당 레벨의 이름을 지정합니다.(""##"",""Header 2""),
]# Markdown 문서를 헤더 레벨에 따라 분할합니다.markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on, strip_headers=False)
md_header_splits = markdown_splitter.split_text(markdown_document)# 분할된 결과를 출력합니다.forheaderinmd_header_splits:print(f""{header.page_content}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
# Intro  
## History  
Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  
Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.
{'Header 1': 'Intro', 'Header 2': 'History'}
=====================
## Rise and divergence  
As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  
additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  
#### Standardization  
From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.
{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}
=====================
## Implementations  
Implementations of Markdown are available for over a dozen programming languages.
{'Header 1': 'Intro', 'Header 2': 'Implementations'}
=====================
```
이전의MarkdownHeaderTextSplitter로 분할된 결과를 다시RecursiveCharacterTextSplitter로 분할합니다.
```
Copychunk_size =200# 분할된 청크의 크기를 지정합니다.chunk_overlap =20# 분할된 청크 간의 중복되는 문자 수를 지정합니다.text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size, chunk_overlap=chunk_overlap
)# 문서를 문자 단위로 분할합니다.splits = text_splitter.split_documents(md_header_splits)# 분할된 결과를 출력합니다.forheaderinsplits:print(f""{header.page_content}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
# Intro  
## History
{'Header 1': 'Intro', 'Header 2': 'History'}
=====================
Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its
{'Header 1': 'Intro', 'Header 2': 'History'}
=====================
readers in its source code form.[9]
{'Header 1': 'Intro', 'Header 2': 'History'}
=====================
Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.
{'Header 1': 'Intro', 'Header 2': 'History'}
=====================
## Rise and divergence  
As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for
{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}
=====================
additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  
#### Standardization
{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}
=====================
From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.
{'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}
=====================
## Implementations  
Implementations of Markdown are available for over a dozen programming languages.
{'Header 1': 'Intro', 'Header 2': 'Implementations'}
=====================
```"
07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter),https://wikidocs.net/234006,"# 07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter)

MarkdownHeaderTextSplitter와 개념적으로 유사한HTMLHeaderTextSplitter는 텍스트를 요소 수준에서 분할하고 각 헤더에 대한 메타데이터를 추가하는 ""구조 인식"" 청크 생성기입니다.
이는 각 청크와 ""관련된"" 메타데이터를 추가합니다.
HTMLHeaderTextSplitter는 요소별로 청크를 반환하거나 동일한 메타데이터를 가진 요소를 결합할 수 있으며
- (a) 관련 텍스트를 의미론적으로 (대략적으로) 그룹화하고
- (b) 문서 구조에 인코딩된 컨텍스트 풍부한 정보를 보존하는 것을 목표로 합니다.
(a) 관련 텍스트를 의미론적으로 (대략적으로) 그룹화하고
(b) 문서 구조에 인코딩된 컨텍스트 풍부한 정보를 보존하는 것을 목표로 합니다.
## ① HTML 문자열을 사용하는 경우
```
Copy%pip install -qU langchain-text-splitters
```
- headers_to_split_on리스트에 분할 기준이 되는 헤더 태그와 해당 헤더의 이름을 튜플 형태로 지정합니다.
- HTMLHeaderTextSplitter객체를 생성하면서headers_to_split_on매개변수에 분할 기준 헤더 리스트를 전달합니다.
headers_to_split_on리스트에 분할 기준이 되는 헤더 태그와 해당 헤더의 이름을 튜플 형태로 지정합니다.
HTMLHeaderTextSplitter객체를 생성하면서headers_to_split_on매개변수에 분할 기준 헤더 리스트를 전달합니다.
```
Copyfromlangchain_text_splittersimportHTMLHeaderTextSplitter

html_string =""""""
<!DOCTYPE html>
<html>
<body>
    <div>
        <h1>Foo</h1>
        <p>Some intro text about Foo.</p>
        <div>
            <h2>Bar main section</h2>
            <p>Some intro text about Bar.</p>
            <h3>Bar subsection 1</h3>
            <p>Some text about the first subtopic of Bar.</p>
            <h3>Bar subsection 2</h3>
            <p>Some text about the second subtopic of Bar.</p>
        </div>
        <div>
            <h2>Baz</h2>
            <p>Some text about Baz</p>
        </div>
        <br>
        <p>Some concluding text about Foo</p>
    </div>
</body>
</html>
""""""headers_to_split_on = [
    (""h1"",""Header 1""),# 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.(""h2"",""Header 2""),
    (""h3"",""Header 3""),
]# 지정된 헤더를 기준으로 HTML 텍스트를 분할하는 HTMLHeaderTextSplitter 객체를 생성합니다.html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)# HTML 문자열을 분할하여 결과를 html_header_splits 변수에 저장합니다.html_header_splits = html_splitter.split_text(html_string)# 분할된 결과를 출력합니다.forheaderinhtml_header_splits:print(f""{header.page_content}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
Foo
{}
=====================
Some intro text about Foo.  
Bar main section Bar subsection 1 Bar subsection 2
{'Header 1': 'Foo'}
=====================
Some intro text about Bar.
{'Header 1': 'Foo', 'Header 2': 'Bar main section'}
=====================
Some text about the first subtopic of Bar.
{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}
=====================
Some text about the second subtopic of Bar.
{'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}
=====================
Baz
{'Header 1': 'Foo'}
=====================
Some text about Baz
{'Header 1': 'Foo', 'Header 2': 'Baz'}
=====================
Some concluding text about Foo
{'Header 1': 'Foo'}
=====================
```
이 예시에서는 웹 URL로부터 HTML 콘텐츠를 로드한 후, 이를 다른 splitter와 파이프라인으로 연결하여 처리하는 과정입니다.
```
Copyfromlangchain_text_splittersimportRecursiveCharacterTextSplitter

url =""https://plato.stanford.edu/entries/goedel/""# 분할할 텍스트의 URL을 지정합니다.headers_to_split_on = [# 분할할 HTML 헤더 태그와 해당 헤더의 이름을 지정합니다.(""h1"",""Header 1""),
    (""h2"",""Header 2""),
    (""h3"",""Header 3""),
    (""h4"",""Header 4""),
]# HTML 헤더를 기준으로 텍스트를 분할하는 HTMLHeaderTextSplitter 객체를 생성합니다.html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)# URL에서 텍스트를 가져와 HTML 헤더를 기준으로 분할합니다.html_header_splits = html_splitter.split_text_from_url(url)

chunk_size =500# 텍스트를 분할할 청크의 크기를 지정합니다.chunk_overlap =30# 분할된 청크 간의 중복되는 문자 수를 지정합니다.text_splitter = RecursiveCharacterTextSplitter(# 텍스트를 재귀적으로 분할하는 RecursiveCharacterTextSplitter 객체를 생성합니다.chunk_size=chunk_size, chunk_overlap=chunk_overlap
)# HTML 헤더로 분할된 텍스트를 다시 청크 크기에 맞게 분할합니다.splits = text_splitter.split_documents(html_header_splits)# 분할된 텍스트 중 80번째부터 85번째까지의 청크를 출력합니다.forheaderinsplits[80:85]:print(f""{header.page_content}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
We see that Gödel first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (“This sentence is false”) and Berry’s paradox (“The least number not defined by an expression consisting of just fourteen English words”). Gödel then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth
{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}
=====================
means that arithmetic truth and arithmetic provability are not co-extensive — whence the First Incompleteness Theorem.
{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}
=====================
This account of Gödel’s discovery was told to Hao Wang very much after the fact; but in Gödel’s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See Gödel 2003a and Gödel 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by Gödel by 1931. But he neither publicized nor published the result; the biases logicians
{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}
=====================
result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to Gödel’s publication of that theorem.
{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}
=====================
We now describe the proof of the two theorems, formulating Gödel’s results in Peano arithmetic. Gödel himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following Gödel’s notation.
{'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}
=====================
```
## 한계
HTMLHeaderTextSplitter는 HTML 문서 간의 구조적 차이를 처리하려고 시도하지만, 때로는 특정 헤더를 누락할 수 있습니다.
예를 들어, 이 알고리즘은 헤더가 항상 관련 텍스트보다 ""위""에 있는 노드, 즉 이전 형제 노드, 조상 노드 및 이들의 조합에 위치한다고 가정합니다.
다음 뉴스 기사(이 문서 작성 시점 기준)에서는 최상위 헤드라인의 텍스트가 ""h1""으로 태그되어 있지만, 우리가 예상하는 텍스트 요소와는별개의 하위 트리에 있는 것을 볼 수 있습니다.
따라서 ""h1"" 요소와 관련 텍스트는 청크 메타데이터에 나타나지 않지만, 해당되는 경우 ""h2""와 관련 텍스트는 볼 수 있습니다.
```
Copy# 분할할 HTML 페이지의 URL을 지정합니다.url =""https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html""headers_to_split_on = [
    (""h1"",""Header 1""),# 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.(""h2"",""Header 2""),# 분할할 헤더 태그와 해당 헤더의 이름을 지정합니다.]# 지정된 헤더를 기준으로 HTML 텍스트를 분할하는 HTMLHeaderTextSplitter 객체를 생성합니다.html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)# 지정된 URL의 HTML 페이지를 분할하여 결과를 html_header_splits 변수에 저장합니다.html_header_splits = html_splitter.split_text_from_url(url)# 분할된 결과를 출력합니다.forheaderinhtml_header_splits:print(f""{header.page_content[:100]}"")print(f""{header.metadata}"", end=""\n=====================\n"")
```
```
CNN values your feedback  
1. How relevant is this ad to you?  
2. Did you encounter any technical i
{}
=====================
No two El Niño winters are the same, but many have temperature and precipitation trends in common.  
{'Header 2': 'What could this winter look like?'}
=====================
Ad Feedback  
Ad Feedback  
Ad Feedback  
Ad Feedback  
Ad Feedback  
Live TV Listen Watch Terms of 
{}
=====================
```"
08. 재귀적 JSON 분할(RecursiveJsonSplitter),https://wikidocs.net/234007,"# 08. 재귀적 JSON 분할(RecursiveJsonSplitter)

이 JSON 분할기는 JSON 데이터를 깊이 우선 탐색(depth-first traversal)하여 더 작은 JSON 청크(chunk)를 생성합니다.
이 분할기는 중첩된 JSON 객체를 가능한 한 유지하려고 시도하지만, 청크의 크기를 min_chunk_size와 max_chunk_size 사이로 유지하기 위해 필요한 경우 객체를 분할합니다. 값이 중첩된 JSON이 아니라 매우 큰 문자열인 경우, 해당 문자열은 분할되지 않습니다.
청크 크기에 대한 엄격한 제한이 필요한 경우, 이 분할기 이후에 Recursive Text Splitter를 사용하여 해당 청크를 처리하는 것을 고려해 볼 수 있습니다.
분할하는 기준
텍스트 분할 방식: JSON 값 기준
청크 크기 측정 방식: 문자 수 기준
```
Copy%pip install -qU langchain-text-splitters
```
- requests.get()함수를 사용하여 ""https://api.smith.langchain.com/openapi.json"" URL에서 JSON 데이터를 가져옵니다.
- 가져온 JSON 데이터는json()메서드를 통해 Python 딕셔너리 형태로 변환되어json_data변수에 저장됩니다.
requests.get()함수를 사용하여 ""https://api.smith.langchain.com/openapi.json"" URL에서 JSON 데이터를 가져옵니다.
가져온 JSON 데이터는json()메서드를 통해 Python 딕셔너리 형태로 변환되어json_data변수에 저장됩니다.
```
Copyimportrequests# JSON 데이터를 로드합니다.json_data = requests.get(""https://api.smith.langchain.com/openapi.json"").json()
```
RecursiveJsonSplitter를 사용하여 JSON 데이터를 분할하는 예제입니다.
```
Copyfromlangchain_text_splittersimportRecursiveJsonSplitter# JSON 데이터를 최대 300 크기의 청크로 분할하는 RecursiveJsonSplitter 객체를 생성합니다.splitter = RecursiveJsonSplitter(max_chunk_size=300)
```
splitter.split_json()함수를 사용하여 JSON 데이터를 재귀적으로 분할합니다.
```
Copy# JSON 데이터를 재귀적으로 분할합니다. 작은 JSON 조각에 접근하거나 조작해야 하는 경우에 사용합니다.json_chunks = splitter.split_json(json_data=json_data)
```
- splitter.create_documents()메서드를 사용하여 JSON 데이터를 문서 형식으로 변환합니다.
- splitter.split_text()메서드를 사용하여 JSON 데이터를 문자열 리스트로 분할합니다.
splitter.create_documents()메서드를 사용하여 JSON 데이터를 문서 형식으로 변환합니다.
splitter.split_text()메서드를 사용하여 JSON 데이터를 문자열 리스트로 분할합니다.
```
Copy# JSON 데이터를 기반으로 문서를 생성합니다.docs = splitter.create_documents(texts=[json_data])# JSON 데이터를 기반으로 문자열 청크를 생성합니다.texts = splitter.split_text(json_data=json_data)# 첫 번째 문자열을 출력합니다.print(docs[0].page_content)print(""===""*20)# 분할된 문자열 청크를 출력합니다.print(texts[0])
```
```
{""openapi"": ""3.1.0"", ""info"": {""title"": ""LangSmith"", ""version"": ""0.1.0""}, ""paths"": {""/api/v1/sessions/{session_id}"": {""get"": {""tags"": [""tracer-sessions""], ""summary"": ""Read Tracer Session"", ""description"": ""Get a specific session.""}}}}
============================================================
{""openapi"": ""3.1.0"", ""info"": {""title"": ""LangSmith"", ""version"": ""0.1.0""}, ""paths"": {""/api/v1/sessions/{session_id}"": {""get"": {""tags"": [""tracer-sessions""], ""summary"": ""Read Tracer Session"", ""description"": ""Get a specific session.""}}}}
```
texts[2]을 출력하여 큰 청크 중 하나를 검토한 결과, 해당 청크에 리스트 객체가 포함되어 있음을 확인할 수 있습니다.
- 2번째 청크의 크기가 제한(300) 을 초과하는 데에는 리스트 객체인 이유가 있습니다.
- 이는RecursiveJsonSplitter가리스트 객체는 분할하지 않기 때문입니다.
2번째 청크의 크기가 제한(300) 을 초과하는 데에는 리스트 객체인 이유가 있습니다.
이는RecursiveJsonSplitter가리스트 객체는 분할하지 않기 때문입니다.
```
Copy# 청크의 크기를 확인해 봅시다.print([len(text)fortextintexts][:10])# 더 큰 청크 중 하나를 검토해 보면 리스트 객체가 있는 것을 볼 수 있습니다.print(texts[1])
```
```
[232, 197, 469, 210, 213, 237, 271, 191, 232, 215]
{""paths"": {""/api/v1/sessions/{session_id}"": {""get"": {""operationId"": ""read_tracer_session_api_v1_sessions__session_id__get"", ""security"": [{""API Key"": []}, {""Tenant ID"": []}, {""Bearer Auth"": []}]}}}}
```
2번 index 청크를 다음과 같이json모듈을 사용하여 파싱할 수 있습니다.
```
Copyimportjson

json_data = json.loads(texts[2])
json_data[""paths""]
```
```
{'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id',     'in': 'path',     'required': True,     'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}},    {'name': 'include_stats',     'in': 'query',     'required': False,     'schema': {'type': 'boolean',      'default': False,      'title': 'Include Stats'}},    {'name': 'accept',     'in': 'header',     'required': False,     'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}],      'title': 'Accept'}}]}}}
```
convert_lists매개변수를True로 설정하여 JSON 내의 리스트를index:item형태의key:value쌍으로 변환할 수 있습니다.
```
Copy# 다음은 JSON을 전처리하고 리스트를 인덱스:항목을 키:값 쌍으로 하는 딕셔너리로 변환합니다.texts = splitter.split_text(json_data=json_data, convert_lists=True)
```
```
Copy# 리스트가 딕셔너리로 변환되었고, 그 결과를 확인합니다.print(texts[2])
```
```
{""paths"": {""/api/v1/sessions/{session_id}"": {""get"": {""parameters"": [{""name"": ""session_id"", ""in"": ""path"", ""required"": true, ""schema"": {""type"": ""string"", ""format"": ""uuid"", ""title"": ""Session Id""}}, {""name"": ""include_stats"", ""in"": ""query"", ""required"": false, ""schema"": {""type"": ""boolean"", ""default"": false, ""title"": ""Include Stats""}}, {""name"": ""accept"", ""in"": ""header"", ""required"": false, ""schema"": {""anyOf"": [{""type"": ""string""}, {""type"": ""null""}], ""title"": ""Accept""}}]}}}}
```
docs리스트의 특정 인덱스에 해당하는 문서를 확인할 수 있습니다.
```
Copy# 2번 문서를 확인합니다.docs[2]
```
```
Document(page_content='{""paths"": {""/api/v1/sessions/{session_id}"": {""get"": {""parameters"": [{""name"": ""session_id"", ""in"": ""path"", ""required"": true, ""schema"": {""type"": ""string"", ""format"": ""uuid"", ""title"": ""Session Id""}}, {""name"": ""include_stats"", ""in"": ""query"", ""required"": false, ""schema"": {""type"": ""boolean"", ""default"": false, ""title"": ""Include Stats""}}, {""name"": ""accept"", ""in"": ""header"", ""required"": false, ""schema"": {""anyOf"": [{""type"": ""string""}, {""type"": ""null""}], ""title"": ""Accept""}}]}}}}')
```"
CH09 임베딩(Embedding),https://wikidocs.net/233777,"# CH09 임베딩(Embedding)

임베딩은 Retrieval-Augmented Generation(RAG) 시스템의 세 번째 단계로, 문서분할 단계에서 생성된 문서 단위들을기계가 이해할 수 있는 수치적 형태로 변환하는 과정입니다. 이 단계는 RAG 시스템의 핵심적인 부분 중 하나로, 문서의 의미를 벡터(숫자의 배열) 형태로 표현함으로써, 사용자가 입력한 질문(Query) 에 대하여 DB 에 저장한문서 조각/단락(Chunk) 을 검색하여 가져올 때유사도 계산시 활용될 수 있습니다.
## 임베딩의 필요성
의미 이해: 자연 언어는 매우 복잡하고 다양한 의미를 내포하고 있습니다. 임베딩을 통해 이러한 텍스트를 정량화된 형태로 변환함으로써, 컴퓨터가 문서의 내용과 의미를 더 잘 이해하고 처리할 수 있습니다.
의미 이해: 자연 언어는 매우 복잡하고 다양한 의미를 내포하고 있습니다. 임베딩을 통해 이러한 텍스트를 정량화된 형태로 변환함으로써, 컴퓨터가 문서의 내용과 의미를 더 잘 이해하고 처리할 수 있습니다.
정보 검색 향상: 수치화된 벡터 형태로의 변환은 문서 간의 유사성을 계산하는 데 있어 필수적입니다. 이는 관련 문서를 검색하거나, 질문에 가장 적합한 문서를 찾는 작업을 용이하게 합니다.
정보 검색 향상: 수치화된 벡터 형태로의 변환은 문서 간의 유사성을 계산하는 데 있어 필수적입니다. 이는 관련 문서를 검색하거나, 질문에 가장 적합한 문서를 찾는 작업을 용이하게 합니다.
## 예시


- 1번 단락: [0.1, 0.5, 0.9, ... , 0.1, 0.2]
- 2번 단락: [0.7, 0.1, 0.3, ... , 0.5, 0.6]
- 3번 단락: [0.9, 0.4, 0.5, ... , 0.4, 0.3]
1번 단락: [0.1, 0.5, 0.9, ... , 0.1, 0.2]
2번 단락: [0.7, 0.1, 0.3, ... , 0.5, 0.6]
3번 단락: [0.9, 0.4, 0.5, ... , 0.4, 0.3]
질문: ""시장조사기관 IDC 가 예측한 AI 소프트웨어 시장의 연평균 성장률은 어떻게 되나요?""
- [0.1, 0.5, 0.9, ..., 0.2, 0.4]
[0.1, 0.5, 0.9, ..., 0.2, 0.4]
유사도 계산 예시
- 1번: 80%-> 선택!
- 2번: 30%
- 3번: 25%
1번: 80%-> 선택!
2번: 30%
3번: 25%
## 코드
```
Copyfromlangchain_openaiimportOpenAIEmbeddings# 단계 3: 임베딩(Embedding) 생성embeddings = OpenAIEmbeddings()
```
## 참고
- 임베딩
- LangChain Text Embeddings
임베딩
LangChain Text Embeddings"
01. OpenAIEmbeddings,https://wikidocs.net/233815,"# 01. OpenAIEmbeddings

문서 임베드(Embed documents)는 문서의 내용을 벡터 형태로 표현하는 과정을 의미합니다.
이를 통해 문서의 의미론적 정보를 수치화할 수 있으며, 문서 간의 유사도 비교, 문서 분류, 검색 등 다양한 자연어 처리 작업에 활용될 수 있습니다.
문서 임베드는 주로 사전 학습된 언어 모델을 사용하여 수행되며, 대표적인 예로는 BERT, GPT 등이 있습니다.
이러한 모델들은 대규모 텍스트 데이터로 학습되어 문맥적 정보를 포착할 수 있기 때문에, 문서의 의미를 효과적으로 인코딩할 수 있습니다.
문서 임베드 과정에서는 문서를 토큰화하고, 각 토큰을 언어 모델에 입력하여 해당 토큰의 임베딩 벡터를 얻습니다.
이후 문서 전체의 임베딩 벡터를 계산하기 위해 토큰 임베딩 벡터들을 평균하거나 가중 평균하는 등의 방법을 사용할 수 있습니다.
문서 임베드를 통해 얻은 벡터 표현은 문서 간의 유사도를 계산하는 데 사용될 수 있으며, 이는 문서 검색, 추천 시스템, 문서 클러스터링 등 다양한 응용 분야에서 활용됩니다.
또한 임베딩 벡터를 기반으로 문서 분류나 감성 분석 등의 작업을 수행할 수 있습니다.
문서 임베드는 자연어 처리 분야에서 중요한 역할을 하며, 대량의 문서를 효율적으로 처리하고 분석하는 데 핵심적인 기술로 자리잡고 있습니다.
- 관련 문서
관련 문서
## Setup
먼저 langchain-openai를 설치하고 필요한 환경 변수를 설정합니다.
```
Copy%pip install -qU langchain-openai
```
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
- OpenAIEmbeddings클래스를langchain_openai모듈에서 임포트합니다.
OpenAIEmbeddings클래스를langchain_openai모듈에서 임포트합니다.
```
Copyfromlangchain_openaiimportOpenAIEmbeddings
```
- OpenAIEmbeddings 클래스를 사용하여 임베딩 객체를 생성합니다.
- 임베딩 모델로는 ""text-embedding-3-large""를 사용합니다.
- 이 모델은 OpenAI에서 제공하는 대규모 텍스트 임베딩 모델 중 하나입니다.
- 생성된embeddings객체는 텍스트를 벡터로 변환하는 데 사용될 수 있습니다.
OpenAIEmbeddings 클래스를 사용하여 임베딩 객체를 생성합니다.
임베딩 모델로는 ""text-embedding-3-large""를 사용합니다.
이 모델은 OpenAI에서 제공하는 대규모 텍스트 임베딩 모델 중 하나입니다.
생성된embeddings객체는 텍스트를 벡터로 변환하는 데 사용될 수 있습니다.
```
Copy# OpenAI의 ""text-embedding-3-large"" 모델을 사용하여 임베딩을 생성합니다.embeddings = OpenAIEmbeddings(model=""text-embedding-3-large"")
```
- text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
```
Copytext = (""임베딩 테스트를 하기 위한 샘플 문장입니다.""# 테스트용 문서 텍스트를 정의합니다.)
```
## 쿼리 임베딩
embeddings.embed_query(text)는 주어진 텍스트를 임베딩 벡터로 변환하는 함수입니다.
- text매개변수로 전달된 텍스트를 입력으로 받습니다.
- 텍스트를 임베딩 모델에 전달하여 해당 텍스트의 벡터 표현을 생성합니다.
- 생성된 임베딩 벡터를query_result변수에 저장합니다.
text매개변수로 전달된 텍스트를 입력으로 받습니다.
텍스트를 임베딩 모델에 전달하여 해당 텍스트의 벡터 표현을 생성합니다.
생성된 임베딩 벡터를query_result변수에 저장합니다.
이 함수는 텍스트를 벡터 공간에 매핑하여 의미적으로 유사한 텍스트를 찾거나 텍스트 간의 유사도를 계산하는 데 사용될 수 있습니다.
```
Copy# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.query_result = embeddings.embed_query(text)
```
query_result[:5]는query_result리스트의 처음 5개 요소를 슬라이싱(slicing)하여 선택합니다.
- 리스트 슬라이싱 문법인[start:end]를 사용하여 리스트의 일부분을 추출합니다.
- start인덱스는 포함되고end인덱스는 제외됩니다.
- start를 생략하면 리스트의 처음부터 슬라이싱합니다.
- 따라서query_result[:5]는query_result리스트의 인덱스 0부터 4까지의 요소를 선택하여 새로운 리스트를 반환합니다.
리스트 슬라이싱 문법인[start:end]를 사용하여 리스트의 일부분을 추출합니다.
start인덱스는 포함되고end인덱스는 제외됩니다.
start를 생략하면 리스트의 처음부터 슬라이싱합니다.
따라서query_result[:5]는query_result리스트의 인덱스 0부터 4까지의 요소를 선택하여 새로운 리스트를 반환합니다.
```
Copy# 쿼리 결과의 처음 5개 항목을 선택합니다.query_result[:5]
```
```
[-0.003218927512230862, -0.00790991377231665, -0.012806527680822976, 0.030056136583834044, 0.016404038761725557]
```
## 도큐먼트 임베딩
embeddings.embed_documents()함수를 사용하여 텍스트 문서를 임베딩합니다.
- [text]를 인자로 전달하여 단일 문서를 리스트 형태로 임베딩 함수에 전달합니다.
- 함수 호출 결과로 반환된 임베딩 벡터를doc_result변수에 할당합니다.
[text]를 인자로 전달하여 단일 문서를 리스트 형태로 임베딩 함수에 전달합니다.
함수 호출 결과로 반환된 임베딩 벡터를doc_result변수에 할당합니다.
```
Copydoc_result = embeddings.embed_documents(
    [text]
)# 텍스트를 임베딩하여 문서 벡터를 생성합니다.
```
doc_result[0][:5]는doc_result리스트의 첫 번째 요소에서 처음 5개의 문자를 슬라이싱하여 선택합니다.
```
Copy# 문서 결과의 첫 번째 요소에서 처음 5개 항목을 선택합니다.doc_result[0][:5]
```
```
[-0.003218927512230862, -0.00790991377231665, -0.012806527680822976, 0.030056136583834044, 0.016404038761725557]
```
## 차원 지정
text-embedding-3모델 클래스를 사용하면 반환되는 임베딩의 크기를 지정할 수 있습니다.
예를 들어, 기본적으로text-embedding-3-large는 3072 차원의 임베딩을 반환합니다.
```
Copy# 문서 결과의 첫 번째 요소의 길이를 반환합니다.len(doc_result[0])
```
```
3072
```
하지만dimensions=1024를 전달함으로써 임베딩의 크기를 1024로 줄일 수 있습니다.
- OpenAI의 ""text-embedding-3-large"" 모델을 사용하여 1024차원의 임베딩을 생성하는OpenAIEmbeddings객체를 초기화합니다.
- dimensions매개변수를 통해 임베딩 벡터의 차원을 1024로 설정합니다.
- 생성된embeddings_1024객체를 사용하여 텍스트 데이터를 1024차원의 벡터로 변환할 수 있습니다.
OpenAI의 ""text-embedding-3-large"" 모델을 사용하여 1024차원의 임베딩을 생성하는OpenAIEmbeddings객체를 초기화합니다.
dimensions매개변수를 통해 임베딩 벡터의 차원을 1024로 설정합니다.
생성된embeddings_1024객체를 사용하여 텍스트 데이터를 1024차원의 벡터로 변환할 수 있습니다.
```
Copy# OpenAI의 ""text-embedding-3-large"" 모델을 사용하여 1024차원의 임베딩을 생성하는 객체를 초기화합니다.embeddings_1024 = OpenAIEmbeddings(
    model=""text-embedding-3-large"", dimensions=1024)
```
embeddings_1024.embed_documents()함수를 사용하여 단일 텍스트(text)를 임베딩한 결과의 길이를 반환합니다.
- embeddings_1024.embed_documents()함수는 리스트 형태의 텍스트를 입력받아 각 텍스트에 대한 임베딩 벡터를 생성합니다.
- [text]와 같이 단일 텍스트를 리스트로 감싸서 함수에 전달합니다.
- 함수의 반환값은 임베딩 벡터들의 리스트입니다. 이 예시에서는 단일 텍스트를 임베딩했으므로 반환되는 리스트의 길이는 1입니다.
- 반환된 리스트의 첫 번째 요소([0])는text에 대한 임베딩 벡터입니다.
- len()함수를 사용하여 임베딩 벡터의 길이를 계산합니다.
embeddings_1024.embed_documents()함수는 리스트 형태의 텍스트를 입력받아 각 텍스트에 대한 임베딩 벡터를 생성합니다.
[text]와 같이 단일 텍스트를 리스트로 감싸서 함수에 전달합니다.
함수의 반환값은 임베딩 벡터들의 리스트입니다. 이 예시에서는 단일 텍스트를 임베딩했으므로 반환되는 리스트의 길이는 1입니다.
반환된 리스트의 첫 번째 요소([0])는text에 대한 임베딩 벡터입니다.
len()함수를 사용하여 임베딩 벡터의 길이를 계산합니다.
따라서 이 코드는text를 임베딩한 벡터의 길이를 반환합니다.
```
Copy# 주어진 텍스트를 임베딩하고 첫 번째 임베딩 벡터의 길이를 반환합니다.len(embeddings_1024.embed_documents([text])[0])
```
```
1024
```
## 유사도 계산
```
Copysentence1 =""안녕하세요? 반갑습니다.""sentence2 =""안녕하세요? 반갑습니다!""sentence3 =""안녕하세요? 만나서 반가워요.""sentence4 =""Hi, nice to meet you.""sentence5 =""I like to eat apples.""
```
```
Copyfromsklearn.metrics.pairwiseimportcosine_similarity
```
```
Copysentences = [sentence1, sentence2, sentence3, sentence4, sentence5]
embedded_sentences = embeddings_1024.embed_documents(sentences)
```
```
Copydefsimilarity(a, b):returncosine_similarity([a], [b])[0][0]
```
```
Copy# sentence1 = ""안녕하세요? 반갑습니다.""# sentence2 = ""안녕하세요? 만나서 반가워요.""# sentence3 = ""Hi, nice to meet you.""# sentence4 = ""I like to eat apples.""fori, sentenceinenumerate(embedded_sentences):forj, other_sentenceinenumerate(embedded_sentences):ifi < j:print(f""[유사도{similarity(sentence, other_sentence):.4f}]{sentences[i]}\t <=====> \t{sentences[j]}"")
```
```
[유사도 0.9652] 안녕하세요? 반갑습니다.   <=====>     안녕하세요? 반갑습니다!
[유사도 0.8576] 안녕하세요? 반갑습니다.   <=====>     안녕하세요? 만나서 반가워요.
[유사도 0.5451] 안녕하세요? 반갑습니다.   <=====>     Hi, nice to meet you.
[유사도 0.1725] 안녕하세요? 반갑습니다.   <=====>     I like to eat apples.
[유사도 0.8402] 안녕하세요? 반갑습니다!   <=====>     안녕하세요? 만나서 반가워요.
[유사도 0.5340] 안녕하세요? 반갑습니다!   <=====>     Hi, nice to meet you.
[유사도 0.1857] 안녕하세요? 반갑습니다!   <=====>     I like to eat apples.
[유사도 0.5856] 안녕하세요? 만나서 반가워요.    <=====>     Hi, nice to meet you.
[유사도 0.1575] 안녕하세요? 만나서 반가워요.    <=====>     I like to eat apples.
[유사도 0.2474] Hi, nice to meet you.   <=====>     I like to eat apples.
```"
02. 캐시 임베딩(CacheBackedEmbeddings),https://wikidocs.net/233816,"# 02. 캐시 임베딩(CacheBackedEmbeddings)

Embeddings는 재계산을 피하기 위해 저장되거나 일시적으로 캐시될 수 있습니다.
Embeddings를 캐싱하는 것은CacheBackedEmbeddings를 사용하여 수행될 수 있습니다. 캐시 지원 embedder는 embeddings를 키-값 저장소에 캐싱하는 embedder 주변에 래퍼입니다. 텍스트는 해시되고 이 해시는 캐시에서 키로 사용됩니다.
CacheBackedEmbeddings를 초기화하는 주요 지원 방법은from_bytes_store입니다. 이는 다음 매개변수를 받습니다:
- underlying_embedder: 임베딩을 위해 사용되는 embedder.
- document_embedding_cache: 문서 임베딩을 캐싱하기 위한ByteStore중 하나.
- namespace: (선택 사항, 기본값은"""") 문서 캐시를 위해 사용되는 네임스페이스. 이 네임스페이스는 다른 캐시와의 충돌을 피하기 위해 사용됩니다. 예를 들어, 사용된 임베딩 모델의 이름으로 설정하십시오.
underlying_embedder: 임베딩을 위해 사용되는 embedder.
document_embedding_cache: 문서 임베딩을 캐싱하기 위한ByteStore중 하나.
namespace: (선택 사항, 기본값은"""") 문서 캐시를 위해 사용되는 네임스페이스. 이 네임스페이스는 다른 캐시와의 충돌을 피하기 위해 사용됩니다. 예를 들어, 사용된 임베딩 모델의 이름으로 설정하십시오.
주의: 동일한 텍스트가 다른 임베딩 모델을 사용하여 임베딩될 때 충돌을 피하기 위해namespace매개변수를 설정하는 것이 중요합니다.
```
Copyfromlangchain.embeddingsimportCacheBackedEmbeddings
```
## VectorStore 에서 임베딩 사용
먼저, 로컬 파일 시스템을 사용하여 임베딩을 저장하고 FAISS 벡터 스토어를 사용하여 검색하는 예제를 살펴보겠습니다.
```
Copy%pip install --upgrade --quiet  langchain-openai faiss-cpu
```
- LocalFileStore,TextLoader,FAISS,OpenAIEmbeddings, 그리고CharacterTextSplitter모듈을 임포트합니다.
- OpenAIEmbeddings를 사용하여 언더라이닝 임베딩을 초기화합니다.
- LocalFileStore를 사용하여 로컬 파일 저장소 인스턴스를./cache/디렉토리에 생성합니다.
- CacheBackedEmbeddings를 사용하여 캐시 지원 임베딩을 초기화하며, 이는underlying_embeddings와 로컬 파일 저장소 인스턴스를 사용하고, 네임스페이스로underlying_embeddings.model을 설정합니다.
LocalFileStore,TextLoader,FAISS,OpenAIEmbeddings, 그리고CharacterTextSplitter모듈을 임포트합니다.
OpenAIEmbeddings를 사용하여 언더라이닝 임베딩을 초기화합니다.
LocalFileStore를 사용하여 로컬 파일 저장소 인스턴스를./cache/디렉토리에 생성합니다.
CacheBackedEmbeddings를 사용하여 캐시 지원 임베딩을 초기화하며, 이는underlying_embeddings와 로컬 파일 저장소 인스턴스를 사용하고, 네임스페이스로underlying_embeddings.model을 설정합니다.
```
Copyfromlangchain.storageimportLocalFileStorefromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportCharacterTextSplitter# OpenAI 임베딩을 사용하여 기본 임베딩 설정embedding = OpenAIEmbeddings()# 로컬 파일 저장소 설정store = LocalFileStore(""./cache/"")# 캐시를 지원하는 임베딩 생성cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embedding,
    store,
    namespace=embedding.model,# 기본 임베딩과 저장소를 사용하여 캐시 지원 임베딩을 생성)
```
```
Copy# store에서 키들을 순차적으로 가져옵니다.list(store.yield_keys())
```
```
['version.txt', 'hub/version.txt']
```
문서를 로드하고, 청크로 분할한 다음, 각 청크를 임베딩하고 벡터 저장소에 로드합니다.
- TextLoader를 사용하여 ""../../state_of_the_union.txt"" 파일에서 텍스트 데이터를 로드합니다.
- CharacterTextSplitter를 초기화하여 문서를 1000자 단위로 분할합니다. 이때, 분할된 텍스트 간의 겹침은 없습니다.
- 분할된 문서들은documents변수에 저장됩니다.
TextLoader를 사용하여 ""../../state_of_the_union.txt"" 파일에서 텍스트 데이터를 로드합니다.
CharacterTextSplitter를 초기화하여 문서를 1000자 단위로 분할합니다. 이때, 분할된 텍스트 간의 겹침은 없습니다.
분할된 문서들은documents변수에 저장됩니다.
```
Copyfromlangchain.document_loadersimportTextLoaderfromlangchain_text_splittersimportCharacterTextSplitter# 문서 로드raw_documents = TextLoader(""./data/appendix-keywords.txt"").load()# 문자 단위로 텍스트 분할 설정text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)# 문서 분할documents = text_splitter.split_documents(raw_documents)
```
```
Copy# 코드 실행 시간을 측정합니다.%time db = FAISS.from_documents(documents, cached_embedder)# 문서로부터 FAISS 데이터베이스 생성
```
```
CPU times: user 218 ms, sys: 26.7 ms, total: 245 ms
Wall time: 951 ms
```
벡터 저장소를 다시 생성하려고 하면, 임베딩을 다시 계산할 필요가 없기 때문에 훨씬 더 빠르게 처리됩니다.
```
Copy# 캐싱된 임베딩을 사용하여 FAISS 데이터베이스 생성%time db2 = FAISS.from_documents(documents, cached_embedder)
```
```
CPU times: user 3.6 ms, sys: 1.25 ms, total: 4.85 ms
Wall time: 4.23 ms
```
```
Copy# VectorStore에서 처음 5개의 키를 가져옵니다.list(store.yield_keys())[:5]
```
```
['text-embedding-ada-0020fd71f95-1342-512d-9d5b-3e3ab3c6bbe0', 'text-embedding-ada-00274ae75af-9058-555e-aefa-082f0b4e0560', 'text-embedding-ada-00241e7391b-b68f-5e9f-bb07-3609bb83c3e2', 'text-embedding-ada-0027494a7c8-3399-52a1-85ef-f4d0a563d31f', 'text-embedding-ada-0029db9e1cd-62d8-50fc-94f4-24bef3cacaf5']
```
## ByteStore로 변경
다른ByteStore를 사용하기 위해서는CacheBackedEmbeddings를 생성할 때 해당ByteStore를 사용하면 됩니다.
아래에서는, 비영구적인InMemoryByteStore를 사용하여 동일한 캐시된 임베딩 객체를 생성하는 예시를 보여줍니다.
```
Copyfromlangchain.embeddingsimportCacheBackedEmbeddingsfromlangchain.storageimportInMemoryByteStore

store = InMemoryByteStore()# 메모리 내 바이트 저장소 생성# 캐시 지원 임베딩 생성cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embedding, store, namespace=embedding.model
)
```"
03. 허깅페이스 임베딩(HuggingFace Hub),https://wikidocs.net/233817,"# 03. 허깅페이스 임베딩(HuggingFace Hub)

Hugging Face Embedding 클래스를 로드해 보겠습니다.
```
Copy%pip install --upgrade --quiet  langchain sentence_transformers
```
- HuggingFaceEmbeddings클래스를langchain_community.embeddings모듈에서 임포트합니다.
HuggingFaceEmbeddings클래스를langchain_community.embeddings모듈에서 임포트합니다.
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
```
Copyimportos# 사용자로부터 HuggingFace Inference API 키를 입력받습니다.inference_api_key = os.environ.get(""HUGGINGFACEHUB_API_TOKEN"")
```
```
Copy# 허깅페이스 모델/토크나이저를 다운로드 받을 경로# (예시)importos# ./cache/ 경로에 다운로드 받도록 설정os.environ[""HF_HOME""] =""./cache/""
```
```
Copyfromlangchain_community.embeddingsimport(
    HuggingFaceEmbeddings,
    HuggingFaceBgeEmbeddings,
)
```
HuggingFaceEmbeddings클래스를 사용하여 임베딩 객체를 생성합니다.
- 이 클래스는 Hugging Face의 Transformers 라이브러리에서 제공하는 사전 훈련된 언어 모델을 활용하여 텍스트 임베딩을 생성합니다.
- 생성된embeddings객체는 텍스트를 벡터 표현으로 변환하는 데 사용될 수 있습니다.
이 클래스는 Hugging Face의 Transformers 라이브러리에서 제공하는 사전 훈련된 언어 모델을 활용하여 텍스트 임베딩을 생성합니다.
생성된embeddings객체는 텍스트를 벡터 표현으로 변환하는 데 사용될 수 있습니다.
```
Copyembeddings = HuggingFaceEmbeddings()# HuggingFace 임베딩을 생성합니다.embeddings = HuggingFaceBgeEmbeddings()
```
```
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
```
- text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
```
Copytext = (""임베딩 테스트를 하기 위한 샘플 문장입니다.""# 테스트용 문서 텍스트를 정의합니다.)
```
embeddings.embed_query(text)는 주어진 텍스트를 임베딩 벡터로 변환하는 함수입니다.
- text매개변수로 전달된 텍스트를 임베딩 모델에 입력하여 벡터 표현을 생성합니다.
- 생성된 임베딩 벡터는query_result변수에 저장됩니다.
text매개변수로 전달된 텍스트를 임베딩 모델에 입력하여 벡터 표현을 생성합니다.
생성된 임베딩 벡터는query_result변수에 저장됩니다.
이 함수는 텍스트를 벡터 공간에 매핑하여 의미적 유사성을 계산하거나 검색에 활용할 수 있는 벡터 표현을 얻는 데 사용됩니다.
```
Copy# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.query_result = embeddings.embed_query(text)
```
query_result[:3]은query_result리스트의 처음 3개 요소를 슬라이싱(slicing)하여 선택합니다.
- 리스트 슬라이싱 문법인[start:end]를 사용하여query_result리스트의 일부분을 추출합니다.
- start인덱스는 포함되고end인덱스는 제외됩니다. 따라서[:3]은 인덱스 0부터 2까지의 요소를 선택합니다.
- 결과적으로query_result리스트의 첫 번째, 두 번째, 세 번째 요소가 선택됩니다.
리스트 슬라이싱 문법인[start:end]를 사용하여query_result리스트의 일부분을 추출합니다.
start인덱스는 포함되고end인덱스는 제외됩니다. 따라서[:3]은 인덱스 0부터 2까지의 요소를 선택합니다.
결과적으로query_result리스트의 첫 번째, 두 번째, 세 번째 요소가 선택됩니다.
```
Copy# 쿼리 결과의 처음 3개 항목을 선택합니다.query_result[:3]
```
```
[0.032299287617206573, -0.10691168159246445, -0.02121555432677269]
```
embeddings.embed_documents()함수를 사용하여 텍스트 문서를 임베딩합니다.
- [text]를 인자로 전달하여 단일 문서를 리스트 형태로 임베딩 함수에 전달합니다.
- 함수 호출 결과로 반환된 임베딩 벡터를doc_result변수에 할당합니다.
[text]를 인자로 전달하여 단일 문서를 리스트 형태로 임베딩 함수에 전달합니다.
함수 호출 결과로 반환된 임베딩 벡터를doc_result변수에 할당합니다.
```
Copydoc_result = embeddings.embed_documents(
    [text]
)# 텍스트를 임베딩하여 문서 벡터를 생성합니다.
```
## Hugging Face Inference API
Hugging Face Inference API를 통해 임베딩 모델에 접근할 수도 있습니다.
이 방법은 sentence_transformers를 설치하거나 모델을 로컬에 다운로드할 필요가 없다는 장점이 있습니다.
HuggingFaceInferenceAPIEmbeddings를 사용하여 텍스트를 임베딩하는 과정을 보여줍니다.
- HuggingFaceInferenceAPIEmbeddings클래스를 초기화할 때api_key와model_name을 전달합니다.
- api_key는 Hugging Face Inference API의 인증 키입니다.
- model_name은 사용할 임베딩 모델의 이름입니다. 여기서는 ""sentence-transformers/all-MiniLM-l6-v2"" 모델을 사용합니다.
- embed_query메서드를 호출하여 주어진text를 임베딩합니다.
- 임베딩 결과인query_result의 첫 3개 요소를 출력합니다.
HuggingFaceInferenceAPIEmbeddings클래스를 초기화할 때api_key와model_name을 전달합니다.
api_key는 Hugging Face Inference API의 인증 키입니다.
model_name은 사용할 임베딩 모델의 이름입니다. 여기서는 ""sentence-transformers/all-MiniLM-l6-v2"" 모델을 사용합니다.
embed_query메서드를 호출하여 주어진text를 임베딩합니다.
임베딩 결과인query_result의 첫 3개 요소를 출력합니다.
```
Copyfromlangchain_community.embeddingsimportHuggingFaceInferenceAPIEmbeddings

embeddings = HuggingFaceInferenceAPIEmbeddings(# Hugging Face Inference API 키를 설정합니다.api_key=inference_api_key,# 사용할 임베딩 모델의 이름을 지정합니다.model_name=""sentence-transformers/all-MiniLM-l6-v2"",
)# 주어진 텍스트에 대한 쿼리 임베딩을 생성합니다.query_result = embeddings.embed_query(text)# 쿼리 임베딩 결과의 첫 3개 요소를 가져옵니다.query_result[:3]
```
```
[-0.02381495200097561, 0.08707402646541595, 0.07979416847229004]
```
## Hugging Face Hub
Hugging Face Hub 패키지를 통해 로컬에서 임베딩을 생성할 수도 있습니다.
이를 위해서는huggingface_hub패키지를 설치해야 합니다.
huggingface_hub 라이브러리를 설치합니다.
- !pip install 명령어를 사용하여 huggingface_hub 라이브러리를 설치합니다.
- huggingface_hub는 Hugging Face에서 제공하는 모델, 데이터셋 등을 쉽게 액세스하고 사용할 수 있도록 도와주는 라이브러리입니다.
!pip install 명령어를 사용하여 huggingface_hub 라이브러리를 설치합니다.
huggingface_hub는 Hugging Face에서 제공하는 모델, 데이터셋 등을 쉽게 액세스하고 사용할 수 있도록 도와주는 라이브러리입니다.
```
Copy# !pip install huggingface_hub
```
- HuggingFaceHubEmbeddings클래스를langchain_community.embeddings모듈에서 임포트합니다.
- 이 클래스는 Hugging Face Hub에 호스팅된 임베딩 모델을 사용하여 텍스트를 벡터로 변환하는 기능을 제공합니다.
- HuggingFaceHubEmbeddings는 LangChain 프레임워크에서 임베딩 기능을 확장하기 위해 커뮤니티에서 개발된 모듈입니다.
HuggingFaceHubEmbeddings클래스를langchain_community.embeddings모듈에서 임포트합니다.
이 클래스는 Hugging Face Hub에 호스팅된 임베딩 모델을 사용하여 텍스트를 벡터로 변환하는 기능을 제공합니다.
HuggingFaceHubEmbeddings는 LangChain 프레임워크에서 임베딩 기능을 확장하기 위해 커뮤니티에서 개발된 모듈입니다.
```
Copyfromlangchain_community.embeddingsimportHuggingFaceHubEmbeddings
```
HuggingFaceHubEmbeddings 클래스를 사용하여 임베딩 객체를 생성합니다.
- HuggingFaceHubEmbeddings클래스는 Hugging Face Hub에 호스팅된 사전 훈련된 임베딩 모델을 사용하여 텍스트를 벡터로 변환합니다.
- embeddings변수에HuggingFaceHubEmbeddings클래스의 인스턴스를 할당합니다.
- 이렇게 생성된embeddings객체는 텍스트 데이터를 벡터로 변환하는 데 사용될 수 있습니다.
HuggingFaceHubEmbeddings클래스는 Hugging Face Hub에 호스팅된 사전 훈련된 임베딩 모델을 사용하여 텍스트를 벡터로 변환합니다.
embeddings변수에HuggingFaceHubEmbeddings클래스의 인스턴스를 할당합니다.
이렇게 생성된embeddings객체는 텍스트 데이터를 벡터로 변환하는 데 사용될 수 있습니다.
```
Copy# HuggingFaceHub에서 제공하는 임베딩 모델을 사용하여 임베딩 객체를 생성합니다.embeddings = HuggingFaceHubEmbeddings()
```
- text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
```
Copytext = (""임베딩 테스트를 하기 위한 샘플 문장입니다.""# 테스트용 문서 텍스트를 정의합니다.)
```
embeddings.embed_query(text)는 주어진 텍스트를 임베딩 벡터로 변환하는 함수입니다.
- text매개변수로 전달된 텍스트를 임베딩 모델에 입력하여 벡터 표현을 생성합니다.
- 생성된 임베딩 벡터는query_result변수에 저장됩니다.
text매개변수로 전달된 텍스트를 임베딩 모델에 입력하여 벡터 표현을 생성합니다.
생성된 임베딩 벡터는query_result변수에 저장됩니다.
이 함수는 텍스트를 벡터 공간에 매핑하여 의미적 유사성을 계산하거나 검색에 활용할 수 있는 벡터 표현을 얻는 데 사용됩니다.
```
Copy# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.query_result = embeddings.embed_query(text)
```
임베딩된 차원을 수를 확인합니다.
```
Copy# 차원을 확인합니다.len(query_result)
```
```
768
```
```
Copy# 쿼리 결과의 처음 3개 항목을 선택합니다.query_result[:3]
```
```
[0.032299287617206573, -0.10691168159246445, -0.02121555432677269]
```"
04. GPT4ALL 임베딩,https://wikidocs.net/233818,"# 04. GPT4ALL 임베딩

GPT4All은 무료로 사용할 수 있는 로컬 실행 기반의 개인정보 보호를 고려한 챗봇입니다.
GPU나 인터넷 연결이 필요하지 않으며, GPT4All Falcon, Wizard 등 인기 있는 모델과 자체 모델을 제공합니다.
이 노트북에서는 LangChain과 함께GPT4All embeddings를 사용하는 방법을 설명합니다.
## GPT4All의 Python 바인딩 설치
GPT4All의 Python 바인딩을 설치하려면 다음 명령을 실행하세요
```
Copy%pip install --upgrade --quiet  gpt4all > /dev/null
```
- GPT4AllEmbeddings클래스를langchain_community.embeddings모듈에서 임포트합니다.
GPT4AllEmbeddings클래스를langchain_community.embeddings모듈에서 임포트합니다.
GPT4AllEmbeddings는 GPT4All 모델을 사용하여 텍스트 데이터를 벡터로 임베딩하는 기능을 제공하는 클래스입니다. 이 클래스는 LangChain 프레임워크의 임베딩 인터페이스를 구현하여, LangChain의 다양한 기능과 함께 사용할 수 있습니다.
```
Copyfromlangchain_community.embeddingsimportGPT4AllEmbeddings
```
GPT4All은 CPU에 최적화된 대조 학습 문장 변환기를 사용하여 임의 길이의 텍스트 문서에 대한 고품질 임베딩 생성을 지원합니다. 이러한 임베딩은 OpenAI를 사용하는 많은 작업에서 품질이 비슷합니다.
GPT4AllEmbeddings클래스의 인스턴스를 생성합니다.
- GPT4AllEmbeddings는 GPT4All 모델을 사용하여 텍스트 데이터를 벡터로 변환하는 임베딩 모델입니다.
- 이 코드에서는gpt4all_embd변수에GPT4AllEmbeddings인스턴스를 할당합니다.
- 이후gpt4all_embd를 사용하여 텍스트 데이터를 벡터로 변환할 수 있습니다.
GPT4AllEmbeddings는 GPT4All 모델을 사용하여 텍스트 데이터를 벡터로 변환하는 임베딩 모델입니다.
이 코드에서는gpt4all_embd변수에GPT4AllEmbeddings인스턴스를 할당합니다.
이후gpt4all_embd를 사용하여 텍스트 데이터를 벡터로 변환할 수 있습니다.
```
Copygpt4all_embd = GPT4AllEmbeddings()# GPT4All 임베딩 객체를 생성합니다.
```
```
100%|██████████| 45.9M/45.9M [00:05<00:00, 8.75MiB/s]
```
```
bert_load_from_file: gguf version     = 2
bert_load_from_file: gguf alignment   = 32
bert_load_from_file: gguf data offset = 695552
bert_load_from_file: model name           = BERT
bert_load_from_file: model architecture   = bert
bert_load_from_file: model file type      = 1
bert_load_from_file: bert tokenizer vocab = 30522
```
```

```
```
Copygpt4all_embd = GPT4AllEmbeddings()# GPT4All 임베딩 객체를 생성합니다.
```
- text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
text변수에 ""임베딩 테스트를 하기 위한 샘플 문장입니다."" 라는 문자열을 할당합니다.
```
Copytext = (""임베딩 테스트를 하기 위한 샘플 문장입니다.""# 테스트용 문서 텍스트를 정의합니다.)
```
## Embed the Textual Data
텍스트 데이터를 임베딩하는 과정은 다음과 같습니다.
먼저, 텍스트 데이터를 토큰화하여 숫자 형태로 변환합니다.
이때, 사전 학습된 토크나이저(tokenizer)를 활용하여 텍스트를 토큰 단위로 분리하고, 각 토큰을 고유한 정수로 매핑합니다.
다음으로, 토큰화된 데이터를 임베딩 레이어에 입력하여 고차원의 밀집 벡터(dense vector) 형태로 변환합니다.
이 과정에서 각 토큰은 해당 토큰의 의미와 문맥을 포착하는 실수 값들의 벡터로 표현됩니다.
마지막으로, 임베딩된 벡터는 다양한 자연어 처리 작업에 활용될 수 있습니다.
예를 들어, 문서 분류, 감성 분석, 기계 번역 등의 작업에서 입력 데이터로 사용되어 모델의 성능을 향상시킬 수 있습니다.
이러한 텍스트 데이터 임베딩 과정은 자연어 처리 분야에서 매우 중요한 역할을 하며, 대량의 텍스트 데이터를 효과적으로 처리하고 분석하는 데 필수적입니다.
gpt4all_embd객체의embed_query메서드를 사용하여 주어진 텍스트(text)를 임베딩합니다.
- text변수에 임베딩할 텍스트가 저장되어 있습니다.
- gpt4all_embd객체는 GPT4All 모델을 사용하여 텍스트 임베딩을 수행하는 객체입니다.
- embed_query메서드는 주어진 텍스트를 벡터 형태로 변환하여 반환합니다.
- 임베딩 결과는query_result변수에 저장됩니다.
text변수에 임베딩할 텍스트가 저장되어 있습니다.
gpt4all_embd객체는 GPT4All 모델을 사용하여 텍스트 임베딩을 수행하는 객체입니다.
embed_query메서드는 주어진 텍스트를 벡터 형태로 변환하여 반환합니다.
임베딩 결과는query_result변수에 저장됩니다.
```
Copyquery_result = gpt4all_embd.embed_query(
    text
)# 주어진 텍스트에 대한 쿼리 임베딩을 생성합니다.
```
embed_documents 함수를 사용하면 여러 개의 텍스트 조각을 임베딩할 수 있습니다.
또한 이러한 임베딩을 Nomic의 Atlas(https://docs.nomic.ai/index.html)와 매핑하여 데이터의 시각적 표현을 확인할 수 있습니다.
임베딩된 차원의 크기를 확인합니다.
```
Copy# 임베딩된 차원의 크기를 확인합니다.len(query_result)
```
```
384
```
gpt4all_embd객체의embed_documents메서드를 사용하여text문서를 임베딩합니다.
- text문서를 리스트로 감싸서embed_documents메서드의 인자로 전달합니다.
- embed_documents메서드는 문서의 임베딩 벡터를 계산하여 반환합니다.
- 반환된 임베딩 벡터는doc_result변수에 저장됩니다.
text문서를 리스트로 감싸서embed_documents메서드의 인자로 전달합니다.
embed_documents메서드는 문서의 임베딩 벡터를 계산하여 반환합니다.
반환된 임베딩 벡터는doc_result변수에 저장됩니다.
```
Copy# 주어진 텍스트를 임베딩하여 문서 벡터를 생성합니다.doc_result = gpt4all_embd.embed_documents([text])
```
```
Copy# 임베딩된 차원의 크기를 확인합니다.len(doc_result[0])
```
```
384
```"
05. Llama CPP 임베딩,https://wikidocs.net/233819,"# 05. Llama CPP 임베딩

작성중"
CH10 벡터저장소(VectorStore),https://wikidocs.net/233778,"# CH10 벡터저장소(VectorStore)

벡터스토어 저장 단계는 Retrieval-Augmented Generation(RAG) 시스템의 네 번째 단계로, 이전 단계에서생성된 임베딩 벡터들을 효율적으로 저장하고 관리하는 과정입니다. 이 단계는 향후검색 과정에서 벡터들을 빠르게 조회하고,관련 문서를 신속하게 찾아내는 데 필수적입니다.
## 벡터스토어 저장의 필요성
빠른 검색 속도: 임베딩 벡터들을 효과적으로 저장하고 색인화함으로써, 대량의 데이터 중에서도관련된 정보를 빠르게 검색할 수 있습니다.
스케일러빌리티: 데이터가 지속적으로 증가함에 따라,벡터스토어는 이를 수용할 수 있는 충분한 스케일러빌리티를 제공해야 합니다. 효율적인 저장 구조는 데이터베이스의 확장성을 보장하며, 시스템의 성능 저하 없이 대규모 데이터를 관리할 수 있도록 합니다.
의미 검색(Semantic Search)지원: 키워드 기반 검색이 아닌 사용자의 질문과 의미상으로 유사한 단락을 조회해야하는데, 벡터스토어는 이러한 기능을 지원합니다. 텍스트 자체가 저장되는 DB의 경우 키워드 기반 검색에 의존해야 하는 한계성이 있지만, 벡터스토어는 의미적으로 유사한 단락 검색을 가능케합니다.
(예시)
질문: ""모바일 디바이스 상에서 동작하는 인공지능 기술을 소개한 기업명은?""

## 벡터스토어 중요성
벡터스토어 저장 단계는RAG 시스템의 검색 기능과 직접적으로 연결되어 있으며, 전체 시스템의응답 시간과 정확성에 큰 영향을 미칩니다. 이 단계를 통해 데이터가 잘 관리되고, 필요할 때 즉시 접근할 수 있도록 함으로써, 사용자에게 신속하고 정확한 정보를 제공할 수 있습니다.
## 코드
```
Copyfromlangchain_community.vectorstoresimportFAISS# 단계 4: DB 생성(Create DB) 및 저장# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)
```
## 참고
- 벡터스토어 사용방법
- LangChain VectorStores
벡터스토어 사용방법
LangChain VectorStores"
01. 벡터저장소(VectorStore) 사용법 톺아보기,https://wikidocs.net/234013,"# 01. 벡터저장소(VectorStore) 사용법 톺아보기

```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```

이번 튜토리얼은 벡터 스토어와 관련된 기본 기능을 소개합니다. 벡터 스토어 작업의 핵심은 벡터 스토어에 넣을 벡터를 만드는 것이며, 이는 일반적으로 임베딩을 통해 만들어집니다. 따라서 이 작업을 시작하기 전에 텍스트 임베딩 모델 인터페이스에 익숙해지는 것이 좋습니다.
훌륭한 벡터 저장소 옵션이 많이 있으며, 여기에는 무료 오픈 소스이며 로컬 컴퓨터에서 완전히 실행되는 몇 가지 옵션이 있습니다. 모든 통합을 검토하여 여러 가지 훌륭한 호스팅 서비스를 살펴보세요.
참고링크
- LangChain 지원 VectorStore 리스트
LangChain 지원 VectorStore 리스트
## ① Chroma
크로마는 오픈소스 벡터 데이터베이스입니다. Chroma는 Apache 2.0 라이선스가 부여됩니다.
- 공식 도큐먼트
- 홈페이지
공식 도큐먼트
홈페이지
Apache 2.0 License 에 대해
Apache 2.0 라이선스는 오픈 소스 라이선스 중 하나로,누구나 소프트웨어를 자유롭게 사용, 수정, 배포할 수 있도록 허용하는 라이선스입니다.
이 라이선스의 주요 특징은 다음과 같습니다:
- 상업적 사용 가능: Apache 2.0 라이선스 하에 배포된 소프트웨어는상업적 목적을 포함하여 어떤 목적으로든 사용할 수 있습니다. 이는 기업이나 개인이 이 라이선스로 된 소프트웨어를 상업적 제품이나 서비스에 통합하는 것을 가능하게 합니다.
- 소스 코드 공개 의무 없음: 소프트웨어를 수정하거나 확장한 경우, 그 변경된 소스 코드를 공개할 의무가 없습니다. 하지만 원본 소프트웨어가 Apache 2.0 라이선스를 따른다면, 변경된 버전 역시 Apache 2.0 라이선스 하에 배포되어야 합니다.
상업적 사용 가능: Apache 2.0 라이선스 하에 배포된 소프트웨어는상업적 목적을 포함하여 어떤 목적으로든 사용할 수 있습니다. 이는 기업이나 개인이 이 라이선스로 된 소프트웨어를 상업적 제품이나 서비스에 통합하는 것을 가능하게 합니다.
상업적 사용 가능: Apache 2.0 라이선스 하에 배포된 소프트웨어는상업적 목적을 포함하여 어떤 목적으로든 사용할 수 있습니다. 이는 기업이나 개인이 이 라이선스로 된 소프트웨어를 상업적 제품이나 서비스에 통합하는 것을 가능하게 합니다.
소스 코드 공개 의무 없음: 소프트웨어를 수정하거나 확장한 경우, 그 변경된 소스 코드를 공개할 의무가 없습니다. 하지만 원본 소프트웨어가 Apache 2.0 라이선스를 따른다면, 변경된 버전 역시 Apache 2.0 라이선스 하에 배포되어야 합니다.
소스 코드 공개 의무 없음: 소프트웨어를 수정하거나 확장한 경우, 그 변경된 소스 코드를 공개할 의무가 없습니다. 하지만 원본 소프트웨어가 Apache 2.0 라이선스를 따른다면, 변경된 버전 역시 Apache 2.0 라이선스 하에 배포되어야 합니다.
```
Copy# 경고 메시지 무시importwarnings

warnings.filterwarnings(""ignore"")
```
```
Copyfromlangchain_community.document_loadersimportTextLoaderfromlangchain_openai.embeddingsimportOpenAIEmbeddingsfromlangchain.text_splitterimportCharacterTextSplitterfromlangchain_community.vectorstoresimportChroma# 텍스트를 600자 단위로 분할text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)# TextLoader 를 통해 텍스트 파일을 로드split_docs = TextLoader(""data/appendix-keywords.txt"").load_and_split(text_splitter)# Chroma 를 통해 벡터 저장소 생성chroma_db = Chroma.from_documents(split_docs, OpenAIEmbeddings())
```
query에 질문을 담아서db에서 유사한 문장을 찾아냅니다.
```
Copy# 유사도 검색(쿼리)similar_docs = chroma_db.similarity_search(""TF IDF 에 대하여 알려줘"")print(similar_docs[0].page_content)
```
```
정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.
예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.
연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝

Deep Learning

정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.
예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.
연관키워드: 인공신경망, 머신러닝, 데이터 분석

Schema

정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.
예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.
연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리

DataFrame
```
다음은OpenAIEmbeddings을 사용하여query를 임베딩하고,Chroma에서 유사도 검색을 수행한 결과입니다.
이전에 수행한 코드와 결과는 동일합니다.
```
Copy# embedding_Vector 를 통해 유사도 검색embedding_vector = OpenAIEmbeddings().embed_query(""TF IDF 에 대하여 알려줘"")
similar_docs = chroma_db.similarity_search_by_vector(embedding_vector)print(similar_docs[0].page_content)
```
```
정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.
예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.
연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝

Deep Learning

정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.
예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.
연관키워드: 인공신경망, 머신러닝, 데이터 분석

Schema

정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.
예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.
연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리

DataFrame
```
```
Copy# 유사도 검색(쿼리)similar_docs = chroma_db.similarity_search(""Word2Vec 에 대하여 알려줘"")print(similar_docs[0].page_content)
```
```
정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)

정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.
예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.
연관키워드: 자연어 처리, 딥러닝, 텍스트 생성

FAISS (Facebook AI Similarity Search)

정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.
예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.
연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화

Open Source
```
## VectorStoreRetriever: as_retreiver()
VectorStoreRetreiver는 벡터 저장소를 사용하여 문서를 검색하는 Retriever 입니다.
이는 벡터 저장소 클래스를 경량 래퍼로 감싸서 리트리버 인터페이스를 따르도록 하는 것입니다.
유사성 검색 및 MMR과 같이 벡터 저장소가 구현한 검색 방법을 사용하여 벡터 저장소의 텍스트를 쿼리합니다.
생성된 VectorStore 에 as_retriever() 함수로 반환받습니다.
```
Copy# retriever 생성retriever = chroma_db.as_retriever()# similarity_search 를 통해 유사도 높은 1개 문서를 검색relevant_docs = retriever.get_relevant_documents(""VectorStore 에 대하여 알려줘"")print(f""문서의 개수:{len(relevant_docs)}"")print(""[검색 결과]\n"")print(relevant_docs[0].page_content)
```
```
문서의 개수: 4
[검색 결과]

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.
연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.
연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV
```
```
Copy# retriever 생성retriever = chroma_db.as_retriever(search_kwargs={""k"":1})# similarity_search 를 통해 유사도 높은 1개 문서를 검색relevant_docs = retriever.get_relevant_documents(""VectorStore 에 대하여 알려줘"")print(f""문서의 개수:{len(relevant_docs)}"")print(""[검색 결과]\n"")print(relevant_docs[0].page_content)
```
```
문서의 개수: 1
[검색 결과]

정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.
연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.
연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV
```
search_type매개변수에 검색 알고리즘을 지정할 수 있습니다.
- similarity(기본값),mmr,similarity_score_threshold등의 옵션을 지정할 수 있습니다.
- 아래에서 지정 옵션 별 차이에 대해 다룹니다.
similarity(기본값),mmr,similarity_score_threshold등의 옵션을 지정할 수 있습니다.
아래에서 지정 옵션 별 차이에 대해 다룹니다.
search_type='similarity'
기본 값으로 설정되어 있습니다. vector store 의 유사도 알고리즘 기반으로 상위 K개의 문서를 검색합니다.
```
Copyretriever = chroma_db.as_retriever(search_type=""similarity"", search_kwargs={""k"":2})
relevant_docs = retriever.get_relevant_documents(""VectorStore, TF IDF 에 대하여 알려줘"")print(f""문서의 개수:{len(relevant_docs)}"")print(""[검색 결과]\n"")foriinrange(len(relevant_docs)):print(relevant_docs[i].page_content)print(""===""*20)
```
```
문서의 개수: 2
[검색 결과]

정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.
예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.
연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝

Deep Learning

정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.
예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.
연관키워드: 인공신경망, 머신러닝, 데이터 분석

Schema

정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.
예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.
연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리

DataFrame
============================================================
정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.
연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.
연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV
============================================================
```
Maximal Marginal Relevance(MMR) 검색
MMR은 쿼리와 관련된 항목을 검색하면서 동시에 내용의 중복을 최소화하는 기법입니다.
이 방법은 단순히 관련성이 높은 항목들을 선택하는 대신,관련성과 다양성 사이의 균형을 찾는 데 중점을 둡니다.
MMR 에 대해 이해해보기
모임에 참석해 친구를 위해 새로운 사람들을 소개하는 상황을 상상해 보세요. 친구가 만나고 싶어하는 사람들의 특징(embedding vector)을 알고 있습니다.
MMR 방식을 적용하면 다음과 같은 접근법을 사용합니다:
모임에 있는 모든 사람들의 프로필(list of embedding vector)을 살펴봅니다.
친구의 관심사와 특성에 부합하는 사람을 찾아 소개합니다.
그 후, 다시 참여한 사람들을 살펴보되, 이번에는 이미 소개한 사람과는 다른 특성을 가진 사람을 찾습니다.
여기서 lambda mult 매개변수는 친구의 취향과 새로운 특성 사이의 균형을 조정하는 역할을 합니다.
이 과정을 친구가 만나길 원하는 사람들의 수(k 매개변수)에 도달할 때까지 반복합니다.
마지막으로, 친구가 만난 사람들의 목록을 제공합니다.
이 예시를 통해 MMR이관련성 높은 항목을 선택하는 동시에내용의 다양성을 유지하려는 방식을 이해할 수 있습니다.
search_type='mmr'을 통해 MMR 을 사용한 검색을 수행할 수 있습니다.
```
Copyretriever = chroma_db.as_retriever(search_type=""mmr"", search_kwargs={""k"":2})
relevant_docs = retriever.get_relevant_documents(""VectorStore, TF IDF 에 대하여 알려줘"")print(f""문서의 개수:{len(relevant_docs)}"")print(""[검색 결과]\n"")foriinrange(len(relevant_docs)):print(relevant_docs[i].page_content)print(""===""*20)
```
```
Number of requested results 20 is greater than number of elements in index 11, updating n_results = 11
```
```
문서의 개수: 2
[검색 결과]

정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.
예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.
연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝

Deep Learning

정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.
예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.
연관키워드: 인공신경망, 머신러닝, 데이터 분석

Schema

정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.
예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.
연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리

DataFrame
============================================================
정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.
연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.
연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV
============================================================
```
search_type='similarity_score_threshold'을 지정하면score_threshold기준을 충족하는 유사도 문서가 반환됩니다.
만약{""k"": 3}이지만,{""score_threshold"": 0.5}로 설정되었는데 score 가 0.5를 넘는 문서가 2개 밖에 없다면, 결과는 3개 문서가 아닌 2개 문서가 반환됩니다.
```
Copyretriever = chroma_db.as_retriever(
    search_type=""similarity_score_threshold"",
    search_kwargs={""k"":2,""score_threshold"":0.5},
)
relevant_docs = retriever.get_relevant_documents(""VectorStore, TF IDF 에 대하여 알려줘"")print(f""문서의 개수:{len(relevant_docs)}"")print(""[검색 결과]\n"")foriinrange(len(relevant_docs)):print(relevant_docs[i].page_content)print(""===""*20)
```
```
문서의 개수: 2
[검색 결과]

정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.
예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.
연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝

Deep Learning

정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.
예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.
연관키워드: 인공신경망, 머신러닝, 데이터 분석

Schema

정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.
예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.
연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리

DataFrame
============================================================
정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.
연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.
연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV
============================================================
```
search_kwargs에 다양한 옵션을 지정할 수 있습니다.
다음의search_kwargs로 Retriever 의 검색 결과를 세부조정할 수 있습니다.
- k:""k""매개변수로 찾을 문서의 개수를 지정할 수 있습니다. 만약,{""k"": 3}지정한다면 3개의 유사도 높은 문서만 선택하겠다는 의미 입니다. 기본 값은 1입니다.
- score_threshold: Minimum relevance threshold forsimilarity_score_threshold
- fetch_k: Amount of documents to pass toMMRalgorithm (Default: 20)
- lambda_mult: Diversity of results returned byMMR. 1 for minimum diversity and 0 for maximum. (Default: 0.5)
- filter: document를 메타데이터 기준으로 필터링 합니다. (예시)search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}
k:""k""매개변수로 찾을 문서의 개수를 지정할 수 있습니다. 만약,{""k"": 3}지정한다면 3개의 유사도 높은 문서만 선택하겠다는 의미 입니다. 기본 값은 1입니다.
score_threshold: Minimum relevance threshold forsimilarity_score_threshold
fetch_k: Amount of documents to pass toMMRalgorithm (Default: 20)
lambda_mult: Diversity of results returned byMMR. 1 for minimum diversity and 0 for maximum. (Default: 0.5)
filter: document를 메타데이터 기준으로 필터링 합니다. (예시)search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}
```
Copyretriever = chroma_db.as_retriever(
    search_type=""mmr"", search_kwargs={""k"":2,""fetch_k"":10,""lambda_mult"":0.75}
)
relevant_docs = retriever.get_relevant_documents(""VectorStore, TF IDF 에 대하여 알려줘"")print(f""문서의 개수:{len(relevant_docs)}"")print(""[검색 결과]\n"")foriinrange(len(relevant_docs)):print(relevant_docs[i].page_content)print(""===""*20)
```
```
문서의 개수: 2
[검색 결과]

정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.
예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.
연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝

Deep Learning

정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.
예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.
연관키워드: 인공신경망, 머신러닝, 데이터 분석

Schema

정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.
예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.
연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리

DataFrame
============================================================
정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.
예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.
연관키워드: 토큰화, 자연어 처리, 구문 분석

VectorStore

정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.
예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.
연관키워드: 임베딩, 데이터베이스, 벡터화

SQL

정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.
예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.
연관키워드: 데이터베이스, 쿼리, 데이터 관리

CSV
============================================================
```
## ② FAISS
FAISS는 semantic search를 도와주는 Meta(구 Facebook)에서 만든 라이브러리 입니다.
- 링크: https://github.com/facebookresearch/faiss
링크: https://github.com/facebookresearch/faiss
리이선스는MIT License입니다.
[참고]
MIT 라이선스는 자유롭고 유연한 소프트웨어 라이선스 중 하나입니다. MIT 라이선스 하에 배포된 소프트웨어는상업적으로 이용할 수 있습니다
주요 특징은 다음과 같습니다:
- 간결함과 넓은 범위의 사용 허가: MIT 라이선스는 매우 간결하며, 사용자에게 소프트웨어를 거의 제한 없이 사용, 복사, 수정, 합병, 출판, 배포, 하위 라이선스 부여, 판매할 수 있는 권리를 부여합니다.
- 저작권과 라이선스 고지 유지 요구: 라이선스는 사용자가 MIT 라이선스하에 배포된 모든 복사본과 상당한 부분에 원래의 저작권 고지와 이 라이선스 고지를 포함하도록 요구합니다.
- 책임의 부인: MIT 라이선스는 소프트웨어가 '있는 그대로' 제공되며, 소프트웨어의 사용으로 인한 어떠한 보증도 제공하지 않습니다. 이는 소프트웨어 사용으로 인한 모든 위험은 사용자가 부담한다는 것을 의미합니다.
간결함과 넓은 범위의 사용 허가: MIT 라이선스는 매우 간결하며, 사용자에게 소프트웨어를 거의 제한 없이 사용, 복사, 수정, 합병, 출판, 배포, 하위 라이선스 부여, 판매할 수 있는 권리를 부여합니다.
간결함과 넓은 범위의 사용 허가: MIT 라이선스는 매우 간결하며, 사용자에게 소프트웨어를 거의 제한 없이 사용, 복사, 수정, 합병, 출판, 배포, 하위 라이선스 부여, 판매할 수 있는 권리를 부여합니다.
저작권과 라이선스 고지 유지 요구: 라이선스는 사용자가 MIT 라이선스하에 배포된 모든 복사본과 상당한 부분에 원래의 저작권 고지와 이 라이선스 고지를 포함하도록 요구합니다.
저작권과 라이선스 고지 유지 요구: 라이선스는 사용자가 MIT 라이선스하에 배포된 모든 복사본과 상당한 부분에 원래의 저작권 고지와 이 라이선스 고지를 포함하도록 요구합니다.
책임의 부인: MIT 라이선스는 소프트웨어가 '있는 그대로' 제공되며, 소프트웨어의 사용으로 인한 어떠한 보증도 제공하지 않습니다. 이는 소프트웨어 사용으로 인한 모든 위험은 사용자가 부담한다는 것을 의미합니다.
책임의 부인: MIT 라이선스는 소프트웨어가 '있는 그대로' 제공되며, 소프트웨어의 사용으로 인한 어떠한 보증도 제공하지 않습니다. 이는 소프트웨어 사용으로 인한 모든 위험은 사용자가 부담한다는 것을 의미합니다.
MIT 라이선스는 그 간결함과 유연성으로 인해 많은 오픈 소스 프로젝트와 상업적 프로젝트에서 널리 사용됩니다. 이 라이선스는 소프트웨어 개발자가 자신의 소프트웨어를 거의 제한 없이 공유할 수 있도록 해주며, 소프트웨어 산업 전반에 걸쳐 협력과 혁신을 촉진하는 데 기여하고 있습니다.
API 사용방법은 Chroma와 동일합니다. (동일한 인터페이스를 제공합니다.)
```
Copyfromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.embeddingsimportOpenAIEmbeddingsfromlangchain.text_splitterimportCharacterTextSplitterfromlangchain_community.vectorstoresimportFAISS# TextSplitter 를 통해 텍스트를 500자 단위로 분할text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)# TextLoader 를 통해 텍스트 파일을 로드split_docs = TextLoader(""data/appendix-keywords.txt"").load_and_split(text_splitter)# FAISS 를 통해 벡터 저장소 생성faiss_db = FAISS.from_documents(split_docs, OpenAIEmbeddings())
```
```
Copy# 유사도 검색(쿼리)similar_docs = faiss_db.similarity_search(""Embedding 에 대한 내용을 알려줘"")print(f""문서의 개수:{len(similar_docs)}"")print(""[검색 결과]\n"")print(similar_docs[0].page_content)
```
```
문서의 개수: 4
[검색 결과]

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token
```"
02. FAISS,https://wikidocs.net/234014,"# 02. FAISS

Facebook AI Similarity Search (Faiss)는 밀집 벡터의 효율적인 유사도 검색과 클러스터링을 위한 라이브러리입니다.
Faiss는 RAM에 맞지 않을 수도 있는 벡터 집합을 포함하여 모든 크기의 벡터 집합을 검색하는 알고리즘을 포함하고 있습니다.
또한 평가와 매개변수 튜닝을 위한 지원 코드도 포함되어 있습니다.
FAISS 문서를 참조하세요.
## 설치
langchain-community 패키지에 통합 기능이 포함되어 있습니다. 또한 faiss 패키지 자체도 설치해야 합니다.
임베딩에는OpenAIEmbeddings를 사용할 예정이므로 해당 요구사항도 설치해야 합니다.
GPU 지원 버전을 사용하려면 faiss-gpu를 설치할 수도 있습니다.
```
Copy%pip install -U langchain-community faiss-cpu langchain-openai tiktoken
```
```
Copyimportos# 사용자로부터 OpenAI API 키를 입력받아 환경 변수로 설정합니다.os.environ[""OPENAI_API_KEY""] =""OPENAI API KEY 입력""
```
혹은dot_env라이브러리를 사용하여.env파일을 만들어 환경 변수로 설정할 수 있습니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
추적을 위한 LangSmith 설정(필수는 아닙니다.)
```
Copy# LANGCHAIN_TRACING_V2 환경 변수를 ""true""로 설정합니다.# os.environ[""LANGCHAIN_TRACING_V2""] = ""true""# LANGCHAIN_API_KEY 환경 변수를 getpass.getpass() 함수를 통해 입력받은 값으로 설정합니다.# os.environ[""LANGCHAIN_API_KEY""] = ""LANGCHAIN API KEY 입력""
```
## DB에 저장
여기에서는 문서를 벡터 저장소에 수집합니다.
- TextLoader를 사용하여 텍스트 데이터를 로드합니다.
- CharacterTextSplitter를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.
- OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.
- FAISS벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.
TextLoader를 사용하여 텍스트 데이터를 로드합니다.
CharacterTextSplitter를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.
OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.
FAISS벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.
참고
- AVX2: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우AVX2를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.
- 필요한 경우os.environ['FAISS_NO_AVX2'] = '1'코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다.
AVX2: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우AVX2를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.
필요한 경우os.environ['FAISS_NO_AVX2'] = '1'코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다.
```
Copyfromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportCharacterTextSplitter# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.# import os## os.environ['FAISS_NO_AVX2'] = '1'# TextLoader를 사용하여 텍스트 파일을 로드합니다.loader = TextLoader(""./data/appendix-keywords.txt"")# 로드된 문서를 가져옵니다.documents = loader.load()# CharacterTextSplitter를 사용하여 문서를 분할합니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)# 분할된 문서를 가져옵니다.docs = text_splitter.split_documents(documents)# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.embeddings = OpenAIEmbeddings()# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.db = FAISS.from_documents(docs, embeddings)
```
## 쿼리
DB 에 데이터를 저장했으므로, 이제 우리는 vectorstore를 쿼리할 수 있습니다.
이를 수행하는 몇 가지 방법이 있는데, 가장 일반적인 방법은similarity_search를 사용하는 것입니다.
similarity_search메서드는 주어진 쿼리와 유사한 벡터를 vectorstore에서 검색하여 가장 유사한 결과를 반환합니다.
쿼리를 위하여 일부 파일의 내용을 확인합니다.
```
Copy# data/appendix-keywords.txt 파일 내용을 읽어서 file 변수에 저장합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# file 변수에 저장된 내용을 출력합니다.print(file[:500])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.
연관키워드: 토큰화, 자연어
```
db.similarity_search(query)를 호출하여 질문과 유사한 문서를 검색합니다.
```
Copy# 쿼리(query) 변수에 저장된 질문과 유사한 문서를 데이터베이스에서 검색합니다.query =""임베딩(Embedding)이란 무엇인가요?""docs = db.similarity_search(query)# 질문과 유사한 문서를 데이터베이스에서 검색
```
쿼리에 대한 유사도가 가장 높은 문서를 출력합니다.
```
Copy# docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.print(docs[0].page_content)
```
```
정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token
```
## Retriever로 활용
vectorstore를Retriever클래스로 변환할 수도 있습니다.
db.as_retriever()메서드를 호출하여 데이터베이스를 검색기(retriever)로 사용할 수 있는 객체를 생성합니다.
- 이 메서드는 데이터베이스 객체db를 검색기로 변환합니다.
- 반환된retriever객체는 질의에 대한 관련 문서를 데이터베이스에서 검색하는 데 사용됩니다.
이 메서드는 데이터베이스 객체db를 검색기로 변환합니다.
반환된retriever객체는 질의에 대한 관련 문서를 데이터베이스에서 검색하는 데 사용됩니다.
```
Copy# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당합니다.retriever = db.as_retriever()
```
retriever.invoke(query)는 검색 질의query를 사용하여retriever객체에서 관련 문서를 검색합니다.
- retriever는 벡터 저장소 또는 검색 엔진과 같은 문서 검색 시스템을 나타내는 객체입니다.
- invoke메서드는retriever객체에 정의된 검색 기능을 호출하여 질의에 관련된 문서를 검색합니다.
retriever는 벡터 저장소 또는 검색 엔진과 같은 문서 검색 시스템을 나타내는 객체입니다.
invoke메서드는retriever객체에 정의된 검색 기능을 호출하여 질의에 관련된 문서를 검색합니다.
```
Copy# 검색 질의를 사용하여 관련 문서를 검색합니다.query =""임베딩(Embedding)이란 무엇인가요?""docs = retriever.invoke(query)
```
반환된 결과를 확인합니다.
```
Copy# docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.print(docs[0].page_content)
```
```
정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token
```
## 점수에 기반한 유사도 검색
FAISS에는 몇 가지 특정 메서드가 있습니다.
그 중 하나는similarity_search_with_score로, 문서뿐만 아니라 쿼리와 문서 간의 거리 점수도 반환할 수 있습니다.
반환되는 거리 점수는 L2 거리입니다. 따라서점수가 낮을수록 더 좋은 결과입니다.
db.similarity_search_with_score(query)메서드를 사용하여 질의와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.
- 반환된 결과는 (문서, 점수) 튜플의 리스트 형태로 구성됩니다.
반환된 결과는 (문서, 점수) 튜플의 리스트 형태로 구성됩니다.
```
Copy# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.docs_and_scores = db.similarity_search_with_score(query)
content, score = docs_and_scores[0]# 문서와 점수 리스트에서 첫 번째 요소를 선택합니다print(""[Content]"")print(content.page_content)# 선택된 문서의 page_content 속성을 출력합니다print(""\n[Score]"")print(score)# 선택된 문서의 점수를 출력합니다
```
```
[Content]
정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

[Score]
0.29661512
```
similarity_search_by_vector함수를 사용하면 주어진 임베딩 벡터와 유사한 문서를 검색할 수 있습니다.
이 함수는 문자열 대신임베딩 벡터를 매개변수로 받아들입니다.
```
Copy# 질의를 임베딩 벡터로 변환합니다.query =""임베딩(Embedding)이란 무엇인가요?""embedding_vector = embeddings.embed_query(query)# 임베딩 벡터를 사용하여 유사도 검색을 수행하고, 문서와 점수를 반환합니다.docs_and_scores = db.similarity_search_by_vector(embedding_vector)
docs_and_scores[0]# 문서와 점수 리스트에서 첫 번째 요소를 선택합니다
```
```
Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'})
```
## 저장 및 로드
FAISS 인덱스를 저장하고 불러올 수도 있습니다.
이는 인덱스를 사용할 때마다 매번 다시 생성할 필요가 없어 유용합니다.
- db.save_local(""저장할 인덱스 이름"")를 사용하여 FAISS 인덱스를 로컬 디렉토리에 저장합니다.
db.save_local(""저장할 인덱스 이름"")를 사용하여 FAISS 인덱스를 로컬 디렉토리에 저장합니다.
```
Copy# 로컬에 ""MY_FIRST_DB_INDEX""라는 이름으로 데이터베이스를 저장합니다.DB_INDEX =""MY_FIRST_DB_INDEX""db.save_local(DB_INDEX)
```
- FAISS.load_local(""불러올 인덱스 이름"", embeddings)를 사용하여 저장된 FAISS 인덱스를 로드합니다.
FAISS.load_local(""불러올 인덱스 이름"", embeddings)를 사용하여 저장된 FAISS 인덱스를 로드합니다.
```
Copy# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.new_db = FAISS.load_local(DB_INDEX, embeddings,
                          allow_dangerous_deserialization=True)

query =""임베딩(Embedding)이란 무엇인가요?""# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.docs = new_db.similarity_search(query)# 문서 리스트의 첫 번째 문서를 가져옵니다.docs[0]
```
```
Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'})
```
FAISS 인덱스를 직렬화(Serializing)하고 역직렬화(De-Serializing)하는 데에는pickle함수를 사용하여 저장할 수 있습니다. 하지만, 만약 90MB 크기의 임베딩 모델(예:sentence-transformers/all-MiniLM-L6-v2또는 다른 모델)을 사용한다면, 결과로 생성되는 pickle 파일의 크기는 90MB 이상이 될 것입니다. 이는 모델의 크기 또한 전체 크기에 포함되기 때문입니다.
따라서, 아래의 함수들을 사용하는 것이 좋습니다. 이 함수들은FAISS 인덱스만 직렬화하므로 크기가 훨씬 작아집니다.
```
Copyfromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportFAISSfromlangchain_text_splittersimportCharacterTextSplitterfromlangchain_community.embeddings.huggingfaceimportHuggingFaceEmbeddings# TextLoader를 사용하여 텍스트 파일을 로드합니다.loader = TextLoader(""./data/appendix-keywords.txt"")# 로드된 문서를 가져옵니다.documents = loader.load()# CharacterTextSplitter를 사용하여 문서를 분할합니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)# 분할된 문서를 가져옵니다.docs = text_splitter.split_documents(documents)# HuggingFaceEmbeddings 사용하여 임베딩을 생성합니다.hf_embeddings = HuggingFaceEmbeddings(model_name=""all-MiniLM-L6-v2"")# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.db = FAISS.from_documents(docs, hf_embeddings)
```
serialize_to_bytes함수로 DB 인덱스를 직렬화합니다.
```
Copy# FAISS 인덱스를 직렬화합니다.serialized_db_index = db.serialize_to_bytes()
```
직렬화된serialized_db_index의 사이즈를 확인합니다.
```
Copyimportsys# 사이즈 측정을 위한 함수를 정의합니다.defget_size(path):
    size = sys.getsizeof(path)ifsize <1024:returnf""{size}bytes""elifsize <pow(1024,2):returnf""{round(size/1024,2)}KB""elifsize <pow(1024,3):returnf""{round(size/(pow(1024,2)),2)}MB""elifsize <pow(1024,4):returnf""{round(size/(pow(1024,3)),2)}GB""# 직렬화된 FAISS 인덱스의 사이즈를 출력합니다.get_size(serialized_db_index)
```
```
'60.81 KB'
```
이번에는deserialize_from_bytes함수로 역직렬화를 수행합니다.
```
Copy# 직렬화된 인덱스를 로드합니다.deserialized_db = FAISS.deserialize_from_bytes(
    embeddings=hf_embeddings,# 직렬화 할 때의 임베딩과 동일하게 지정serialized=serialized_db_index,# 직렬화된 인덱스)
```
```
Copyquery =""임베딩(Embedding)이란 무엇인가요?""# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.docs = deserialized_db.similarity_search(query)# 문서 리스트의 첫 번째 문서를 가져옵니다.docs[0]
```
```
Document(page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n예시: {""이름"": ""홍길동"", ""나이"": 30, ""직업"": ""개발자""}는 JSON 형식의 데이터입니다.\n연관키워드: 데이터 교환, 웹 개발, API\n\nTransformer', metadata={'source': './data/appendix-keywords.txt'})
```
## 병합(merge)
FAISS 벡터 저장소를 병합하는 것도 가능합니다.
두 개의 FAISS 벡터 저장소를 하나로 합칠 수 있습니다.
```
Copy# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.embeddings = OpenAIEmbeddings()# db1 생성db1 = FAISS.from_texts([""LangChain!!""], embeddings)# db2 생성db2 = FAISS.from_texts([""좋아요^^""], embeddings)
```
db1의 내용 확인
```
Copy# db1 벡터 데이터베이스의 내부 문서 저장소 딕셔너리에 접근합니다.db1.docstore._dict
```
```
{'869410ed-8353-4deb-9517-4fca77e291cc': Document(page_content='LangChain!!')}
```
db2의 내용 확인
```
Copy# db2 문서 저장소의 내부 딕셔너리에 접근합니다.db2.docstore._dict
```
```
{'5695551b-a697-4ed5-bbf4-ae9fff86f51f': Document(page_content='좋아요^^')}
```
db1객체의merge_from메서드를 사용하여db2객체의 내용을db1에 병합합니다.
- merge_from메서드는db2의 데이터를db1에 병합하는 기능을 수행합니다.
- 병합 작업 후에는db1에db2의 데이터가 포함됩니다.
merge_from메서드는db2의 데이터를db1에 병합하는 기능을 수행합니다.
병합 작업 후에는db1에db2의 데이터가 포함됩니다.
```
Copydb1.merge_from(db2)# db1에 db2를 병합합니다.
```
병합된 결과를 확인합니다.
```
Copy# db1 문서 저장소의 내부 딕셔너리에 접근합니다.db1.docstore._dict
```
```
{'869410ed-8353-4deb-9517-4fca77e291cc': Document(page_content='LangChain!!'), '5695551b-a697-4ed5-bbf4-ae9fff86f51f': Document(page_content='좋아요^^')}
```
## 필터링
FAISS vectorstore는 필터링 기능도 지원할 수 있습니다.
FAISS는 기본적으로 필터링을 지원하지 않기 때문에, 이를 수동으로 처리해야 합니다.
방식
이는 먼저k보다 더 많은 결과를 가져온 다음 필터링하는 방식으로 이루어집니다.
이는 먼저k보다 더 많은 결과를 가져온 다음 필터링하는 방식으로 이루어집니다.
이 필터는 메타데이터 dict를 입력으로 받아 bool을 반환하는 호출 가능한 함수이거나, 누락된 각 키는 무시되고 존재하는 각 키는 값 목록에 있어야 하는 메타데이터 dict입니다.
이 필터는 메타데이터 dict를 입력으로 받아 bool을 반환하는 호출 가능한 함수이거나, 누락된 각 키는 무시되고 존재하는 각 키는 값 목록에 있어야 하는 메타데이터 dict입니다.
또한 검색 메서드를 호출할 때fetch_k매개변수를 설정하여 필터링 전에 가져올 문서 수를 지정할 수 있습니다.
```
Copyfromlangchain_core.documentsimportDocument

list_of_documents = [# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 1을 가진 문서Document(page_content=""foo"", metadata=dict(page=1)),# 페이지 내용이 ""bar""이고 메타데이터로 페이지 번호 1을 가진 문서Document(page_content=""bar"", metadata=dict(page=1)),# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 2를 가진 문서Document(page_content=""foo"", metadata=dict(page=2)),# 페이지 내용이 ""barbar""이고 메타데이터로 페이지 번호 2를 가진 문서Document(page_content=""barbar"", metadata=dict(page=2)),# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 3을 가진 문서Document(page_content=""foo"", metadata=dict(page=3)),# 페이지 내용이 ""bar burr""이고 메타데이터로 페이지 번호 3을 가진 문서Document(page_content=""bar burr"", metadata=dict(page=3)),# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 4를 가진 문서Document(page_content=""foo"", metadata=dict(page=4)),# 페이지 내용이 ""bar bruh""이고 메타데이터로 페이지 번호 4를 가진 문서Document(page_content=""bar bruh"", metadata=dict(page=4)),
]# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성db = FAISS.from_documents(list_of_documents, embeddings)# ""foo""와 유사한 문서를 검색하고 점수와 함께 결과 반환results_with_scores = db.similarity_search_with_score(""foo"")fordoc, scoreinresults_with_scores:# 검색 결과를 반복하면서# 각 문서의 내용, 메타데이터, 점수를 출력print(f""Content:{doc.page_content}, Metadata:{doc.metadata}, Score:{score}"")
```
```
Content: foo, Metadata: {'page': 1}, Score: 0.0
Content: foo, Metadata: {'page': 2}, Score: 0.0
Content: foo, Metadata: {'page': 3}, Score: 0.0
Content: foo, Metadata: {'page': 4}, Score: 0.0
```
이제 동일한 쿼리 호출을 수행하지만,page = 1인 경우만 필터링합니다.
- filter매개변수를 사용하여 검색 결과를 필터링할 수 있습니다. 딕셔너리 형태로 메타데이터 필드와 값을 지정하거나, 호출 가능한 객체(callable)를 사용할 수 있습니다.
- 검색 결과는(문서, 점수)튜플의 리스트 형태로 반환됩니다.
filter매개변수를 사용하여 검색 결과를 필터링할 수 있습니다. 딕셔너리 형태로 메타데이터 필드와 값을 지정하거나, 호출 가능한 객체(callable)를 사용할 수 있습니다.
검색 결과는(문서, 점수)튜플의 리스트 형태로 반환됩니다.
```
Copy# 방법1) 유사도 검색을 수행하고 필터를 적용하여 결과와 점수를 반환합니다.results_with_scores = db.similarity_search_with_score(""foo"",filter=dict(page=1))# 방법2) 혹은 callable 을 사용하여 필터링 하는 경우# results_with_scores = db.similarity_search_with_score(""foo"", filter=lambda d: d[""page""] == 1)fordoc, scoreinresults_with_scores:# 결과와 점수를 반복합니다.# 각 문서의 내용, 메타데이터, 점수를 출력합니다.print(f""[Content]{doc.page_content}, [metadata]{doc.metadata}, [Score]{score}"")
```
```
[Content] foo, [metadata] {'page': 1}, [Score] 0.0
[Content] bar, [metadata] {'page': 1}, [Score] 0.31470414996147156
```
## Max Marginal Relevance (MMR)
MMR(Maximal Marginal Relevance)방식은 쿼리에 대한 관련 항목을 검색할 때 중복을 피하는 방법 중 하나입니다. 단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 검색된 항목들 사이에관련성과 다양성 사이의 균형을 보장합니다. 이는자주 발생할 수 있는, 매우 유사한 항목들만이 검색되는 상황을 방지하는 데에 유용합니다.
예를 들어, 특정 주제에 대해 정보를 찾고 있다고 가정해봅시다. 가장 관련성 높은 문서만을 반환하는 시스템은 비슷비슷한 정보를 담은 문서들을 제공할 수 있습니다. 하지만, MMR 방식을 사용하면, 검색된 문서들이 해당 주제에 대해 서로 다른 관점이나 새로운 정보를 제공하도록 합니다. 이로써 사용자는 주제에 대해 보다 폭넓은 이해를 할 수 있게 됩니다.
MMR은 두 가지 주요 요소, 즉 쿼리에 대한 문서의 관련성과이미 선택된 문서들과의 차별성을 동시에 고려합니다.
첫째로, 쿼리와의 관련성이 높은 문서를 찾는 것이 중요합니다.
둘째로, 이미 선택된 문서들과는 다른 새로운 정보나 관점을 제공하는 문서를 찾는 것입니다.
이 두 가지 요소의 균형을 맞추는 것이 MMR의 핵심입니다.
max_marginal_relevance_search함수에도 동일한 방식을 적용할 수 있습니다.
- db.max_marginal_relevance_search()메서드를 사용하여 ""foo""라는 검색어로 문서를 검색합니다.
- 이때filter매개변수를 사용하여page메타데이터 값이 1인 문서만 검색합니다.
db.max_marginal_relevance_search()메서드를 사용하여 ""foo""라는 검색어로 문서를 검색합니다.
이때filter매개변수를 사용하여page메타데이터 값이 1인 문서만 검색합니다.
```
Copy# ""foo""를 검색어로 사용하여 최대 한계 관련성 검색을 수행하고, 메타데이터의 'page' 필드가 1인 문서만 필터링합니다.results = db.max_marginal_relevance_search(""foo"",filter=dict(page=1))fordocinresults:# 각 문서의 내용과 메타데이터를 출력합니다.print(f""[Content]{doc.page_content}, [metadata]{doc.metadata}"")
```
```
[Content] foo, [metadata] {'page': 1}
[Content] bar, [metadata] {'page': 1}
```
similarity_search함수를 호출할 때fetch_k매개변수를 설정하는 방법에 대한 예시입니다.
일반적으로fetch_k매개변수는k매개변수보다 훨씬 큰 값으로 설정하는 것이 좋습니다. 그 이유는fetch_k매개변수가 필터링 전에 가져올 문서의 수를 나타내기 때문입니다.
만약fetch_k를 낮은 값으로 설정하면, 필터링할 문서가 충분하지 않을 수 있습니다.
```
Copyresults = db.similarity_search(""foo"",# 검색 쿼리# 메타데이터의 'page' 필드가 1인 문서만 필터링filter=dict(page=1),
    k=1,# 가장 유사한 1개의 문서를 반환fetch_k=4,
)# 4개의 문서까지 검색fordocinresults:# 각 문서의 내용과 메타데이터를 출력합니다.print(f""[Content]{doc.page_content}, [metadata]{doc.metadata}"")
```
```
[Content] foo, [metadata] {'page': 1}
```
## 문서 삭제
벡터 저장소에 저장된 문서를 삭제할 수도 있습니다. 이때 삭제할 문서의 ID는 DB 에 저장된 ID와 일치해야 합니다.
db.delete()메서드를 사용하여 문서를 삭제합니다.
```
Copydb.index_to_docstore_id[0]
```
```
'ae2212d7-a646-469a-bbca-d54f5f34bd75'
```
```
Copy# 인덱스 0에 해당하는 문서 저장소 ID를 사용하여 데이터베이스에서 문서를 삭제합니다.db.delete([db.index_to_docstore_id[0]])
```
```
True
```"
03. FAISS 비동기(Asynchronous),https://wikidocs.net/234015,"# 03. FAISS 비동기(Asynchronous)

본 튜토리얼은 FAISS DB 를 비동기로 활용하는 방법을 다룹니다.
더욱 자세한 사용법은Faiss 공식 문서를 참조하세요.
faiss-gpu 또는 faiss-cpu 라이브러리를 설치합니다.
- CUDA 7.5 이상을 지원하는 GPU가 있는 경우faiss-gpu를 설치합니다.
- GPU가 없거나 CPU에서 실행하려는 경우faiss-cpu를 설치합니다.
CUDA 7.5 이상을 지원하는 GPU가 있는 경우faiss-gpu를 설치합니다.
GPU가 없거나 CPU에서 실행하려는 경우faiss-cpu를 설치합니다.
```
Copy# %pip install --upgrade --quiet faiss-gpu # CUDA 7.5 이상을 지원하는 GPU용 설치# 또는# %pip install --upgrade --quiet faiss-cpu # CPU용 설치
```
혹은dot_env라이브러리를 사용하여.env파일을 만들어 환경 변수로 설정할 수 있습니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
추적을 위한 LangSmith 설정(필수는 아닙니다.)
```
Copy# LANGCHAIN_TRACING_V2 환경 변수를 ""true""로 설정합니다.# os.environ[""LANGCHAIN_TRACING_V2""] = ""true""# LANGCHAIN_API_KEY 환경 변수를 getpass.getpass() 함수를 통해 입력받은 값으로 설정합니다.# os.environ[""LANGCHAIN_API_KEY""] = ""LANGCHAIN API KEY 입력""
```
- TextLoader를 사용하여 텍스트 데이터를 로드합니다.
- CharacterTextSplitter를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.
- OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.
- FAISS벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.
TextLoader를 사용하여 텍스트 데이터를 로드합니다.
CharacterTextSplitter를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.
OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.
FAISS벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.
참고
- AVX2: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우AVX2를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.
- 필요한 경우os.environ['FAISS_NO_AVX2'] = '1'코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다.
AVX2: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우AVX2를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.
필요한 경우os.environ['FAISS_NO_AVX2'] = '1'코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다.
샘플 데이터(appendix-keywords.txt) 일부 파일의 내용을 확인합니다.
```
Copy# data/appendix-keywords.txt 파일 내용을 읽어서 file 변수에 저장합니다.withopen(""./data/appendix-keywords.txt"")asf:
    file = f.read()# 파일의 내용을 읽어서 file 변수에 저장합니다.# file 변수에 저장된 내용을 출력합니다.print(file[:500])
```
```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.
연관키워드: 토큰화, 자연어
```
```
Copyfromlangchain_text_splittersimportCharacterTextSplitterfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_community.vectorstoresimportFAISSfromlangchain_community.document_loadersimportTextLoader# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.# import os## os.environ['FAISS_NO_AVX2'] = '1'# TextLoader를 사용하여 텍스트 파일을 로드합니다.loader = TextLoader(""./data/appendix-keywords.txt"")# 로드된 문서를 가져옵니다.documents = loader.load()# CharacterTextSplitter를 사용하여 문서를 분할합니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)# 분할된 문서를 가져옵니다.docs = text_splitter.split_documents(documents)# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.embeddings = OpenAIEmbeddings()# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.db =awaitFAISS.afrom_documents(docs, embeddings)
```
```
Copy# 검색할 쿼리 설정query =""임베딩(Embedding)이란 무엇인가요?""# 쿼리와 유사한 문서 검색docs =awaitdb.asimilarity_search(query)# 검색된 문서의 내용 출력print(docs[0].page_content)
```
```
정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token
```
## 점수에 기반한 유사도 검색
FAISS에는 몇 가지 특정 메서드가 있습니다.
그 중 하나는similarity_search_with_score로, 문서뿐만 아니라 쿼리와 문서 간의 거리 점수도 반환할 수 있습니다.
반환되는 거리 점수는 L2 거리입니다.
따라서 점수가 낮을수록 더 좋은 결과입니다.
```
Copy# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.docs_and_scores =awaitdb.asimilarity_search_with_score(query)# 검색 결과 중 가장 유사도가 높은 문서와 점수를 가져옵니다.docs_and_scores[0]
```
```
(Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'}), 0.29650298)
```
similarity_search_by_vector함수를 사용하면 주어진 임베딩 벡터와 유사한 문서를 검색할 수 있습니다.
이 함수는 문자열 대신임베딩 벡터를 매개변수로 받아들입니다.
```
Copy# 검색할 쿼리 설정query =""임베딩(Embedding)이란 무엇인가요?""# 쿼리를 임베딩 벡터로 변환합니다.embedding_vector =awaitembeddings.aembed_query(query)# 임베딩 벡터를 사용하여 유사도 검색을 수행하고 문서와 점수를 반환합니다.docs_and_scores =awaitdb.asimilarity_search_by_vector(embedding_vector)
```
## 저장 및 로드
FAISS 인덱스를 저장하고 불러올 수도 있습니다.
이는 인덱스를 사용할 때마다 매번 다시 생성할 필요가 없어 유용합니다.
- db.save_local(""저장할 인덱스 이름"")를 사용하여 FAISS 인덱스를 로컬 디렉토리에 저장합니다.
db.save_local(""저장할 인덱스 이름"")를 사용하여 FAISS 인덱스를 로컬 디렉토리에 저장합니다.
```
Copy# 로컬에 ""MY_ASYNC_DB_INDEX""라는 이름으로 데이터베이스를 저장합니다.DB_INDEX =""MY_ASYNC_DB_INDEX""db.save_local(DB_INDEX)
```
- FAISS.load_local(""불러올 인덱스 이름"", embeddings)를 사용하여 저장된 FAISS 인덱스를 로드합니다.
FAISS.load_local(""불러올 인덱스 이름"", embeddings)를 사용하여 저장된 FAISS 인덱스를 로드합니다.
```
Copy# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.new_db = FAISS.load_local(DB_INDEX, embeddings, allow_dangerous_deserialization=True)

query =""임베딩(Embedding)이란 무엇인가요?""# new_db에서 query와 유사한 문서를 비동기적으로 검색하여 docs 변수에 할당합니다.docs =awaitnew_db.asimilarity_search(query)

docs[0]# 검색 결과 중 가장 유사한 문서를 반환합니다.
```
```
Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'})
```
FAISS 인덱스를 직렬화(Serializing)하고 역직렬화(De-Serializing)하는 데에는pickle함수를 사용하여 저장할 수 있습니다. 하지만, 만약 90MB 크기의 임베딩 모델(예:sentence-transformers/all-MiniLM-L6-v2또는 다른 모델)을 사용한다면, 결과로 생성되는 pickle 파일의 크기는 90MB 이상이 될 것입니다. 이는 모델의 크기 또한 전체 크기에 포함되기 때문입니다.
따라서, 아래의 함수들을 사용하는 것이 좋습니다. 이 함수들은FAISS 인덱스만 직렬화하므로 크기가 훨씬 작아집니다.
```
Copyfromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportFAISSfromlangchain_text_splittersimportCharacterTextSplitterfromlangchain_community.embeddings.huggingfaceimportHuggingFaceEmbeddings# TextLoader를 사용하여 텍스트 파일을 로드합니다.loader = TextLoader(""./data/appendix-keywords.txt"")# 로드된 문서를 가져옵니다.documents = loader.load()# CharacterTextSplitter를 사용하여 문서를 분할합니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)# 분할된 문서를 가져옵니다.docs = text_splitter.split_documents(documents)# HuggingFaceEmbeddings 사용하여 임베딩을 생성합니다.hf_embeddings = HuggingFaceEmbeddings(model_name=""all-MiniLM-L6-v2"")# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.db = FAISS.from_documents(docs, hf_embeddings)
```
```
Copy# FAISS 인덱스를 직렬화합니다.serialized_db_index = db.serialize_to_bytes()
```
직렬화된serialized_db_index의 사이즈를 확인합니다.
```
Copyimportsys# 사이즈 측정을 위한 함수를 정의합니다.defget_size(path):
    size = sys.getsizeof(path)ifsize <1024:returnf""{size}bytes""elifsize <pow(1024,2):returnf""{round(size/1024,2)}KB""elifsize <pow(1024,3):returnf""{round(size/(pow(1024,2)),2)}MB""elifsize <pow(1024,4):returnf""{round(size/(pow(1024,3)),2)}GB""# 직렬화된 FAISS 인덱스의 사이즈를 출력합니다.get_size(serialized_db_index)
```
```
'60.81 KB'
```
이번에는deserialize_from_bytes함수로 역직렬화를 수행합니다.
```
Copy# 직렬화된 인덱스를 로드합니다.deserialized_db = FAISS.deserialize_from_bytes(
    embeddings=hf_embeddings,# 직렬화 할 때의 임베딩과 동일하게 지정serialized=serialized_db_index,# 직렬화된 인덱스)
```
```
Copyquery =""임베딩(Embedding)이란 무엇인가요?""# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.docs = deserialized_db.similarity_search(query)# 문서 리스트의 첫 번째 문서를 가져옵니다.docs[0]
```
```
Document(page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n예시: {""이름"": ""홍길동"", ""나이"": 30, ""직업"": ""개발자""}는 JSON 형식의 데이터입니다.\n연관키워드: 데이터 교환, 웹 개발, API\n\nTransformer', metadata={'source': './data/appendix-keywords.txt'})
```
## 병합(merge)
FAISS 벡터 저장소를 병합하는 것도 가능합니다.
두 개의 FAISS 벡터 저장소를 하나로 합칠 수 있습니다.
- FAISS.afrom_texts메서드를 사용하여 두 개의 FAISS 데이터베이스db1과db2를 비동기적으로 생성합니다.
- db1은[""foo""]텍스트 리스트로부터 생성되며,embeddings를 사용하여 벡터화됩니다.
- db2는[""bar""]텍스트 리스트로부터 생성되며, 동일한embeddings를 사용하여 벡터화됩니다.
FAISS.afrom_texts메서드를 사용하여 두 개의 FAISS 데이터베이스db1과db2를 비동기적으로 생성합니다.
db1은[""foo""]텍스트 리스트로부터 생성되며,embeddings를 사용하여 벡터화됩니다.
db2는[""bar""]텍스트 리스트로부터 생성되며, 동일한embeddings를 사용하여 벡터화됩니다.
```
Copy# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.embeddings = OpenAIEmbeddings()# db1 생성db1 = FAISS.from_texts([""LangChain DB 1의 내용""], embeddings)# db2 생성db2 = FAISS.from_texts([""DB 2 의 내용""], embeddings)
```
db1의 내용 확인
```
Copy# db1 벡터 데이터베이스의 내부 문서 저장소 딕셔너리에 접근합니다.db1.docstore._dict
```
```
{'5ad3475d-3a95-469c-b868-8ab93b644708': Document(page_content='LangChain DB 1의 내용')}
```
db2의 내용 확인
```
Copy# db2 문서 저장소의 내부 딕셔너리에 접근합니다.db2.docstore._dict
```
```
{'3439056f-c28f-4366-baa0-c79a949378c6': Document(page_content='DB 2 의 내용')}
```
db1객체의merge_from메서드를 사용하여db2객체의 내용을db1에 병합합니다.
- merge_from메서드는db2의 데이터를db1에 병합하는 기능을 수행합니다.
- 병합 작업 후에는db1에db2의 데이터가 포함됩니다.
merge_from메서드는db2의 데이터를db1에 병합하는 기능을 수행합니다.
병합 작업 후에는db1에db2의 데이터가 포함됩니다.
```
Copydb1.merge_from(db2)# db1에 db2를 병합합니다.
```
병합된 결과를 확인합니다.
```
Copy# db1 문서 저장소의 내부 딕셔너리에 접근합니다.db1.docstore._dict
```
```
{'5ad3475d-3a95-469c-b868-8ab93b644708': Document(page_content='LangChain DB 1의 내용'), '3439056f-c28f-4366-baa0-c79a949378c6': Document(page_content='DB 2 의 내용')}
```
## 필터링
FAISS vectorstore는 필터링 기능도 지원할 수 있습니다.
FAISS는 기본적으로 필터링을 지원하지 않기 때문에, 이를 수동으로 처리해야 합니다.
방식
이는 먼저k보다 더 많은 결과를 가져온 다음 필터링하는 방식으로 이루어집니다.
이는 먼저k보다 더 많은 결과를 가져온 다음 필터링하는 방식으로 이루어집니다.
이 필터는 메타데이터 dict를 입력으로 받아 bool을 반환하는 호출 가능한 함수이거나, 누락된 각 키는 무시되고 존재하는 각 키는 값 목록에 있어야 하는 메타데이터 dict입니다.
이 필터는 메타데이터 dict를 입력으로 받아 bool을 반환하는 호출 가능한 함수이거나, 누락된 각 키는 무시되고 존재하는 각 키는 값 목록에 있어야 하는 메타데이터 dict입니다.
또한 검색 메서드를 호출할 때fetch_k매개변수를 설정하여 필터링 전에 가져올 문서 수를 지정할 수 있습니다.
```
Copyfromlangchain_core.documentsimportDocument

list_of_documents = [# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 1을 가진 문서Document(page_content=""foo"", metadata=dict(page=1)),# 페이지 내용이 ""bar""이고 메타데이터로 페이지 번호 1을 가진 문서Document(page_content=""bar"", metadata=dict(page=1)),# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 2를 가진 문서Document(page_content=""foo"", metadata=dict(page=2)),# 페이지 내용이 ""barbar""이고 메타데이터로 페이지 번호 2를 가진 문서Document(page_content=""barbar"", metadata=dict(page=2)),# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 3을 가진 문서Document(page_content=""foo"", metadata=dict(page=3)),# 페이지 내용이 ""bar burr""이고 메타데이터로 페이지 번호 3을 가진 문서Document(page_content=""bar burr"", metadata=dict(page=3)),# 페이지 내용이 ""foo""이고 메타데이터로 페이지 번호 4를 가진 문서Document(page_content=""foo"", metadata=dict(page=4)),# 페이지 내용이 ""bar bruh""이고 메타데이터로 페이지 번호 4를 가진 문서Document(page_content=""bar bruh"", metadata=dict(page=4)),
]# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성db = FAISS.from_documents(list_of_documents, embeddings)
results_with_scores = db.asimilarity_search_with_score(""foo"")# ""foo""와 유사한 문서를 검색하고 점수와 함께 결과 반환
```
이제 동일한 쿼리 호출을 수행하지만,page = 1인 경우만 필터링합니다.
```
Copy# 유사도 검색을 수행하고 결과를 점수와 함께 가져옵니다. 필터로 page가 1인 문서만 검색합니다.results_with_scores =awaitdb.asimilarity_search_with_score(""foo"",filter=dict(page=1))for(
    doc,
    score,
)inresults_with_scores:# 검색 결과를 반복하면서 각 문서와 점수를 가져옵니다.# 문서의 내용, 메타데이터, 점수를 출력합니다.print(f""Content:{doc.page_content}, Metadata:{doc.metadata}, Score:{score}"")
```
```
Content: foo, Metadata: {'page': 1}, Score: 0.0
Content: bar, Metadata: {'page': 1}, Score: 0.31470414996147156
```
## Max Marginal Relevance (MMR)
MMR(Maximal Marginal Relevance)방식은 쿼리에 대한 관련 항목을 검색할 때 중복을 피하는 방법 중 하나입니다. 단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 검색된 항목들 사이에관련성과 다양성 사이의 균형을 보장합니다. 이는자주 발생할 수 있는, 매우 유사한 항목들만이 검색되는 상황을 방지하는 데에 유용합니다.
예를 들어, 특정 주제에 대해 정보를 찾고 있다고 가정해봅시다. 가장 관련성 높은 문서만을 반환하는 시스템은 비슷비슷한 정보를 담은 문서들을 제공할 수 있습니다. 하지만, MMR 방식을 사용하면, 검색된 문서들이 해당 주제에 대해 서로 다른 관점이나 새로운 정보를 제공하도록 합니다. 이로써 사용자는 주제에 대해 보다 폭넓은 이해를 할 수 있게 됩니다.
MMR은 두 가지 주요 요소, 즉 쿼리에 대한 문서의 관련성과이미 선택된 문서들과의 차별성을 동시에 고려합니다.
첫째로, 쿼리와의 관련성이 높은 문서를 찾는 것이 중요합니다.
둘째로, 이미 선택된 문서들과는 다른 새로운 정보나 관점을 제공하는 문서를 찾는 것입니다.
이 두 가지 요소의 균형을 맞추는 것이 MMR의 핵심입니다.
```
Copy# ""foo""를 검색어로 사용하여 최대 한계 관련성 검색을 수행하고, 필터로 page가 1인 문서만 검색합니다.results =awaitdb.amax_marginal_relevance_search(""foo"",filter=dict(page=1))fordocinresults:# 검색 결과를 반복합니다.# 각 문서의 내용과 메타데이터를 출력합니다.print(f""Content:{doc.page_content}, Metadata:{doc.metadata}"")
```
```
Content: foo, Metadata: {'page': 1}
Content: bar, Metadata: {'page': 1}
```
similarity_search함수를 호출할 때fetch_k매개변수를 설정하는 방법에 대한 예시입니다.
일반적으로fetch_k매개변수는k매개변수보다 훨씬 큰 값으로 설정하는 것이 좋습니다. 그 이유는fetch_k매개변수가 필터링 전에 가져올 문서의 수를 나타내기 때문입니다.
만약fetch_k를 낮은 값으로 설정하면, 필터링할 문서가 충분하지 않을 수 있습니다.
```
Copy# ""foo""와 유사한 문서를 검색하고, 'page' 메타데이터가 1인 문서만 필터링하여 가장 유사한 1개의 문서를 반환하되, 4개의 문서를 가져옵니다.results =awaitdb.asimilarity_search(""foo"",filter=dict(page=1), k=1, fetch_k=4)fordocinresults:# 검색 결과를 반복하면서 각 문서에 대해 다음을 수행합니다.# 문서의 내용과 메타데이터를 출력합니다.print(f""Content:{doc.page_content}, Metadata:{doc.metadata}"")
```
```
Content: foo, Metadata: {'page': 1}
```
## 문서 삭제
벡터 저장소에 저장된 문서를 삭제할 수도 있습니다. 이때 삭제할 문서의 ID는 DB 에 저장된 ID와 일치해야 합니다.
db.delete()메서드를 사용하여 문서를 삭제합니다.
```
Copydb.index_to_docstore_id[0]
```
```
'660aafd1-5952-4caf-b55d-e7cc3db10712'
```
```
Copy# 인덱스 0에 해당하는 문서 저장소 ID를 사용하여 데이터베이스에서 문서를 삭제합니다.db.delete([db.index_to_docstore_id[0]])
```
```
True
```"
04. Chroma,https://wikidocs.net/234094,"# 04. Chroma

Chroma는 개발자의 생산성에 초점을 맞춘 AI 기반의 오픈 소스 벡터 데이터베이스입니다. Chroma는 Apache 2.0 라이선스로 제공됩니다.
Chroma를 설치하려면 다음 명령어를 사용하세요
```
Copy%pip install -qU chromadb
```
Chroma는 다양한 모드로 실행될 수 있습니다. 아래에서 각각의 예시를 확인할 수 있습니다.
- in-memory- Python 스크립트 또는 Jupyter 노트북에서 실행
- in-memory with persistance- 스크립트 또는 노트북에서 실행하고 디스크에 저장/로드
- in a docker container- 로컬 머신이나 클라우드에서 서버로 실행
in-memory- Python 스크립트 또는 Jupyter 노트북에서 실행
in-memory with persistance- 스크립트 또는 노트북에서 실행하고 디스크에 저장/로드
in a docker container- 로컬 머신이나 클라우드에서 서버로 실행
다른 데이터베이스와 마찬가지로 다음과 같은 작업을 수행할 수 있습니다:
- .add
- .get
- .update
- .upsert
- .delete
- .peek
- .query: 유사도 검색을 실행합니다.
.add
.get
.update
.upsert
.delete
.peek
.query: 유사도 검색을 실행합니다.
[참고]
전체 문서는docs에서 확인할 수 있습니다.
- 이러한 메서드에 직접 액세스하려면._collection.method()를 사용할 수 있습니다.
이러한 메서드에 직접 액세스하려면._collection.method()를 사용할 수 있습니다.
## 기본 사용법
이 기본 예제에서는 샘플 데이터(appendix-keywords.txt) 를 가져와 청크로 분할하고, 오픈 소스 임베딩 모델을 사용하여 임베딩한 다음, Chroma에 로드하고 쿼리합니다.
- TextLoader를 사용하여 문서를 로드합니다.
- CharacterTextSplitter를 사용하여 로드된 문서를 청크 단위로 분할합니다.
- SentenceTransformerEmbeddings를 사용하여 오픈 소스 임베딩 함수를 생성합니다. 모델로는""all-MiniLM-L6-v2""를 사용합니다.
- Chroma 벡터 스토어에 분할된 문서와 임베딩 함수를 로드합니다.
- 질의(query) 를 사용하여 Chroma 벡터 스토어에서 유사도 검색을 수행합니다.
- 검색 결과 중 가장 유사한 문서의 내용을 출력합니다.
TextLoader를 사용하여 문서를 로드합니다.
CharacterTextSplitter를 사용하여 로드된 문서를 청크 단위로 분할합니다.
SentenceTransformerEmbeddings를 사용하여 오픈 소스 임베딩 함수를 생성합니다. 모델로는""all-MiniLM-L6-v2""를 사용합니다.
Chroma 벡터 스토어에 분할된 문서와 임베딩 함수를 로드합니다.
질의(query) 를 사용하여 Chroma 벡터 스토어에서 유사도 검색을 수행합니다.
검색 결과 중 가장 유사한 문서의 내용을 출력합니다.
```
Copy# importfromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.embeddings.sentence_transformerimport(
    SentenceTransformerEmbeddings,
)fromlangchain_community.vectorstoresimportChromafromlangchain_text_splittersimportCharacterTextSplitter# 문서를 로드하고 청크로 분할합니다.loader = TextLoader(""./data/appendix-keywords.txt"")
documents = loader.load()# 문서를 청크로 분할합니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
docs = text_splitter.split_documents(documents)# 오픈 소스 임베딩 함수를 생성합니다.stf_embeddings = SentenceTransformerEmbeddings(model_name=""all-MiniLM-L6-v2"")# Chroma에 로드합니다.db = Chroma.from_documents(docs, stf_embeddings)# 질의합니다.query =""What is Word2Vec?""docs = db.similarity_search(query)# 결과를 출력합니다.print(docs[0].page_content)
```
```
정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)
```
## 로컬 디스크 공간에 저장
이전 예제를 확장하여, 디스크에 저장하려는 경우에는 Chroma 클라이언트를 초기화하고 데이터를 저장할 디렉토리를 전달하기만 하면 됩니다.
[주의]
- Chroma는 데이터를 자동으로 디스크에 저장하기 위해 최선을 다하지만, 여러 개의 인메모리 클라이언트가 서로의 작업을 중단시킬 수 있습니다. 따라서, 한 번에 경로당 하나의 클라이언트만 실행하는 것이 좋습니다.
- Chroma.from_documents()메서드를 사용하여docs와embedding_function으로부터 Chroma 데이터베이스를 생성하고,persist_directory를 지정하여 디스크에 저장합니다.
- db2.similarity_search()메서드를 사용하여query와 유사한 문서를 검색합니다.
- Chroma클래스의 생성자를 사용하여persist_directory에 저장된 Chroma 데이터베이스를 로드하고,embedding_function을 지정합니다.
- db3.similarity_search()메서드를 사용하여query와 유사한 문서를 검색합니다.
- 검색된 문서 중 첫 번째 문서의page_content를 출력합니다.
Chroma는 데이터를 자동으로 디스크에 저장하기 위해 최선을 다하지만, 여러 개의 인메모리 클라이언트가 서로의 작업을 중단시킬 수 있습니다. 따라서, 한 번에 경로당 하나의 클라이언트만 실행하는 것이 좋습니다.
Chroma는 데이터를 자동으로 디스크에 저장하기 위해 최선을 다하지만, 여러 개의 인메모리 클라이언트가 서로의 작업을 중단시킬 수 있습니다. 따라서, 한 번에 경로당 하나의 클라이언트만 실행하는 것이 좋습니다.
Chroma.from_documents()메서드를 사용하여docs와embedding_function으로부터 Chroma 데이터베이스를 생성하고,persist_directory를 지정하여 디스크에 저장합니다.
Chroma.from_documents()메서드를 사용하여docs와embedding_function으로부터 Chroma 데이터베이스를 생성하고,persist_directory를 지정하여 디스크에 저장합니다.
db2.similarity_search()메서드를 사용하여query와 유사한 문서를 검색합니다.
Chroma클래스의 생성자를 사용하여persist_directory에 저장된 Chroma 데이터베이스를 로드하고,embedding_function을 지정합니다.
db3.similarity_search()메서드를 사용하여query와 유사한 문서를 검색합니다.
검색된 문서 중 첫 번째 문서의page_content를 출력합니다.
```
Copy# 저장할 경로 지정DB_PATH =""./chroma_db""# 문서를 디스크에 저장합니다. 저장시 persist_directory에 저장할 경로를 지정합니다.db2 = Chroma.from_documents(docs, stf_embeddings, persist_directory=DB_PATH)# 질의합니다.query =""What is Word2Vec?""docs = db2.similarity_search(query)# 결과를 출력합니다.docs[0]
```
```
Document(page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)', metadata={'source': './data/appendix-keywords.txt'})
```
로컬 디스크에 저장된 Chroma 데이터베이스를로드한 뒤 쿼리합니다.
```
Copy# 디스크에서 문서를 로드합니다.db3 = Chroma(persist_directory=""./chroma_db"",
             embedding_function=stf_embeddings)# 질의합니다.query =""What is Word2Vec?""docs = db3.similarity_search(query)print(docs[0].page_content)
```
```
정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)
```
## Langchain API - Chroma Client 활용
Chroma Client를 생성하여 LangChain에 전달할 수도 있습니다. 이는 기본 데이터베이스에 더 쉽게 액세스하려는 경우에 특히 유용합니다.
또한 LangChain에서 사용할collection name을 지정할 수 있습니다.
- chromadb모듈을 사용하여PersistentClient를 생성합니다.
- PersistentClient를 사용하여 ""chroma_collection""이라는 이름의 컬렉션을 가져오거나 생성합니다.
- collection.add()메서드를 사용하여 컬렉션에 문서를 추가합니다. 문서의 ID와 내용을 각각 ""1"", ""2"", ""3""과 ""a"", ""b"", ""c""로 지정합니다.
- Chroma클래스를 사용하여langchain_chroma객체를 생성합니다. 이 객체는PersistentClient, 컬렉션 이름, 임베딩 함수를 매개변수로 받습니다.
- langchain_chroma._collection.count()메서드를 사용하여 컬렉션의 문서 수를 출력합니다.
chromadb모듈을 사용하여PersistentClient를 생성합니다.
PersistentClient를 사용하여 ""chroma_collection""이라는 이름의 컬렉션을 가져오거나 생성합니다.
collection.add()메서드를 사용하여 컬렉션에 문서를 추가합니다. 문서의 ID와 내용을 각각 ""1"", ""2"", ""3""과 ""a"", ""b"", ""c""로 지정합니다.
Chroma클래스를 사용하여langchain_chroma객체를 생성합니다. 이 객체는PersistentClient, 컬렉션 이름, 임베딩 함수를 매개변수로 받습니다.
langchain_chroma._collection.count()메서드를 사용하여 컬렉션의 문서 수를 출력합니다.
```
Copyimportchromadb# ChromaDB의 PersistentClient를 생성합니다.persistent_client = chromadb.PersistentClient()# ""my_chroma_collection""이라는 이름의 컬렉션을 가져오거나 생성합니다.collection = persistent_client.get_or_create_collection(""chroma_collection"")# 컬렉션에 ID와 문서를 추가합니다.collection.add(ids=[""1"",""2"",""3""], documents=[""a"",""b"",""c""])# Chroma 객체를 생성합니다.langchain_chroma = Chroma(# PersistentClient를 전달합니다.client=persistent_client,# 사용할 컬렉션의 이름을 지정합니다.collection_name=""chroma_collection"",# 임베딩 함수를 전달합니다.embedding_function=stf_embeddings,
)# 컬렉션의 항목 수를 출력합니다.print(""현재 저장된 Collection 의 개수는 "",
    langchain_chroma._collection.count(),"" 개 입니다."",
)
```
```
현재 저장된 Collection 의 개수는  3  개 입니다.
```
## 도커 컨테이너 사용
Chroma Server를 별도의 Docker 컨테이너에서 실행하고, 이에 연결할 Client를 생성한 다음, 이를 LangChain에 전달할 수도 있습니다.
Chroma는 문서의 여러Collections을 처리할 수 있지만, LangChain 인터페이스는 하나의 컬렉션만 처리하므로 컬렉션 이름을 지정해야 합니다.
LangChain에서 사용하는 기본 컬렉션 이름은""langchain""입니다.
다음은 Docker 이미지를 복제, 빌드 및 실행하는 방법입니다
!git clone git@github.com:chroma-core/chroma.git
docker-compose.yml파일을 편집하고environment아래에ALLOW_RESET=TRUE를 추가합니다.
```
Copy...command:uvicornchromadb.app:app--reload--workers1--host0.0.0.0--port8000--log-configlog_config.ymlenvironment:-IS_PERSISTENT=TRUE-ALLOW_RESET=TRUEports:-8000:8000...
```
편집이 완료된 예시

그런 다음docker-compose up -d --build를 실행합니다.
도커 구동 후 성공적으로 실행되는지 확인합니다.

chromadb패키지를 사용하여 Chroma 클라이언트를 생성합니다.
- allow_reset=True로 설정하여 데이터베이스를 초기화할 수 있도록 합니다.
- client.reset()을 호출하여 데이터베이스를 초기화합니다.
- client.create_collection()을 사용하여 ""chroma_docker""이라는 새로운 컬렉션을 생성합니다.
- docs리스트의 각 문서에 대해collection.add()를 호출하여 고유한 ID, 메타데이터, 문서 내용을 컬렉션에 추가합니다.
allow_reset=True로 설정하여 데이터베이스를 초기화할 수 있도록 합니다.
client.reset()을 호출하여 데이터베이스를 초기화합니다.
client.create_collection()을 사용하여 ""chroma_docker""이라는 새로운 컬렉션을 생성합니다.
docs리스트의 각 문서에 대해collection.add()를 호출하여 고유한 ID, 메타데이터, 문서 내용을 컬렉션에 추가합니다.
```
Copy# Chroma 클라이언트 생성importuuidimportrandomimportchromadbfromchromadb.configimportSettings

client = chromadb.HttpClient(
    host=""0.0.0.0"", port=8000, settings=Settings(allow_reset=True)
)
client.reset()# 데이터베이스 초기화collection = client.create_collection(""chroma_docker"")# 문서를 로드하고 청크로 분할합니다.loader = TextLoader(""./data/appendix-keywords.txt"")
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
docs = loader.load_and_split(text_splitter)fori, docinenumerate(docs):
    doc.metadata[""chunk_id""] = i# Chunk ID 추가# 무작위로 임의의 페이지 번호를 삽입합니다.doc.metadata[""page_number""] = random.randint(0,5)
    collection.add(
        ids=[str(uuid.uuid1())],
        metadatas=doc.metadata,
        documents=doc.page_content,
    )# LangChain에게 클라이언트와 컬렉션 이름 사용 지시db4 = Chroma(
    client=client,
    collection_name=""chroma_docker"",
    embedding_function=stf_embeddings,
)
query =""What is Word2Vec?""docs = db4.similarity_search(query)print(docs[0].page_content)
```
```
정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)
```
## 업데이트 & 삭제
실제 애플리케이션을 구축할 때는 데이터 추가를 넘어 데이터 업데이트와 삭제도 수행하고자 합니다.
Chroma는 이를 단순화하기 위해 사용자가ids를 제공하도록 합니다.ids는 파일 이름이나filename_paragraphNumber와 같은 결합된 해시가 될 수 있습니다.
Chroma는 이러한 모든 작업을 지원하지만, 일부는 여전히 LangChain 인터페이스를 통해 완전히 통합되는 과정에 있습니다. 추가적인 워크플로 개선 사항이 곧 추가될 예정입니다.
다음은 다양한 작업을 수행하는 방법을 보여주는 기본 예제입니다:
- 문서 개수에 해당하는 간단한 ID 리스트를 생성합니다.
- Chroma.from_documents()메서드를 사용하여 문서, 임베딩 함수, ID를 전달하고 Chroma 데이터베이스를 생성합니다.
- similarity_search()메서드를 사용하여 쿼리와 유사한 문서를 검색하고, 첫 번째 문서의 메타데이터를 출력합니다.
- 첫 번째 문서의 메타데이터를 업데이트하고,update_document()메서드를 사용하여 데이터베이스에 반영합니다.
- 업데이트된 문서의 정보를 출력합니다.
- 마지막 문서를 삭제하기 전후의 문서 개수를 출력하여 삭제 작업을 확인합니다.
문서 개수에 해당하는 간단한 ID 리스트를 생성합니다.
Chroma.from_documents()메서드를 사용하여 문서, 임베딩 함수, ID를 전달하고 Chroma 데이터베이스를 생성합니다.
similarity_search()메서드를 사용하여 쿼리와 유사한 문서를 검색하고, 첫 번째 문서의 메타데이터를 출력합니다.
첫 번째 문서의 메타데이터를 업데이트하고,update_document()메서드를 사용하여 데이터베이스에 반영합니다.
업데이트된 문서의 정보를 출력합니다.
마지막 문서를 삭제하기 전후의 문서 개수를 출력하여 삭제 작업을 확인합니다.
```
Copy# 간단한 ID 생성ids = [str(i)foriinrange(1,len(docs) +1)]# 데이터 추가example_db = Chroma.from_documents(docs, stf_embeddings, ids=ids)
docs = example_db.similarity_search(query)print(docs[0].metadata)# 문서의 메타데이터 업데이트docs[0].metadata = {""source"":""./images/appendix-keywords.txt"",""new_value"":""테스트용으로 업데이트할 내용입니다."",
}# DB 에 업데이트example_db.update_document(ids[0], docs[0])print(example_db._collection.get(ids=[ids[0]]))# 문서 개수 출력print(""count before"", example_db._collection.count())# 마지막 문서 삭제example_db._collection.delete(ids=[ids[-1]])# 삭제 후 문서 개수 출력print(""count after"", example_db._collection.count())
```
```
{'source': './data/appendix-keywords.txt'}
{'ids': ['1'], 'embeddings': None, 'metadatas': [{'chunk_id': 12, 'new_value': '테스트용으로 업데이트할 내용입니다.', 'page_number': 1, 'source': './images/appendix-keywords.txt'}], 'documents': ['정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)'], 'uris': None, 'data': None}
count before 64
count after 63
```
## OpenAI Embeddings 사용하기
많은 사람들이OpenAIEmbeddings를 사용하는 것을 선호합니다.
다음은OpenAIEmbeddings을 사용한 예제입니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
- chromadb.EphemeralClient를 사용하여 새로운 Chroma 클라이언트를 생성합니다.
- Chroma.from_documents메서드를 사용하여 문서(docs)와 임베딩(embedings)을 기반으로 Chroma 컬렉션을 생성합니다.
- 새로 생성된 클라이언트(new_client)를 사용합니다.
- 컬렉션 이름은 ""openai_collection""으로 지정합니다.
- 질의(query)를 정의합니다: ""Word2Vec 에 대해서 설명해 주세요.""
- openai_lc_client.similarity_search메서드를 사용하여 질의와 유사한 문서를 검색합니다.
chromadb.EphemeralClient를 사용하여 새로운 Chroma 클라이언트를 생성합니다.
Chroma.from_documents메서드를 사용하여 문서(docs)와 임베딩(embedings)을 기반으로 Chroma 컬렉션을 생성합니다.
새로 생성된 클라이언트(new_client)를 사용합니다.
컬렉션 이름은 ""openai_collection""으로 지정합니다.
질의(query)를 정의합니다: ""Word2Vec 에 대해서 설명해 주세요.""
openai_lc_client.similarity_search메서드를 사용하여 질의와 유사한 문서를 검색합니다.
```
Copyfromlangchain_openaiimportOpenAIEmbeddings# 문서를 로드하고 청크로 분할합니다.loader = TextLoader(""./data/appendix-keywords.txt"")
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
docs = loader.load_and_split(text_splitter)# OpenAIEmbeddings 를 사용openai_embeddings = OpenAIEmbeddings()# client 생성new_client = chromadb.EphemeralClient()# OpenAI 임베딩과 Chroma 클라이언트를 사용하여 문서에서 Chroma 검색 클라이언트를 생성합니다.openai_lc_client = Chroma.from_documents(
    docs, openai_embeddings, client=new_client, collection_name=""openai_collection"")

query =""Word2Vec 에 대해서 설명해 주세요.""# 질의를 사용하여 유사도 검색을 수행합니다.docs = openai_lc_client.similarity_search(query)# 검색 결과에서 첫 번째 문서의 내용을 출력합니다.print(docs[0].page_content)
```
```
정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)
```
코사인 유사도를 산정합니다. score가 높을 수록 유사도가 높다고 판단할 수 있습니다.
- db객체의similarity_search_with_score메서드를 사용하여query와 유사한 문서를 검색하고, 그 결과를docs변수에 저장합니다.
db객체의similarity_search_with_score메서드를 사용하여query와 유사한 문서를 검색하고, 그 결과를docs변수에 저장합니다.
```
Copyquery =""Word2Vec 에 대해서 설명해 주세요.""# 유사도 검색을 수행하고 점수와 함께 결과를 반환합니다.docs = db.similarity_search_with_score(query)
```
- docs[0]을 사용하여 문서 리스트의 첫 번째 문서에 접근합니다.
docs[0]을 사용하여 문서 리스트의 첫 번째 문서에 접근합니다.
```
Copy# 문서 리스트의 첫 번째 문서를 가져옵니다.document, score = docs[0]print(document, end=""\n\n"")print(f""유사도 점수:{score}"")
```
```
page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)' metadata={'source': './data/appendix-keywords.txt'}

유사도 점수: 0.8870154023170471
```
이 섹션에서는 Chroma를 retriever로 사용하는 다양한 옵션에 대해 설명합니다.
retriever 객체에서 유사도 검색을 사용하는 것 외에도mmr을 사용할 수 있습니다.
- db객체를 사용하여retriever를 생성합니다.
- search_type매개변수를 ""mmr""로 설정하여 MMR(Maximal Marginal Relevance) 검색 알고리즘을 사용합니다.
db객체를 사용하여retriever를 생성합니다.
search_type매개변수를 ""mmr""로 설정하여 MMR(Maximal Marginal Relevance) 검색 알고리즘을 사용합니다.
```
Copy# 검색 유형을 ""mmr""로 설정하여 데이터베이스를 검색기로 사용retriever = db.as_retriever(search_type=""mmr"")
```
- retriever객체의get_relevant_documents()메서드를 사용하여 주어진query와 관련된 문서를 검색합니다.
- get_relevant_documents()메서드는 관련 문서의 리스트를 반환하며, 이 중 첫 번째 문서([0])를 선택합니다.
retriever객체의get_relevant_documents()메서드를 사용하여 주어진query와 관련된 문서를 검색합니다.
get_relevant_documents()메서드는 관련 문서의 리스트를 반환하며, 이 중 첫 번째 문서([0])를 선택합니다.
```
Copyquery =""Word2Vec 에 대해서 설명해 주세요.""# 질의와 관련된 문서 중 가장 관련성이 높은 문서를 가져옵니다.retriever.get_relevant_documents(query)[0]
```
```
Document(page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)', metadata={'source': './data/appendix-keywords.txt'})
```
## 메타데이터를 기반으로 필터링
컬렉션을 다루기 전에 컬렉션의 범위를 좁히는 것이 도움이 될 수 있습니다.
예를 들어,get메서드를 사용하여 메타데이터를 기반으로 컬렉션을 필터링할 수 있습니다.
- example_db.get()메서드를 사용하여 메타데이터 필터링을 해보겠습니다.
example_db.get()메서드를 사용하여 메타데이터 필터링을 해보겠습니다.
```
Copy# metadatas 중 chunk_id 기준으로 필터링example_db.get(where={""chunk_id"":0})
```
```
{'ids': ['2'], 'embeddings': None, 'metadatas': [{'chunk_id': 0,   'page_number': 0,   'source': './data/appendix-keywords.txt'}], 'documents': ['Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'], 'uris': None, 'data': None}
```
```
Copy# page_number 기준으로 필터링example_db.get(where={""page_number"":0})
```
```
{'ids': ['2'], 'embeddings': None, 'metadatas': [{'chunk_id': 0,   'page_number': 0,   'source': './data/appendix-keywords.txt'}], 'documents': ['Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'], 'uris': None, 'data': None}
```
```
Copy# source 기준으로 필터링example_db.get(where={""source"":""./data/appendix-keywords.txt""})
```
```
{'ids': ['038f8d1a-3791-4478-a1f3-270ee38f6198',  '04d5339d-2a5b-43b6-8e27-4ef618912aeb',  '07625557-455a-48e7-ac03-58a6836ec615',  'cc9b1230-db40-4807-a2a0-59697b597583',  'd4652daf-309d-430a-be0d-0741fe67af7f',  'e3da97b8-378f-4978-b3ed-ee517fad2b8c',  'e529b625-4193-4af8-9517-bd8982e371ef',  'e7152f9a-549d-45c0-884a-3c55a2485985',  'e91f813c-3431-4ab7-82c1-6387dfa13acb',  'eab327eb-79a0-43be-ae01-5723893deec7',  'eb0aba7a-094a-42f0-aa55-48514ec2020c',  'f02c2127-dbf1-4bac-a67c-a3c871378b46',  'fb0442c6-e5c6-41ff-bcdf-f011f1f8a956'], 'embeddings': None, 'metadatas': [{'source': './data/appendix-keywords.txt'}, {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'},  {'source': './data/appendix-keywords.txt'}], 'documents': ['정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\n\nParser',  '정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\n예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\n연관키워드: 데이터 융합, 인공지능, 딥러닝',  '정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\n\nSchema',  ""정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 '주의'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서 사용됩니다.\n예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링\n\n판다스 (Pandas)"",  '정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n예시: {""이름"": ""홍길동"", ""나이"": 30, ""직업"": ""개발자""}는 JSON 형식의 데이터입니다.\n연관키워드: 데이터 교환, 웹 개발, API\n\nTransformer',  '정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n\nOpen Source',  '정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\n연관키워드: 임베딩, 데이터베이스, 벡터화\n\nSQL',  '정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\n예시: 사용자가 ""이메일 초안 작성""과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\n연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\n\nKeyword Search',  '정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.\n연관키워드: 토큰화, 자연어 처리, 구문 분석\n\nTokenizer',  '정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\n\nInstructGPT',  '정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\n\nDataFrame',  '정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken',  '정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.\n연관키워드: 토큰화, 자연어 처리, 구문 분석\n\nVectorStore',  '정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.\n예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\n연관키워드: 빅데이터, 패턴 인식, 예측 분석\n\n멀티모달 (Multimodal)',  '정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)',  '정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n연관키워드: 혁신, 기술, 비즈니스 모델\n\nCrawling',  '정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.\n예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.\n연관키워드: 데이터 분석, 판다스, 데이터 처리\n\nAttention 메커니즘','정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\n\nStructured Data'], 'uris': None, 'data': None}
```"
CH11 검색기(Retriever),https://wikidocs.net/233779,"# CH11 검색기(Retriever)

검색기(Retriever) 단계는 Retrieval-Augmented Generation(RAG) 시스템의 다섯 번째 단계로, 저장된 벡터 데이터베이스에서사용자의 질문과 관련된 문서를 검색하는 과정입니다. 이 단계는 사용자 질문에가장 적합한 정보를 신속하게 찾아내는 것이 목표이며, RAG 시스템의 전반적인 성능과 직결되는 매우 중요한 과정입니다.
## 검색기의 필요성
정확한 정보 제공: 검색기는 사용자의 질문과가장 관련성 높은 정보를 검색하여, 시스템이정확하고 유용한 답변을 생성할 수 있도록 합니다. 이 과정이 효과적으로 이루어지지 않으면, 결과적으로 제공되는답변의 품질이 떨어질 수있습니다.
정확한 정보 제공: 검색기는 사용자의 질문과가장 관련성 높은 정보를 검색하여, 시스템이정확하고 유용한 답변을 생성할 수 있도록 합니다. 이 과정이 효과적으로 이루어지지 않으면, 결과적으로 제공되는답변의 품질이 떨어질 수있습니다.
응답 시간 단축: 효율적인 검색 알고리즘을 사용하여 데이터베이스에서 적절한정보를 빠르게 검색함으로써, 전체적인시스템 응답 시간을 단축시킵니다. 사용자 경험 향상에 직접적인 영향을 미칩니다.
응답 시간 단축: 효율적인 검색 알고리즘을 사용하여 데이터베이스에서 적절한정보를 빠르게 검색함으로써, 전체적인시스템 응답 시간을 단축시킵니다. 사용자 경험 향상에 직접적인 영향을 미칩니다.
최적화: 효과적인 검색 과정을 통해필요한 정보만을 추출함으로써 시스템 자원의 사용을 최적화하고,불필요한 데이터 처리를 줄일 수있습니다.
최적화: 효과적인 검색 과정을 통해필요한 정보만을 추출함으로써 시스템 자원의 사용을 최적화하고,불필요한 데이터 처리를 줄일 수있습니다.
## 동작 방식
질문의 벡터화: 사용자의 질문을 벡터 형태로 변환합니다. 이 과정은 임베딩 단계와 유사한 기술을 사용하여 진행됩니다. 변환된 질문 벡터는 후속 검색 작업의 기준점으로 사용됩니다.
질문의 벡터화: 사용자의 질문을 벡터 형태로 변환합니다. 이 과정은 임베딩 단계와 유사한 기술을 사용하여 진행됩니다. 변환된 질문 벡터는 후속 검색 작업의 기준점으로 사용됩니다.
벡터 유사성 비교: 저장된 문서 벡터들과 질문 벡터 사이의유사성을 계산합니다. 이는 주로 코사인 유사성(cosine similarity), Max Marginal Relevance(MMR) 등의 수학적 방법을 사용하여 수행됩니다.
벡터 유사성 비교: 저장된 문서 벡터들과 질문 벡터 사이의유사성을 계산합니다. 이는 주로 코사인 유사성(cosine similarity), Max Marginal Relevance(MMR) 등의 수학적 방법을 사용하여 수행됩니다.
상위 문서 선정: 계산된 유사성 점수를 기준으로상위 N개의 가장 관련성 높은 문서를 선정합니다. 이 문서들은 다음 단계에서 사용자의 질문에 대한 답변을 생성하는 데 사용됩니다.
상위 문서 선정: 계산된 유사성 점수를 기준으로상위 N개의 가장 관련성 높은 문서를 선정합니다. 이 문서들은 다음 단계에서 사용자의 질문에 대한 답변을 생성하는 데 사용됩니다.
문서 정보 반환: 선정된 문서들의 정보를 다음 단계(프롬프트 생성)로 전달합니다. 이 정보에는 문서의 내용, 위치, 메타데이터 등이 포함될 수 있습니다.
문서 정보 반환: 선정된 문서들의 정보를 다음 단계(프롬프트 생성)로 전달합니다. 이 정보에는 문서의 내용, 위치, 메타데이터 등이 포함될 수 있습니다.
## 검색기의 중요성
검색기는 RAG 시스템에서정보 검색의 질을 결정하는 핵심적인 역할을 합니다. 효율적인 검색기 없이는 대규모 데이터베이스에서 관련 정보를 신속하고 정확하게 찾아내는 것이 매우 어렵습니다.
또한, 검색기는사용자 질문에 대한 적절한 컨텍스트를 제공하여, 언어 모델이 보다정확한 답변을 생성할 수 있도록 돕습니다. 따라서 검색기의 성능은RAG 시스템의 전반적인 효율성과 사용자 만족도에 직접적인 영향을 미칩니다.
## Sparse Retriever & Dense Retriever
Sparse Retriever와Dense Retriever는 정보 검색 시스템에서 사용되는 두 가지 주요 방법입니다. 이들은 자연어 처리 분야, 특히 대규모 문서 집합에서 관련 문서를 검색할 때 사용됩니다.
Sparse Retriever는 문서와 쿼리(질문)를 이산적인키워드 벡터로 변환하여 처리합니다. 이 방법은 주로 텀 빈도-역문서 빈도(TF-IDF)나 BM25와 같은 전통적인 정보 검색 기법을 사용합니다.
- TF-IDF (Term Frequency-Inverse Document Frequency): 단어가 문서에 나타나는 빈도와 그 단어가 몇 개의 문서에서 나타나는지를 반영하여 단어의 중요도를 계산합니다. 여기서, 자주 나타나면서도 문서 집합 전체에서 드물게 나타나는 단어가 높은 가중치를 받습니다.
- BM25: TF-IDF를 개선한 모델로, 문서의 길이를 고려하여 검색 정확도를 향상시킵니다. 긴 문서와 짧은 문서 간의 가중치를 조정하여, 단어 빈도의 영향을 상대적으로 조절합니다.
TF-IDF (Term Frequency-Inverse Document Frequency): 단어가 문서에 나타나는 빈도와 그 단어가 몇 개의 문서에서 나타나는지를 반영하여 단어의 중요도를 계산합니다. 여기서, 자주 나타나면서도 문서 집합 전체에서 드물게 나타나는 단어가 높은 가중치를 받습니다.
BM25: TF-IDF를 개선한 모델로, 문서의 길이를 고려하여 검색 정확도를 향상시킵니다. 긴 문서와 짧은 문서 간의 가중치를 조정하여, 단어 빈도의 영향을 상대적으로 조절합니다.
Sparse Retriever의 특징은 각 단어의 존재 여부만을 고려하기 때문에 계산 비용이 낮고, 구현이 간단하다는 점입니다. 그러나 이 방법은 단어의 의미적 연관성을 고려하지 않으며,검색 결과의 품질이 키워드의 선택에 크게 의존합니다.
Dense Retriever는 최신 딥러닝 기법을 사용하여 문서와 쿼리를 연속적인 고차원 벡터로 인코딩합니다. Dense Retriever는 문서의 의미적 내용을 보다 풍부하게 표현할 수 있으며,키워드가 완벽히 일치하지 않더라도 의미적으로 관련된 문서를 검색할 수 있습니다.
Dense Retriever는 벡터 공간에서의 거리(예: 코사인 유사도)를 사용하여쿼리와 가장 관련성 높은 문서를 찾습니다. 이 방식은 특히 언어의 뉘앙스와 문맥을 이해하는 데 유리하며,복잡한 쿼리에 대해 더 정확한 검색 결과를 제공할 수 있습니다.
표현 방식: Sparse Retriever는 이산적인 키워드 기반의 표현을 사용하는 반면, Dense Retriever는 연속적인 벡터 공간에서 의미적 표현을 사용합니다.
의미적 처리 능력: Dense Retriever는 문맥과 의미를 더 깊이 파악할 수 있어, 키워드가 정확히 일치하지 않아도 관련 문서를 검색할 수 있습니다. Sparse Retriever는 이러한 의미적 뉘앙스를 덜 반영합니다.
적용 범위: 복잡한 질문이나 자연어 쿼리에 대해서는 Dense Retriever가 더 적합할 수 있으며, 간단하고 명확한 키워드 검색에는 Sparse Retriever가 더 유용할 수 있습니다.
이 두 방법은 각각의 장단점을 가지고 있으며, 사용 사례와 요구 사항에 따라 적절히 선택되어야 합니다.
## 코드
```
Copyfromlangchain_community.vectorstoresimportFAISS# 단계 4: DB 생성(Create DB) 및 저장# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)# 단계 5: Dense Retriever 생성# 문서에 포함되어 있는 정보를 검색하고 생성합니다.faiss_retriever = vectorstore.as_retriever()
```
```
Copyfromlangchain_community.retrieversimportBM25Retriever# 단계 5: Sparse Retriever 생성# 문서에 포함되어 있는 정보를 검색하고 생성합니다.bm25_retriever = BM25Retriever.from_documents(split_documents)
```
## 참고
- 벡터저장소 지원 검색기
- LangChain Retrievers
벡터저장소 지원 검색기
LangChain Retrievers"
01. 벡터저장소 지원 검색기(VectorStore-backed Retriever),https://wikidocs.net/234016,"# 01. 벡터저장소 지원 검색기(VectorStore-backed Retriever)

VectorStore 지원 검색기는 vector store를 사용하여 문서를 검색하는 retriever입니다.
이는 vector store 클래스를 retriever 인터페이스에 맞게 래핑한 경량 래퍼(wrapper)입니다.
Vector store에 구현된유사도 검색(similarity search)이나MMR과 같은 검색 메서드를 사용하여 vector store 내의 텍스트를 쿼리합니다.
샘플 데이터를TextLoader로 로드합니다.
```
Copyfromlangchain_community.document_loadersimportTextLoader# TextLoader를 사용하여 ""./data/appendix-keywords.txt"" 파일을 로드합니다.loader = TextLoader(""./data/appendix-keywords.txt"")
```
- loader를 사용하여 문서를 로드합니다.
- CharacterTextSplitter를 사용하여 로드된 문서를 분할하고, 분할된 텍스트를texts변수에 저장합니다.
- OpenAIEmbeddings를 사용하여 임베딩 객체를 생성합니다.
- FAISS.from_documents메서드를 사용하여texts와embeddings로부터 FAISS 벡터 데이터베이스db를 생성합니다.
loader를 사용하여 문서를 로드합니다.
CharacterTextSplitter를 사용하여 로드된 문서를 분할하고, 분할된 텍스트를texts변수에 저장합니다.
OpenAIEmbeddings를 사용하여 임베딩 객체를 생성합니다.
FAISS.from_documents메서드를 사용하여texts와embeddings로부터 FAISS 벡터 데이터베이스db를 생성합니다.
```
Copyfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportCharacterTextSplitter# 문서를 로드합니다.documents = loader.load()# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 1000이고 청크 간 중복은 없습니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)# 로드된 문서를 분할합니다.texts = text_splitter.split_documents(documents)# OpenAI 임베딩을 생성합니다.embeddings = OpenAIEmbeddings()# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.db = FAISS.from_documents(texts, embeddings)
```
db.as_retriever()메서드를 호출하여retriever객체를 생성합니다.
- as_retriever()메서드는 벡터 데이터베이스를 검색기(retriever)로 변환하여 반환합니다.
- 검색기(Retriever)는 질의(query)에 대해 유사도가 높은 문서를 검색하는 역할을 수행합니다.
as_retriever()메서드는 벡터 데이터베이스를 검색기(retriever)로 변환하여 반환합니다.
검색기(Retriever)는 질의(query)에 대해 유사도가 높은 문서를 검색하는 역할을 수행합니다.
```
Copy# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당합니다.retriever = db.as_retriever()
```
retriever객체의get_relevant_documents메서드를 사용하여 질의에 관련된 문서를 검색합니다.
- get_relevant_documents메서드는 질의를 입력으로 받습니다. 질의와 관련된 문서를 검색하고 반환합니다.
get_relevant_documents메서드는 질의를 입력으로 받습니다. 질의와 관련된 문서를 검색하고 반환합니다.
```
Copy# 관련 문서를 검색합니다.docs = retriever.get_relevant_documents(""임베딩(Embedding)은 무엇인가요?"")
docs[0]
```
```
Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'})
```
## Max Marginal Relevance (MMR)
MMR(Maximal Marginal Relevance)방식은 쿼리에 대한 관련 항목을 검색할 때 중복을 피하는 방법 중 하나입니다. 단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 검색된 항목들 사이에관련성과 다양성 사이의 균형을 보장합니다. 이는자주 발생할 수 있는, 매우 유사한 항목들만이 검색되는 상황을 방지하는 데에 유용합니다.
예를 들어, 특정 주제에 대해 정보를 찾고 있다고 가정해봅시다. 가장 관련성 높은 문서만을 반환하는 시스템은 비슷비슷한 정보를 담은 문서들을 제공할 수 있습니다. 하지만, MMR 방식을 사용하면, 검색된 문서들이 해당 주제에 대해 서로 다른 관점이나 새로운 정보를 제공하도록 합니다. 이로써 사용자는 주제에 대해 보다 폭넓은 이해를 할 수 있게 됩니다.
MMR은 두 가지 주요 요소, 즉 쿼리에 대한 문서의 관련성과이미 선택된 문서들과의 차별성을 동시에 고려합니다.
- db객체를as_retriever메서드를 사용하여 retriever로 변환합니다.
- search_type매개변수를""mmr""로 설정하여MMR(Maximal Marginal Relevance)검색 알고리즘을 사용합니다.
db객체를as_retriever메서드를 사용하여 retriever로 변환합니다.
search_type매개변수를""mmr""로 설정하여MMR(Maximal Marginal Relevance)검색 알고리즘을 사용합니다.
```
Copy# 데이터베이스를 검색기로 사용하며, MMR(Maximal Marginal Relevance) 검색 유형을 지정합니다.retriever = db.as_retriever(search_type=""mmr"")
```
retriever객체의get_relevant_documents메서드를 사용하여 질의에 관련된 문서를 검색합니다.
- get_relevant_documents메서드는 질의를 입력으로 받습니다. 질의와 관련된 문서를 검색하고 반환합니다.
get_relevant_documents메서드는 질의를 입력으로 받습니다. 질의와 관련된 문서를 검색하고 반환합니다.
```
Copy# 관련 문서를 검색합니다.docs = retriever.get_relevant_documents(""임베딩(Embedding)은 무엇인가요?"")
docs[0]
```
```
Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'})
```
## 유사도 점수 임계값 검색기(Similarity Score Threshold Retrieval)
유사도 점수 임계값을 설정하고 해당 임계값 이상의 점수를 가진 문서만 반환하는 검색 방법을 설정할 수 있습니다.
임계값을 적절히 설정함으로써관련성이 낮은 문서를 필터링하고, 질의와가장 유사한 문서만 선별할 수 있습니다.
- db.as_retriever()메서드를 사용하여 데이터베이스를 기반으로 하는 검색기(retriever)를 생성합니다.
- search_type매개변수를 ""similarity_score_threshold""로 설정하여 유사도 점수 임계값을 기준으로 검색을 수행합니다.
- search_kwargs매개변수에{""score_threshold"": 0.5}를 전달하여 유사도 점수 임계값을 0.5로 설정합니다. 이는 검색 결과의유사도 점수가 0.5 이상인 문서만 반환됨을 의미합니다.
db.as_retriever()메서드를 사용하여 데이터베이스를 기반으로 하는 검색기(retriever)를 생성합니다.
search_type매개변수를 ""similarity_score_threshold""로 설정하여 유사도 점수 임계값을 기준으로 검색을 수행합니다.
search_kwargs매개변수에{""score_threshold"": 0.5}를 전달하여 유사도 점수 임계값을 0.5로 설정합니다. 이는 검색 결과의유사도 점수가 0.5 이상인 문서만 반환됨을 의미합니다.
```
Copyretriever = db.as_retriever(# 검색 유형을 ""유사도 점수 임계값""으로 설정합니다.search_type=""similarity_score_threshold"",# 검색 인자로 점수 임계값을 0.5로 지정합니다.search_kwargs={""score_threshold"":0.75},
)
```
retriever객체의get_relevant_documents메서드를 사용하여 질의에 관련된 문서를 검색합니다.
- get_relevant_documents메서드는 질의를 입력으로 받습니다. 질의와 관련된 문서를 검색하고 반환합니다.
get_relevant_documents메서드는 질의를 입력으로 받습니다. 질의와 관련된 문서를 검색하고 반환합니다.
```
Copy# 관련 문서를 검색합니다.docs = retriever.get_relevant_documents(""임베딩(Embedding)은 무엇인가요?"")# 전체 검색 결과를 출력합니다.docs
```
```
[Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'}), Document(page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)', metadata={'source': './data/appendix-keywords.txt'}), Document(page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding', metadata={'source': './data/appendix-keywords.txt'})]
```
## Top K 명시
검색 시 사용할k와 같은 검색 키워드 인자(kwargs)를 지정할 수 있습니다.
k매개변수는 검색 결과에서 반환할 상위 결과의 개수를 나타냅니다. 적절한k값을 설정함으로써 검색 성능과 사용자 경험을 최적화할 수 있습니다.
- db객체를retriever로 변환합니다.
- as_retriever메서드를 사용하여 검색 인자search_kwargs를 전달합니다.
- search_kwargs에서k매개변수를 1로 설정하여 검색 결과로 반환할 문서의 수를 지정합니다.
db객체를retriever로 변환합니다.
as_retriever메서드를 사용하여 검색 인자search_kwargs를 전달합니다.
search_kwargs에서k매개변수를 1로 설정하여 검색 결과로 반환할 문서의 수를 지정합니다.
```
Copy# 데이터베이스를 검색하여 가장 관련성이 높은 1개의 결과를 반환하는 retriever를 생성합니다.retriever = db.as_retriever(search_kwargs={""k"":1})
```
retriever객체의get_relevant_documents메서드를 사용하여 질의에 관련된 문서를 검색합니다.
- len(docs)를 통해 검색된 관련 문서의 개수를 확인합니다.
len(docs)를 통해 검색된 관련 문서의 개수를 확인합니다.
```
Copy# 관련 문서를 검색합니다.docs = retriever.get_relevant_documents(""임베딩(Embedding)은 무엇인가요?"")# 전체 검색 결과를 출력합니다.print(docs)# 문서의 개수를 출력합니다.print(f""문서의 개수:{len(docs)}"")
```
```
[Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken', metadata={'source': './data/appendix-keywords.txt'})]
문서의 개수: 1
```"
02. 문맥 압축 검색기(ContextualCompressionRetriever),https://wikidocs.net/234097,"# 02. 문맥 압축 검색기(ContextualCompressionRetriever)

검색 시스템에서 직면하는 어려움 중 하나는 데이터를 시스템에 수집할 때 어떤 특정 질의를 처리해야 할지 미리 알 수 없다는 점입니다.
이는 질의와 가장 관련성이 높은 정보가 많은 양의 무관한 텍스트를 포함한 문서에 묻혀 있을 수 있음을 의미합니다.
이러한 전체 문서를 애플리케이션에 전달하면 더 비용이 많이 드는 LLM 호출과 품질이 낮은 응답으로 이어질 수 있습니다.
ContextualCompressionRetriever은 이 문제를 해결하기 위해 고안되었습니다.
아이디어는 간단합니다. 검색된 문서를 그대로 즉시 반환하는 대신, 주어진 질의의 맥락을 사용하여 문서를 압축함으로써 관련 정보만 반환되도록 할 수 있습니다.
여기서 ""압축""은 개별 문서의 내용을 압축하는 것과 문서를 전체적으로 필터링하는 것 모두를 의미합니다.
Contextual Compression Retriever를 사용하려면 다음이 필요합니다.
- Base Retriever
- Document Compressor
Base Retriever
Document Compressor
ContextualCompressionRetriever는 질의를 base retriever에 전달하고, 초기 문서를 가져와 Document Compressor를 통과시킵니다.
Document Compressor는 문서 목록을 가져와 문서의 내용을 줄이거나 문서를 완전히 삭제하여 목록을 축소합니다.
출처:https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v

pretty_print_docs함수는 문서 리스트를 예쁘게 출력하는 헬퍼 함수입니다.
```
Copy# 문서를 예쁘게 출력하기 위한 도우미 함수defpretty_print_docs(docs):print(f""\n{'-'*100}\n"".join(
            [f""문서{i+1}:\n\n""+ d.page_contentfori, dinenumerate(docs)]
        )
    )
```
## 바닐라 벡터 스토어 retriever 사용하기
간단한 벡터 스토어 retriever를 초기화하고 텍스트 문서를 청크 단위로 저장하는 것부터 시작해 보겠습니다.
예시 질문을 던졌을 때, retriever는 관련 있는 문서 1~2개와 관련 없는 문서 몇 개를 반환하는 것을 확인할 수 있습니다.
심지어 관련 있는 문서조차도 많은 무관한 정보를 포함하고 있습니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
```
Copyfromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportCharacterTextSplitter# TextLoader를 사용하여 ""appendix-keywords.txt"" 파일에서 문서를 로드합니다.loader = TextLoader(""./data/appendix-keywords.txt"")# CharacterTextSplitter를 사용하여 문서를 청크 크기 300자와 청크 간 중복 0으로 분할합니다.text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = loader.load_and_split(text_splitter)# OpenAIEmbeddings를 사용하여 FAISS 벡터 저장소를 생성하고 검색기로 변환합니다.retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()# 쿼리에 질문을 정의하고 관련 문서를 검색합니다.docs = retriever.get_relevant_documents(""Semantic Search 에 대해서 알려줘."")# 검색된 문서를 예쁘게 출력합니다.pretty_print_docs(docs)
```
```
문서 1:

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding
----------------------------------------------------------------------------------------------------
문서 2:

정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.
예시: 사용자가 ""커피숍 서울""이라고 검색하면, 관련된 커피숍 목록을 반환합니다.
연관키워드: 검색 엔진, 데이터 검색, 정보 검색

Page Rank
----------------------------------------------------------------------------------------------------
문서 3:

정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.
예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.
연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

Word2Vec
----------------------------------------------------------------------------------------------------
문서 4:

정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.
예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.
연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석

데이터 마이닝
```
## LLMChainExtractor를 사용하여 맥락적 압축 추가하기
이제 기본 retriever를ContextualCompressionRetriever로 감싸 보겠습니다.
LLMChainExtractor를 추가할 것인데, 이는 초기에 반환된 문서들을 반복하면서 각 문서에서 질의와 관련된 내용만 추출하는 역할을 합니다.
ContextualCompressionRetriever를 사용하여 질문에 대한 관련 문서를 압축하고 검색하는 과정을 보여줍니다.
- LLMChainExtractor를 사용하여compressor를 생성합니다.
- compressor와retriever를 기반으로ContextualCompressionRetriever를 초기화합니다.
LLMChainExtractor를 사용하여compressor를 생성합니다.
compressor와retriever를 기반으로ContextualCompressionRetriever를 초기화합니다.
```
Copyfromlangchain.retrieversimportContextualCompressionRetrieverfromlangchain.retrievers.document_compressorsimportLLMChainExtractorfromlangchain_openaiimportOpenAI

llm = OpenAI(temperature=0)# OpenAI 언어 모델 초기화# LLM을 사용하여 문서 압축기 생성compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(# 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성base_compressor=compressor,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.get_relevant_documents(# 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색""Semantic Search 에 대해서 알려줘."")
pretty_print_docs(compressed_docs)# 검색된 문서를 예쁘게 출력
```
```
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
```
```
문서 1:

Semantic Search 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝
----------------------------------------------------------------------------------------------------
문서 2:

검색 엔진, 데이터 검색, 정보 검색
----------------------------------------------------------------------------------------------------
문서 3:

검색 엔진 최적화, 웹 분석, 링크 분석
```
## 필터 사용
LLMChainFilter는 초기에 검색된 문서 중 어떤 문서를 필터링하고 어떤 문서를 반환할지 결정하기 위해 LLM 체인을 사용하는 보다 단순하지만 강력한 압축기입니다.
이 필터는 문서 내용을 변경(압축)하지 않고 문서를선택적으로 반환합니다.
- LLMChainFilter클래스를 사용하여_filter객체를 생성합니다. 이 객체는 LLM을 기반으로 문서를 필터링하는 역할을 합니다.
- ContextualCompressionRetriever클래스를 사용하여compression_retriever객체를 생성합니다. 이 객체는_filter를 기반 압축기로,retriever를 기반 검색기로 사용합니다.
- compression_retriever의get_relevant_documents메서드를 호출하여 주어진 질문과 관련된 문서를 압축하여 가져옵니다.
LLMChainFilter클래스를 사용하여_filter객체를 생성합니다. 이 객체는 LLM을 기반으로 문서를 필터링하는 역할을 합니다.
ContextualCompressionRetriever클래스를 사용하여compression_retriever객체를 생성합니다. 이 객체는_filter를 기반 압축기로,retriever를 기반 검색기로 사용합니다.
compression_retriever의get_relevant_documents메서드를 호출하여 주어진 질문과 관련된 문서를 압축하여 가져옵니다.
```
Copyfromlangchain.retrievers.document_compressorsimportLLMChainFilter# LLM을 사용하여 LLMChainFilter 객체를 생성합니다._filter= LLMChainFilter.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(# LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.base_compressor=_filter,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.get_relevant_documents(# 쿼리""Semantic Search 에 대해서 알려줘."")
pretty_print_docs(compressed_docs)# 압축된 문서를 예쁘게 출력합니다.
```
```
문서 1:

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding
----------------------------------------------------------------------------------------------------
문서 2:

정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.
예시: 사용자가 ""커피숍 서울""이라고 검색하면, 관련된 커피숍 목록을 반환합니다.
연관키워드: 검색 엔진, 데이터 검색, 정보 검색

Page Rank
```
각각의 검색된 문서에 대해 추가적인 LLM 호출을 수행하는 것은 비용이 많이 들고 속도가 느립니다.
EmbeddingsFilter는 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환함으로써 더 저렴하고 빠른 옵션을 제공합니다.
이를 통해 검색 결과의 관련성을 유지하면서도 계산 비용과 시간을 절약할 수 있습니다.
EmbeddingsFilter와ContextualCompressionRetriever를 사용하여 관련 문서를 압축하고 검색하는 과정입니다.
- EmbeddingsFilter를 초기화하며, 임베딩 객체와유사도 임계값(0.86)을 설정합니다.
- ContextualCompressionRetriever를 초기화하며, EmbeddingsFilter를 기본 압축기로, 이전에 정의된retriever를 기본 검색기로 사용합니다.
- compression_retriever.get_relevant_documents()메서드를 호출하여 주어진 질문과 관련된 문서를 압축하고 검색합니다.
EmbeddingsFilter를 초기화하며, 임베딩 객체와유사도 임계값(0.86)을 설정합니다.
ContextualCompressionRetriever를 초기화하며, EmbeddingsFilter를 기본 압축기로, 이전에 정의된retriever를 기본 검색기로 사용합니다.
compression_retriever.get_relevant_documents()메서드를 호출하여 주어진 질문과 관련된 문서를 압축하고 검색합니다.
```
Copyfromlangchain.retrievers.document_compressorsimportEmbeddingsFilterfromlangchain_openaiimportOpenAIEmbeddings

embeddings = OpenAIEmbeddings()# 유사도 임계값이 0.76인 EmbeddingsFilter 객체를 생성합니다.embeddings_filter = EmbeddingsFilter(
    embeddings=embeddings, similarity_threshold=0.86)# 기본 압축기로 embeddings_filter를, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter, base_retriever=retriever
)# ContextualCompressionRetriever 객체를 사용하여 관련 문서를 검색합니다.compressed_docs = compression_retriever.get_relevant_documents(# 쿼리""Semantic Search 에 대해서 알려줘."")# 압축된 문서를 예쁘게 출력합니다.pretty_print_docs(compressed_docs)
```
```
문서 1:

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding
```
## 파이프라인 생성(압축기+문서 변환기)
DocumentCompressorPipeline을 사용하면 여러 compressor를 순차적으로 결합할 수 있습니다.
Compressor와 함께BaseDocumentTransformer를 파이프라인에 추가할 수 있는데, 이는 맥락적 압축을 수행하지 않고 단순히 문서 집합에 대한 변환을 수행합니다.
예를 들어,TextSplitter는 문서를 더 작은 조각으로 분할하기 위해 document transformer로 사용될 수 있으며,EmbeddingsRedundantFilter는 문서 간의 임베딩 유사성을 기반으로 중복 문서를 필터링하는 데 사용될 수 있습니다.
아래에서는 먼저 문서를 더 작은 청크로 분할한 다음, 중복 문서를 제거하고, 쿼리와의 관련성을 기준으로 필터링하여 compressor pipeline을 생성합니다.
DocumentCompressorPipeline을 사용하여 문서 압축 파이프라인을 구성합니다.
- CharacterTextSplitter를 사용하여 문서를 분할합니다.
- EmbeddingsRedundantFilter를 사용하여 중복된 내용을 필터링합니다.
- EmbeddingsFilter를 사용하여 유사도 임계값이 0.86 이상인 관련 내용만 필터링합니다.
- 위의 세 가지 변환기를DocumentCompressorPipeline에 전달하여 파이프라인을 구성합니다.
CharacterTextSplitter를 사용하여 문서를 분할합니다.
EmbeddingsRedundantFilter를 사용하여 중복된 내용을 필터링합니다.
EmbeddingsFilter를 사용하여 유사도 임계값이 0.86 이상인 관련 내용만 필터링합니다.
위의 세 가지 변환기를DocumentCompressorPipeline에 전달하여 파이프라인을 구성합니다.
```
Copyfromlangchain.retrievers.document_compressorsimportDocumentCompressorPipelinefromlangchain_community.document_transformersimportEmbeddingsRedundantFilterfromlangchain_text_splittersimportCharacterTextSplitter# 문자 기반 텍스트 분할기를 생성하고, 청크 크기를 300으로, 청크 간 중복을 0으로 설정합니다.splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)# 임베딩을 사용하여 중복 필터를 생성합니다.redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)# 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정합니다.relevant_filter = EmbeddingsFilter(
    embeddings=embeddings, similarity_threshold=0.86)

pipeline_compressor = DocumentCompressorPipeline(# 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터를 변환기로 설정합니다.transformers=[splitter, redundant_filter, relevant_filter]
)
```
ContextualCompressionRetriever를 초기화하며,base_compressor로pipeline_compressor를,base_retriever로retriever를 사용합니다.
```
Copycompression_retriever = ContextualCompressionRetriever(# 기본 압축기로 pipeline_compressor를 사용하고, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever를 초기화합니다.base_compressor=pipeline_compressor,
    base_retriever=retriever,
)

compressed_docs = compression_retriever.get_relevant_documents(# 쿼리""Semantic Search 에 대해서 알려줘."")# 압축된 문서를 예쁘게 출력합니다.pretty_print_docs(compressed_docs)
```
```
문서 1:

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding
```"
03. 앙상블 검색기(EnsembleRetriever),https://wikidocs.net/234100,"# 03. 앙상블 검색기(EnsembleRetriever)

EnsembleRetriever는 여러 retriever를 입력으로 받아get_relevant_documents()메서드의 결과를 앙상블하고,Reciprocal Rank Fusion알고리즘을 기반으로 결과를 재순위화합니다.
서로 다른 알고리즘의 장점을 활용함으로써,EnsembleRetriever는 단일 알고리즘보다 더 나은 성능을 달성할 수 있습니다.
가장 일반적인 패턴은sparse retriever(예: BM25)와dense retriever(예: embedding similarity)를 결합하는 것인데, 이는 두 retriever의 장점이 상호 보완적이기 때문입니다. 이를""hybrid search""라고도 합니다.
Sparse retriever는 키워드를 기반으로 관련 문서를 찾는 데 효과적이며, dense retriever는 의미적 유사성을 기반으로 관련 문서를 찾는 데 효과적입니다.
```
Copy%pip install -qU rank_bm25 deeplake > /dev/null
```
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
```
Copyfromlangchain.retrieversimportBM25Retriever, EnsembleRetrieverfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddings
```
- 두 개의 문서 리스트(doc_list_1과doc_list_2)를 정의합니다.
- EnsembleRetriever를 초기화하여BM25Retriever와FAISS검색기를 결합합니다. 각 검색기의 가중치를 설정됩니다.
두 개의 문서 리스트(doc_list_1과doc_list_2)를 정의합니다.
EnsembleRetriever를 초기화하여BM25Retriever와FAISS검색기를 결합합니다. 각 검색기의 가중치를 설정됩니다.
```
Copy# 비타민 별 섭취할 수 있는 음식 정보doc_list_1 = [""비타민A : 당근, 시금치, 감자 등의 주황색과 녹색 채소에서 섭취할 수 있습니다."",""비타민B : 전곡물, 콩, 견과류, 육류 등 다양한 식품에서 찾을 수 있습니다."",""비타민C : 오렌지, 키위, 딸기, 브로콜리, 피망 등의 과일과 채소에 많이 들어 있습니다."",""비타민D : 연어, 참치, 버섯, 우유, 계란 노른자 등에 함유되어 있습니다."",""비타민E : 해바라기씨, 아몬드, 시금치, 아보카도 등에서 섭취할 수 있습니다."",
]# 비타민 별 효능 정보doc_list_2 = [""비타민A : 시력과 피부 건강을 지원합니다."",""비타민B : 에너지 대사와 신경계 기능을 돕습니다."",""비타민C : 면역 체계를 강화하고 콜라겐 생성을 촉진합니다."",""비타민D : 뼈 건강과 면역 체계를 지원합니다."",""비타민E : 항산화 작용을 통해 세포를 보호합니다."",
]# bm25 retriever와 faiss retriever를 초기화합니다.bm25_retriever = BM25Retriever.from_texts(# doc_list_1의 텍스트와 메타데이터를 사용하여 BM25Retriever를 초기화합니다.doc_list_1,
    metadatas=[{""source"":1}] *len(doc_list_1),
)
bm25_retriever.k =1# BM25Retriever의 검색 결과 개수를 1로 설정합니다.embedding = OpenAIEmbeddings()# OpenAI 임베딩을 사용합니다.faiss_vectorstore = FAISS.from_texts(# doc_list_2의 텍스트와 임베딩, 메타데이터를 사용하여 FAISS 벡터 저장소를 초기화합니다.doc_list_2,
    embedding,
    metadatas=[{""source"":2}] *len(doc_list_2),
)# FAISS 벡터 저장소를 사용하여 retriever를 생성하고, 검색 결과 개수를 1로 설정합니다.faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={""k"":1})# 앙상블 retriever를 초기화합니다.ensemble_retriever = EnsembleRetriever(# BM25Retriever와 FAISS retriever를 사용하여 EnsembleRetriever를 초기화하고, 각 retriever의 가중치를 0.6:0.4로 설정합니다.retrievers=[bm25_retriever, faiss_retriever],
    weights=[0.6,0.4],
    search_type=""mmr"",
)
```
ensemble_retriever객체의get_relevant_documents()메서드를 호출하여 관련성 높은 문서를 검색합니다.
```
Copy# 검색 결과 문서를 가져옵니다.query =""비타민A 의 효능은?""ensemble_result = ensemble_retriever.get_relevant_documents(query)
bm25_result = bm25_retriever.get_relevant_documents(query)
faiss_result = faiss_retriever.get_relevant_documents(query)# 가져온 문서를 출력합니다.print(""[Ensemble Retriever]\n"", ensemble_result, end=""\n\n"")print(""[BM25 Retriever]\n"", bm25_result, end=""\n\n"")print(""[FAISS Retriever]\n"", faiss_result, end=""\n\n"")
```
```
[Ensemble Retriever]
 [Document(page_content='비타민A : 당근, 시금치, 감자 등의 주황색과 녹색 채소에서 섭취할 수 있습니다.', metadata={'source': 1}), Document(page_content='비타민A : 시력과 피부 건강을 지원합니다.', metadata={'source': 2})]

[BM25 Retriever]
 [Document(page_content='비타민A : 당근, 시금치, 감자 등의 주황색과 녹색 채소에서 섭취할 수 있습니다.', metadata={'source': 1})]

[FAISS Retriever]
 [Document(page_content='비타민A : 시력과 피부 건강을 지원합니다.', metadata={'source': 2})]
```
```
Copy# 검색 결과 문서를 가져옵니다.query =""시력에 좋은 비타민은?""ensemble_result = ensemble_retriever.get_relevant_documents(query)
bm25_result = bm25_retriever.get_relevant_documents(query)
faiss_result = faiss_retriever.get_relevant_documents(query)# 가져온 문서를 출력합니다.print(""[Ensemble Retriever]\n"", ensemble_result, end=""\n\n"")print(""[BM25 Retriever]\n"", bm25_result, end=""\n\n"")print(""[FAISS Retriever]\n"", faiss_result, end=""\n\n"")
```
```
[Ensemble Retriever]
 [Document(page_content='비타민E : 해바라기씨, 아몬드, 시금치, 아보카도 등에서 섭취할 수 있습니다.', metadata={'source': 1}), Document(page_content='비타민A : 시력과 피부 건강을 지원합니다.', metadata={'source': 2})]

[BM25 Retriever]
 [Document(page_content='비타민E : 해바라기씨, 아몬드, 시금치, 아보카도 등에서 섭취할 수 있습니다.', metadata={'source': 1})]

[FAISS Retriever]
 [Document(page_content='비타민A : 시력과 피부 건강을 지원합니다.', metadata={'source': 2})]
```
```
Copy# 검색 결과 문서를 가져옵니다.query =""비타민E 는 어떻게 섭취할 수 있나요?""ensemble_result = ensemble_retriever.get_relevant_documents(query)
bm25_result = bm25_retriever.get_relevant_documents(query)
faiss_result = faiss_retriever.get_relevant_documents(query)# 가져온 문서를 출력합니다.print(""[Ensemble Retriever]\n"", ensemble_result, end=""\n\n"")print(""[BM25 Retriever]\n"", bm25_result, end=""\n\n"")print(""[FAISS Retriever]\n"", faiss_result, end=""\n\n"")
```
```
[Ensemble Retriever]
 [Document(page_content='비타민E : 해바라기씨, 아몬드, 시금치, 아보카도 등에서 섭취할 수 있습니다.', metadata={'source': 1}), Document(page_content='비타민E : 항산화 작용을 통해 세포를 보호합니다.', metadata={'source': 2})]

[BM25 Retriever]
 [Document(page_content='비타민E : 해바라기씨, 아몬드, 시금치, 아보카도 등에서 섭취할 수 있습니다.', metadata={'source': 1})]

[FAISS Retriever]
 [Document(page_content='비타민E : 항산화 작용을 통해 세포를 보호합니다.', metadata={'source': 2})]
```
## 런타임 설정
런타임에서도 retriever를 구성할 수 있습니다. 이를 위해서는 필드를 구성 가능한 것으로 표시해야 합니다.
- ConfigurableField클래스를langchain_core.runnables모듈에서 임포트합니다.
ConfigurableField클래스를langchain_core.runnables모듈에서 임포트합니다.
```
Copyfromlangchain_core.runnablesimportConfigurableField
```
- faiss_retriever객체를 생성하며,faiss_vectorstore의as_retriever메서드를 사용합니다.
- search_kwargs매개변수를 통해 검색 시 반환할 결과의 개수(k)를 2로 설정합니다.
- configurable_fields메서드를 사용하여faiss_retriever의 설정 가능한 필드를 정의합니다.
- search_kwargs필드를ConfigurableField객체로 정의합니다.
- 필드의 ID는 ""search_kwargs_faiss""로 설정합니다.
- 필드의 이름은 ""Search Kwargs""로 설정합니다.
- 필드에 대한 설명을 ""The search kwargs to use""로 설정합니다.
faiss_retriever객체를 생성하며,faiss_vectorstore의as_retriever메서드를 사용합니다.
search_kwargs매개변수를 통해 검색 시 반환할 결과의 개수(k)를 2로 설정합니다.
configurable_fields메서드를 사용하여faiss_retriever의 설정 가능한 필드를 정의합니다.
search_kwargs필드를ConfigurableField객체로 정의합니다.
필드의 ID는 ""search_kwargs_faiss""로 설정합니다.
필드의 이름은 ""Search Kwargs""로 설정합니다.
필드에 대한 설명을 ""The search kwargs to use""로 설정합니다.
```
Copyfaiss_retriever = faiss_vectorstore.as_retriever(# 검색 시 반환할 결과의 개수를 설정합니다.search_kwargs={""k"":2}
).configurable_fields(
    search_kwargs=ConfigurableField(# 검색 매개변수의 고유 식별자를 설정합니다.id=""search_kwargs_faiss"",# 검색 매개변수의 이름을 설정합니다.name=""Search Kwargs"",# 검색 매개변수에 대한 설명을 작성합니다.description=""The search kwargs to use"",
    )
)
```
EnsembleRetriever를 사용하여 두 개의 검색기를 앙상블로 결합하는 코드입니다.
- bm25_retriever와faiss_retriever를 리스트로 전달하여 앙상블에 포함시킵니다.
- weights파라미터를 통해 각 검색기의 가중치를 [0.5, 0.5]로 설정합니다. 이는 두 검색기의 결과를 동일한 비율로 반영함을 의미합니다.
bm25_retriever와faiss_retriever를 리스트로 전달하여 앙상블에 포함시킵니다.
weights파라미터를 통해 각 검색기의 가중치를 [0.5, 0.5]로 설정합니다. 이는 두 검색기의 결과를 동일한 비율로 반영함을 의미합니다.
```
Copyensemble_retriever = EnsembleRetriever(# 리트리버 목록을 설정합니다. 여기서는 bm25_retriever와 faiss_retriever를 사용합니다.retrievers=[bm25_retriever, faiss_retriever],# 각 리트리버의 가중치를 설정합니다. 여기서는 두 리트리버에 동일한 가중치 0.5를 부여합니다.weights=[0.5,0.5],
)
```
ensemble_retriever를 사용하여 ""apples""라는 질의어에 대한 문서를 검색합니다.
- 검색 시config매개변수를 통해 검색 설정을 지정합니다.
- search_kwargs_faiss옵션의k값을 1로 설정하여 가장 관련성이 높은 문서 1개를 반환하도록 합니다.
검색 시config매개변수를 통해 검색 설정을 지정합니다.
search_kwargs_faiss옵션의k값을 1로 설정하여 가장 관련성이 높은 문서 1개를 반환하도록 합니다.
FAISS retriever에서 하나의 source만 반환되는 것에 주목해 주세요. 이는 실행 시점에 관련 설정을 전달하기 때문입니다.
```
Copy# 검색 설정을 지정합니다. Faiss 검색에서 k=1로 설정하여 가장 유사한 문서 1개를 반환하도록 합니다.config = {""configurable"": {""search_kwargs_faiss"": {""k"":1}}}# 앙상블 검색기를 사용하여 ""apples""에 대한 검색을 수행하고, 설정된 config를 적용합니다. 검색 결과는 docs 변수에 저장됩니다.docs = ensemble_retriever.invoke(""apples"", config=config)
docs# 검색 결과인 docs를 출력합니다.
```
```
[Document(page_content='You like apples', metadata={'source': 2}), Document(page_content='I like apples', metadata={'source': 1}), Document(page_content='Apples and oranges are fruits', metadata={'source': 1})]
```"
04. 긴 문맥 재정렬(LongContextReorder),https://wikidocs.net/234101,"# 04. 긴 문맥 재정렬(LongContextReorder)

모델의 아키텍처와 상관없이, 10개 이상의 검색된 문서를 포함할 경우 성능이 상당히 저하됩니다.
간단히 말해, 모델이 긴 컨텍스트 중간에 있는 관련 정보에 접근해야 할 때, 제공된 문서를 무시하는 경향이 있습니다.
자세한 내용은 다음 논문을 참조하세요
- https://arxiv.org/abs/2307.03172
https://arxiv.org/abs/2307.03172
이 문제를 피하기 위해, 검색 후 문서의 순서를 재배열하여 성능 저하를 방지할 수 있습니다.
```
Copy%pip install --upgrade --quiet  sentence-transformers > /dev/null
```
- Chroma벡터 저장소를 사용하여 텍스트 데이터를 저장하고 검색할 수 있는retriever를 생성합니다.
- retriever의get_relevant_documents메서드를 사용하여 주어진 쿼리에 대해 관련성이 높은 문서를 검색합니다.
Chroma벡터 저장소를 사용하여 텍스트 데이터를 저장하고 검색할 수 있는retriever를 생성합니다.
retriever의get_relevant_documents메서드를 사용하여 주어진 쿼리에 대해 관련성이 높은 문서를 검색합니다.
```
Copyfromlangchain.promptsimportPromptTemplatefromlangchain_community.document_transformersimportLongContextReorderfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_community.vectorstoresimportChroma# 임베딩을 가져옵니다.embeddings = OpenAIEmbeddings()

texts = [""이건 그냥 내가 아무렇게나 적어본 글입니다."",""사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다."",""아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다."",""챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다."",""챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다."",""애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다."",""ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다."",""비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다."",""ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다."",""FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다."",
]# 검색기를 생성합니다. (K는 10으로 설정합니다)retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(
    search_kwargs={""k"":10}
)
```
get_relevant_documents메서드는 검색기에게 쿼리를 전달하고, 결과를 반환합니다.
```
Copyquery =""ChatGPT에 대해 무엇을 말해줄 수 있나요?""# 관련성 점수에 따라 정렬된 관련 문서를 가져옵니다.docs = retriever.get_relevant_documents(query)
docs
```
```
[Document(page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.'), Document(page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'), Document(page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'), Document(page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'), Document(page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'), Document(page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'), Document(page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'), Document(page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'), Document(page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'), Document(page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.')]
```
LongContextReorder클래스의 인스턴스인reordering을 생성합니다.
- reordering.transform_documents(docs)를 호출하여 문서 목록docs를 재정렬합니다.
- 덜 관련된 문서는 목록의 중간에 위치하고, 더 관련된 문서는 시작과 끝에 위치하도록 재정렬됩니다.
reordering.transform_documents(docs)를 호출하여 문서 목록docs를 재정렬합니다.
덜 관련된 문서는 목록의 중간에 위치하고, 더 관련된 문서는 시작과 끝에 위치하도록 재정렬됩니다.
```
Copy# 문서를 재정렬합니다# 덜 관련된 문서는 목록의 중간에 위치하고 더 관련된 요소는 시작/끝에 위치합니다.reordering = LongContextReorder()
reordered_docs = reordering.transform_documents(docs)# 4개의 관련 문서가 시작과 끝에 위치하는지 확인합니다.reordered_docs
```
```
[Document(page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'), Document(page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'), Document(page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'), Document(page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'), Document(page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.'), Document(page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'), Document(page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'), Document(page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'), Document(page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'), Document(page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.')]
```
## Context Reordering을 사용하여 질의-응답 체인 생성
```
Copyfromlangchain_core.promptsimportformat_document# 기본 문서 프롬프트를 생성합니다. (source, metadata 등을 추가할 수 있습니다)DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(
    template=""{page_content} [source: teddylee777@gmail.com]"")defcombine_documents(docs,# 문서 목록# 문서 프롬프트 (기본값: DEFAULT_DOCUMENT_PROMPT)document_prompt=DEFAULT_DOCUMENT_PROMPT,
    document_separator=""\n"",# 문서 구분자 (기본값: 두 개의 줄바꿈)):# context 에 입력으로 넣기 위한 문서 병합doc_strings = [f""[{i}]{format_document(doc, document_prompt)}""fori, docinenumerate(docs)
    ]# 각 문서를 주어진 프롬프트로 포맷팅하여 문자열 목록 생성returndocument_separator.join(
        doc_strings
    )# 포맷팅된 문서 문자열을 구분자로 연결하여 반환defreorder_documents(docs):# 재정렬reordering = LongContextReorder()
    reordered_docs = reordering.transform_documents(docs)
    combined = combine_documents(reordered_docs, document_separator=""\n"")print(combined)returncombined
```
재정렬된 문서를 출력합니다.
```
Copy# 재정렬된 문서를 출력_ = reorder_documents(docs)
```
```
[0] ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다. [source: teddylee777@gmail.com]
[1] 챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다. [source: teddylee777@gmail.com]
[2] 이건 그냥 내가 아무렇게나 적어본 글입니다. [source: teddylee777@gmail.com]
[3] 애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다. [source: teddylee777@gmail.com]
[4] FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다. [source: teddylee777@gmail.com]
[5] 아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다. [source: teddylee777@gmail.com]
[6] 비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다. [source: teddylee777@gmail.com]
[7] 챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. [source: teddylee777@gmail.com]
[8] 사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다. [source: teddylee777@gmail.com]
[9] ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. [source: teddylee777@gmail.com]
```
```
Copyfromlangchain.promptsimportChatPromptTemplatefromoperatorimportitemgetterfromlangchain_openaiimportChatOpenAIfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.runnablesimportRunnableLambda# 프롬프트 템플릿template =""""""Given this text extracts:
{context}

-----
Please answer the following question:
{question}

Answer in the following languages: {language}
""""""# 프롬프트 정의prompt = ChatPromptTemplate.from_template(template)# Chain 정의chain = (
    {""context"": itemgetter(""question"")
        | retriever
        | RunnableLambda(reorder_documents),# 질문을 기반으로 문맥을 검색합니다.""question"": itemgetter(""question""),# 질문을 추출합니다.""language"": itemgetter(""language""),# 답변 언어를 추출합니다.}
    | prompt# 프롬프트 템플릿에 값을 전달합니다.| ChatOpenAI()# 언어 모델에 프롬프트를 전달합니다.| StrOutputParser()# 모델의 출력을 문자열로 파싱합니다.)
```
question에 쿼리를 입력하고language에 언어를 입력합니다.
- 재정렬된 문서의 검색 결과도 확인합니다.
재정렬된 문서의 검색 결과도 확인합니다.
```
Copyanswer = chain.invoke(
    {""question"":""ChatGPT에 대해 무엇을 말해줄 수 있나요?"",""language"":""KOREAN""}
)
```
```
[0] ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다. [source: teddylee777@gmail.com]
[1] 챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다. [source: teddylee777@gmail.com]
[2] 이건 그냥 내가 아무렇게나 적어본 글입니다. [source: teddylee777@gmail.com]
[3] 애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다. [source: teddylee777@gmail.com]
[4] FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다. [source: teddylee777@gmail.com]
[5] 아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다. [source: teddylee777@gmail.com]
[6] 비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다. [source: teddylee777@gmail.com]
[7] 챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. [source: teddylee777@gmail.com]
[8] 사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다. [source: teddylee777@gmail.com]
[9] ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. [source: teddylee777@gmail.com]
```
답변을 출력합니다.
```
Copyprint(answer)
```
```
ChatGPT는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습한 AI이며, 다양한 질문에 답할 수 있습니다. 또한 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. ChatGPT의 기능은 지속적으로 발전하고 있으며, OpenAI에 의해 개발되었습니다.
```"
05. 상위 문서 검색기(ParentDocumentRetriever),https://wikidocs.net/234164,"# 05. 상위 문서 검색기(ParentDocumentRetriever)

문서 검색과 문서 분할의 균형 잡기
문서 검색 과정에서 문서를 적절한 크기의 조각(청크)으로 나누는 것은 다음의상충되는 두 가지 중요한 요소를 고려해야 합니다.
작은 문서를 원하는 경우: 이렇게 하면 문서의 임베딩이 그 의미를 가장 정확하게 반영할 수 있습니다. 문서가 너무 길면 임베딩이 의미를 잃어버릴 수 있습니다.
각 청크의 맥락이 유지되도록 충분히 긴 문서를 원하는 경우입니다.
ParentDocumentRetriever의 역할
이 두 요구 사항 사이의 균형을 맞추기 위해ParentDocumentRetriever라는 도구가 사용됩니다. 이 도구는 문서를 작은 조각으로 나누고, 이 조각들을 관리합니다. 검색을 진행할 때는, 먼저 이 작은 조각들을 찾아낸 다음, 이 조각들이 속한 원본 문서(또는 더 큰 조각)의 식별자(ID)를 통해 전체적인 맥락을 파악할 수 있습니다.
여기서 '부모 문서'란, 작은 조각이 나누어진 원본 문서를 말합니다. 이는 전체 문서일 수도 있고, 비교적 큰 다른 조각일 수도 있습니다. 이 방식을 통해 문서의 의미를 정확하게 파악하면서도, 전체적인 맥락을 유지할 수 있게 됩니다.
정리
- 문서 간의 계층 구조 활용:ParentDocumentRetriever는 문서 검색의 효율성을 높이기 위해 문서 간의 계층 구조를 활용합니다.
- 검색 성능 향상: 관련성 높은 문서를 빠르게 찾아내며, 주어진 질문에 대한 가장 적합한 답변을 제공하는 문서를 효과적으로 찾아낼 수 있습니다.
  문서를 검색할 때 자주 발생하는 두 가지 상충되는 요구 사항이 있습니다:
문서 간의 계층 구조 활용:ParentDocumentRetriever는 문서 검색의 효율성을 높이기 위해 문서 간의 계층 구조를 활용합니다.
검색 성능 향상: 관련성 높은 문서를 빠르게 찾아내며, 주어진 질문에 대한 가장 적합한 답변을 제공하는 문서를 효과적으로 찾아낼 수 있습니다.
  문서를 검색할 때 자주 발생하는 두 가지 상충되는 요구 사항이 있습니다:
```
Copy%pip install -qU deeplake
```
여러 개의 텍스트 파일을 로드하기 위해TextLoader객체를 생성하고 데이터를 로드합니다.
```
Copyfromlangchain.storageimportInMemoryStorefromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportChromafromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportRecursiveCharacterTextSplitterfromlangchain.retrieversimportParentDocumentRetriever
```
```
Copyloaders = [# 파일을 로드합니다.TextLoader(""./data/ai-story.txt""),# 파일을 로드합니다.TextLoader(""./data/appendix-keywords.txt""),
]
docs = []# 빈 리스트를 생성합니다.forloaderinloaders:# loaders 리스트의 각 로더에 대해 반복합니다.docs.extend(
        loader.load()
    )# 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.
```
## 전체 문서 검색
이 모드에서는 전체 문서를 검색하고자 합니다. 따라서child_splitter만 지정하도록 하겠습니다.
- 나중에는parent_splitter도 지정하여 결과를 비교해 보겠습니다.
나중에는parent_splitter도 지정하여 결과를 비교해 보겠습니다.
```
Copy# 자식 분할기를 생성합니다.child_splitter = RecursiveCharacterTextSplitter(chunk_size=300)# DB를 생성합니다.vectorstore = Chroma(
    collection_name=""full_documents"", embedding_function=OpenAIEmbeddings()
)

store = InMemoryStore()# Retriever 를 생성합니다.retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
)
```
retriever.add_documents(docs, ids=None)함수로 문서목록을 추가합니다.
- ids가None이면 자동으로 생성됩니다.
- add_to_docstore=False로 설정시 document 를 중복으로 추가하지 않습니다. 단, 중복을 체크하기 위한ids값이 필수 값으로 요구됩니다.
ids가None이면 자동으로 생성됩니다.
add_to_docstore=False로 설정시 document 를 중복으로 추가하지 않습니다. 단, 중복을 체크하기 위한ids값이 필수 값으로 요구됩니다.
```
Copy# 문서를 검색기에 추가합니다. docs는 문서 목록이고, ids는 문서의 고유 식별자 목록입니다.retriever.add_documents(docs, ids=None, add_to_docstore=True)
```
이 코드는 두 개의 키를 반환해야 합니다. 그 이유는 우리가 두 개의 문서를 추가했기 때문입니다.
- store객체의yield_keys()메서드를 호출하여 반환된 키(key) 값들을 리스트로 변환합니다.
store객체의yield_keys()메서드를 호출하여 반환된 키(key) 값들을 리스트로 변환합니다.
```
Copy# 저장소의 모든 키를 리스트로 반환합니다.list(store.yield_keys())
```
```
['477b9ba2-d39c-4f42-80ce-08c8f0d1b08a', '1ea8adb5-5316-465d-8a41-f01b8d123152']
```
이제 벡터 스토어 검색 기능을 호출해 보겠습니다.
우리가 작은 청크(chunk)들을 저장하고 있기 때문에, 검색 결과로 작은 청크들이 반환되는 것을 확인할 수 있을 것입니다.
vectorstore객체의similarity_search메서드를 사용하여 유사도 검색을 수행합니다.
```
Copy# 유사도 검색을 수행합니다.sub_docs = vectorstore.similarity_search(""Word2Vec"")
```
sub_docs[0].page_content를 출력합니다.
```
Copy# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.print(sub_docs[0].page_content)
```
```
Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)
```
이제 전체 retriever에서 검색해 보겠습니다. 이 과정에서는 작은 청크(chunk)들이 위치한문서를 반환하기 때문에 상대적으로 큰 문서들이 반환될 것입니다.
retriever객체의get_relevant_documents()메서드를 사용하여 쿼리와 관련된 문서를 검색합니다.
```
Copy# 문서를 검색하여 가져옵니다.retrieved_docs = retriever.get_relevant_documents(""Word2Vec"")
```
검색된 문서(retrieved_docs[0])의 일부 내용을 출력합니다.
```
Copy# 검색된 문서의 문서의 페이지 내용의 길이를 출력합니다.print(f""문서의 길이:{len(retrieved_docs[0].page_content)}"",
    end=""\n\n=====================\n\n"",
)# 문서의 일부를 출력합니다.print(retrieved_docs[0].page_content[2000:2500])
```
```
문서의 길이: 5733

=====================

 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.
연관키워드: 혁신, 기술, 비즈니스 모델

Crawling

정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.
예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.
연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)

정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을
```
## 더 큰 크기의 청크로부터 검색
이전의 결과처럼전체 문서가 너무 커서 있는 그대로 검색하기에는 부적합할 수 있습니다.
이런 경우, 실제로 우리가 하고 싶은 것은 먼저 원시 문서를 더 큰 청크로 분할한 다음, 더 작은 청크로 분할하는 것입니다.
그런 다음 작은 청크들을 인덱싱하지만, 검색 시에는 더 큰 청크를 검색합니다 (그러나 여전히 전체 문서는 아닙니다).
- RecursiveCharacterTextSplitter를 사용하여 부모 문서와 자식 문서를 생성합니다.
- 부모 문서는chunk_size가 1200으로 설정되어 있습니다.
- 자식 문서는chunk_size가 300으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성됩니다.
RecursiveCharacterTextSplitter를 사용하여 부모 문서와 자식 문서를 생성합니다.
부모 문서는chunk_size가 1200으로 설정되어 있습니다.
자식 문서는chunk_size가 300으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성됩니다.
```
Copy# 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.parent_splitter = RecursiveCharacterTextSplitter(chunk_size=900)# 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.# 부모보다 작은 문서를 생성해야 합니다.child_splitter = RecursiveCharacterTextSplitter(chunk_size=300)# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소입니다.vectorstore = Chroma(
    collection_name=""split_parents"", embedding_function=OpenAIEmbeddings()
)# 부모 문서의 저장 계층입니다.store = InMemoryStore()
```
ParentDocumentRetriever를 초기화하는 코드입니다.
- vectorstore매개변수는 문서 벡터를 저장하는 벡터 저장소를 지정합니다.
- docstore매개변수는 문서 데이터를 저장하는 문서 저장소를 지정합니다.
- child_splitter매개변수는 하위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.
- parent_splitter매개변수는 상위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.
vectorstore매개변수는 문서 벡터를 저장하는 벡터 저장소를 지정합니다.
docstore매개변수는 문서 데이터를 저장하는 문서 저장소를 지정합니다.
child_splitter매개변수는 하위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.
parent_splitter매개변수는 상위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.
ParentDocumentRetriever는 계층적 문서 구조를 처리하며, 상위 문서와 하위 문서를 별도로 분할하고 저장합니다. 이를 통해 검색 시 상위 문서와 하위 문서를 효과적으로 활용할 수 있습니다.
```
Copyretriever = ParentDocumentRetriever(# 벡터 저장소를 지정합니다.vectorstore=vectorstore,# 문서 저장소를 지정합니다.docstore=store,# 하위 문서 분할기를 지정합니다.child_splitter=child_splitter,# 상위 문서 분할기를 지정합니다.parent_splitter=parent_splitter,
)
```
retriever객체에docs를 추가합니다.retriever가 검색할 수 있는 문서 집합에 새로운 문서들을 추가하는 역할을 합니다.
```
Copyretriever.add_documents(docs)# 문서를 retriever에 추가합니다.
```
이제 문서의 수가 훨씬 더 많아진 것을 볼 수 있습니다. 이는 더 큰 청크(chunk)들입니다.
```
Copy# 저장소에서 키를 생성하고 리스트로 변환한 후 길이를 반환합니다.len(list(store.yield_keys()))
```
```
20
```
기본 벡터 저장소가 여전히 작은 청크를 검색하는지 확인해 보겠습니다.
vectorstore객체의similarity_search메서드를 사용하여 유사도 검색을 수행합니다.
```
Copy# 유사도 검색을 수행합니다.sub_docs = vectorstore.similarity_search(""Word2Vec"")# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.print(sub_docs[0].page_content)
```
```
Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)
```
이번에는retriever객체의get_relevant_documents메서드를 사용하여 문서를 검색합니다.
```
Copy# 문서를 검색하여 가져옵니다.retrieved_docs = retriever.get_relevant_documents(""Word2Vec"")# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.print(retrieved_docs[0].page_content)
```
```
정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.
예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.
연관키워드: 혁신, 기술, 비즈니스 모델

Crawling

정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.
예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.
연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)

정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.
예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.
연관키워드: 자연어 처리, 딥러닝, 텍스트 생성

FAISS (Facebook AI Similarity Search)
```
```
Copy# 검색된 문서 중 첫 번째 문서의 내용을 출력합니다.print(retrieved_docs[0].page_content)
```
```
정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.
예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.
연관키워드: 혁신, 기술, 비즈니스 모델

Crawling

정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.
예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.
연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진

Word2Vec

정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.
예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.
연관키워드: 자연어 처리, 임베딩, 의미론적 유사성
LLM (Large Language Model)

정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.
예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.
연관키워드: 자연어 처리, 딥러닝, 텍스트 생성

FAISS (Facebook AI Similarity Search)
```"
06. 다중 쿼리 검색기(MultiQueryRetriever),https://wikidocs.net/234109,"# 06. 다중 쿼리 검색기(MultiQueryRetriever)

거리 기반 벡터 데이터베이스 검색은 고차원 공간에서의 쿼리 임베딩(표현)과 '거리'를 기준으로 유사한 임베딩을 가진 문서를 찾는 방식입니다. 하지만 쿼리의 세부적인 차이나 임베딩이 데이터의 의미를 제대로 포착하지 못할 경우, 검색 결과가 달라질 수 있습니다. 이를 수동으로 조정하는 프롬프트 엔지니어링이나 튜닝 작업은 번거로울 수 있습니다.
이런 문제를 해결하기 위해,MultiQueryRetriever는 주어진 사용자 입력 쿼리에 대해 다양한 관점에서 여러 쿼리를 자동으로 생성하는 LLM(Language Learning Model)을 활용해 프롬프트 튜닝 과정을 자동화합니다.
이 방식은 각각의 쿼리에 대해 관련 문서 집합을 검색하고, 모든 쿼리를 아우르는 고유한 문서들의 합집합을 추출해, 잠재적으로 관련된 더 큰 문서 집합을 얻을 수 있게 해줍니다. 여러 관점에서 동일한 질문을 생성함으로써,MultiQueryRetriever는 거리 기반 검색의 제한을 일정 부분 극복하고, 더욱 풍부한 검색 결과를 제공할 수 있습니다.
- WebBaseLoader를 사용하여 블로그 게시물(https://lilianweng.github.io/posts/2023-06-23-agent/)을 로드합니다.
- RecursiveCharacterTextSplitter를 사용하여 로드된 데이터를 청크 크기 500자와 청크 간 중복 0으로 분할합니다.
- OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.
- 분할된 문서와 생성된 임베딩을 사용하여Chroma벡터 데이터베이스를 구축합니다.
WebBaseLoader를 사용하여 블로그 게시물(https://lilianweng.github.io/posts/2023-06-23-agent/)을 로드합니다.
RecursiveCharacterTextSplitter를 사용하여 로드된 데이터를 청크 크기 500자와 청크 간 중복 0으로 분할합니다.
OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.
분할된 문서와 생성된 임베딩을 사용하여Chroma벡터 데이터베이스를 구축합니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
```
Copy# 샘플 벡터DB 구축fromlangchain_community.document_loadersimportWebBaseLoaderfromlangchain.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportRecursiveCharacterTextSplitter# 블로그 포스트 로드loader = WebBaseLoader(""https://teddylee777.github.io/openai/openai-assistant-tutorial/"", encoding=""utf-8"")# 문서 분할text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = loader.load_and_split(text_splitter)# 임베딩 정의openai_embedding = OpenAIEmbeddings()# 벡터DB 생성db = FAISS.from_documents(docs, openai_embedding)# retriever 생성retriever = db.as_retriever()# 문서 검색query =""OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.""relevant_docs = retriever.get_relevant_documents(query)# 검색된 문서의 개수 출력len(relevant_docs)
```
```
4
```
검색된 결과 중 1개 문서의 내용을 출력합니다.
```
Copy# 1번 문서를 출력합니다.print(relevant_docs[1].page_content)
```
```
가장 강력한 도구로서, Assistant에게 사용자 정의 함수를 지정할 수 있습니다. 이는 Chat Completions API에서의 함수 호출과 매우 유사합니다.


Function calling(함수 호출) 도구를 사용하면 Assistant 에게 사용자 정의 함수 를 설명하여 호출해야 하는 함수를 인자와 함께 지능적으로 반환하도록 할 수 있습니다.


Assistant API는 실행 중에 함수를 호출할 때 실행을 일시 중지하며, 함수 호출 결과를 다시 제공하여 Run 실행을 계속할 수 있습니다. (이는 사용자 피드백을 받아 재게할 수 있는 의미이기도 합니다. 아래 튜토리얼에서 상세히 다룹니다).
```
## 기본 예제
MultiQueryRetriever에 사용할 LLM을 지정하고 질의 생성에 사용하면, retriever가 나머지 작업을 처리합니다.
- ChatOpenAI를 사용하여 LLM을 초기화합니다.
- MultiQueryRetriever.from_llm메서드를 사용하여retriever_from_llm객체를 생성합니다.
- 이 메서드는 벡터 저장소의 검색기(vectordb.as_retriever())와 LLM(llm)을 인자로 받습니다.
ChatOpenAI를 사용하여 LLM을 초기화합니다.
MultiQueryRetriever.from_llm메서드를 사용하여retriever_from_llm객체를 생성합니다.
이 메서드는 벡터 저장소의 검색기(vectordb.as_retriever())와 LLM(llm)을 인자로 받습니다.
MultiQueryRetriever는 LLM을 활용하여 질문을 분석하고, 관련된 문서를 벡터 저장소에서 검색하여 최종 답변을 생성하는 역할을 합니다. 이를 통해 질문에 대한 보다 정확하고 포괄적인 답변을 제공할 수 있습니다.
```
Copyfromlangchain.retrievers.multi_queryimportMultiQueryRetrieverfromlangchain_openaiimportChatOpenAI# ChatOpenAI 언어 모델을 초기화합니다. temperature는 0으로 설정합니다.llm = ChatOpenAI(temperature=0)

multiquery_retriever = MultiQueryRetriever.from_llm(# MultiQueryRetriever를 언어 모델을 사용하여 초기화합니다.# 벡터 데이터베이스의 retriever와 언어 모델을 전달합니다.retriever=db.as_retriever(),
    llm=llm,
)
```
아래는 다중 쿼리를 생성하는 중간 과정을 디버깅하기 위하여 실행하는 코드입니다.
- logging.getLogger(""langchain.retrievers.multi_query"")함수를 사용하여""langchain.retrievers.multi_query""로거를 가져옵니다.
- setLevel(logging.INFO)메서드를 호출하여 해당 로거의 로그 레벨을INFO로 설정합니다. 이는INFO레벨 이상의 로그 메시지만 출력되도록 합니다.
logging.getLogger(""langchain.retrievers.multi_query"")함수를 사용하여""langchain.retrievers.multi_query""로거를 가져옵니다.
setLevel(logging.INFO)메서드를 호출하여 해당 로거의 로그 레벨을INFO로 설정합니다. 이는INFO레벨 이상의 로그 메시지만 출력되도록 합니다.
```
Copy# 쿼리에 대한 로깅 설정importlogging

logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO)
```
- retriever_from_llm객체의get_relevant_documents메서드를 호출하여question과 관련된 문서를 검색합니다.
- unique_docs에는 검색된 관련 문서가 저장됩니다.
- len(unique_docs)를 통해 검색된 관련 문서의 개수를 확인합니다.
retriever_from_llm객체의get_relevant_documents메서드를 호출하여question과 관련된 문서를 검색합니다.
unique_docs에는 검색된 관련 문서가 저장됩니다.
len(unique_docs)를 통해 검색된 관련 문서의 개수를 확인합니다.
```
Copy# 질문을 정의합니다.question =""OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.""# 문서 검색relevant_docs = multiquery_retriever.get_relevant_documents(query=question)# 검색된 고유한 문서의 개수를 반환합니다.print(f""===============\n검색된 문서 개수:{len(relevant_docs)}"",
    end=""\n===============\n"",
)# 검색된 문서의 내용을 출력합니다.print(relevant_docs[0].page_content)
```
```
INFO:langchain.retrievers.multi_query:Generated queries: ['1. OpenAI Assistant API의 Functions는 어떻게 사용되는지 설명해 주실 수 있나요?', '2. OpenAI Assistant API의 Functions를 어떻게 활용할 수 있는지 알려주세요.', '3. OpenAI Assistant API의 Functions를 어떻게 구현하고 활용할 수 있는지에 대해 자세히 설명해 주세요.']
```
```
===============
검색된 문서 개수: 4
===============
OpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval, Functions 를 활용하는 방법에 대해 다룹니다. 이와 더불어 파일을 업로드 하는 내용과 사용자의 피드백을 제출하는 내용도 튜토리얼 말미에 포함하고 있습니다.
```
## LCEL Chain 활용하는 방법
- 사용자 정의 프롬프트 정의하고, 정의한 프롬프트와 함께 Chain 을 생성합니다.
- Chain 은 사용자의 질문을 입력 받으면 (아래의 예제에서는) 5개의 질문을 생성한 뒤""\n""구분자로 구분하여 생성된 5개 질문을 반환합니다.
사용자 정의 프롬프트 정의하고, 정의한 프롬프트와 함께 Chain 을 생성합니다.
Chain 은 사용자의 질문을 입력 받으면 (아래의 예제에서는) 5개의 질문을 생성한 뒤""\n""구분자로 구분하여 생성된 5개 질문을 반환합니다.
```
Copyfromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_core.promptsimportPromptTemplatefromlangchain_core.output_parsersimportStrOutputParser# 프롬프트 템플릿을 정의합니다.(5개의 질문을 생성하도록 프롬프트를 작성하였습니다)prompt = PromptTemplate.from_template(""""""You are an AI language model assistant. 
Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. 
By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. 
Your response should be a list of values separated by new lines, eg: `foo\nbar\nbaz\n`

#ORIGINAL QUESTION: 
{question}
"""""")# 언어 모델 인스턴스를 생성합니다.llm = ChatOpenAI(temperature=0)# LLMChain을 생성합니다.chain = {""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser()# 질문을 정의합니다.question =""OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.""# 체인을 실행하여 생성된 다중 쿼리를 확인합니다.multi_queries = chain.invoke({""question"": question})# 결과를 확인합니다.(5개 질문 생성)multi_queries
```
```
""1. How can I learn about the usage of Functions in the OpenAI Assistant API?\n2. Can you provide information on how to use Functions in the OpenAI Assistant API?\n3. What is the proper way to utilize Functions in the OpenAI Assistant API?\n4. Please explain the usage guidelines for Functions in the OpenAI Assistant API.\n5. I'm looking for instructions on how to implement Functions in the OpenAI Assistant API.""
```
이전에 생성한 Chain을MultiQueryRetriever에 전달하여 retrieve 할 수 있습니다.
```
Copymultiquery_retriever = MultiQueryRetriever.from_llm(
    llm=chain, retriever=db.as_retriever()
)
```
MultiQueryRetriever를 사용하여 문서를 검색하고 결과를 확인합니다.
```
Copy# 결과relevant_docs = multiquery_retriever.get_relevant_documents(query=question)# 검색된 고유한 문서의 개수를 반환합니다.print(f""===============\n검색된 문서 개수:{len(relevant_docs)}"",
    end=""\n===============\n"",
)# 검색된 문서의 내용을 출력합니다.print(relevant_docs[0].page_content)
```
```
INFO:langchain.retrievers.multi_query:Generated queries: ['How can I use the Functions of the OpenAI Assistant API?', 'Can you provide guidance on using the Functions of the OpenAI Assistant API?', 'What are the steps to utilize the Functions of the OpenAI Assistant API?', 'I need help understanding how to use the Functions of the OpenAI Assistant API.', 'Please explain the usage of the Functions in the OpenAI Assistant API.']
```
```
===============
검색된 문서 개수: 5
===============
OpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval, Functions 를 활용하는 방법에 대해 다룹니다. 이와 더불어 파일을 업로드 하는 내용과 사용자의 피드백을 제출하는 내용도 튜토리얼 말미에 포함하고 있습니다.
```"
07. 다중 벡터저장소 검색기(MultiVectorRetriever),https://wikidocs.net/234281,"# 07. 다중 벡터저장소 검색기(MultiVectorRetriever)

LangChain에서는 문서를 다양한 상황에서 효율적으로 쿼리할 수 있는 특별한 기능, 바로MultiVectorRetriever를 제공합니다. 이 기능을 사용하면 문서를 여러 벡터로 저장하고 관리할 수 있어, 정보 검색의 정확도와 효율성을 대폭 향상시킬 수 있습니다. 오늘은 이MultiVectorRetriever를 활용해 문서당 여러 벡터를 생성하는 몇 가지 방법을 살펴보겠습니다.
문서당 여러 벡터 생성 방법 소개
작은 청크 생성: 문서를 더 작은 단위로 나눈 후, 각 청크에 대해 별도의 임베딩을 생성합니다. 이 방식을 사용하면 문서의 특정 부분에 좀 더 세심한 주의를 기울일 수 있습니다. 이 과정은ParentDocumentRetriever를 통해 구현할 수 있어, 세부 정보에 대한 탐색이 용이해집니다.
작은 청크 생성: 문서를 더 작은 단위로 나눈 후, 각 청크에 대해 별도의 임베딩을 생성합니다. 이 방식을 사용하면 문서의 특정 부분에 좀 더 세심한 주의를 기울일 수 있습니다. 이 과정은ParentDocumentRetriever를 통해 구현할 수 있어, 세부 정보에 대한 탐색이 용이해집니다.
요약 임베딩: 각 문서의 요약을 생성하고, 이 요약으로부터 임베딩을 만듭니다. 이 요약 임베딩은 문서의 핵심 내용을 신속하게 파악하는 데 큰 도움이 됩니다. 문서 전체를 분석하는 대신 핵심적인 요약 부분만을 활용하여 효율성을 극대화할 수 있습니다.
요약 임베딩: 각 문서의 요약을 생성하고, 이 요약으로부터 임베딩을 만듭니다. 이 요약 임베딩은 문서의 핵심 내용을 신속하게 파악하는 데 큰 도움이 됩니다. 문서 전체를 분석하는 대신 핵심적인 요약 부분만을 활용하여 효율성을 극대화할 수 있습니다.
가설 질문 활용: 각 문서에 대해 적합한 가설 질문을 만들고, 이 질문에 기반한 임베딩을 생성합니다. 특정 주제나 내용에 대해 깊이 있는 탐색을 원할 때 이 방법이 유용합니다. 가설 질문은 문서의 내용을 다양한 관점에서 접근하게 해주며, 더 광범위한 이해를 가능하게 합니다.
가설 질문 활용: 각 문서에 대해 적합한 가설 질문을 만들고, 이 질문에 기반한 임베딩을 생성합니다. 특정 주제나 내용에 대해 깊이 있는 탐색을 원할 때 이 방법이 유용합니다. 가설 질문은 문서의 내용을 다양한 관점에서 접근하게 해주며, 더 광범위한 이해를 가능하게 합니다.
수동 추가 방식: 사용자가 문서 검색 시 고려해야 할 특정 질문이나 쿼리를 직접 추가할 수 있습니다. 이 방법을 통해 사용자는 검색 과정에서 보다 세밀한 제어를 할 수 있으며, 자신의 요구 사항에 맞춘 맞춤형 검색이 가능해집니다.
수동 추가 방식: 사용자가 문서 검색 시 고려해야 할 특정 질문이나 쿼리를 직접 추가할 수 있습니다. 이 방법을 통해 사용자는 검색 과정에서 보다 세밀한 제어를 할 수 있으며, 자신의 요구 사항에 맞춘 맞춤형 검색이 가능해집니다.
MultiVectorRetriever를 통해 이러한 다양한 접근 방식을 유연하게 활용함으로써, 사용자는 필요한 정보를 보다 정확하고 빠르게 찾을 수 있습니다. LangChain의 이 기능은 정보 검색 작업을 보다 효과적으로 만들어주는 훌륭한 도구입니다.
## 사용 방법
텍스트 파일에서 데이터를 로드하고, 로드된 문서들을 지정된 크기로 분할하는 전처리 과정을 수행합니다.
분할된 문서들은 추후 벡터화 및 검색 등의 작업에 사용될 수 있습니다.
```
Copyfromlangchain.storageimportInMemoryByteStorefromlangchain_community.document_loadersimportTextLoaderfromlangchain_community.vectorstoresimportChromafromlangchain_openaiimportOpenAIEmbeddingsfromlangchain_text_splittersimportRecursiveCharacterTextSplitterfromlangchain.retrievers.multi_vectorimportMultiVectorRetriever

loaders = [# 첫 번째 데이터를 로드합니다.TextLoader(""./data/ai-story.txt""),# 두 번째 데이터를 로드합니다.TextLoader(""./data/appendix-keywords.txt""),
]
docs = []# 빈 문서 리스트를 초기화합니다.forloaderinloaders:
    docs.extend(loader.load())# 각 로더에서 문서를 로드하여 docs 리스트에 추가합니다.
```
데이터로부터 로드한 원본 도큐먼트는docs변수에 담았습니다.
## 작은 청크 생성
대용량 정보를 검색하는 경우, 더 작은 단위로 정보를 임베딩하는 것이 유용할 수 있습니다.
이를 통해 임베딩은 의미론적 의미를 최대한 근접하게 포착하면서도, 가능한 한 많은 맥락을 하위 단계로 전달할 수 있습니다.
ParentDocumentRetriever가 수행하는 작업이 바로 이것입니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
```
Copy# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소importuuid

vectorstore = Chroma(
    collection_name=""full_documents"", embedding_function=OpenAIEmbeddings()
)# 부모 문서의 저장소 계층store = InMemoryByteStore()

id_key =""doc_id""# 검색기 (시작 시 비어 있음)retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=store,
    id_key=id_key,
)# 문서 ID를 생성합니다.doc_ids = [str(uuid.uuid4())for_indocs]# 두개의 생성된 id를 확인합니다.doc_ids
```
```
['72cda610-ea10-4953-b6c5-92bbfc6d5afd', '2d5252a7-3080-42c9-8cbe-6bf625d41c9f']
```
여기서 큰 청크로 분할하기 위한parent_text_splitter
더 작은 청크로 분할하기 위한child_text_splitter를 정의합니다.
```
Copy# RecursiveCharacterTextSplitter 객체를 생성합니다.parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000)# 더 작은 청크를 생성하는 데 사용할 분할기child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
```
- parent_text_splitter를 사용하여 문서를 큰 청크 단위로 분할합니다.
- 각 문서의 메타데이터에""doc_id""를 키로 하고 생성한uuid를 입력합니다.
- 최종적으로parent_docs리스트에는 원본 문서들이 큰 단위로 분할된 하위 문서들이 저장되며, 각 문서에는 원본 문서의 ID가 메타데이터로 포함됩니다.
parent_text_splitter를 사용하여 문서를 큰 청크 단위로 분할합니다.
각 문서의 메타데이터에""doc_id""를 키로 하고 생성한uuid를 입력합니다.
최종적으로parent_docs리스트에는 원본 문서들이 큰 단위로 분할된 하위 문서들이 저장되며, 각 문서에는 원본 문서의 ID가 메타데이터로 포함됩니다.
```
Copyparent_docs = []fori, docinenumerate(docs):
    _id= doc_ids[i]# 현재 문서의 ID를 가져옵니다.# 현재 문서를 하위 문서로 분할합니다.parent_doc = parent_text_splitter.split_documents([doc])for_docinparent_doc:# 분할된 문서에 대해 반복합니다.# 문서의 메타데이터에 ID를 저장합니다._doc.metadata[id_key] = _idparent_docs.extend(parent_doc)# 분할된 문서를 리스트에 추가합니다.
```
- child_text_splitter를 사용하여 문서를 더 작은 청크로 분할합니다.
- 각 문서의 메타데이터에""doc_id""를 키로 하고 생성한uuid를 입력합니다. 이는 작게 분할된 청크에 문서의 ID 를 부여하기 위함입니다.
- 최종적으로child_docs리스트에는 원본 문서들이 작게 분할된 하위 문서들이 저장되며, 각 하위 문서에는 원본 문서의 ID가 메타데이터로 포함됩니다.
child_text_splitter를 사용하여 문서를 더 작은 청크로 분할합니다.
각 문서의 메타데이터에""doc_id""를 키로 하고 생성한uuid를 입력합니다. 이는 작게 분할된 청크에 문서의 ID 를 부여하기 위함입니다.
최종적으로child_docs리스트에는 원본 문서들이 작게 분할된 하위 문서들이 저장되며, 각 하위 문서에는 원본 문서의 ID가 메타데이터로 포함됩니다.
```
Copychild_docs = []# 하위 문서를 저장할 리스트를 초기화합니다.fori, docinenumerate(docs):
    _id= doc_ids[i]# 현재 문서의 ID를 가져옵니다.# 현재 문서를 하위 문서로 분할합니다.child_doc = child_text_splitter.split_documents([doc])for_docinchild_doc:# 분할된 하위 문서에 대해 반복합니다.# 하위 문서의 메타데이터에 ID를 저장합니다._doc.metadata[id_key] = _idchild_docs.extend(child_doc)# 분할된 하위 문서를 리스트에 추가합니다.
```
각각 분할된 청크의 수를 확인합니다.
```
Copyprint(f""분할된 parent_docs의 개수:{len(parent_docs)}"")print(f""분할된 child_docs의 개수:{len(child_docs)}"")
```
```
분할된 parent_docs의 개수: 4
분할된 child_docs의 개수: 54
```
벡터저장소에 새롭게 생성한 작게 쪼개진 하위문서 집합을 추가합니다.
다음으로는 상위 문서는 생성한 UUID 와 맵핑하여docstore에 추가합니다.
- mset()메서드를 통해 문서 ID와 문서 내용을 key-value 쌍으로 문서 저장소에 저장합니다.
mset()메서드를 통해 문서 ID와 문서 내용을 key-value 쌍으로 문서 저장소에 저장합니다.
(예시)list(zip(doc_ids, docs))[0]
```
Copy('36d475a5-9f1a-40ab-aeb1-ba720fa229d8',Document(page_content='Scikit Learn\n\nScikit-learn은 Python 언어를 위한 또 다른 핵심 라이브러리로, 기계 학습의 다양한 알고리즘을 구현하기 위해 설계되었습니다. 이 라이브러리는 2007년 David Cournapeau에 의해 프로젝트가 시작되었으며, 그 후로 커뮤니티의 광범위한 기여를 받아 현재까지 발전해왔습니다. ...))
```
```
Copy# 벡터 저장소에 하위 문서를 추가합니다.retriever.vectorstore.add_documents(parent_docs)
retriever.vectorstore.add_documents(child_docs)# 문서 저장소에 문서 ID와 문서를 매핑하여 저장합니다.retriever.docstore.mset(list(zip(doc_ids, docs)))
```
주어진 키워드에 대한 유사도 검색을 수행합니다. 가장 유사도가 높은 첫 번째 문서 조각을 출력합니다.
```
Copy# vectorstore의 유사도 검색을 수행합니다.retriever.vectorstore.similarity_search(""Word2Vec 의 정의?"")
```
```
[Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': '72cda610-ea10-4953-b6c5-92bbfc6d5afd', 'source': './data/ai-story.txt'}), Document(page_content='Word2Vec은 크게 두 가지 모델 아키텍처로 구성됩니다: Continuous Bag-of-Words (CBOW)와 Skip-Gram입니다. CBOW 모델은 주변 단어(맥락)를 기반으로 특정 단어를 예측하는 반면, Skip-Gram 모델은 하나의 단어로부터 주변 단어들을 예측합니다. 두 모델 모두 딥러닝이 아닌, 단순화된 신경망 구조를 사용하여 대규모 텍스트 데이터에서 학습할 수 있으며, 매우 효율적입니다.', metadata={'doc_id': '72cda610-ea10-4953-b6c5-92bbfc6d5afd', 'source': './data/ai-story.txt'}), Document(page_content='Word2Vec의 벡터 표현은 다양한 NLP 작업에 활용될 수 있습니다. 예를 들어, 단어의 유사도 측정, 문장이나 문서의 벡터 표현 생성, 기계 번역, 감정 분석 등이 있습니다. 또한, 벡터 연산을 통해 단어 간의 의미적 관계를 추론하는 것이 가능해집니다. 예를 들어, ""king"" - ""man"" + ""woman""과 같은 벡터 연산을 수행하면, 결과적으로 ""queen""과 유사한 벡터를 가진 단어를 찾을 수 있습니다.', metadata={'doc_id': '72cda610-ea10-4953-b6c5-92bbfc6d5afd', 'source': './data/ai-story.txt'}), Document(page_content='Crawling\n\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n\nWord2Vec\n\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n예시: Word2Vec 모델에서 ""왕""과 ""여왕""은 서로 가까운 위치에 벡터로 표현됩니다.\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\nLLM (Large Language Model)', metadata={'doc_id': '2d5252a7-3080-42c9-8cbe-6bf625d41c9f', 'source': './data/appendix-keywords.txt'})]
```
다음과 같이score_threshold를 추가하여 유사도 검색을 수행할 수 있습니다.
유사도 검색 결과에 유사도 점수가 0.5 이상인 결과만 반환합니다.
또한,k의 계수도 지정할 수 있으며,k는 검색되는 문서의 개수를 의미합니다.
```
Copy# score_threshold를 사용하여 유사도 검색을 수행합니다.retriever.vectorstore.similarity_search_with_relevance_scores(""Word2Vec 의 정의?"", score_threshold=0.5, k=3)
```
```
[(Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': '72cda610-ea10-4953-b6c5-92bbfc6d5afd', 'source': './data/ai-story.txt'}),  0.8502118705599201), (Document(page_content='Word2Vec은 크게 두 가지 모델 아키텍처로 구성됩니다: Continuous Bag-of-Words (CBOW)와 Skip-Gram입니다. CBOW 모델은 주변 단어(맥락)를 기반으로 특정 단어를 예측하는 반면, Skip-Gram 모델은 하나의 단어로부터 주변 단어들을 예측합니다. 두 모델 모두 딥러닝이 아닌, 단순화된 신경망 구조를 사용하여 대규모 텍스트 데이터에서 학습할 수 있으며, 매우 효율적입니다.', metadata={'doc_id': '72cda610-ea10-4953-b6c5-92bbfc6d5afd', 'source': './data/ai-story.txt'}),  0.843839109014391), (Document(page_content='Word2Vec의 벡터 표현은 다양한 NLP 작업에 활용될 수 있습니다. 예를 들어, 단어의 유사도 측정, 문장이나 문서의 벡터 표현 생성, 기계 번역, 감정 분석 등이 있습니다. 또한, 벡터 연산을 통해 단어 간의 의미적 관계를 추론하는 것이 가능해집니다. 예를 들어, ""king"" - ""man"" + ""woman""과 같은 벡터 연산을 수행하면, 결과적으로 ""queen""과 유사한 벡터를 가진 단어를 찾을 수 있습니다.', metadata={'doc_id': '72cda610-ea10-4953-b6c5-92bbfc6d5afd', 'source': './data/ai-story.txt'}),  0.8371520268093797)]
```
retriever객체의get_relevant_documents메서드를 호출하여 관련된 문서를 검색합니다.
주어진 쿼리에 관련성이 높은 문서를 검색합니다.
여기서는 2개의 도큐먼트 안에 ""Word2Vec"" 의 정의가 포함되었기 때문에 2개의 문서 모두 검색 결과로 나왔습니다.
```
Copyrelevant_doc = retriever.get_relevant_documents(""Word2Vec 의 정의?"")len(relevant_doc)
```
```
2
```
첫 번째로 찾은 문서의 내용의 문자열 길이를 확인하면 문서 전체가 출력됨을 확인할 수 있습니다.
```
Copy# Retriever가 반환하는 문서의 길이를 확인합니다.len(retriever.get_relevant_documents(""Word2Vec 의 정의?"")[0].page_content)
```
```
7482
```
리트리버(retriever)가 벡터 데이터베이스에서 기본적으로 수행하는 검색 유형은 유사도 검색입니다.
LangChain Vector Stores는Max Marginal Relevance를 통한 검색도 지원하므로, 이를 대신 사용하고 싶다면 다음과 같이search_type속성을 설정하면 됩니다.
- retriever객체의search_type속성을SearchType.mmr로 설정합니다.
- 이는 검색 시 MMR(Maximal Marginal Relevance) 알고리즘을 사용하도록 지정하는 것입니다.
retriever객체의search_type속성을SearchType.mmr로 설정합니다.
이는 검색 시 MMR(Maximal Marginal Relevance) 알고리즘을 사용하도록 지정하는 것입니다.
```
Copyfromlangchain.retrievers.multi_vectorimportSearchType# 검색 유형을 MMR(Maximal Marginal Relevance)로 설정retriever.search_type = SearchType.mmr# 검색어로 관련 문서를 검색하고, 첫 번째 문서의 페이지 내용 길이를 반환len(retriever.get_relevant_documents(""Word2Vec의 정의"")[0].page_content)
```
```
7482
```
## 요약본(summary)을 벡터저장소에 저장
요약은 종종 청크(chunk)의 내용을 보다 정확하게 추출할 수 있어 더 나은 검색 결과를 얻을 수 있습니다.
여기서는 요약을 생성하는 방법과 이를 임베딩하는 방법에 대해 설명합니다.
```
Copyimportuuidfromlangchain_core.documentsimportDocumentfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_openaiimportChatOpenAI


chain = (
    {""doc"":lambdax: x.page_content}# 입력 데이터에서 페이지 내용을 추출하는 함수# 문서 요약을 위한 프롬프트 템플릿 생성| ChatPromptTemplate.from_template(""Summarize the following document in Korean:\n\n{doc}"")# OpenAI의 ChatGPT 모델을 사용하여 요약 생성 (최대 재시도 횟수: 0)| ChatOpenAI(max_retries=0)
    | StrOutputParser()# 생성된 요약 결과를 문자열로 파싱)
```
- chain.batch메서드를 사용하여docs리스트의 문서들을 일괄 처리합니다.
- 여기서max_concurrency매개변수를 5로 설정하여 최대 5개의 문서를 동시에 처리할 수 있도록 합니다.
chain.batch메서드를 사용하여docs리스트의 문서들을 일괄 처리합니다.
여기서max_concurrency매개변수를 5로 설정하여 최대 5개의 문서를 동시에 처리할 수 있도록 합니다.
```
Copy# 최대 동시성을 5로 설정하여 문서 배치 처리summaries = chain.batch(docs, {""max_concurrency"":5})
```
요약된 내용을 출력하여 결과를 확인합니다.
```
Copy# 요약을 출력합니다.print(summaries[0])
```
```
Scikit-learn은 Python을 위한 핵심 머신 러닝 라이브러리로, 다양한 알고리즘을 구현하기 위해 만들어졌다. 이 라이브러리는 2007년 David Cournapeau가 시작한 프로젝트로, 지금까지 커뮤니티의 기여를 받아 계속 발전해왔다. Scikit-learn은 분류, 회귀, 군집화, 차원 축소 등 다양한 작업을 지원하며, 사용이 쉬운 API로 제공되어 연구자와 개발자가 복잡한 데이터 과학 문제를 해결할 수 있도록 돕는다. 또한, 다양한 기계 학습 모델을 통합하는 점이 강점이며, 데이터 처리와 모델 평가를 위한 도구들도 제공한다. Scikit-learn은 범용성이 높아서 다양한 분야에서 사용될 수 있고, 지속적인 개선과 업데이트로 사용자들에게 최신 기술을 제공한다. 이러한 특징으로 Scikit-learn은 기계 학습을 시작하는 사람들에게 인기 있는 선택지로 자리를 잡았다.
```
```
Copy# 요약을 출력합니다.print(summaries[1])
```
```
의미론적 검색은 사용자의 질의를 이해하여 관련된 결과를 반환하는 방식이며, 임베딩은 텍스트 데이터를 저차원 벡터로 변환하는 과정을 뜻합니다. 토큰은 텍스트를 작은 단위로 분할하고, 토크나이저는 이를 처리하는 도구입니다. 벡터스토어는 벡터 데이터를 저장하는 시스템이고, SQL은 데이터베이스 관리 언어입니다. CSV와 JSON은 데이터 형식을 저장하거나 교환하는 데 사용되며, 트랜스포머는 딥러닝 모델 중 하나입니다. HuggingFace는 NLP를 위한 라이브러리이고, 디지털 변환은 기업을 혁신하는 과정을 말합니다. 크롤링은 웹 데이터 수집을 의미하며, Word2Vec은 단어 간 의미적 관계를 나타내는 기술입니다. LLM은 대규모 언어 모델을 의미하며, FAISS는 벡터 검색을 위한 라이브러리입니다. 오픈 소스는 소프트웨어 협업과 혁신을 위한 개방된 소스 코드를 말하고, 구조화된 데이터는 정해진 형식에 따라 조직된 데이터를 의미합니다. 파서는 데이터를 구조화된 형태로 변환하는 도구이며, TF-IDF는 문서 내 단어의 중요도를 평가하는 통계적 척도입니다. 딥러닝은 복잡한 문제 해결을 위한 기계 학습 기술이고, 스키마는 데이터의 구조를 정의하는 청사진입니다. DataFrame은 데이터 분석에 사용되는 테이블 형태의 데이터 구조이며, Attention 메커니즘은 중요한 정보에 더 집중하는 기법입니다. 판다스는 데이터 분석을 위한 파이썬 라이브러리이고, GPT는 생성적 언어 모델입니다. InstructGPT는 특정 지시에 따라 작업을 수행하는 GPT 모델이며, 키워드 검색은 정보 검색 방법 중 하나입니다. 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘이고, 데이터 마이닝은 유용한 정보를 발굴하는 과정입니다. 멀티모달은 여러 종류의 데이터를 결합하여 처리하는 기술을 의미합니다.
```
Chroma벡터 저장소를 초기화하여 자식 청크(child chunks)를 인덱싱합니다. 이때OpenAIEmbeddings를 임베딩 함수로 사용합니다.
- 문서 ID를 나타내는 키로""doc_id""를 사용합니다.
문서 ID를 나타내는 키로""doc_id""를 사용합니다.
```
Copy# 요약 정보를 저장할 벡터 저장소를 생성합니다.vectorstore = Chroma(collection_name=""summaries"", embedding_function=OpenAIEmbeddings())# 부모 문서를 저장할 저장소를 생성합니다.store = InMemoryByteStore()# 문서 ID를 저장할 키 이름을 지정합니다.id_key =""doc_id""# 검색기를 초기화합니다. (시작 시 비어 있음)retriever = MultiVectorRetriever(
    vectorstore=vectorstore,# 벡터 저장소byte_store=store,# 바이트 저장소id_key=id_key,# 문서 ID 키)# 문서 ID를 생성합니다.doc_ids = [str(uuid.uuid4())for_indocs]
```
요약된 문서와 메타데이터(여기서는 생성한 요약본에 대한Document ID입니다)를 저장합니다.
```
Copysummary_docs = [# 요약된 내용을 페이지 콘텐츠로 하고, 문서 ID를 메타데이터로 포함하는 Document 객체를 생성합니다.Document(page_content=s, metadata={id_key: doc_ids[i]})fori, sinenumerate(
        summaries
    )# summaries 리스트의 각 요약과 인덱스에 대해 반복합니다.]
```
요약본의 문서의 개수는 원본 문서의 개수와 일치합니다.
```
Copy# 요약본의 문서의 개수len(summary_docs)
```
```
2
```
- retriever.vectorstore.add_documents(summary_docs)를 통해summary_docs를 벡터 저장소에 추가합니다.
- retriever.docstore.mset(list(zip(doc_ids, docs)))를 사용하여doc_ids와docs를 매핑하여 문서 저장소에 저장합니다.
retriever.vectorstore.add_documents(summary_docs)를 통해summary_docs를 벡터 저장소에 추가합니다.
retriever.docstore.mset(list(zip(doc_ids, docs)))를 사용하여doc_ids와docs를 매핑하여 문서 저장소에 저장합니다.
```
Copyretriever.vectorstore.add_documents(
    summary_docs
)# 요약된 문서를 벡터 저장소에 추가합니다.# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.retriever.docstore.mset(list(zip(doc_ids, docs)))
```
다음으로는 원본 청크(chunk) 데이터를 벡터 저장소에 추가하는 코드입니다.
```
Copy# RecursiveCharacterTextSplitter 객체를 생성합니다.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)

split_docs = []
split_docs_ids = []fori, docinenumerate(docs):
    _id= doc_ids[i]# 현재 문서의 ID를 가져옵니다.# 현재 문서를 하위 문서로 분할합니다.split_doc = text_splitter.split_documents([doc])for_docinsplit_doc:# 분할된 문서에 대해 반복합니다.# 문서의 메타데이터에 ID를 저장합니다._doc.metadata[id_key] = _idsplit_docs_ids.append(_id)
    split_docs.extend(split_doc)# 분할된 문서를 리스트에 추가합니다.
```
분할된 문서의 개수를 출력합니다.
```
Copyprint(f""분할된 문서의 개수:{len(split_docs)}"")
```
```
분할된 문서의 개수: 17
```
마지막으로 분할된 문서를 벡터 저장소에 추가합니다.
```
Copy# 문서를 검색기의 벡터 저장소에 추가합니다.retriever.vectorstore.add_documents(split_docs)
```
```
['e5c629c1-569a-4e94-ad84-35aafb766caf', '50635cd4-4783-478c-bd98-f07dc32cd498', 'e4b66c21-3b6b-4985-acae-e327fedefc01', '8897cc61-6002-479c-9531-9ea0f43065f0', '9396aaef-77d0-4ed8-a8bf-682e85ad6ae3', '76bc6c95-dcf0-4656-900e-f50e72539fb3', '497f4790-57ac-4234-a1f1-80c2389d0ca8', 'f67434d4-d4ea-40f8-8538-9c4ddcb6c485', '8475a19a-b181-4483-8ef5-2d465467a34a', 'b9c68687-d11d-41a5-ab27-d700993a7ad5', '06476f8b-7deb-4318-8abd-0357e1474b8e', '09877995-c050-47f1-813a-87eaab69c0f7', 'e694a261-1b52-4d11-9f15-da9ef3c5b35d', '6e909fa4-d402-4079-a9d1-38a7b4fd3575', '96acceb4-5d20-418c-9bfe-661d0a9b989e', '04d000ee-5050-4c32-aeee-37c98cbc4677', '9f01c835-8102-4931-8ef1-0f872e1da4ff']
```
vectorstore객체의similarity_search메서드를 사용하여 유사도 검색을 수행합니다.
```
Copy# 유사도 검색을 수행합니다.result_docs = vectorstore.similarity_search(""Word2Vec의 정의가 뭐야?"")
```
```
Copy# 1개의 결과 문서를 출력합니다.result_docs[0]
```
```
Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': 'e0c4ac38-5b34-4c78-af87-d39bd1f32d2a', 'source': './data/ai-story.txt'})
```
retriever객체의get_relevant_documents메서드를 사용하여 질문과 관련된 문서를 검색합니다.
```
Copy# 관련된 문서를 검색하여 가져옵니다.retrieved_docs = retriever.get_relevant_documents(""Word2Vec 의 정의가 뭐야?"")len(retrieved_docs)
```
```
2
```
retrieved_docs[0].page_content의 길이를 반환합니다. 문서를 반환하기 때문에 페이지 내용의 길이는 청크보다 일반적으로 큽니다.
```
Copy# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.len(retrieved_docs[0].page_content)
```
```
7482
```
## 가설 쿼리(Hypothetical Queries)
LLM은 특정 문서에 대해 가정할 수 있는 질문 목록을 생성하는 데에도 사용될 수 있습니다.
이렇게 생성된 질문들은 임베딩(embedding)될 수 있으며, 이를 통해 문서의 내용을 더욱 깊이 있게 탐색하고 이해할 수 있습니다.
가정 질문 생성은 문서의 주요 주제와 개념을 파악하는 데 도움이 되며, 독자들이 문서 내용에 대해 더 많은 궁금증을 갖도록 유도할 수 있습니다.
- functions리스트에는 하나의 딕셔너리 요소가 포함되어 있습니다.
- 딕셔너리는name,description,parameters키를 가지고 있습니다.
- name은 함수의 이름을 나타내는 문자열입니다.
- description은 함수의 설명을 나타내는 문자열입니다.
- parameters는 함수의 매개변수를 정의하는 딕셔너리입니다.
- type은 매개변수의 타입을 나타내며, 여기서는 ""object""로 설정되어 있습니다.
- properties는 객체의 속성을 정의하는 딕셔너리입니다.questions는 ""array"" 타입의 속성으로, 각 요소는 ""string"" 타입입니다.
- questions는 ""array"" 타입의 속성으로, 각 요소는 ""string"" 타입입니다.
- required는 필수 속성을 나타내는 리스트이며, 여기서는questions가 필수로 지정되어 있습니다.
functions리스트에는 하나의 딕셔너리 요소가 포함되어 있습니다.
딕셔너리는name,description,parameters키를 가지고 있습니다.
name은 함수의 이름을 나타내는 문자열입니다.
description은 함수의 설명을 나타내는 문자열입니다.
parameters는 함수의 매개변수를 정의하는 딕셔너리입니다.
type은 매개변수의 타입을 나타내며, 여기서는 ""object""로 설정되어 있습니다.
properties는 객체의 속성을 정의하는 딕셔너리입니다.questions는 ""array"" 타입의 속성으로, 각 요소는 ""string"" 타입입니다.
- questions는 ""array"" 타입의 속성으로, 각 요소는 ""string"" 타입입니다.
questions는 ""array"" 타입의 속성으로, 각 요소는 ""string"" 타입입니다.
required는 필수 속성을 나타내는 리스트이며, 여기서는questions가 필수로 지정되어 있습니다.
```
Copyfunctions = [
    {""name"":""hypothetical_questions"",# 함수의 이름을 지정합니다.""description"":""Generate hypothetical questions"",# 함수에 대한 설명을 작성합니다.""parameters"": {# 함수의 매개변수를 정의합니다.""type"":""object"",# 매개변수의 타입을 객체로 지정합니다.""properties"": {# 객체의 속성을 정의합니다.""questions"": {# 'questions' 속성을 정의합니다.""type"":""array"",# 'questions'의 타입을 배열로 지정합니다.""items"": {""type"":""string""},# 배열의 요소 타입을 문자열로 지정합니다.},
            },""required"": [""questions""],# 필수 매개변수로 'questions'를 지정합니다.},
    }
]
```
ChatPromptTemplate을 사용하여 주어진 문서를 기반으로 3개의 가상 질문을 생성하는 프롬프트 템플릿을 정의합니다.
- ChatOpenAI를 사용하여 GPT 모델을 초기화하고,functions와function_call을 설정하여 가상 질문 생성 함수를 호출합니다.
- JsonKeyOutputFunctionsParser를 사용하여 생성된 가상 질문을 파싱하고,questions키에 해당하는 값을 추출합니다.
ChatOpenAI를 사용하여 GPT 모델을 초기화하고,functions와function_call을 설정하여 가상 질문 생성 함수를 호출합니다.
JsonKeyOutputFunctionsParser를 사용하여 생성된 가상 질문을 파싱하고,questions키에 해당하는 값을 추출합니다.
```
Copyfromlangchain.output_parsers.openai_functionsimportJsonKeyOutputFunctionsParser

chain = (
    {""doc"":lambdax: x.page_content}# 아래 문서를 사용하여 답변할 수 있는 가상의 질문을 정확히 3개 생성하도록 요청합니다. 이 숫자는 조정될 수 있습니다.| ChatPromptTemplate.from_template(""Generate a list of exactly 3 hypothetical questions that the below document could be used to answer. Answer in Korean:\n\n{doc}"")
    | ChatOpenAI(max_retries=0, model=""gpt-4-turbo-preview"").bind(
        functions=functions, function_call={""name"":""hypothetical_questions""}
    )# 출력에서 ""questions"" 키에 해당하는 값을 추출합니다.| JsonKeyOutputFunctionsParser(key_name=""questions"")
)
```
chain.invoke(docs[0])호출하여 첫 번째 문서에 대한 답변을 출력합니다.
- 출력은 생성한 3개의 가설 쿼리(Hypothetical Queries) 가 담겨 있습니다.
출력은 생성한 3개의 가설 쿼리(Hypothetical Queries) 가 담겨 있습니다.
```
Copy# 주어진 문서에 대해 체인을 실행합니다.chain.invoke(docs[0])
```
```
['scikit-learn을 사용하여 어떤 종류의 기계 학습 모델을 개발할 수 있습니까?', 'NLP 기술은 어떤 산업 분야에서 활용될 수 있습니까?', 'SciPy 라이브러리를 사용하여 어떤 종류의 과학적 계산을 수행할 수 있습니까?']
```
chain.batch메서드를 사용하여docs데이터에 대해 동시에 여러 개의 요청을 처리합니다.
- docs매개변수는 처리할 문서 데이터를 나타냅니다.
- max_concurrency매개변수는 동시에 처리할 수 있는 최대 요청 수를 지정합니다. 이 예시에서는 5로 설정되어 있습니다.
- 이 메서드는docs데이터의 각 항목에 대해chain객체의 작업을 수행하고, 최대 5개의 요청을 동시에 처리합니다.
- 처리 결과는hypothetical_questions변수에 저장됩니다.
docs매개변수는 처리할 문서 데이터를 나타냅니다.
max_concurrency매개변수는 동시에 처리할 수 있는 최대 요청 수를 지정합니다. 이 예시에서는 5로 설정되어 있습니다.
이 메서드는docs데이터의 각 항목에 대해chain객체의 작업을 수행하고, 최대 5개의 요청을 동시에 처리합니다.
처리 결과는hypothetical_questions변수에 저장됩니다.
```
Copy# 문서 목록에 대해 가설적 질문을 일괄 처리하여 생성합니다. 최대 동시성은 5로 설정되어 있습니다.hypothetical_questions = chain.batch(docs, {""max_concurrency"":5})
```
```
Copyprint(hypothetical_questions[0])print(hypothetical_questions[1])
```
```
['Scikit-learn 라이브러리를 사용하여 어떤 유형의 기계 학습 문제를 해결할 수 있습니까?', '자연어 처리(NLP) 기술을 활용하여 어떤 종류의 문제를 해결할 수 있습니까?', 'Word2Vec을 사용하여 어떻게 단어 간의 의미적 관계를 파악할 수 있습니까?']
['자연어 처리 기술을 활용한 의미론적 검색의 장점은 무엇인가요?', '인공 지능 모델에서 텍스트 데이터를 처리하는 과정은 어떻게 이루어지나요?', '대규모 언어 모델(GPT)이 자연어 이해와 생성 작업에서 어떻게 활용되나요?']
```
아래는 이전에 진행했던 방식과 동일하게 생성한 가설 쿼리(Hypothetical Queries) 를 벡터저장소에 저장하는 과정입니다.
```
Copy# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소vectorstore = Chroma(
    collection_name=""hypo-questions"", embedding_function=OpenAIEmbeddings()
)# 부모 문서의 저장소 계층store = InMemoryByteStore()
id_key =""doc_id""# 검색기 (시작 시 비어 있음)retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=store,
    id_key=id_key,
)
doc_ids = [str(uuid.uuid4())for_indocs]# 문서 ID 생성
```
question_docs리스트에 메타데이터(문서 ID) 를 추가합니다.
```
Copyquestion_docs = []# hypothetical_questions 리스트를 순회하면서 인덱스와 질문 리스트를 가져옵니다.fori, question_listinenumerate(hypothetical_questions):
    question_docs.extend(# question_docs 리스트에 Document 객체를 추가합니다.# 질문 리스트의 각 질문에 대해 Document 객체를 생성하고, 메타데이터에 해당 질문의 문서 ID를 포함시킵니다.[Document(page_content=s, metadata={id_key: doc_ids[i]})forsinquestion_list]
    )
```
가설 쿼리를 문서에 추가하고, 원본 문서를docstore에 추가합니다.
```
Copyretriever.vectorstore.add_documents(
    question_docs
)# 질문 문서를 벡터 저장소에 추가합니다.# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.retriever.docstore.mset(list(zip(doc_ids, docs)))
```
vectorstore객체의similarity_search메서드를 사용하여 유사도 검색을 수행합니다.
```
Copy# 유사한 문서를 벡터 저장소에서 검색합니다.result_docs = vectorstore.similarity_search(""Word2Vec에 대한 정의는 뭐야?"")
```
아래는 유사도 검색 결과입니다.
여기서는 생성한 가설 쿼리만 추가해 놓은 상태이기 때문에, 생성한 가설 쿼리 중 유사도가 가장 높은 문서를 반환합니다.
```
Copy# 유사도 검색 결과를 출력합니다.result_docs
```
```
[Document(page_content='Word2Vec을 사용하여 어떻게 단어 간의 의미적 관계를 파악할 수 있습니까?', metadata={'doc_id': 'e013fa57-0570-4193-ab77-5f8f068594bf'}), Document(page_content='인공 지능 모델에서 텍스트 데이터를 처리하는 과정은 어떻게 이루어지나요?', metadata={'doc_id': '8163db6a-5927-4c3a-ab37-094e335ec8ee'}), Document(page_content='자연어 처리 기술을 활용한 의미론적 검색의 장점은 무엇인가요?', metadata={'doc_id': '8163db6a-5927-4c3a-ab37-094e335ec8ee'}), Document(page_content='대규모 언어 모델(GPT)이 자연어 이해와 생성 작업에서 어떻게 활용되나요?', metadata={'doc_id': '8163db6a-5927-4c3a-ab37-094e335ec8ee'})]
```
이전 단계에서 분할한 문서도 벡터저장소에 추가합니다.
```
Copy# 문서를 검색기의 벡터 저장소에 추가합니다.retriever.vectorstore.add_documents(split_docs)
```
```
['3ea6e39d-3c12-4505-bdda-dc41165c98ba', '00f6a5f8-3c31-488a-9a15-d85b19253a09', '0c4e2a80-993a-4b45-87c8-80d6f47e4aa1', '1b00caf9-a3bd-424e-8aee-0f622ad29f98', 'a7dcf8ff-2d39-4b7d-a1c3-9546c1dbfa87', 'e1c61135-2d7b-460c-8191-3ea158b46f5a', 'abb2cebb-8c43-442b-b7ee-5edfe1c4c3c2', 'fc5aa98a-3bfd-4fdf-930a-da492518cf69', '06b2ea20-4351-4555-8db3-e6e5a8fb1bcc', '8a1cda27-a12b-4026-9a81-a77717ba404e', '27cda25e-fa76-4e27-b849-b60c2046800a', '874257d1-c7f7-45af-88f2-0d80b4b3a0b5', 'e618645b-163b-4792-a2c1-8b05672e9098', 'af145a18-b974-4d4b-b2e3-28bd7d782b4c', '0062342b-aca7-40da-b0a7-f667c65c8622', 'cc270694-a8fb-4514-8692-6e700518b890', 'ade11598-78ec-4bd7-82f8-998055ae79af']
```
retriever객체의get_relevant_documents메서드를 사용하여 쿼리와 관련된 문서를 검색합니다.
```
Copy# 관련된 문서를 검색하여 가져옵니다.retrieved_docs = retriever.get_relevant_documents(""Word2Vec에 대한 정의가 뭐야?"")len(retrieved_docs)
```
```
2
```
검색 결과인retrieved_docs[0].page_content의 길이를 확인합니다.
```
Copy# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.len(retrieved_docs[0].page_content)
```
```
7482
```"
08. 셀프 쿼리 검색기(SelfQueryRetriever),https://wikidocs.net/234475,"# 08. 셀프 쿼리 검색기(SelfQueryRetriever)

SelfQueryRetriever는 자체적으로 질문을 생성하고 해결할 수 있는 기능을 갖춘 검색 도구입니다. 이는 사용자가 제공한 자연어 질의를 바탕으로, query-constructing LLM chain을 사용해 구조화된 질의를 만듭니다. 그 후, 이 구조화된 질의를 기본 벡터 데이터 저장소(VectorStore)에 적용하여 검색을 수행합니다.
이 과정을 통해,SelfQueryRetriever는 단순히 사용자의 입력 질의를 저장된 문서의 내용과 의미적으로 비교하는 것을 넘어서, 사용자의 질의에서 문서의 메타데이터에 대한 필터를 추출하고, 이 필터를 실행하여 관련된 문서를 찾을 수 있습니다. 이를 통해, 사용자의 질의에 대한 더 정확하고 관련성 높은 결과를 제공할 수 있게 됩니다.
[참고]
- LangChain 이 지원하는 셀프 쿼리 번역기(Self-query Retriever) 목록은여기에서 확인해 주시기 바랍니다.
LangChain 이 지원하는 셀프 쿼리 번역기(Self-query Retriever) 목록은여기에서 확인해 주시기 바랍니다.
## 시작하기
데모를 위해Chromavector store를 사용할 예정입니다. 이번 튜토리얼에서는 영화 요약이 포함된 작은 데모 문서 세트를 만들었습니다.
참고:SelfQueryRetriever를 사용하려면lark패키지를 설치해야 합니다.
```
Copy# 설치%pip install -qU lark chromadb
```
영화 설명과 메타데이터를 기반으로 유사도 검색이 가능한 벡터 저장소가 구축합니다.
- Document클래스를 사용하여 영화에 대한 간략한 설명과 메타데이터를 포함하는 문서 객체 리스트인docs를 생성합니다.
- OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.
- Chroma.from_documents메서드를 사용하여docs와OpenAIEmbeddings로부터 Chroma 벡터 저장소인vectorstore를 생성합니다.
Document클래스를 사용하여 영화에 대한 간략한 설명과 메타데이터를 포함하는 문서 객체 리스트인docs를 생성합니다.
OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.
Chroma.from_documents메서드를 사용하여docs와OpenAIEmbeddings로부터 Chroma 벡터 저장소인vectorstore를 생성합니다.
```
Copyfromlangchain_community.vectorstoresimportChromafromlangchain_core.documentsimportDocumentfromlangchain_openaiimportOpenAIEmbeddings


docs = [
    Document(
        page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"",
        metadata={""year"":1993,""rating"":7.7,""genre"":""science fiction""},
    ),
    Document(
        page_content=""Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."",
        metadata={""year"":2010,""director"":""Christopher Nolan"",""rating"":8.2},
    ),
    Document(
        page_content=""A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"",
        metadata={""year"":2006,""director"":""Satoshi Kon"",""rating"":8.6},
    ),
    Document(
        page_content=""A bunch of normal-sized women are supremely wholesome and some men pine after them"",
        metadata={""year"":2019,""director"":""Greta Gerwig"",""rating"":8.3},
    ),
    Document(
        page_content=""Toys come alive and have a blast doing so"",
        metadata={""year"":1995,""genre"":""animated""},
    ),
    Document(
        page_content=""Three men walk into the Zone, three men walk out of the Zone"",
        metadata={""year"":1979,""director"":""Andrei Tarkovsky"",""genre"":""thriller"",""rating"":9.9,
        },
    ),
]
vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())
```
## SelfQueryRetriever 생성하기
이제 retriever를 인스턴스화할 수 있습니다. 이를 위해서는 문서가 지원하는메타데이터 필드와 문서 내용에 대한간단한 설명을 미리 제공해야 합니다.
AttributeInfo클래스를 사용하여 영화 메타데이터 필드에 대한 정보를 정의합니다.
- 장르(genre): 문자열 타입, 영화의 장르를 나타내며 ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated'] 중 하나의 값을 가집니다.
- 연도(year): 정수 타입, 영화가 개봉된 연도를 나타냅니다.
- 감독(director): 문자열 타입, 영화 감독의 이름을 나타냅니다.
- 평점(rating): 실수 타입, 1-10 범위의 영화 평점을 나타냅니다.
장르(genre): 문자열 타입, 영화의 장르를 나타내며 ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated'] 중 하나의 값을 가집니다.
연도(year): 정수 타입, 영화가 개봉된 연도를 나타냅니다.
감독(director): 문자열 타입, 영화 감독의 이름을 나타냅니다.
평점(rating): 실수 타입, 1-10 범위의 영화 평점을 나타냅니다.
```
Copyfromlangchain.chains.query_constructor.baseimportAttributeInfofromlangchain.retrievers.self_query.baseimportSelfQueryRetrieverfromlangchain_openaiimportChatOpenAI

metadata_field_info = [
    AttributeInfo(
        name=""genre"",
        description=""The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']"",type=""string"",
    ),
    AttributeInfo(
        name=""year"",
        description=""The year the movie was released"",type=""integer"",
    ),
    AttributeInfo(
        name=""director"",
        description=""The name of the movie director"",type=""string"",
    ),
    AttributeInfo(
        name=""rating"", description=""A 1-10 rating for the movie"",type=""float""),
]
```
document_content_description변수에 영화에 대한 간략한 요약 설명을 할당합니다.
```
Copy# 문서의 내용에 대한 간략한 설명document_content_description =""Brief summary of a movie""
```
SelfQueryRetriever.from_llm()메서드를 사용하여retriever객체를 생성합니다.
- llm: 언어 모델
- vectorstore: 벡터 저장소
- document_content_description: 문서 내용 설명
- metadata_field_info: 메타데이터 필드 정보
llm: 언어 모델
vectorstore: 벡터 저장소
document_content_description: 문서 내용 설명
metadata_field_info: 메타데이터 필드 정보
```
Copy# LLM 정의llm = ChatOpenAI(model=""gpt-4-turbo-preview"", temperature=0)# SelfQueryRetriever 생성retriever = SelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
)
```
## 테스트
이제 우리가 만든 retriever를 실제로 사용해 볼 수 있습니다!
retriever객체의invoke메서드를 호출하여 필터링된 검색을 수행합니다.
- 검색 쿼리로 ""I want to watch a movie rated higher than 8.5"" 를 전달하여 평점이 8.5 이상인 영화를 검색하도록 지정합니다.
검색 쿼리로 ""I want to watch a movie rated higher than 8.5"" 를 전달하여 평점이 8.5 이상인 영화를 검색하도록 지정합니다.
```
Copy# 8.5 이상의 평점을 받은 영화를 보고 싶다는 필터만 지정합니다.retriever.invoke(""I want to watch a movie rated higher than 8.5"")
```
```
[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}), Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006})]
```
질의 내용은 ""Has Greta Gerwig directed any movies about women""으로, Greta Gerwig가 여성에 관한 영화를 연출했는지 묻는 질문입니다.
```
Copy# Greta Gerwig가 여성에 관한 영화를 연출한 적이 있는지 질의합니다.retriever.invoke(""Has Greta Gerwig directed any movies about women"")
```
```
[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019})]
```
이번 쿼리에서는복합 필터(composite filter)를 사용하여 검색 조건을 지정했음을 알 수 있습니다.
- 복합 필터: rating above 8.5, science fiction
복합 필터: rating above 8.5, science fiction
```
Copy# 8.5 이상의 평점을 가진 SF 영화를 검색하는 복합 필터를 지정합니다.retriever.invoke(""What's a highly rated (above 8.5) science fiction film?"")
```
```
[]
```
이 질의 역시 복합 필터를 사용하여 검색 결과를 필터링합니다.
- 복합 필터: 1990 ~ 2005년, 장난관 관련 영화, 에니메이션 영화 선호
복합 필터: 1990 ~ 2005년, 장난관 관련 영화, 에니메이션 영화 선호
```
Copy# 에러가 발생하는 쿼리retriever.invoke(# 1990년 이후 2005년 이전에 제작된 장난감에 관한 영화를 검색하며, 애니메이션 영화를 선호한다는 쿼리와 복합 필터를 지정합니다.""What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated"")
```
하지만, retriever.invoke()를 호출할 때마다 에러가 발생합니다.
이 문제를 해결하기 위한 방법은 노트북의 제일 하단에 나옵니다.
## K 계수 필터링
k는 가져올 문서의 수를 의미합니다.
SelfQueryRetriever를 사용하여k를 지정할 수도 있습니다. 이는 생성자에enable_limit=True를 전달하여 수행할 수 있습니다.
SelfQueryRetriever클래스를 사용하여retriever객체를 생성합니다.
- document_content_description: 문서 내용에 대한 설명
- metadata_field_info: 메타데이터 필드 정보
- enable_limit: 검색 결과 제한 여부
document_content_description: 문서 내용에 대한 설명
metadata_field_info: 메타데이터 필드 정보
enable_limit: 검색 결과 제한 여부
(방법1) 아래의 경우는search_kwargs={""k"": 2}을 명시적으로 지정하여 2개의 검색 결과를 반환하도록 지정합니다.
```
Copyretriever = SelfQueryRetriever.from_llm(
    llm,# 언어 모델(Language Model)을 지정합니다.vectorstore,# 벡터 저장소(Vector Store)를 지정합니다.document_content_description,# 문서 내용에 대한 설명을 지정합니다.metadata_field_info,# 메타데이터 필드 정보를 지정합니다.enable_limit=True,# 검색 결과 제한 기능을 활성화합니다.search_kwargs={""k"":2},# k 의 값을 2로 지정하여 검색 결과를 2개로 제한합니다.)# 공룡에 관한 두 영화가 무엇인지 질의합니다.retriever.invoke(""What are movies about dinosaurs"")
```
```
[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]
```
하지만 코드로 명시적으로search_kwargs를 지정하지 않고 query 에서two, three등의 숫자를 사용하여 검색 결과를 제한할 수 있습니다.
```
Copyretriever = SelfQueryRetriever.from_llm(
    llm,# 언어 모델(Language Model)을 지정합니다.vectorstore,# 벡터 저장소(Vector Store)를 지정합니다.document_content_description,# 문서 내용에 대한 설명을 지정합니다.metadata_field_info,# 메타데이터 필드 정보를 지정합니다.enable_limit=True,# 검색 결과 제한 기능을 활성화합니다.)# 공룡에 관한 두 영화가 무엇인지 질의합니다.(two)retriever.invoke(""What are two movies about dinosaurs"")
```
```
[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}), Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]
```
하나만 반환하도록 질의 내용을 조금 수정해 보겠습니다.
```
Copy# 공룡에 관한 두 영화가 무엇인지 질의합니다.(one)retriever.invoke(""What are one movies about dinosaurs"")
```
```
[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993})]
```
## LCEL을 사용하여 Chain 구성하기
내부에서 어떤 일이 일어나는지 확인하고 더 많은 사용자 정의 제어를 하기 위해, 우리는 retriever를 처음부터 재구성할 수 있습니다.
먼저, query-construction chain을 생성해야 합니다.
이 chain은 사용자 query를 받아 사용자가 지정한 필터를 캡처하는StructuredQuery객체를 생성합니다.
get_query_constructor_prompt함수를 사용하여 쿼리 생성기 프롬프트를 가져옵니다.
- 이 함수는document_content_description과metadata_field_info를 인자로 받습니다.
이 함수는document_content_description과metadata_field_info를 인자로 받습니다.
StructuredQueryOutputParser.from_components()메서드를 사용하여 구조화된 쿼리 출력 파서를 초기화합니다.
- 쿼리 생성기 프롬프트(prompt)와 언어 모델(llm), 그리고 출력 파서(output_parser)를 파이프라인으로 연결하여query_constructor를 생성합니다.
- 이 파이프라인은 프롬프트에 따라 쿼리를 생성하고, 언어 모델을 통해 처리한 후, 출력 파서를 사용하여 구조화된 형식으로 변환합니다.
쿼리 생성기 프롬프트(prompt)와 언어 모델(llm), 그리고 출력 파서(output_parser)를 파이프라인으로 연결하여query_constructor를 생성합니다.
이 파이프라인은 프롬프트에 따라 쿼리를 생성하고, 언어 모델을 통해 처리한 후, 출력 파서를 사용하여 구조화된 형식으로 변환합니다.
```
Copyfromlangchain.chains.query_constructor.baseimport(
    StructuredQueryOutputParser,
    get_query_constructor_prompt,
)# 문서 내용 설명과 메타데이터 필드 정보를 사용하여 쿼리 생성기 프롬프트를 가져옵니다.prompt = get_query_constructor_prompt(
    document_content_description,
    metadata_field_info,
)# 구성 요소에서 구조화된 쿼리 출력 파서를 생성합니다.output_parser = StructuredQueryOutputParser.from_components()# 프롬프트, 언어 모델, 출력 파서를 연결하여 쿼리 생성기를 만듭니다.query_constructor = prompt | llm | output_parser
```
prompt.format()메서드를 사용하여query매개변수에 ""dummy question"" 문자열을 전달하고 그 결과를 출력하여 Prompt 내용을 확인해 보겠습니다.
```
Copy# prompt 문자열에 query 매개변수를 ""dummy question""으로 설정하여 포맷팅한 결과를 출력합니다.print(prompt.format(query=""dummy question""))
```
```
Your goal is to structure the user's query to match the request schema provided below.

<< Structured Request Schema >>
When responding use a markdown code snippet with a JSON object formatted in the following schema:Copy{""query"":string \ text string to compare to document contents""filter"":string \ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.

A logical condition statement is composed of one or more comparison and logical operation statements.

A comparison statement takes the form: `comp(attr, val)`:
- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator
- `attr` (string):  name of attribute to apply the comparison to
- `val` (string): is the comparison value

A logical operation statement takes the form `op(statement1, statement2, ...)`:
- `op` (and | or | not): logical operator
- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to

Make sure that you only use the comparators and logical operators listed above and no others.
Make sure that filters only refer to attributes that exist in the data source.
Make sure that filters only use the attributed names with its function names if there are functions applied on them.
Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.
Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.
Make sure that filters are only used as needed. If there are no filters that should be applied return ""NO_FILTER"" for the filter value.

<< Example 1. >>
Data Source:Copy{""content"":""Lyrics of a song"",""attributes"":{""artist"":{""type"":""string"",""description"":""Name of the song artist""},""length"":{""type"":""integer"",""description"":""Length of the song in seconds""},""genre"":{""type"":""string"",""description"":""The song genre, one of ""pop"", ""rock"" or ""rap""""}}}User Query:
What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre

Structured Request:Copy{""query"":""teenager love"",""filter"":""and(or(eq(\""artist\"", \""Taylor Swift\""), eq(\""artist\"", \""Katy Perry\"")), lt(\""length\"", 180), eq(\""genre\"", \""pop\""))""}<< Example 2. >>
Data Source:Copy{""content"":""Lyrics of a song"",""attributes"":{""artist"":{""type"":""string"",""description"":""Name of the song artist""},""length"":{""type"":""integer"",""description"":""Length of the song in seconds""},""genre"":{""type"":""string"",""description"":""The song genre, one of ""pop"", ""rock"" or ""rap""""}}}User Query:
What are songs that were not published on Spotify

Structured Request:Copy{""query"":"""",""filter"":""NO_FILTER""}<< Example 3. >>
Data Source:Copy{""content"":""Brief summary of a movie"",""attributes"":{""genre"":{""description"":""The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']"",""type"":""string""},""year"":{""description"":""The year the movie was released"",""type"":""integer""},""director"":{""description"":""The name of the movie director"",""type"":""string""},""rating"":{""description"":""A 1-10 rating for the movie"",""type"":""float""}}}User Query:
dummy question

Structured Request:
```
```
Copy{""query"":string \ text string to compare to document contents""filter"":string \ logical condition statement for filtering documents}
```
```
Copy{""content"":""Lyrics of a song"",""attributes"":{""artist"":{""type"":""string"",""description"":""Name of the song artist""},""length"":{""type"":""integer"",""description"":""Length of the song in seconds""},""genre"":{""type"":""string"",""description"":""The song genre, one of ""pop"", ""rock"" or ""rap""""}}}
```
```
Copy{""query"":""teenager love"",""filter"":""and(or(eq(\""artist\"", \""Taylor Swift\""), eq(\""artist\"", \""Katy Perry\"")), lt(\""length\"", 180), eq(\""genre\"", \""pop\""))""}
```
```
Copy{""content"":""Lyrics of a song"",""attributes"":{""artist"":{""type"":""string"",""description"":""Name of the song artist""},""length"":{""type"":""integer"",""description"":""Length of the song in seconds""},""genre"":{""type"":""string"",""description"":""The song genre, one of ""pop"", ""rock"" or ""rap""""}}}
```
```
Copy{""query"":"""",""filter"":""NO_FILTER""}
```
```
Copy{""content"":""Brief summary of a movie"",""attributes"":{""genre"":{""description"":""The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']"",""type"":""string""},""year"":{""description"":""The year the movie was released"",""type"":""integer""},""director"":{""description"":""The name of the movie director"",""type"":""string""},""rating"":{""description"":""A 1-10 rating for the movie"",""type"":""float""}}}
```
query_constructor.invoke()메서드를 호출하여 주어진 쿼리에 대한 처리를 수행합니다.
```
Copyquery_constructor.invoke(
    {# 쿼리 생성기를 호출하여 주어진 질문에 대한 쿼리를 생성합니다.""query"":""What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers""}
)
```
```
StructuredQuery(query='taxi drivers', filter=Operation(operator=, arguments=[Comparison(comparator=, attribute='genre', value='science fiction'), Comparison(comparator=, attribute='year', value=1990), Comparison(comparator=, attribute='year', value=1999), Comparison(comparator=, attribute='director', value='Luc Besson')]), limit=None)
```
Self-query retriever의 핵심 요소는 query constructor입니다. 훌륭한 검색 시스템을 만들기 위해서는 query constructor가 잘 작동하도록 해야 합니다.
이를 위해서는프롬프트(Prompt), 프롬프트 내의 예시, 속성 설명 등을 조정해야 합니다.
[참고]
- 호텔 인벤토리 데이터에 대한 query constructor를 개선하는 과정을 보여주는 예시는쿡북 튜토리얼을 참조하세요.
호텔 인벤토리 데이터에 대한 query constructor를 개선하는 과정을 보여주는 예시는쿡북 튜토리얼을 참조하세요.
다음으로 중요한 요소는 structured query translator입니다. 이는 일반적인StructuredQuery객체를 사용 중인 vector store의 구문에 맞는 메타데이터 필터로 변환하는 역할을 담당합니다.
SelfQueryRetriever를 사용하여 질문에 대한 답변을 생성하는 검색기를 구현합니다.
- query_constructor를 사용하여 질문을 구성합니다.
- vectorstore를 사용하여 벡터 저장소에 접근합니다.
- ChromaTranslator를 사용하여 구조화된 질의를 Chroma 벡터 저장소에 맞게 변환합니다.
query_constructor를 사용하여 질문을 구성합니다.
vectorstore를 사용하여 벡터 저장소에 접근합니다.
ChromaTranslator를 사용하여 구조화된 질의를 Chroma 벡터 저장소에 맞게 변환합니다.
```
Copyfromlangchain.retrievers.self_query.chromaimportChromaTranslator

retriever = SelfQueryRetriever(
    query_constructor=query_constructor,# 이전에 생성한 쿼리 생성기vectorstore=vectorstore,# 벡터 저장소를 지정structured_query_translator=ChromaTranslator(),# 쿼리 변환기
```
retriever.invoke()메서드를 사용하여 주어진 질문에 대한 답변을 생성합니다.
```
Copyretriever.invoke(# 1990년 이후 2005년 이전에 제작된 장난감에 관한 영화를 검색하며, 애니메이션 영화가 선호됩니다.""What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated"")
```
```
[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]
```
이렇게 구조화된 쿼리 생성기 + 쿼리 변환기를 사용하여 오류 없이 데이터를 필터링하고 검색할 수 있습니다."
09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever),https://wikidocs.net/234604,"# 09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)

TimeWeightedVectorStoreRetriever는 의미론적 유사성과 시간에 따른 감쇠를 결합해 사용하는 검색 도구입니다. 이를 통해 문서 또는 데이터의""신선함""과""관련성""을 모두 고려하여 결과를 제공합니다.
스코어링 알고리즘은 다음과 같이 구성됩니다
semantic_similarity+(1.0−decay_rate)hourspassed
여기서semantic_similarity는 문서 또는 데이터 간의 의미적 유사도를 나타내고,decay_rate는 시간이 지남에 따라 점수가 얼마나 감소하는지를 나타내는 비율입니다.hours_passed는 객체가 마지막으로 접근된 후부터 현재까지 경과한 시간(시간 단위)을 의미합니다.
이 방식의 주요 특징은, 객체가 마지막으로 접근된 시간을 기준으로 하여""정보의 신선함""을 평가한다는 점입니다. 즉,자주 접근되는 객체는 시간이 지나도 높은 점수를 유지하며, 이를 통해자주 사용되거나 중요하게 여겨지는 정보가 검색 결과 상위에 위치할 가능성이 높아집니다.이런 방식은 최신성과 관련성을 모두 고려하는 동적인 검색 결과를 제공합니다.
특히,decay_rate는 리트리버의 객체가 생성된 이후가 아니라 **마지막으로 액세스된 이후 경과된 시간을 의미합니다. 즉, 자주 액세스하는 객체는 '최신'으로 유지됩니다.
패키지를 업그레이드 합니다.
```
Copy%pip install -qU deeplake lark
```
```
Copyfromdatetimeimportdatetime, timedeltaimportfaissfromlangchain.docstoreimportInMemoryDocstorefromlangchain.retrieversimportTimeWeightedVectorStoreRetrieverfromlangchain_community.vectorstoresimportFAISSfromlangchain_core.documentsimportDocumentfromlangchain_openaiimportOpenAIEmbeddings
```
## 낮은 감쇠율(low decay_rate)
- decay rate가 낮다는 것은 (여기서는 극단적으로 0에 가깝게 설정할 것입니다)기억이 더 오래 ""기억될""것임을 의미합니다.
- decay rate가0 이라는 것은 기억이 절대 잊혀지지 않는다는 것을 의미하며, 이는 이 retriever를 vector lookup과 동등하게 만듭니다.
decay rate가 낮다는 것은 (여기서는 극단적으로 0에 가깝게 설정할 것입니다)기억이 더 오래 ""기억될""것임을 의미합니다.
decay rate가 낮다는 것은 (여기서는 극단적으로 0에 가깝게 설정할 것입니다)기억이 더 오래 ""기억될""것임을 의미합니다.
decay rate가0 이라는 것은 기억이 절대 잊혀지지 않는다는 것을 의미하며, 이는 이 retriever를 vector lookup과 동등하게 만듭니다.
decay rate가0 이라는 것은 기억이 절대 잊혀지지 않는다는 것을 의미하며, 이는 이 retriever를 vector lookup과 동등하게 만듭니다.
TimeWeightedVectorStoreRetriever를 초기화하며, 벡터 저장소, 감쇠율(decay_rate)을 매우 작은 값으로 설정하고, 검색할 벡터의 개수(k)를 1로 지정합니다.
```
Copy# 임베딩 모델을 정의합니다.embeddings_model = OpenAIEmbeddings()# 벡터 저장소를 빈 상태로 초기화합니다.embedding_size =1536index = faiss.IndexFlatL2(embedding_size)
vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})# 시간 가중치가 적용된 벡터 저장소 검색기를 초기화합니다.retriever = TimeWeightedVectorStoreRetriever(
    vectorstore=vectorstore, decay_rate=0.0000000000000000000000001, k=1)
```
retriever.add_documents()메서드를 사용하여 ""hello world"" 문자열을 가진Document객체를 추가하며, 메타데이터로last_accessed_at에 어제 시간을 설정합니다.
다시retriever.add_documents()메서드를 사용하여 ""hello foo"" 문자열을 가진Document객체를 추가합니다.
```
Copyyesterday = datetime.now() - timedelta(days=1)# 어제 날짜를 계산합니다.retriever.add_documents(# ""hello world"" 내용의 문서를 추가하고, 메타데이터에 어제 날짜를 설정합니다.[Document(page_content=""hello world"", metadata={""last_accessed_at"": yesterday})]
)# ""hello foo"" 내용의 문서를 추가합니다.retriever.add_documents([Document(page_content=""hello foo"")])
```
```
['d3c2b942-2fa2-4d78-9c87-931fbbc7646d']
```
retriever.get_relevant_documents(""hello world"")메서드를 호출하여 ""hello world""와 관련된 문서를 검색합니다.
- ""Hello World""가 가장 먼저 반환되는데, 이는 가장 두드러진(salient) 문서이기 때문입니다.
- decay_rate가0에 가깝기 때문에 ""Hello World"" 문서는 여전히 최신(recent)으로 간주됩니다.
""Hello World""가 가장 먼저 반환되는데, 이는 가장 두드러진(salient) 문서이기 때문입니다.
decay_rate가0에 가깝기 때문에 ""Hello World"" 문서는 여전히 최신(recent)으로 간주됩니다.
```
Copy# ""Hello World""가 가장 먼저 반환되는 이유는 가장 두드러지기 때문이며, 감쇠율이 0에 가깝기 때문에 여전히 최신 상태를 유지하고 있음을 의미합니다.retriever.get_relevant_documents(""hello world"")
```
```
[Document(page_content='hello world', metadata={'last_accessed_at': datetime.datetime(2024, 3, 23, 0, 25, 31, 137309), 'created_at': datetime.datetime(2024, 3, 23, 0, 25, 30, 544821), 'buffer_idx': 0})]
```
## 높음 감쇠율(high decay_rate)
높은decay_rate(예: 0.9999...)를 사용하면recency score가 빠르게 0으로 수렴합니다.
(만약 이 값을 1로 설정하면 모든 객체의recency값이 0이 되어, Vector Lookup 과 동일한 결과를 얻게 됩니다.)
TimeWeightedVectorStoreRetriever를 사용하여 검색기를 초기화합니다.decay_rate를 0.999로 설정하여 시간에 따른 가중치 감소율을 조정합니다.
```
Copy# 임베딩 모델을 정의합니다.embeddings_model = OpenAIEmbeddings()# 벡터 저장소를 빈 상태로 초기화합니다.embedding_size =1536index = faiss.IndexFlatL2(embedding_size)
vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})# 시간 가중치가 적용된 벡터 저장소 검색기를 초기화합니다.retriever = TimeWeightedVectorStoreRetriever(
    vectorstore=vectorstore, decay_rate=0.999, k=1)
```
retriever.add_documents()메서드를 사용하여 ""hello world"" 문자열을 가진Document객체를 추가하며, 메타데이터로last_accessed_at에 어제 시간을 설정합니다.
다시retriever.add_documents()메서드를 사용하여 ""hello foo"" 문자열을 가진Document객체를 추가합니다.
```
Copyyesterday = datetime.now() - timedelta(days=1)# 어제 날짜를 계산합니다.retriever.add_documents(# ""hello world"" 내용의 문서를 추가하고, 메타데이터에 어제 날짜를 설정합니다.[Document(page_content=""hello world"", metadata={""last_accessed_at"": yesterday})]
)# ""hello foo"" 내용의 문서를 추가합니다.retriever.add_documents([Document(page_content=""hello foo"")])
```
```
['076d7c8c-5ee3-4316-b201-55140bbb686f']
```
- retriever.get_relevant_documents(""hello world"")를 호출하면 ""Hello Foo""가 먼저 반환됩니다.
- 이는 retriever가 ""hello world""와 관련된 문서를 대부분 잊어버렸기 때문입니다.
retriever.get_relevant_documents(""hello world"")를 호출하면 ""Hello Foo""가 먼저 반환됩니다.
이는 retriever가 ""hello world""와 관련된 문서를 대부분 잊어버렸기 때문입니다.
```
Copy# ""hello world""와 가장 관련된 문서를 검색합니다.# ""Hello Foo""가 먼저 반환되는 이유는 ""hello world""가 대부분 잊혀졌기 때문입니다.retriever.get_relevant_documents(""hello world"")
```
```
[Document(page_content='hello foo', metadata={'last_accessed_at': datetime.datetime(2024, 3, 23, 0, 25, 31, 972916), 'created_at': datetime.datetime(2024, 3, 23, 0, 25, 31, 718842), 'buffer_idx': 1})]
```
## 감쇠율(decay_rate) 정리
- decay_rate를 0.000001 로 매우 작게 설정한 경우: 감쇠율(즉, 정보를 망각하는 비율)이 매우 낮기 때문에 정보를 거의 잊지 않습니다. 따라서,최신 정보이든 오래된 정보든 시간 가중치 차이가 거의 없습니다.이럴때는 유사도에 더 높은 점수를 주게 됩니다.
- decay_rate를 0.999 로 1에 가깝게 설정한 경우: 감쇠율(즉, 정보를 망각하는 비율)이 매우 높습니다. 따라서, 과거의 정보는 거의다 잊어버립니다. 따라서, 이러한 경우는 최신 정보에 더 높은 점수를 주게 됩니다.
decay_rate를 0.000001 로 매우 작게 설정한 경우: 감쇠율(즉, 정보를 망각하는 비율)이 매우 낮기 때문에 정보를 거의 잊지 않습니다. 따라서,최신 정보이든 오래된 정보든 시간 가중치 차이가 거의 없습니다.이럴때는 유사도에 더 높은 점수를 주게 됩니다.
decay_rate를 0.999 로 1에 가깝게 설정한 경우: 감쇠율(즉, 정보를 망각하는 비율)이 매우 높습니다. 따라서, 과거의 정보는 거의다 잊어버립니다. 따라서, 이러한 경우는 최신 정보에 더 높은 점수를 주게 됩니다.
## 가상의 시간(Virtual time)
LangChain의 일부 유틸리티를 사용하면 시간 구성 요소를 모의(mock) 테스트 할 수 있습니다.
mock_now함수는 LangChain에서 제공하는 유틸리티 함수로, 현재 시간을 모의(mock)하는 데 사용됩니다.
- 아래는 시간을 2023년 6월 8일로 설정해 보겠습니다.
아래는 시간을 2023년 6월 8일로 설정해 보겠습니다.
```
Copyimportdatetimefromlangchain.utilsimportmock_now# 현재 시간을 특정 시점으로 설정mock_now(datetime.datetime(2023,6,8,0,0,0))# 현재 시간 출력print(datetime.datetime.now())
```
```
2024-03-23 00:25:32.221079
```
mock_now함수를 사용하여 현재 시간을 2024년 3월 28일 10시 11분으로 설정합니다.
- retriever.get_relevant_documents메서드를 호출하여 ""hello world""라는 쿼리에 대한 관련 문서를 검색합니다.
- 검색 결과를 출력합니다. 이때 문서의 마지막 접근 시간이 설정된 시간(2024년 3월 28일 10시 11분)으로 표시됩니다.
retriever.get_relevant_documents메서드를 호출하여 ""hello world""라는 쿼리에 대한 관련 문서를 검색합니다.
검색 결과를 출력합니다. 이때 문서의 마지막 접근 시간이 설정된 시간(2024년 3월 28일 10시 11분)으로 표시됩니다.
```
Copy# 현재 시간을 2024년 6월 3일 10시 11분으로 설정합니다.withmock_now(datetime.datetime(2024,3,28,10,11)):# ""hello world""와 관련된 문서를 검색하고 출력합니다.print(retriever.get_relevant_documents(""hello world""))
```
```
[Document(page_content='hello world', metadata={'last_accessed_at': MockDateTime(2024, 3, 28, 10, 11), 'created_at': datetime.datetime(2024, 3, 23, 0, 25, 31, 454121), 'buffer_idx': 0})]
```"
CH12 Retrieval Augmented Generation(RAG),https://wikidocs.net/233780,"# CH12 Retrieval Augmented Generation(RAG)

Retrieval-Augmented Generation(RAG)는 자연어 처리(NLP) 분야에서의 혁신적인 기술로, 기존의 언어 모델의 한계를 넘어서정보 검색과생성을 통합하는 방법론입니다.
기본적으로, RAG는 풍부한 정보를 담고 있는대규모 문서 데이터베이스에서 관련 정보를 검색하고, 이를 통해 언어모델이 더 정확하고 상세한 답변을 생성할 수 있게 합니다.
예를 들어,최신 뉴스 이벤트나 특정 분야의 전문 지식과 같은 주제에 대해 물어보면,RAG는 관련 문서를 찾아 그 내용을 바탕으로 답변을 구성합니다.
## RAG의 8단계 프로세스

도큐먼트 로드 (Document Loader): 이 단계에서는 외부 데이터 소스에서 필요한 문서를 로드하고 초기 처리를 합니다. 이것은 마치 책을 여러 권 챙겨 도서관에서 공부하는 것과 비슷합니다. 학생이 공부하기 전에 필요한 책들을 책장에서 골라오는 과정입니다.
도큐먼트 로드 (Document Loader): 이 단계에서는 외부 데이터 소스에서 필요한 문서를 로드하고 초기 처리를 합니다. 이것은 마치 책을 여러 권 챙겨 도서관에서 공부하는 것과 비슷합니다. 학생이 공부하기 전에 필요한 책들을 책장에서 골라오는 과정입니다.
텍스트 분할 (Text Splitter): 로드된 문서를 처리 가능한 작은 단위로 분할합니다. 큰 책을 챕터별로 나누는 것과 유사합니다.
텍스트 분할 (Text Splitter): 로드된 문서를 처리 가능한 작은 단위로 분할합니다. 큰 책을 챕터별로 나누는 것과 유사합니다.
임베딩 (Embedding): 각 문서 또는 문서의 일부를 벡터 형태로 변환하여, 문서의 의미를 수치화합니다. 이는 책의 내용을 요약하여 핵심 키워드로 표현하는 것과 비슷합니다.
임베딩 (Embedding): 각 문서 또는 문서의 일부를 벡터 형태로 변환하여, 문서의 의미를 수치화합니다. 이는 책의 내용을 요약하여 핵심 키워드로 표현하는 것과 비슷합니다.
벡터스토어(Vector Store) 저장: 임베딩된 벡터들을 데이터베이스에 저장합니다. 이는 요약된 키워드를 색인화하여 나중에 빠르게 찾을 수 있도록 하는 과정입니다.
벡터스토어(Vector Store) 저장: 임베딩된 벡터들을 데이터베이스에 저장합니다. 이는 요약된 키워드를 색인화하여 나중에 빠르게 찾을 수 있도록 하는 과정입니다.

검색기 (Retriever): 질문이 주어지면, 이와 관련된 벡터를 벡터 데이터베이스에서 검색합니다. 질문에 가장 잘 맞는 책의 챕터를 찾는 것과 유사합니다.
검색기 (Retriever): 질문이 주어지면, 이와 관련된 벡터를 벡터 데이터베이스에서 검색합니다. 질문에 가장 잘 맞는 책의 챕터를 찾는 것과 유사합니다.
프롬프트 (Prompt): 검색된 정보를 바탕으로 언어 모델을 위한 질문을 구성합니다. 이는 정보를 바탕으로 어떻게 질문할지 결정하는 과정입니다.
프롬프트 (Prompt): 검색된 정보를 바탕으로 언어 모델을 위한 질문을 구성합니다. 이는 정보를 바탕으로 어떻게 질문할지 결정하는 과정입니다.
LLM (Large Language Model): 구성된 프롬프트를 사용하여 언어 모델이 답변을 생성합니다. 즉, 수집된 정보를 바탕으로 과제나 보고서를 작성하는 학생과 같습니다.
LLM (Large Language Model): 구성된 프롬프트를 사용하여 언어 모델이 답변을 생성합니다. 즉, 수집된 정보를 바탕으로 과제나 보고서를 작성하는 학생과 같습니다.
체인(Chain) 생성: 이전의 모든 과정의 하나의 파이프라인으로 묶어주는 체인(Chain) 을 생성합니다.
체인(Chain) 생성: 이전의 모든 과정의 하나의 파이프라인으로 묶어주는 체인(Chain) 을 생성합니다."
01. 기본내용 - 네이버 뉴스기사 QA 봇,https://wikidocs.net/234008,"# 01. 기본내용 - 네이버 뉴스기사 QA 봇

## 개요
LangChain은 질문과 답변(Q&A) 애플리케이션을 구축하는 데 필요한 다양한 구성 요소를 제공하는 툴킷입니다. 이러한 구성 요소들은 개발자가 텍스트 데이터 소스를 활용하여 고급 Q&A 애플리케이션을 쉽게 만들 수 있도록 돕습니다. 본 튜토리얼에서는 LangChain을 사용하여 간단한 Q&A 애플리케이션을 구축하는 과정을 소개하고, LangChain의 주요 구성 요소와 이들이 어떻게 상호 작용하는지 살펴보겠습니다. 또한, 애플리케이션 개발 과정에서 LangSmith가 어떻게 도움이 될 수 있는지도 탐구할 것입니다.
## LangChain Q&A 아키텍처
LangChain Q&A 애플리케이션의 아키텍처는 크게 두 가지 주요 단계로 구성됩니다:질문 처리와답변 생성. 이 과정에서 여러 LangChain 구성 요소가 사용됩니다.
질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:
- 데이터 소스 연결: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.
- 데이터 인덱싱 및 검색: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.
데이터 소스 연결: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.
데이터 인덱싱 및 검색: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.
관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:
- 답변 생성 모델: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.
답변 생성 모델: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.
LangSmith의 역할
LangSmith는 애플리케이션의 성능을 추적하고 분석하는 데 도움을 주는 도구입니다. Q&A 애플리케이션의 복잡성이 증가함에 따라, LangSmith는 다음과 같은 방법으로 유용하게 사용될 수 있습니다:
- 성능 모니터링: LangSmith를 사용하여 애플리케이션의 질문 처리 및 답변 생성 성능을 모니터링할 수 있습니다. 이를 통해 애플리케이션의 성능을 지속적으로 개선할 수 있습니다.
- 오류 분석: LangSmith는 애플리케이션이 잘못된 답변을 생성했을 때, 이의 원인을 분석하는 데 도움을 줍니다. 이를 통해 개발자는 애플리케이션의 오류를 정확하게 파악하고, 이를 수정하는 데 필요한 정보를 얻을 수 있습니다.
성능 모니터링: LangSmith를 사용하여 애플리케이션의 질문 처리 및 답변 생성 성능을 모니터링할 수 있습니다. 이를 통해 애플리케이션의 성능을 지속적으로 개선할 수 있습니다.
오류 분석: LangSmith는 애플리케이션이 잘못된 답변을 생성했을 때, 이의 원인을 분석하는 데 도움을 줍니다. 이를 통해 개발자는 애플리케이션의 오류를 정확하게 파악하고, 이를 수정하는 데 필요한 정보를 얻을 수 있습니다.
LangChain을 사용하여 간단한 Q&A 애플리케이션을 구축하는 과정은 개발자에게 매우 유용한 경험을 제공합니다. LangChain의 다양한 구성 요소는 질문 처리와 답변 생성 과정을 간소화하며, LangSmith는 애플리케이션의 성능을 모니터링하고 개선하는 데 필수적인 도구입니다. 이 튜토리얼을 통해 얻은 지식을 바탕으로, 개발자는 자신만의 고급 Q&A 애플리케이션을 구축할 수 있는 기반을 마련할 수 있을 것입니다.
## 아키텍처
우리는Q&A 소개에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:
- 인덱싱: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다.이 작업은 보통 오프라인에서 발생합니다.
- 검색 및 생성: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.
인덱싱: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다.이 작업은 보통 오프라인에서 발생합니다.
인덱싱: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다.이 작업은 보통 오프라인에서 발생합니다.
검색 및 생성: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.
검색 및 생성: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.
RAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.

로드: 먼저 데이터를 로드해야 합니다. 이를 위해DocumentLoaders를 사용할 것입니다.
분할:Text splitters는 큰Documents를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.
저장: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종VectorStore와Embeddings모델을 사용하여 수행됩니다.

검색: 사용자 입력이 주어지면Retriever를 사용하여 저장소에서 관련 분할을 검색합니다.
생성:ChatModel/LLM은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다
## 환경설정
API KEY 를 설정합니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
LangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은LangSmith를 사용하는 것입니다.
LangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.
```
Copyimportos# 디버깅을 위한 프로젝트명을 기입합니다.os.environ[""LANGCHAIN_PROJECT""] =""4일차 실습""# tracing 을 위해서는 아래 코드의 주석을 해제하고 실행합니다.# os.environ[""LANGCHAIN_TRACING_V2""] = true
```
## 첫 번째 RAG 기반 QA봇
첫 번째 튜토리얼에는 네이버 뉴스기사의 내용에 대해 질문할 수 있는뉴스기사 QA 앱을 구축할 것입니다.
이 가이드에서는 OpenAI 챗 모델과 임베딩, 그리고 Chroma 벡터 스토어를 사용할 것입니다.
하지만 여기서 보여지는 모든 것은 어떤ChatModel이나LLM,임베딩, 그리고VectorStore또는Retriever와도 작동합니다.
먼저 다음의 과정을 통해 간단한 인덱싱 파이프라인과 RAG 체인을 약 20줄의 코드로 구현할 수 있습니다.
라이브러리
- bs4는 웹 페이지를 파싱하기 위한 라이브러리입니다.
- langchain은 AI와 관련된 다양한 기능을 제공하는 라이브러리로, 여기서는 특히 텍스트 분할(RecursiveCharacterTextSplitter), 문서 로딩(WebBaseLoader), 벡터 저장(Chroma,FAISS), 출력 파싱(StrOutputParser), 실행 가능한 패스스루(RunnablePassthrough) 등을 다룹니다.
- langchain_openai모듈을 통해 OpenAI의 챗봇(ChatOpenAI)과 임베딩(OpenAIEmbeddings) 기능을 사용할 수 있습니다.
bs4는 웹 페이지를 파싱하기 위한 라이브러리입니다.
langchain은 AI와 관련된 다양한 기능을 제공하는 라이브러리로, 여기서는 특히 텍스트 분할(RecursiveCharacterTextSplitter), 문서 로딩(WebBaseLoader), 벡터 저장(Chroma,FAISS), 출력 파싱(StrOutputParser), 실행 가능한 패스스루(RunnablePassthrough) 등을 다룹니다.
langchain_openai모듈을 통해 OpenAI의 챗봇(ChatOpenAI)과 임베딩(OpenAIEmbeddings) 기능을 사용할 수 있습니다.
```
Copyimportbs4fromlangchainimporthubfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain_community.document_loadersimportWebBaseLoaderfromlangchain_community.vectorstoresimportFAISSfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_openaiimportChatOpenAI, OpenAIEmbeddings
```
웹 페이지의 내용을 로드하고, 텍스트를 청크로 나누어 인덱싱하는 과정을 거친 후, 관련된 텍스트 스니펫을 검색하여 새로운 내용을 생성하는 과정을 구현합니다.
WebBaseLoader는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해bs4.SoupStrainer를 사용합니다.
[참고]
- bs4.SoupStrainer는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.
bs4.SoupStrainer는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.
(예시)
```
Copybs4.SoupStrainer(""div"",
    attrs={""class"": [""newsct_article _article_body"",""media_end_head_title""]},# 클래스 명을 입력)
```
```
Copy# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.loader = WebBaseLoader(
    web_paths=(""https://n.news.naver.com/article/437/0000378416"",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(""div"",
            attrs={""class"": [""newsct_article _article_body"",""media_end_head_title""]},
        )
    ),
)

docs = loader.load()print(f""문서의 수:{len(docs)}"")
docs
```
```
문서의 수: 1
```
```
[Document(page_content=""\n출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\n\n\n[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]\n\t\t\n"", metadata={'source': 'https://n.news.naver.com/article/437/0000378416'})]
```
RecursiveCharacterTextSplitter는 문서를 지정된 크기의 청크로 나눕니다.
```
Copytext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

splits = text_splitter.split_documents(docs)len(splits)
```
```
3
```
FAISS혹은Chroma와 같은 vectorstore는 이러한 청크를 바탕으로 문서의 벡터 표현을 생성합니다.
```
Copy# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.retriever = vectorstore.as_retriever()
```
vectorstore.as_retriever()를 통해 생성된 검색기는hub.pull로 가져온 프롬프트와ChatOpenAI모델을 사용하여 새로운 내용을 생성합니다.
마지막으로,StrOutputParser는 생성된 결과를 문자열로 파싱합니다.
```
Copyprompt = hub.pull(""rlm/rag-prompt"")
prompt
```
```
ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:""))])
```
```
Copyprint(prompt.messages[0].prompt.template)
```
```
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: {question} 
Context: {context} 
Answer:
```
```
Copyfromlangchain.callbacks.baseimportBaseCallbackHandlerclassStreamCallback(BaseCallbackHandler):defon_llm_new_token(self, token:str, **kwargs):print(token, end="""", flush=True)


llm = ChatOpenAI(
    model_name=""gpt-4-turbo-preview"",
    temperature=0,
    streaming=True,
    callbacks=[StreamCallback()],
)defformat_docs(docs):# 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.return""\n\n"".join(doc.page_contentfordocindocs)# 체인을 생성합니다.rag_chain = (
    {""context"": retriever | format_docs,""question"": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```
이 함수는rag_chain객체의invoke메서드를 사용하여 사용자의 질문을 처리합니다.
LangSmith Trace 보기
```
Copy_ = rag_chain.invoke(""부영그룹의 출산 장려 정책에 대해 설명해주세요."")# 문서에 대한 질의를 입력하고, 답변을 출력합니다.
```
```
부영그룹은 출산 장려 정책으로 2021년 이후 태어난 직원 자녀에게 1억원씩 지원하며, 총 70억원을 지원할 계획입니다. 연년생이나 쌍둥이 자녀가 있는 경우 총 2억원을 받게 되며, 셋째 아이를 낳는 경우 국민주택을 제공하겠다는 정책도 있습니다. 이러한 정책은 직원들에게 금전적인 도움을 주어 출산과 양육에 대한 경제적 부담을 줄이기 위한 것입니다.
```
LangSmith Trace 보기
```
Copyrag_chain.invoke(""부영그룹은 출산 직원에게 얼마의 지원을 제공하나요?"")# 문서에 대한 질의를 입력하고, 답변을 출력합니다.
```
```
부영그룹은 출산 직원에게 1억원을 지원합니다. 2021년 이후 태어난 직원 자녀에게 이 지원금을 제공하며, 연년생이나 쌍둥이 자녀가 있을 경우 총 2억원을 받게 됩니다. 셋째 아이를 낳는 경우에는 국민주택을 제공하겠다는 계획도 밝혔습니다.
```
```
'부영그룹은 출산 직원에게 1억원을 지원합니다. 2021년 이후 태어난 직원 자녀에게 이 지원금을 제공하며, 연년생이나 쌍둥이 자녀가 있을 경우 총 2억원을 받게 됩니다. 셋째 아이를 낳는 경우에는 국민주택을 제공하겠다는 계획도 밝혔습니다.'
```
LangSmith Trace 보기
```
Copyprint(
    rag_chain.invoke(""정부의 저출생 대책을 bullet points 형식으로 작성해 주세요."")
)# 문서에 대한 질의를 입력하고, 답변을 출력합니다.
```
```
- 매달 부모 급여 지급: 0세 아이는 100만원으로 인상
- 첫만남이용권 및 아동수당 제공: 아이 돌까지 1년 동안 총 1520만원 지원
- 지자체별 추가 지원: 예를 들어, 인천시는 새로 태어난 아기에게 18살 될 때까지 1억원 지급, 광주시는 17살 될 때까지 7400만원 지급- 매달 부모 급여 지급: 0세 아이는 100만원으로 인상
- 첫만남이용권 및 아동수당 제공: 아이 돌까지 1년 동안 총 1520만원 지원
- 지자체별 추가 지원: 예를 들어, 인천시는 새로 태어난 아기에게 18살 될 때까지 1억원 지급, 광주시는 17살 될 때까지 7400만원 지급
```
LangSmith Trace 보기
```
Copyrag_chain.invoke(""부영그룹의 임직원 숫자는 몇명인가요?"")# 문서에 대한 질의를 입력하고, 답변을 출력합니다.
```
```
제공된 문맥에서 부영그룹의 임직원 수에 대한 구체적인 정보는 언급되지 않았습니다.
```
```
'제공된 문맥에서 부영그룹의 임직원 수에 대한 구체적인 정보는 언급되지 않았습니다.'
```
이 함수는vectorstore객체의delete_collection메서드를 호출하여 컬렉션을 삭제합니다. 이는 데이터 정리 과정에서 사용되며, 특정 데이터셋이나 정보를 저장하는 컬렉션을 제거함으로써 시스템의 저장 공간을 확보하고, 불필요한 데이터로 인한 혼란을 방지합니다.
```
Copy# 컬렉션을 삭제합니다.vectorstore.delete_collection()
```"
02. RAG 의 기능별 다양한 모듈 활용기,https://wikidocs.net/234009,"# 02. RAG 의 기능별 다양한 모듈 활용기



질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:
- 데이터 소스 연결: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.
- 데이터 인덱싱 및 검색: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.
데이터 소스 연결: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.
데이터 인덱싱 및 검색: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.
관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:
- 답변 생성 모델: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.
답변 생성 모델: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.
## 아키텍처
우리는Q&A 소개에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:
- 인덱싱: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다.이 작업은 보통 오프라인에서 발생합니다.
- 검색 및 생성: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.
인덱싱: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다.이 작업은 보통 오프라인에서 발생합니다.
인덱싱: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다.이 작업은 보통 오프라인에서 발생합니다.
검색 및 생성: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.
검색 및 생성: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.
RAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.

로드: 먼저 데이터를 로드해야 합니다. 이를 위해DocumentLoaders를 사용할 것입니다.
분할:Text splitters는 큰Documents를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.
저장: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종VectorStore와Embeddings모델을 사용하여 수행됩니다.

검색: 사용자 입력이 주어지면Retriever를 사용하여 저장소에서 관련 분할을 검색합니다.
생성:ChatModel/LLM은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다
## 실습에 활용한 문서
소프트웨어정책연구소(SPRi) - 2023년 12월호
- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)
- 링크: https://spri.kr/posts/view/23669
- 파일명:SPRI_AI_Brief_2023년12월호_F.pdf
저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)
링크: https://spri.kr/posts/view/23669
파일명:SPRI_AI_Brief_2023년12월호_F.pdf
실습을 위해 다운로드 받은 파일을data폴더로 복사해 주시기 바랍니다
## 환경설정
API KEY 를 설정합니다.
```
Copy# API 키를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API 키 정보 로드load_dotenv()
```
```
True
```
LangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은LangSmith를 사용하는 것입니다.
LangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.
```
Copyimportos# 디버깅을 위한 프로젝트명을 기입합니다.os.environ[""LANGCHAIN_PROJECT""] =""RAG TUTORIAL""# tracing 을 위해서는 아래 코드의 주석을 해제하고 실행합니다.# os.environ[""LANGCHAIN_TRACING_V2""] = true
```
## 모듈별로 자세히 살펴보기
```
Copyimportbs4fromlangchainimporthubfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain_community.document_loadersimportWebBaseLoaderfromlangchain_community.vectorstoresimportChroma, FAISSfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_openaiimportChatOpenAI, OpenAIEmbeddings
```
아래는에서 다뤘던 기본적인 RAG 모델을 사용하는 예제입니다.
여기서 각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.
```
Copy# 단계 1: 문서 로드(Load Documents)# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.url =""https://n.news.naver.com/article/437/0000378416""loader = WebBaseLoader(
    web_paths=(url,),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(""div"",
            attrs={""class"": [""newsct_article _article_body"",""media_end_head_title""]},
        )
    ),
)
docs = loader.load()# 단계 2: 문서 분할(Split Documents)text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)

splits = text_splitter.split_documents(docs)# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())# 단계 4: 검색(Search)# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.retriever = vectorstore.as_retriever()# 단계 5: 프롬프트 생성(Create Prompt)# 프롬프트를 생성합니다.prompt = hub.pull(""rlm/rag-prompt"")# 단계 6: 언어모델 생성(Create LLM)# 모델(LLM) 을 생성합니다.llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0)defformat_docs(docs):# 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.return""\n\n"".join(doc.page_contentfordocindocs)# 단계 7: 체인 생성(Create Chain)rag_chain = (
    {""context"": retriever | format_docs,""question"": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)# 단계 8: 체인 실행(Run Chain)# 문서에 대한 질의를 입력하고, 답변을 출력합니다.question =""부영그룹의 출산 장려 정책에 대해 설명해주세요""response = rag_chain.invoke(question)# 결과 출력print(f""URL:{url}"")print(f""문서의 수:{len(docs)}"")print(""===""*20)print(f""[HUMAN]\n{question}\n"")print(f""[AI]\n{response}"")
```
```
URL: https://n.news.naver.com/article/437/0000378416
문서의 수: 1
============================================================
[HUMAN]
부영그룹의 출산 장려 정책에 대해 설명해주세요

[AI]
부영그룹은 2021년 이후 태어난 직원 자녀에게 1억원을 지원하는 출산 장려 정책을 내놓았습니다. 또한, 셋째까지 낳는 경우에는 국민주택을 제공하기로 했습니다. 이러한 정책은 직원들의 경제적 부담을 줄이고 출산을 장려하기 위한 것입니다.
```
## 단계 1: 문서 로드(Load Documents)
- 공식문서 링크 - Document loaders
공식문서 링크 - Document loaders
WebBaseLoader는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해bs4.SoupStrainer를 사용합니다.
[참고]
- bs4.SoupStrainer는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.
bs4.SoupStrainer는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.
(예시)
```
Copybs4.SoupStrainer(""div"",
    attrs={""class"": [""newsct_article _article_body"",""media_end_head_title""]},# 클래스 명을 입력)

bs4.SoupStrainer(""article"",
    attrs={""id"": [""dic_area""]},# 클래스 명을 입력)
```
아래의 BBC 뉴스 기사입니다. 영문으로 작성된 기사로 시험해 보고 싶다면, 아래의 주석을 해제하고 실행해 보세요.
```
Copy# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.loader = WebBaseLoader(
    web_paths=(""https://www.bbc.com/news/business-68092814"",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(""main"",
            attrs={""id"": [""main-content""]},
        )
    ),
)
docs = loader.load()print(f""문서의 수:{len(docs)}"")
docs[0].page_content[:500]
```
```
문서의 수: 1
```
```
'Could AI \'trading bots\' transform the world of investing?Published1 FebruaryShareclose panelShare pageCopy linkAbout sharingImage source, Getty ImagesImage caption, It is hard for both humans and computers to predict stock market movementsBy Jonty BloomBusiness reporterSearch for ""AI investing"" online, and you\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI ""trading bots"" could apparently do with my in'
```
```
Copyfromlangchain.document_loadersimportPyPDFLoader# PDF 파일 로드. 파일의 경로 입력loader = PyPDFLoader(""data/SPRI_AI_Brief_2023년12월호_F.pdf"")# 페이지 별 문서 로드docs = loader.load()print(f""문서의 수:{len(docs)}"")# 10번째 페이지의 내용 출력print(f""\n[페이지내용]\n{docs[10].page_content[:500]}"")print(f""\n[metadata]\n{docs[10].metadata}\n"")
```
```
문서의 수: 23

[페이지내용]
SPRi AI Brief |  
2023-12 월호
8코히어 , 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개
n코히어와 12개 기관이  광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태, 
작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기 ’ 플랫폼을 출시
n대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의 
구성과 계보도 추적 가능KEY Contents
£데이터 출처 탐색기 , 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상
nAI 기업 코히어 (Cohere) 가 매사추세츠 공과⼤(MIT), 하버드 ⼤ 로스쿨 , 카네기멜론 ⼤ 등 12개 기관과  
함께 2023 년 10월 25일 ‘데이터 출처 탐색기 (Data Provenance Explorer)’ 플랫폼을 공개
∙AI 모델 훈련에 사용되는 데이터셋의 불분명한 출처로 인해 데이터 투명성이 확보되지 않아 다양한 
법적·윤리적 문제가 발생
∙이에 연구

[metadata]
{'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 10}
```
CSV 는 페이지 번호 대신 행번호로 데이터를 조회합니다.
```
Copyfromlangchain_community.document_loaders.csv_loaderimportCSVLoader# CSV 파일 로드loader = CSVLoader(file_path=""data/titanic.csv"")
docs = loader.load()print(f""문서의 수:{len(docs)}"")# 10번째 페이지의 내용 출력print(f""\n[페이지내용]\n{docs[10].page_content[:500]}"")print(f""\n[metadata]\n{docs[10].metadata}\n"")
```
```
문서의 수: 891

[페이지내용]
PassengerId: 11
Survived: 1
Pclass: 3
Name: Sandstrom, Miss. Marguerite Rut
Sex: female
Age: 4
SibSp: 1
Parch: 1
Ticket: PP 9549
Fare: 16.7
Cabin: G6
Embarked: S

[metadata]
{'source': 'data/titanic.csv', 'row': 10}
```
```
Copyfromlangchain_community.document_loadersimportTextLoader

loader = TextLoader(""data/appendix-keywords.txt"")
docs = loader.load()print(f""문서의 수:{len(docs)}"")# 10번째 페이지의 내용 출력print(f""\n[페이지내용]\n{docs[0].page_content[:500]}"")print(f""\n[metadata]\n{docs[0].metadata}\n"")
```
```
문서의 수: 1

[페이지내용]
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.
연관키워드: 토큰화, 자연어

[metadata]
{'source': 'data/appendix-keywords.txt'}
```
아래는 폴더 내 모든.txt파일을 로드하는 예시입니다.
```
Copyfromlangchain_community.document_loadersimportDirectoryLoader

loader = DirectoryLoader(""."", glob=""data/*.txt"", show_progress=True)
docs = loader.load()print(f""문서의 수:{len(docs)}"")# 10번째 페이지의 내용 출력print(f""\n[페이지내용]\n{docs[0].page_content[:500]}"")print(f""\n[metadata]\n{docs[0].metadata}\n"")
```
```
100%|██████████| 6/6 [00:00<00:00, 36.18it/s]
```
```
문서의 수: 6

[페이지내용]
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다. 연관키워드: 토큰화, 자연어

[metadata]
{'source': 'data/appendix-keywords-CP949.txt'}
```
```

```
다음은 폴더내 모든.pdf파일을 로드하는 예제입니다.
```
Copyfromlangchain_community.document_loadersimportDirectoryLoader

loader = DirectoryLoader(""."", glob=""data/*.pdf"")
docs = loader.load()print(f""문서의 수:{len(docs)}\n"")print(""[메타데이터]\n"")print(docs[0].metadata)print(""\n========= [앞부분] 미리보기 =========\n"")print(docs[0].page_content[2500:3000])
```
다음은.py파일을 로드하는 예제입니다.
```
Copyfromlangchain_community.document_loadersimportPythonLoader

loader = DirectoryLoader(""."", glob=""**/*.py"", loader_cls=PythonLoader)
docs = loader.load()print(f""문서의 수:{len(docs)}\n"")print(""[메타데이터]\n"")print(docs[0].metadata)print(""\n========= [앞부분] 미리보기 =========\n"")print(docs[0].page_content[:500])
```
```
문서의 수: 1

[메타데이터]

{'source': 'data/audio_utils.py'}

========= [앞부분] 미리보기 =========

import re
import os
from pytube import YouTube
from moviepy.editor import AudioFileClip, VideoFileClip
from pydub import AudioSegment
from pydub.silence import detect_nonsilent


def extract_abr(abr):
    youtube_audio_pattern = re.compile(r""\d+"")
    kbps = youtube_audio_pattern.search(abr)
    if kbps:
        kbps = kbps.group()
        return int(kbps)
    else:
        return 0


def get_audio_filepath(filename):
    # audio 폴더가 없으면 생성
    if not os.path.isdir(""audio""):
        os.mkdir(""au
```
## 단계 2: 문서 분할(Split Documents)
```
Copy# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.loader = WebBaseLoader(
    web_paths=(""https://www.bbc.com/news/business-68092814"",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(""main"",
            attrs={""id"": [""main-content""]},
        )
    ),
)
docs = loader.load()print(f""문서의 수:{len(docs)}"")
docs[0].page_content[:500]
```
이것은 가장 간단한 방법입니다. 이 방법은 문자를 기준으로 분할합니다(기본값은 ""\n\n"") 그리고 청크의 길이를 문자의 수로 측정합니다.
텍스트가 어떻게 분할되는지: 단일 문자 단위
청크 크기가 어떻게 측정되는지:lenof characters.
시각화 예제: https://chunkviz.up.railway.app/
CharacterTextSplitter클래스는 텍스트를 특정 크기의 청크로 분할하는 기능을 제공합니다.
- separator매개변수는 청크를 구분하는 데 사용되는 문자열을 지정하며, 여기서는 두 개의 개행 문자(""\n\n"")를 사용합니다
- chunk_size는 각 청크의 최대 길이를 결정합니다
- chunk_overlap은 인접한 청크 간에 겹치는 문자의 수를 지정합니다.
- length_function은 청크의 길이를 계산하는 데 사용되는 함수를 결정하며, 기본적으로 문자열의 길이를 반환하는len함수가 사용됩니다.
- is_separator_regex는separator가 정규 표현식으로 해석될지 여부를 결정하는 불리언 값입니다.
separator매개변수는 청크를 구분하는 데 사용되는 문자열을 지정하며, 여기서는 두 개의 개행 문자(""\n\n"")를 사용합니다
chunk_size는 각 청크의 최대 길이를 결정합니다
chunk_overlap은 인접한 청크 간에 겹치는 문자의 수를 지정합니다.
length_function은 청크의 길이를 계산하는 데 사용되는 함수를 결정하며, 기본적으로 문자열의 길이를 반환하는len함수가 사용됩니다.
is_separator_regex는separator가 정규 표현식으로 해석될지 여부를 결정하는 불리언 값입니다.
```
Copyfromlangchain.text_splitterimportCharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator=""\n\n"",
    chunk_size=100,
    chunk_overlap=10,
    length_function=len,
    is_separator_regex=False,
)
```
이 함수는text_splitter객체의create_documents메소드를 사용하여 주어진 텍스트(state_of_the_union)를 여러 문서로 분할하고, 그 결과를texts변수에 저장합니다. 이후texts의 첫 번째 문서를 출력합니다. 이 과정은 텍스트 데이터를 처리하고 분석하기 위한 초기 단계로 볼 수 있으며, 특히 큰 텍스트 데이터를 관리 가능한 크기의 단위로 나누는 데 유용합니다.
```
Copy# chain of density 논문의 일부 내용을 불러옵니다withopen(""data/chain-of-density.txt"",""r"")asf:
    text = f.read()[:500]
```
```
Copytext_splitter = CharacterTextSplitter(
    chunk_size=100, chunk_overlap=10, separator=""\n\n"")
text_splitter.split_text(text)
```
```
['Selecting the “right” amount of information to include in a summary is a difficult task. \nA good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']
```
```
Copytext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=""\n"")
text_splitter.split_text(text)
```
```
['Selecting the “right” amount of information to include in a summary is a difficult task.', 'A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']
```
```
Copytext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator="" "")
text_splitter.split_text(text)
```
```
['Selecting the “right” amount of information to include in a summary is a difficult task. \nA good', 'A good summary should be detailed and entity-centric without being overly dense and hard to follow.', 'to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with', 'with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial', 'an initial entity-sparse summary before iteratively incorporating missing salient entities without', 'without increasing the length. Summaries genera']
```
```
Copytext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator="" "")
text_splitter.split_text(text)
```
```
['Selecting the “right” amount of information to include in a summary is a difficult task. \nA good', 'summary should be detailed and entity-centric without being overly dense and hard to follow. To', 'better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to', 'as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary', 'before iteratively incorporating missing salient entities without increasing the length. Summaries', 'genera']
```
```
Copytext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator="" "")# text 파일을 청크로 나누어줍니다.text_splitter.split_text(text)# document를 청크로 나누어줍니다.split_docs = text_splitter.split_documents(docs)len(split_docs)
```
```
7
```
```
Copysplit_docs[0]
```
```
Document(page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding\n\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken\n\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.\n연관키워드: 토큰화, 자연어 처리, 구문 분석\n\nTokenizer\n\n정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n예시: ""I love programming.""이라는 문장을 [""I"", ""love"", ""programming"", "".""]으로 분할합니다.\n연관키워드: 토큰화, 자연어 처리, 구문 분석\n\nVectorStore\n\n정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\n연관키워드: 임베딩, 데이터베이스, 벡터화\n\nSQL\n\n정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n예시: SELECT * FROM users WHERE', metadata={'source': 'data/appendix-keywords.txt'})
```
```
Copy# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.loader = WebBaseLoader(
    web_paths=(""https://www.bbc.com/news/business-68092814"",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(""main"",
            attrs={""id"": [""main-content""]},
        )
    ),
)# splitter 를 정의합니다.text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator="" "")# 문서를 로드시 바로 분할까지 수행합니다.split_docs = loader.load_and_split(text_splitter=text_splitter)print(f""문서의 수:{len(docs)}"")
docs[0].page_content[:500]
```
```
문서의 수: 1
```
```
'Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 ""태양계 행성""이라고 검색하면, ""목성"", ""화성"" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding\n\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: ""사과""라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken\n\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n예시: 문장 ""나는 학교에 간다""를 ""나는"", ""학교에"", ""간다""로 분할합니다.\n연관키워드: 토큰화, 자연어'
```
이 텍스트 분할기는 일반 텍스트에 권장되는 텍스트 분할기입니다.
텍스트가 어떻게 분할 규칙: list ofseparators
청크 크기가 어떻게 측정되는가:lenof characters
```
Copy# langchain 패키지에서 RecursiveCharacterTextSplitter 클래스를 가져옵니다.fromlangchain.text_splitterimportRecursiveCharacterTextSplitter
```
RecursiveCharacterTextSplitter클래스는 텍스트를 재귀적으로 분할하는 기능을 제공합니다. 이 클래스는chunk_size로 분할할 청크의 크기,chunk_overlap으로 인접 청크 간의 겹침 크기,length_function으로 청크의 길이를 계산하는 함수, 그리고is_separator_regex로 구분자가 정규 표현식인지 여부를 지정하는 매개변수를 받습니다. 예시에서는 청크 크기를 100, 겹침 크기를 20으로 설정하고, 길이 계산 함수로len을 사용하며, 구분자가 정규 표현식이 아님을 나타내기 위해is_separator_regex를False로 설정합니다.
```
Copyrecursive_text_splitter = RecursiveCharacterTextSplitter(# 정말 작은 청크 크기를 설정합니다.chunk_size=100,
    chunk_overlap=10,
    length_function=len,
    is_separator_regex=False,
)
```
```
Copy# chain of density 논문의 일부 내용을 불러옵니다withopen(""data/chain-of-density.txt"",""r"")asf:
    text = f.read()[:500]
```
```
Copycharacter_text_splitter = CharacterTextSplitter(
    chunk_size=100, chunk_overlap=10, separator="" "")forsentincharacter_text_splitter.split_text(text):print(sent)print(""===""*20)
recursive_text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100, chunk_overlap=10)forsentinrecursive_text_splitter.split_text(text):print(sent)
```
- 지정한 separators 리스트를 순차적으로 시도하며 주어진 문서를 분할합니다.
- 청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 [""\n\n"", ""\n"", "" "", """"]입니다.
- 이는 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각인 것처럼 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.
지정한 separators 리스트를 순차적으로 시도하며 주어진 문서를 분할합니다.
청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 [""\n\n"", ""\n"", "" "", """"]입니다.
이는 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각인 것처럼 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.
```
Copy# recursive_text_splitter 에 기본 지정된 separators 를 확인합니다.recursive_text_splitter._separators
```
의미적 유사성을 기준으로 텍스트를 분할합니다.
출처:Greg Kamradt’s Notebook
높은 수준(high level)에서 문장으로 분할한 다음 3개 문장으로 그룹화한 다음 임베딩 공간에서 유사한 문장을 병합하는 방식입니다.
```
Copy# 최신 버전으로 업데이트합니다.# !pip install -U langchain langchain_experimental -q
```
```
Copyfromlangchain_experimental.text_splitterimportSemanticChunkerfromlangchain_openai.embeddingsimportOpenAIEmbeddings# SemanticChunker 를 생성합니다.semantic_text_splitter = SemanticChunker(
    OpenAIEmbeddings(), add_start_index=True)
```
```
Copy# chain of density 논문의 일부 내용을 불러옵니다withopen(""data/chain-of-density.txt"",""r"")asf:
    text = f.read()forsentinsemantic_text_splitter.split_text(text):print(sent)print(""===""*20)
```
```
Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between infor-mativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace. Introduction

Automatic summarization has come a long way in the past few years, largely due to a paradigm shift away from supervised fine-tuning on labeled datasets to zero-shot prompting with Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023). Without additional training, careful prompting can enable fine-grained control over summary characteristics, such as length (Goyal et al., 2022), topics (Bhaskar et al., 2023), and style (Pu and Demberg, 2023).
============================================================
An overlooked aspect is the information density of an summary. In theory, as a compression of another text, a summary should be denser–containing a higher concentration of information–than the source document. Given the high latency of LLM decoding (Kad-dour et al., 2023), covering more information in fewer words is a worthy goal, especially for real-time applications. Yet, how dense is an open question. A summary is uninformative if it contains insufficient detail. If it contains too much information, however, it can be-come difficult to follow without having to increase the overall length. Conveying more information subject to a fixed token budget requires a combination of abstrac-tion, compression, and fusion. There is a limit to how much space can be made for additional information before becoming illegible or even factually incorrect.
============================================================
```
## 3 단계: 임베딩
참고: https://python.langchain.com/docs/integrations/text_embedding
```
Copyfromlangchain_community.vectorstoresimportFAISSfromlangchain_openai.embeddingsimportOpenAIEmbeddings# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(
    documents=splits, embedding=OpenAIEmbeddings())
```
다음은OpenAI의 지원되는 Embedding 모델들의 목록입니다.
- 기본 값은text-embeding-ada-002입니다.
기본 값은text-embeding-ada-002입니다.
```
Copyvectorstore = FAISS.from_documents(
    documents=splits, embedding=OpenAIEmbeddings(model=""text-embedding-3-small"")
)
```
```
Warning: model not found. Using cl100k_base encoding.
```
```
Copyfromlangchain_community.embeddingsimportHuggingFaceBgeEmbeddings# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(
    documents=splits, embedding=HuggingFaceBgeEmbeddings()
)
```
```
Copy# !pip install fastembed -q
```
```
Copyfromlangchain_community.embeddings.fastembedimportFastEmbedEmbeddings

vectorstore = FAISS.from_documents(
    documents=splits, embedding=FastEmbedEmbeddings())
```
## 4단계: 벡터스토어 생성(Create Vectorstore)
```
Copyfromlangchain_community.vectorstoresimportFAISS# FAISS DB 적용vectorstore = FAISS.from_documents(
    documents=splits, embedding=OpenAIEmbeddings())
```
```
Copyfromlangchain_community.vectorstoresimportChroma# Chroma DB 적용vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
```
## 5단계: Retriever 생성
리트리버는 구조화되지 않은 쿼리가 주어지면 문서를 반환하는 인터페이스입니다.
리트리버는 문서를 저장할 필요 없이 문서를 반환(또는 검색)하기만 합니다.
- 공식 도큐먼트
공식 도큐먼트
생성된 VectorStore 에as_retriver()로 가져와서Retriever를 생성합니다.
- 기본값은 코사인 유사도인similarity가 적용되어 있습니다.
기본값은 코사인 유사도인similarity가 적용되어 있습니다.
```
Copyquery =""회사의 저출생 정책이 뭐야?""retriever = vectorstore.as_retriever(search_type=""similarity"")
search_result = retriever.get_relevant_documents(query)print(search_result)
```
```
[Document(page_content=""출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책"", metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}), Document(page_content='남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]', metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}), Document(page_content=""[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고"", metadata={'source': 'https://n.news.naver.com/article/437/0000378416'})]
```
similarity_score_threshold는 유사도 기반 검색에서score_threshold이상인 결과만 반환합니다.
```
Copyquery =""회사의 저출생 정책이 뭐야?""retriever = vectorstore.as_retriever(
    search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"":0.8}
)
search_result = retriever.get_relevant_documents(query)print(search_result)
```
```
[Document(page_content=""출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책"", metadata={'source': 'https://n.news.naver.com/article/437/0000378416'})]
```
maximum marginal search result를 사용하여 검색합니다.
```
Copyquery =""회사의 저출생 정책이 뭐야?""retriever = vectorstore.as_retriever(search_type=""mmr"", search_kwargs={""k"":2})
search_result = retriever.get_relevant_documents(query)print(search_result)
```
```
[Document(page_content=""출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책"", metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}), Document(page_content='남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]', metadata={'source': 'https://n.news.naver.com/article/437/0000378416'})]
```
```
Copyfromlangchain.retrievers.multi_queryimportMultiQueryRetrieverfromlangchain_openaiimportChatOpenAI

query =""회사의 저출생 정책이 뭐야?""llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo"")

retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(), llm=llm
)
```
```
Copy# Set logging for the queriesimportlogging

logging.basicConfig()
logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO)
```
```
Copyunique_docs = retriever_from_llm.get_relevant_documents(query=question)len(unique_docs)
```
```
INFO:langchain.retrievers.multi_query:Generated queries: ['1. 부영그룹의 출산 장려 정책은 어떻게 운영되고 있나요?', '2. 부영그룹은 출산을 장려하기 위해 어떤 정책을 시행하고 있나요?', '3. 출산율 증가를 위해 부영그룹이 채택한 정책은 어떤 것들이 있나요?']
```
```
2
```
```
Copyfromlangchain.retrieversimportBM25Retriever, EnsembleRetrieverfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddings
```
```
Copydoc_list = [""난 오늘 많이 먹어서 배가 정말 부르다"",""떠나는 저 배가 오늘 마지막 배인가요?"",""내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다."",
]# initialize the bm25 retriever and faiss retrieverbm25_retriever = BM25Retriever.from_texts(doc_list)
bm25_retriever.k =2faiss_vectorstore = FAISS.from_texts(doc_list, OpenAIEmbeddings())
faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={""k"":2})# initialize the ensemble retrieverensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5,0.5]
)
```
```
Copydefpretty_print(docs):fori, docinenumerate(docs):print(f""[{i+1}]{doc.page_content}"")
```
```
Copysample_query =""나 요즘 배에 정말 살이 많이 쪘어...""print(f""[Query]\n{sample_query}\n"")
relevant_docs = bm25_retriever.get_relevant_documents(sample_query)print(""[BM25 Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = faiss_retriever.get_relevant_documents(sample_query)print(""[FAISS Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)print(""[Ensemble Retriever]"")
pretty_print(relevant_docs)
```
```
[Query]
나 요즘 배에 정말 살이 많이 쪘어...

[BM25 Retriever]
[1] 난 오늘 많이 먹어서 배가 정말 부르다
[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
============================================================
[FAISS Retriever]
[1] 난 오늘 많이 먹어서 배가 정말 부르다
[2] 떠나는 저 배가 오늘 마지막 배인가요?
============================================================
[Ensemble Retriever]
[1] 난 오늘 많이 먹어서 배가 정말 부르다
[2] 떠나는 저 배가 오늘 마지막 배인가요?
[3] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
```
```
Copysample_query =""바다 위에 떠다니는 배들이 많다""print(f""[Query]\n{sample_query}\n"")
relevant_docs = bm25_retriever.get_relevant_documents(sample_query)print(""[BM25 Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = faiss_retriever.get_relevant_documents(sample_query)print(""[FAISS Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)print(""[Ensemble Retriever]"")
pretty_print(relevant_docs)
```
```
[Query]
바다 위에 떠다니는 배들이 많다

[BM25 Retriever]
[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
[2] 떠나는 저 배가 오늘 마지막 배인가요?
============================================================
[FAISS Retriever]
[1] 난 오늘 많이 먹어서 배가 정말 부르다
[2] 떠나는 저 배가 오늘 마지막 배인가요?
============================================================
[Ensemble Retriever]
[1] 떠나는 저 배가 오늘 마지막 배인가요?
[2] 난 오늘 많이 먹어서 배가 정말 부르다
[3] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
```
```
Copysample_query =""ships""print(f""[Query]\n{sample_query}\n"")
relevant_docs = bm25_retriever.get_relevant_documents(sample_query)print(""[BM25 Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = faiss_retriever.get_relevant_documents(sample_query)print(""[FAISS Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)print(""[Ensemble Retriever]"")
pretty_print(relevant_docs)
```
```
[Query]
ships

[BM25 Retriever]
[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
[2] 떠나는 저 배가 오늘 마지막 배인가요?
============================================================
[FAISS Retriever]
[1] 떠나는 저 배가 오늘 마지막 배인가요?
[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
============================================================
[Ensemble Retriever]
[1] 떠나는 저 배가 오늘 마지막 배인가요?
[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
```
```
Copysample_query =""pear""print(f""[Query]\n{sample_query}\n"")
relevant_docs = bm25_retriever.get_relevant_documents(sample_query)print(""[BM25 Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = faiss_retriever.get_relevant_documents(sample_query)print(""[FAISS Retriever]"")
pretty_print(relevant_docs)print(""===""*20)
relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)print(""[Ensemble Retriever]"")
pretty_print(relevant_docs)
```
```
[Query]
pear

[BM25 Retriever]
[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
[2] 떠나는 저 배가 오늘 마지막 배인가요?
============================================================
[FAISS Retriever]
[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
[2] 떠나는 저 배가 오늘 마지막 배인가요?
============================================================
[Ensemble Retriever]
[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.
[2] 떠나는 저 배가 오늘 마지막 배인가요?
```
## 6단계: 프롬프트 생성(Create Prompt)
프롬프트 엔지니어링은 주어진 데이터(context)를 토대로 우리가 원하는 결과를 도출할 때 중요한 역할을 합니다.
[TIP1]
만약,retriever에서 도출한 결과에서 중요한 정보가 누락된다면retriever의 로직을 수정해야 합니다.
만약,retriever에서 도출한 결과가 많은 정보를 포함하고 있지만,llm이 그 중에서 중요한 정보를 찾지 못한거나 원하는 형태로 출력하지 않는다면 프롬프트를 수정해야 합니다.
[TIP2]
LangSmith 의hub에는 검증된 프롬프트가 많이 업로드 되어 있습니다.
검증된 프롬프트를 활용하거나 약간 수정한다면 비용과 시간을 절약할 수 있습니다.
검증된 프롬프트를 활용하거나 약간 수정한다면 비용과 시간을 절약할 수 있습니다.
https://smith.langchain.com/hub/search?q=rag
https://smith.langchain.com/hub/search?q=rag
```
Copyfromlangchainimporthub
```
```
Copyprompt = hub.pull(""rlm/rag-prompt"")
prompt
```
```
ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:""))])
```
## 7단계: 언어모델 생성(Create LLM)
OpenAI 모델 중 하나를 선택합니다.
- gpt-3.5-turbo: OpenAI의 GPT-3.5-turbo 모델
- gpt-4-turbo-preview: OpenAI의 GPT-4-turbo-preview 모델
gpt-3.5-turbo: OpenAI의 GPT-3.5-turbo 모델
gpt-4-turbo-preview: OpenAI의 GPT-4-turbo-preview 모델
자세한 비용 체계는OpenAI API 모델 리스트 / 요금표에서 확인할 수 있습니다.
```
Copyfromlangchain_openaiimportChatOpenAI

model = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo"")
```
다음의 방식으로 토큰 사용량을 확인할 수 있습니다.
```
Copyfromlangchain.callbacksimportget_openai_callbackwithget_openai_callback()ascb:
    result = model.invoke(""대한민국의 수도는 어디인가요?"")print(cb)
```
```
Tokens Used: 39
    Prompt Tokens: 24
    Completion Tokens: 15
Successful Requests: 1
Total Cost (USD): $6.6e-05
```
HuggingFace 에 업로드 되어 있는 오픈소스 모델을 손쉽게 다운로드 받아 사용할 수 있습니다.
아래의 리더보드에서 날마다 성능을 개선하는 오픈소스 리더보드를 확인할 수 있습니다.
- HuggingFace LLM Leaderboard
HuggingFace LLM Leaderboard
```
Copy# HuggingFaceHub 객체 생성fromlangchain.llmsimportHuggingFaceHub

repo_id =""google/flan-t5-xxl""t5_model = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={""temperature"":0.1,""max_length"":512}
)
```
```
Copyt5_model.invoke(""Where is the capital of South Korea?"")
```
```
'seoul'
```
## RAG 템플릿 실험
```
Copy# 단계 1: 문서 로드(Load Documents)# 문서를 로드하고, 청크로 나누고, 인덱싱합니다.fromlangchain.document_loadersimportPyPDFLoader# PDF 파일 로드. 파일의 경로 입력file_path =""data/SPRI_AI_Brief_2023년12월호_F.pdf""loader = PyPDFLoader(file_path=file_path)# 단계 2: 문서 분할(Split Documents)text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)

split_docs = loader.load_and_split(text_splitter=text_splitter)# 단계 3, 4: 임베딩 & 벡터스토어 생성(Create Vectorstore)# 벡터스토어를 생성합니다.vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())# 단계 5: 리트리버 생성(Create Retriever)# 사용자의 질문(query) 에 부합하는 문서를 검색합니다.# 유사도 높은 K 개의 문서를 검색합니다.k =3# (Sparse) bm25 retriever and (Dense) faiss retriever 를 초기화 합니다.bm25_retriever = BM25Retriever.from_documents(split_docs)
bm25_retriever.k = k

faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())
faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={""k"": k})# initialize the ensemble retrieverensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5,0.5]
)# 단계 6: 프롬프트 생성(Create Prompt)# 프롬프트를 생성합니다.prompt = hub.pull(""rlm/rag-prompt"")# 단계 7: 언어모델 생성(Create LLM)# 모델(LLM) 을 생성합니다.llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0)defformat_docs(docs):# 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.return""\n\n"".join(doc.page_contentfordocindocs)# 단계 8: 체인 생성(Create Chain)rag_chain = (
    {""context"": ensemble_retriever | format_docs,""question"": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)# 결과 출력print(f""PDF Path:{file_path}"")print(f""문서의 수:{len(docs)}"")print(""===""*20)print(f""[HUMAN]\n{question}\n"")print(f""[AI]\n{response}"")
```
```
PDF Path: data/SPRI_AI_Brief_2023년12월호_F.pdf
문서의 수: 23
============================================================
[HUMAN]
삼성 가우스에 대해 설명해주세요

[AI]
삼성 가우스는 삼성전자가 개발한 생성 AI 모델로, 언어, 코드, 이미지의 3개 모델로 구성되어 있습니다. 이 모델은 온디바이스에서 작동 가능하며, 외부로 사용자 정보가 유출되지 않는 안전한 데이터를 통해 학습되었습니다. 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획을 가지고 있습니다.
```
문서: data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 10)
- LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r
LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r
```
Copy# 단계 8: 체인 실행(Run Chain)# 문서에 대한 질의를 입력하고, 답변을 출력합니다.question =""삼성 가우스에 대해 설명해주세요""response = rag_chain.invoke(question)print(response)
```
```
삼성 가우스는 삼성전자가 개발한 생성 AI 모델로, 언어, 코드, 이미지의 3개 모델로 구성되어 있습니다. 이 모델은 온디바이스에서 작동 가능하며, 외부로 사용자 정보가 유출되지 않는 안전한 데이터를 통해 학습되었습니다. 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획을 가지고 있습니다.
```
문서: data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 12)
- LangSmith: https://smith.langchain.com/public/2b2913c9-6b9c-4a19-bb16-dc2256e2fdbf/r
LangSmith: https://smith.langchain.com/public/2b2913c9-6b9c-4a19-bb16-dc2256e2fdbf/r
```
Copy# 단계 8: 체인 실행(Run Chain)# 문서에 대한 질의를 입력하고, 답변을 출력합니다.question =""미래의 AI 소프트웨어 매출 전망은 어떻게 되나요?""response = rag_chain.invoke(question)print(response)
```
```
2027년까지 AI 소프트웨어 매출은 2,510억 달러로 전망되며, AI 애플리케이션은 2027년까지 21.1%의 연평균 성장률을 기록할 것으로 예상됩니다. AI 플랫폼은 2027년까지 35.8%의 연평균 성장률이 예상되며, AI SIS는 5년간 32.6%의 연평균 성장률을 기록할 것으로 예상됩니다.
```
문서: data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 14)
- LangSmith: https://smith.langchain.com/public/17ef6df2-b012-4f8e-b0a8-62894d82c097/r
LangSmith: https://smith.langchain.com/public/17ef6df2-b012-4f8e-b0a8-62894d82c097/r
```
Copy# 단계 8: 체인 실행(Run Chain)# 문서에 대한 질의를 입력하고, 답변을 출력합니다.question =""YouTube 가 2024년에 의무화 한 것은 무엇인가요?""response = rag_chain.invoke(question)print(response)
```
```
유튜브가 2024년부터 AI 생성 콘텐츠에 AI 라벨 표시를 의무화했습니다. 이 규칙을 준수하지 않는 콘텐츠는 삭제되고 크리에이터에 대한 수익 배분도 중단될 수 있습니다. 유튜브는 AI 생성 콘텐츠가 개인을 모방한 경우 개인정보 침해 신고 절차에 따라 콘텐츠 삭제 요청도 받을 계획입니다.
```"
03. RAPTOR: 긴 문맥 요약(Long Context Summary),https://wikidocs.net/234017,"# 03. RAPTOR: 긴 문맥 요약(Long Context Summary)

## 설치
```
Copypip install -qU langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic
```
RAPTOR논문은 문서의 색인 생성 및 검색에 대한 흥미로운 접근 방식을 제시합니다.
테디노트 논문 요약글(노션)
- leafs는 시작 문서 집합입니다.
- leafs는 임베딩되어 클러스터링됩니다.
- 그런 다음 클러스터는 유사한 문서들 간의 정보를 더 높은 수준(더 추상적인)으로 요약합니다.
leafs는 시작 문서 집합입니다.
leafs는 임베딩되어 클러스터링됩니다.
그런 다음 클러스터는 유사한 문서들 간의 정보를 더 높은 수준(더 추상적인)으로 요약합니다.
이 과정은 재귀적으로 수행되어, 원본 문서(leafs)에서 더 추상적인 요약으로 이어지는 ""트리""를 형성합니다.
이를 다양한 규모에서 적용할 수 있습니다;leafs는 다음과 같을 수 있습니다:
- 단일 문서에서의 텍스트 청크(논문에서 보여준 것처럼)
- 전체 문서(아래에서 보여주는 것처럼)
단일 문서에서의 텍스트 청크(논문에서 보여준 것처럼)
전체 문서(아래에서 보여주는 것처럼)
더 긴 컨텍스트의 LLMs를 사용하면, 전체 문서에 대해 이 작업을 수행할 수 있습니다.
LangChain의 LCEL 문서에 이를 적용해 봅시다.
이 경우, 각doc은 LCEL 문서의 고유한 웹 페이지입니다.
콘텍스트는 2,000 토큰 미만에서 10,000 토큰 이상까지 다양합니다.
웹 문서에서 텍스트 데이터를 추출하고, 텍스트의 토큰 수를 계산하여 히스토그램으로 시각화하는 과정을 설명합니다.
- tiktoken라이브러리를 사용하여 주어진 인코딩 이름에 따라 문자열의 토큰 수를 계산합니다.
- RecursiveUrlLoader클래스를 사용하여 지정된 URL에서 웹 문서를 재귀적으로 로드합니다. 이 과정에서BeautifulSoup를 활용하여 HTML 문서에서 텍스트를 추출합니다.
- 여러 URL에서 문서를 로드하여 모든 텍스트 데이터를 하나의 리스트에 모읍니다.
- 각 문서 텍스트에 대해num_tokens_from_string함수를 호출하여 토큰 수를 계산하고, 이를 리스트에 저장합니다.
- matplotlib를 사용하여 계산된 토큰 수의 분포를 히스토그램으로 시각화합니다. 히스토그램은 토큰 수를 x축에, 해당 토큰 수를 가진 문서의 빈도수를 y축에 나타냅니다.
- 히스토그램은 데이터의 분포를 이해하는 데 도움을 주며, 특히 텍스트 데이터의 길이 분포를 시각적으로 파악할 수 있습니다.
tiktoken라이브러리를 사용하여 주어진 인코딩 이름에 따라 문자열의 토큰 수를 계산합니다.
RecursiveUrlLoader클래스를 사용하여 지정된 URL에서 웹 문서를 재귀적으로 로드합니다. 이 과정에서BeautifulSoup를 활용하여 HTML 문서에서 텍스트를 추출합니다.
여러 URL에서 문서를 로드하여 모든 텍스트 데이터를 하나의 리스트에 모읍니다.
각 문서 텍스트에 대해num_tokens_from_string함수를 호출하여 토큰 수를 계산하고, 이를 리스트에 저장합니다.
matplotlib를 사용하여 계산된 토큰 수의 분포를 히스토그램으로 시각화합니다. 히스토그램은 토큰 수를 x축에, 해당 토큰 수를 가진 문서의 빈도수를 y축에 나타냅니다.
히스토그램은 데이터의 분포를 이해하는 데 도움을 주며, 특히 텍스트 데이터의 길이 분포를 시각적으로 파악할 수 있습니다.
```
Copyfromlangchain_community.document_loaders.recursive_url_loaderimportRecursiveUrlLoaderfrombs4importBeautifulSoupasSoupimporttiktokenimportmatplotlib.pyplotaspltdefnum_tokens_from_string(string:str, encoding_name:str) ->int:# 주어진 문자열에서 토큰의 개수를 반환합니다.encoding = tiktoken.get_encoding(encoding_name)
    num_tokens =len(encoding.encode(string))returnnum_tokens# LCEL 문서 로드url =""https://python.langchain.com/docs/expression_language/""loader = RecursiveUrlLoader(
    url=url, max_depth=20, extractor=lambdax: Soup(x,""html.parser"").text
)
docs = loader.load()# PydanticOutputParser를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)url =""https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start""loader = RecursiveUrlLoader(
    url=url, max_depth=1, extractor=lambdax: Soup(x,""html.parser"").text
)
docs_pydantic = loader.load()# Self Query를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)url =""https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/""loader = RecursiveUrlLoader(
    url=url, max_depth=1, extractor=lambdax: Soup(x,""html.parser"").text
)
docs_sq = loader.load()# 문서 텍스트docs.extend([*docs_pydantic, *docs_sq])
docs_texts = [d.page_contentfordindocs]# 각 문서에 대한 토큰 수 계산counts = [num_tokens_from_string(d,""cl100k_base"")fordindocs_texts]# 토큰 수의 히스토그램을 그립니다.plt.figure(figsize=(10,6))
plt.hist(counts, bins=30, color=""blue"", edgecolor=""black"", alpha=0.7)
plt.title(""Token Counts in LCEL Documents"")
plt.xlabel(""Token Count"")
plt.ylabel(""Frequency"")
plt.grid(axis=""y"", alpha=0.75)# 히스토그램을 표시합니다.plt.show
```
```

```
문서 텍스트를 정렬하고 연결하여 토큰 수를 계산하는 과정을 설명합니다.
- 문서(docs)를 메타데이터의 ""source"" 키를 기준으로 정렬합니다.
- 정렬된 문서 리스트를 역순으로 뒤집습니다.
- 역순으로 된 문서의 내용을 특정 구분자(""\n\n\n --- \n\n\n"")를 사용하여 연결합니다.
- 연결된 내용의 토큰 수를num_tokens_from_string함수를 사용하여 계산하고, 이를 출력합니다. 이때,""cl100k_base""모델을 사용합니다.
문서(docs)를 메타데이터의 ""source"" 키를 기준으로 정렬합니다.
정렬된 문서 리스트를 역순으로 뒤집습니다.
역순으로 된 문서의 내용을 특정 구분자(""\n\n\n --- \n\n\n"")를 사용하여 연결합니다.
연결된 내용의 토큰 수를num_tokens_from_string함수를 사용하여 계산하고, 이를 출력합니다. 이때,""cl100k_base""모델을 사용합니다.
```
Copy# 문서 텍스트를 연결합니다.# 문서를 출처 메타데이터 기준으로 정렬합니다.d_sorted =sorted(docs, key=lambdax: x.metadata[""source""])
d_reversed =list(reversed(d_sorted))# 정렬된 문서를 역순으로 배열합니다.concatenated_content =""\n\n\n --- \n\n\n"".join(
    [# 역순으로 배열된 문서의 내용을 연결합니다.doc.page_contentfordocind_reversed
    ]
)print(""Num tokens in all context: %s""# 모든 문맥에서의 토큰 수를 출력합니다.% num_tokens_from_string(concatenated_content,""cl100k_base"")
)
```

```
Num tokens in all context: 69074
```

RecursiveCharacterTextSplitter를 사용하여 텍스트를 분할하는 과정을 설명합니다.
- chunk_size_tok변수를 설정하여, 각 텍스트 청크의 크기를 2000 토큰으로 지정합니다.
- RecursiveCharacterTextSplitter의from_tiktoken_encoder메소드를 사용하여 텍스트 분할기를 초기화합니다. 여기서 청크 크기(chunk_size)와 청크 간 겹침(chunk_overlap)을 0으로 설정합니다.
- 초기화된 텍스트 분할기의split_text메소드를 호출하여,concatenated_content라는 변수에 저장된 연결된 텍스트를 분할합니다. 분할 결과는texts_split변수에 저장됩니다.
chunk_size_tok변수를 설정하여, 각 텍스트 청크의 크기를 2000 토큰으로 지정합니다.
RecursiveCharacterTextSplitter의from_tiktoken_encoder메소드를 사용하여 텍스트 분할기를 초기화합니다. 여기서 청크 크기(chunk_size)와 청크 간 겹침(chunk_overlap)을 0으로 설정합니다.
초기화된 텍스트 분할기의split_text메소드를 호출하여,concatenated_content라는 변수에 저장된 연결된 텍스트를 분할합니다. 분할 결과는texts_split변수에 저장됩니다.
```
Copy# 텍스트 분할을 위한 코드fromlangchain_text_splittersimportRecursiveCharacterTextSplitter

chunk_size_tok =2000# 토큰의 청크 크기를 설정합니다.# 재귀적 문자 텍스트 분할기를 초기화합니다. 토큰 인코더를 사용하여 청크 크기와 중복을 설정합니다.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=chunk_size_tok, chunk_overlap=0)
texts_split = text_splitter.split_text(
    concatenated_content
)# 주어진 텍스트를 분할합니다.
```
## 모델
다양한 모델을 테스트할 수 있으며, 새로운Claude3계열도 포함됩니다.
관련 API 키를 설정하는 것을 잊지 마세요.
- OPENAI_API_KEY, Anthropic 을 사용하는 경우ANTHROPIC_API_KEY
OPENAI_API_KEY, Anthropic 을 사용하는 경우ANTHROPIC_API_KEY
ChatOpenAI혹은ChatAnthropic+OpenAIEmbeddings를 사용하여 챗봇 모델을 구현합니다.
- OpenAIEmbeddings를 인스턴스화하여 OpenAI의 임베딩 기능을 초기화합니다.
- ChatOpenAI혹은ChatAnthropic을 사용하여temperature를 0으로 설정하고, 챗봇 모델을 초기화합니다.
OpenAIEmbeddings를 인스턴스화하여 OpenAI의 임베딩 기능을 초기화합니다.
ChatOpenAI혹은ChatAnthropic을 사용하여temperature를 0으로 설정하고, 챗봇 모델을 초기화합니다.
```
Copyfromdotenvimportload_dotenv

load_dotenv()
```
```
True
```
Cache Embedding 을 사용합니다.
```
Copyfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain.embeddingsimportCacheBackedEmbeddingsfromlangchain.storageimportLocalFileStore

store = LocalFileStore(""./cache/"")# embeddings 인스턴스를 생성합니다.embd = OpenAIEmbeddings(model=""text-embedding-3-small"", disallowed_special=())

cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embd, store, namespace=embd.model
)
```
모델을 초기화 합니다.
```
Copyfromlangchain_anthropicimportChatAnthropicfromlangchain_openaiimportOpenAIEmbeddings, ChatOpenAIfromlangchain.callbacks.baseimportBaseCallbackHandlerclassStreamCallback(BaseCallbackHandler):defon_llm_new_token(self, token:str, **kwargs):print(token, end="""", flush=True)# ChatOpenAI 모델을 초기화합니다. 모델은 ""gpt-4-turbo-preview""를 사용합니다.model = ChatOpenAI(
    model=""gpt-4-turbo-preview"",
    temperature=0,
    streaming=True,
    callbacks=[StreamCallback()],
)# ChatAnthropic 모델을 초기화합니다. 온도는 0으로 설정하고, 모델은 ""claude-3-opus-20240229""를 사용합니다.# model = ChatAnthropic(temperature=0, model=""claude-3-opus-20240229"")
```
트리 구축에서의 클러스터링 접근 방식에는 몇 가지 흥미로운 아이디어가 포함되어 있습니다.
GMM (가우시안 혼합 모델)
- 다양한 클러스터에 걸쳐 데이터 포인트의 분포를 모델링합니다.
- 모델의 베이지안 정보 기준(BIC)을 평가하여 최적의 클러스터 수를 결정합니다.
다양한 클러스터에 걸쳐 데이터 포인트의 분포를 모델링합니다.
모델의 베이지안 정보 기준(BIC)을 평가하여 최적의 클러스터 수를 결정합니다.
UMAP (Uniform Manifold Approximation and Projection)
- 클러스터링을 지원합니다.
- 고차원 데이터의 차원을 축소합니다.
- UMAP은 데이터 포인트의 유사성에 기반하여 자연스러운 그룹화를 강조하는 데 도움을 줍니다.
클러스터링을 지원합니다.
고차원 데이터의 차원을 축소합니다.
UMAP은 데이터 포인트의 유사성에 기반하여 자연스러운 그룹화를 강조하는 데 도움을 줍니다.
지역 및 전역 클러스터링
- 다양한 규모에서 데이터를 분석하는 데 사용됩니다.
- 데이터 내의 세밀한 패턴과 더 넓은 패턴 모두를 효과적으로 포착합니다.
다양한 규모에서 데이터를 분석하는 데 사용됩니다.
데이터 내의 세밀한 패턴과 더 넓은 패턴 모두를 효과적으로 포착합니다.
임계값 설정
- GMM의 맥락에서 클러스터 멤버십을 결정하기 위해 적용됩니다.
- 확률 분포를 기반으로 합니다(데이터 포인트를 ≥ 1 클러스터에 할당).
GMM의 맥락에서 클러스터 멤버십을 결정하기 위해 적용됩니다.
확률 분포를 기반으로 합니다(데이터 포인트를 ≥ 1 클러스터에 할당).
GMM 및 임계값 설정에 대한 코드는 아래 두 출처에서 언급된 Sarthi et al의 것입니다:
- 원본 저장소
- 소소한 조정
원본 저장소
소소한 조정
두 저자 모두에게 전적인 공로를 인정합니다.
global_cluster_embeddings함수는 임베딩의 글로벌 차원 축소를 수행하기 위해 UMAP을 사용합니다.
- 입력된 임베딩(embeddings)을 UMAP을 사용하여 지정된 차원(dim)으로 차원 축소합니다.
- n_neighbors는 각 포인트를 고려할 이웃의 수를 지정하며, 제공되지 않을 경우 임베딩 수의 제곱근으로 기본 설정됩니다.
- metric은 UMAP에 사용될 거리 측정 기준을 지정합니다.
- 결과로, 지정된 차원으로 축소된 임베딩이 numpy 배열로 반환됩니다.
입력된 임베딩(embeddings)을 UMAP을 사용하여 지정된 차원(dim)으로 차원 축소합니다.
n_neighbors는 각 포인트를 고려할 이웃의 수를 지정하며, 제공되지 않을 경우 임베딩 수의 제곱근으로 기본 설정됩니다.
metric은 UMAP에 사용될 거리 측정 기준을 지정합니다.
결과로, 지정된 차원으로 축소된 임베딩이 numpy 배열로 반환됩니다.
```
CopyfromtypingimportDict,List,Optional,Tupleimportnumpyasnpimportpandasaspdimportumapfromlangchain.promptsimportChatPromptTemplatefromlangchain_core.output_parsersimportStrOutputParserfromsklearn.mixtureimportGaussianMixture

RANDOM_SEED =42# 재현성을 위한 고정된 시드 값### --- 위의 인용된 코드에서 주석과 문서화를 추가함 --- ###defglobal_cluster_embeddings(embeddings: np.ndarray,
    dim:int,
    n_neighbors:Optional[int] =None,
    metric:str=""cosine"",) -> np.ndarray:""""""
    UMAP을 사용하여 임베딩의 전역 차원 축소를 수행합니다.

    매개변수:
    - embeddings: numpy 배열로 된 입력 임베딩.
    - dim: 축소된 공간의 목표 차원.
    - n_neighbors: 선택 사항; 각 점을 고려할 이웃의 수.
                   제공되지 않으면 임베딩 수의 제곱근으로 기본 설정됩니다.
    - metric: UMAP에 사용할 거리 측정 기준.

    반환값:
    - 지정된 차원으로 축소된 임베딩의 numpy 배열.
    """"""ifn_neighborsisNone:
        n_neighbors =int((len(embeddings) -1) **0.5)returnumap.UMAP(
        n_neighbors=n_neighbors, n_components=dim, metric=metric
    ).fit_transform(embeddings)
```
임베딩 데이터에 대해 지역 차원 축소를 수행하는 함수local_cluster_embeddings를 구현합니다.
- 입력된 임베딩(embeddings)을 UMAP을 사용하여 지정된 차원(dim)으로 차원 축소합니다.
- 차원 축소 과정에서 각 점에 대해 고려할 이웃의 수(num_neighbors)와 거리 측정 메트릭(metric)을 파라미터로 사용합니다.
- 최종적으로 차원이 축소된 임베딩을numpy배열로 반환합니다.
입력된 임베딩(embeddings)을 UMAP을 사용하여 지정된 차원(dim)으로 차원 축소합니다.
차원 축소 과정에서 각 점에 대해 고려할 이웃의 수(num_neighbors)와 거리 측정 메트릭(metric)을 파라미터로 사용합니다.
최종적으로 차원이 축소된 임베딩을numpy배열로 반환합니다.
```
Copydeflocal_cluster_embeddings(embeddings: np.ndarray, dim:int, num_neighbors:int=10, metric:str=""cosine"") -> np.ndarray:""""""
    임베딩에 대해 지역 차원 축소를 수행합니다. 이는 일반적으로 전역 클러스터링 이후에 사용됩니다.

    매개변수:
    - embeddings: numpy 배열로서의 입력 임베딩.
    - dim: 축소된 공간의 목표 차원 수.
    - num_neighbors: 각 점에 대해 고려할 이웃의 수.
    - metric: UMAP에 사용할 거리 측정 기준.

    반환값:
    - 지정된 차원으로 축소된 임베딩의 numpy 배열.
    """"""returnumap.UMAP(
        n_neighbors=num_neighbors, n_components=dim, metric=metric
    ).fit_transform(embeddings)
```
get_optimal_clusters함수는 주어진 임베딩 데이터를 기반으로 최적의 클러스터 수를 결정하는 데 사용됩니다. 이 과정은 가우시안 혼합 모델(Gaussian Mixture Model)을 사용하여 베이지안 정보 기준(Bayesian Information Criterion, BIC)을 계산함으로써 수행됩니다.
- 입력 임베딩(embeddings)은 numpy 배열로 제공됩니다.
- 최대 클러스터 수(max_clusters)는 고려할 클러스터의 최대 수를 지정합니다. 기본값은 50입니다.
- 재현성을 위한 난수 상태(random_state)는 고정된 값을 사용합니다.
- 함수는 입력 임베딩에 대해 여러 클러스터 수를 시도하며 각각에 대한 BIC 값을 계산합니다.
- 최소 BIC 값을 가지는 클러스터 수를 최적의 클러스터 수로 결정하고 반환합니다.
입력 임베딩(embeddings)은 numpy 배열로 제공됩니다.
최대 클러스터 수(max_clusters)는 고려할 클러스터의 최대 수를 지정합니다. 기본값은 50입니다.
재현성을 위한 난수 상태(random_state)는 고정된 값을 사용합니다.
함수는 입력 임베딩에 대해 여러 클러스터 수를 시도하며 각각에 대한 BIC 값을 계산합니다.
최소 BIC 값을 가지는 클러스터 수를 최적의 클러스터 수로 결정하고 반환합니다.
이 함수는 클러스터링 문제에서 데이터를 가장 잘 설명하는 클러스터 수를 자동으로 찾는 데 유용하게 사용될 수 있습니다.
```
Copydefget_optimal_clusters(embeddings: np.ndarray, max_clusters:int=50, random_state:int= RANDOM_SEED) ->int:""""""
    가우시안 혼합 모델(Gaussian Mixture Model)을 사용하여 베이지안 정보 기준(BIC)을 통해 최적의 클러스터 수를 결정합니다.

    매개변수:
    - embeddings: numpy 배열로서의 입력 임베딩.
    - max_clusters: 고려할 최대 클러스터 수.
    - random_state: 재현성을 위한 시드.

    반환값:
    - 발견된 최적의 클러스터 수를 나타내는 정수.
    """"""max_clusters =min(
        max_clusters,len(embeddings)
    )# 최대 클러스터 수와 임베딩의 길이 중 작은 값을 최대 클러스터 수로 설정n_clusters = np.arange(1, max_clusters)# 1부터 최대 클러스터 수까지의 범위를 생성bics = []# BIC 점수를 저장할 리스트forninn_clusters:# 각 클러스터 수에 대해 반복gm = GaussianMixture(
            n_components=n, random_state=random_state
        )# 가우시안 혼합 모델 초기화gm.fit(embeddings)# 임베딩에 대해 모델 학습bics.append(gm.bic(embeddings))# 학습된 모델의 BIC 점수를 리스트에 추가returnn_clusters[np.argmin(bics)]# BIC 점수가 가장 낮은 클러스터 수를 반환
```
GMM_cluster함수는 임베딩을 가우시안 혼합 모델(Gaussian Mixture Model, GMM)을 사용하여 클러스터링합니다. 이 과정은 확률 임계값을 기반으로 합니다.
- 입력된 임베딩(embeddings)은 numpy 배열로 제공됩니다.
- threshold는 임베딩을 특정 클러스터에 할당하기 위한 확률 임계값입니다.
- random_state는 결과의 재현성을 위한 시드 값입니다.
- 최적의 클러스터 수를 결정하기 위해get_optimal_clusters함수를 호출합니다.
- 결정된 클러스터 수를 바탕으로 가우시안 혼합 모델을 초기화하고, 입력된 임베딩에 대해 학습을 수행합니다.
- 각 임베딩에 대한 클러스터 할당 확률을 계산하고, 이 확률이 주어진 임계값을 초과하는 경우 해당 임베딩을 클러스터에 할당합니다.
- 함수는 최종적으로 임베딩의 클러스터 레이블과 결정된 클러스터 수를 튜플로 반환합니다.
입력된 임베딩(embeddings)은 numpy 배열로 제공됩니다.
threshold는 임베딩을 특정 클러스터에 할당하기 위한 확률 임계값입니다.
random_state는 결과의 재현성을 위한 시드 값입니다.
최적의 클러스터 수를 결정하기 위해get_optimal_clusters함수를 호출합니다.
결정된 클러스터 수를 바탕으로 가우시안 혼합 모델을 초기화하고, 입력된 임베딩에 대해 학습을 수행합니다.
각 임베딩에 대한 클러스터 할당 확률을 계산하고, 이 확률이 주어진 임계값을 초과하는 경우 해당 임베딩을 클러스터에 할당합니다.
함수는 최종적으로 임베딩의 클러스터 레이블과 결정된 클러스터 수를 튜플로 반환합니다.
```
CopydefGMM_cluster(embeddings: np.ndarray, threshold:float, random_state:int=0):""""""
    확률 임계값을 기반으로 가우시안 혼합 모델(GMM)을 사용하여 임베딩을 클러스터링합니다.

    매개변수:
    - embeddings: numpy 배열로서의 입력 임베딩.
    - threshold: 임베딩을 클러스터에 할당하기 위한 확률 임계값.
    - random_state: 재현성을 위한 시드.

    반환값:
    - 클러스터 레이블과 결정된 클러스터 수를 포함하는 튜플.
    """"""n_clusters = get_optimal_clusters(embeddings)# 최적의 클러스터 수를 구합니다.# 가우시안 혼합 모델을 초기화합니다.gm = GaussianMixture(n_components=n_clusters, random_state=random_state)
    gm.fit(embeddings)# 임베딩에 대해 모델을 학습합니다.probs = gm.predict_proba(
        embeddings
    )# 임베딩이 각 클러스터에 속할 확률을 예측합니다.# 임계값을 초과하는 확률을 가진 클러스터를 레이블로 선택합니다.labels = [np.where(prob > threshold)[0]forprobinprobs]returnlabels, n_clusters# 레이블과 클러스터 수를 반환합니다.
```
perform_clustering함수는 임베딩에 대해 차원 축소, 가우시안 혼합 모델을 사용한 글로벌 클러스터링, 그리고 각 글로벌 클러스터 내에서의 로컬 클러스터링을 수행하여 클러스터링 결과를 반환합니다.
- 입력된 임베딩(embeddings)에 대해 차원 축소를 수행합니다. 이는 UMAP을 사용하여 지정된 차원(dim)으로 임베딩의 차원을 축소하는 과정을 포함합니다.
- 차원이 축소된 임베딩에 대해 가우시안 혼합 모델(GMM)을 사용하여 글로벌 클러스터링을 수행합니다. 클러스터 할당은 주어진 확률 임계값(threshold)을 기준으로 결정됩니다.
- 각 글로벌 클러스터 내에서 추가적인 로컬 클러스터링을 수행합니다. 이 과정은 글로벌 클러스터링 결과를 바탕으로 각 글로벌 클러스터에 속한 임베딩들만을 대상으로 다시 차원 축소 및 GMM 클러스터링을 진행합니다.
- 최종적으로, 모든 임베딩에 대해 글로벌 및 로컬 클러스터 ID를 할당하여, 각 임베딩이 속한 클러스터 ID를 담은 리스트를 반환합니다. 이 리스트는 임베딩의 순서에 따라 각 임베딩에 대한 클러스터 ID 배열을 포함합니다.
입력된 임베딩(embeddings)에 대해 차원 축소를 수행합니다. 이는 UMAP을 사용하여 지정된 차원(dim)으로 임베딩의 차원을 축소하는 과정을 포함합니다.
차원이 축소된 임베딩에 대해 가우시안 혼합 모델(GMM)을 사용하여 글로벌 클러스터링을 수행합니다. 클러스터 할당은 주어진 확률 임계값(threshold)을 기준으로 결정됩니다.
각 글로벌 클러스터 내에서 추가적인 로컬 클러스터링을 수행합니다. 이 과정은 글로벌 클러스터링 결과를 바탕으로 각 글로벌 클러스터에 속한 임베딩들만을 대상으로 다시 차원 축소 및 GMM 클러스터링을 진행합니다.
최종적으로, 모든 임베딩에 대해 글로벌 및 로컬 클러스터 ID를 할당하여, 각 임베딩이 속한 클러스터 ID를 담은 리스트를 반환합니다. 이 리스트는 임베딩의 순서에 따라 각 임베딩에 대한 클러스터 ID 배열을 포함합니다.
이 함수는 고차원 데이터의 클러스터링을 위해 글로벌 및 로컬 차원에서의 클러스터링을 결합한 접근 방식을 제공합니다. 이를 통해 더 세분화된 클러스터링 결과를 얻을 수 있으며, 복잡한 데이터 구조를 보다 효과적으로 분석할 수 있습니다.
```
Copydefperform_clustering(embeddings: np.ndarray,
    dim:int,
    threshold:float,) ->List[np.ndarray]:""""""
    임베딩에 대해 차원 축소, 가우시안 혼합 모델을 사용한 클러스터링, 각 글로벌 클러스터 내에서의 로컬 클러스터링을 순서대로 수행합니다.

    매개변수:
    - embeddings: numpy 배열로 된 입력 임베딩입니다.
    - dim: UMAP 축소를 위한 목표 차원입니다.
    - threshold: GMM에서 임베딩을 클러스터에 할당하기 위한 확률 임계값입니다.

    반환값:
    - 각 임베딩의 클러스터 ID를 포함하는 numpy 배열의 리스트입니다.
    """"""iflen(embeddings) <= dim +1:# 데이터가 충분하지 않을 때 클러스터링을 피합니다.return[np.array([0])for_inrange(len(embeddings))]# 글로벌 차원 축소reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)# 글로벌 클러스터링global_clusters, n_global_clusters = GMM_cluster(
        reduced_embeddings_global, threshold
    )

    all_local_clusters = [np.array([])for_inrange(len(embeddings))]
    total_clusters =0# 각 글로벌 클러스터를 순회하며 로컬 클러스터링 수행foriinrange(n_global_clusters):# 현재 글로벌 클러스터에 속하는 임베딩 추출global_cluster_embeddings_ = embeddings[
            np.array([iingcforgcinglobal_clusters])
        ]iflen(global_cluster_embeddings_) ==0:continueiflen(global_cluster_embeddings_) <= dim +1:# 작은 클러스터는 직접 할당으로 처리local_clusters = [np.array([0])for_inglobal_cluster_embeddings_]
            n_local_clusters =1else:# 로컬 차원 축소 및 클러스터링reduced_embeddings_local = local_cluster_embeddings(
                global_cluster_embeddings_, dim
            )
            local_clusters, n_local_clusters = GMM_cluster(
                reduced_embeddings_local, threshold
            )# 로컬 클러스터 ID 할당, 이미 처리된 총 클러스터 수를 조정forjinrange(n_local_clusters):
            local_cluster_embeddings_ = global_cluster_embeddings_[
                np.array([jinlcforlcinlocal_clusters])
            ]
            indices = np.where(
                (embeddings == local_cluster_embeddings_[:,None]).all(-1)
            )[1]foridxinindices:
                all_local_clusters[idx] = np.append(
                    all_local_clusters[idx], j + total_clusters
                )

        total_clusters += n_local_clustersreturnall_local_clusters
```
텍스트 문서의 목록에 대한 임베딩을 생성하는 함수embed를 구현합니다.
- 입력으로 텍스트 문서의 목록(texts)을 받습니다.
- embd객체의embed_documents메소드를 사용하여 텍스트 문서의 임베딩을 생성합니다.
- 생성된 임베딩을numpy.ndarray형태로 변환하여 반환합니다.
입력으로 텍스트 문서의 목록(texts)을 받습니다.
embd객체의embed_documents메소드를 사용하여 텍스트 문서의 임베딩을 생성합니다.
생성된 임베딩을numpy.ndarray형태로 변환하여 반환합니다.
```
Copydefembed(texts):# 텍스트 문서 목록에 대한 임베딩을 생성합니다.## 이 함수는 `embd` 객체가 존재한다고 가정하며, 이 객체는 텍스트 목록을 받아 그 임베딩을 반환하는 `embed_documents` 메소드를 가지고 있습니다.## 매개변수:# - texts: List[str], 임베딩할 텍스트 문서의 목록입니다.## 반환값:# - numpy.ndarray: 주어진 텍스트 문서들에 대한 임베딩 배열입니다.text_embeddings = embd.embed_documents(
        texts
    )# 텍스트 문서들의 임베딩을 생성합니다.text_embeddings_np = np.array(text_embeddings)# 임베딩을 numpy 배열로 변환합니다.returntext_embeddings_np# 임베딩된 numpy 배열을 반환합니다.
```
embed_cluster_texts함수는 텍스트 목록을 임베딩하고 클러스터링하여, 원본 텍스트, 해당 임베딩, 그리고 할당된 클러스터 라벨을 포함하는pandas.DataFrame을 반환합니다.
- 주어진 텍스트 목록에 대해 임베딩을 생성합니다.
- 생성된 임베딩을 기반으로 클러스터링을 수행합니다. 이 과정은 사전에 정의된perform_clustering함수를 사용합니다.
- 결과를 저장하기 위해pandas.DataFrame을 초기화합니다.
- DataFrame에 원본 텍스트, 임베딩 리스트, 클러스터 라벨을 각각 저장합니다.
주어진 텍스트 목록에 대해 임베딩을 생성합니다.
생성된 임베딩을 기반으로 클러스터링을 수행합니다. 이 과정은 사전에 정의된perform_clustering함수를 사용합니다.
결과를 저장하기 위해pandas.DataFrame을 초기화합니다.
DataFrame에 원본 텍스트, 임베딩 리스트, 클러스터 라벨을 각각 저장합니다.
이 함수는 텍스트 데이터의 임베딩 생성과 클러스터링을 하나의 단계로 결합하여, 텍스트 데이터의 구조적 분석과 그룹화를 용이하게 합니다.
```
Copydefembed_cluster_texts(texts):""""""
    텍스트 목록을 임베딩하고 클러스터링하여, 텍스트, 그들의 임베딩, 그리고 클러스터 라벨이 포함된 DataFrame을 반환합니다.

    이 함수는 임베딩 생성과 클러스터링을 단일 단계로 결합합니다. 임베딩에 대해 클러스터링을 수행하는 `perform_clustering` 함수의 사전 정의된 존재를 가정합니다.

    매개변수:
    - texts: List[str], 처리될 텍스트 문서의 목록입니다.

    반환값:
    - pandas.DataFrame: 원본 텍스트, 그들의 임베딩, 그리고 할당된 클러스터 라벨이 포함된 DataFrame입니다.
    """"""text_embeddings_np = embed(texts)# 임베딩 생성cluster_labels = perform_clustering(
        text_embeddings_np,10,0.1)# 임베딩에 대해 클러스터링 수행df = pd.DataFrame()# 결과를 저장할 DataFrame 초기화df[""text""] = texts# 원본 텍스트 저장df[""embd""] =list(text_embeddings_np)# DataFrame에 리스트로 임베딩 저장df[""cluster""] = cluster_labels# 클러스터 라벨 저장returndf
```
fmt_txt함수는pandas의DataFrame에서 텍스트 문서를 단일 문자열로 포맷팅합니다.
- 입력 파라미터로DataFrame을 받으며, 이DataFrame은 포맷팅할 텍스트 문서를 포함한 'text' 컬럼을 가져야 합니다.
- 모든 텍스트 문서는 특정 구분자(""--- --- \n --- ---"")를 사용하여 연결되어 단일 문자열로 반환됩니다.
- 함수는 연결된 텍스트 문서를 포함하는 단일 문자열을 반환합니다.
입력 파라미터로DataFrame을 받으며, 이DataFrame은 포맷팅할 텍스트 문서를 포함한 'text' 컬럼을 가져야 합니다.
모든 텍스트 문서는 특정 구분자(""--- --- \n --- ---"")를 사용하여 연결되어 단일 문자열로 반환됩니다.
함수는 연결된 텍스트 문서를 포함하는 단일 문자열을 반환합니다.
```
Copydeffmt_txt(df: pd.DataFrame) ->str:""""""
    DataFrame에 있는 텍스트 문서를 단일 문자열로 포맷합니다.

    매개변수:
    - df: 'text' 열에 포맷할 텍스트 문서가 포함된 DataFrame.

    반환값:
    - 모든 텍스트 문서가 특정 구분자로 결합된 단일 문자열.
    """"""unique_txt = df[""text""].tolist()# 'text' 열의 모든 텍스트를 리스트로 변환return""--- --- \n --- --- "".join(
        unique_txt
    )# 텍스트 문서들을 특정 구분자로 결합하여 반환
```
텍스트 데이터를 임베딩하고, 클러스터링하며, 각 클러스터에 대한 요약을 생성하는 과정을 수행합니다.
- 주어진 텍스트 목록에 대해 임베딩을 생성하고 유사성에 기반한 클러스터링을 진행합니다. 이 과정은df_clusters데이터프레임을 결과로 합니다. 이 데이터프레임에는 원본 텍스트, 임베딩, 그리고 클러스터 할당 정보가 포함됩니다.
- 클러스터 할당을 쉽게 처리하기 위해 데이터프레임 항목을 확장합니다. 각 행은 텍스트, 임베딩, 클러스터를 포함하는 새로운 데이터프레임으로 변환됩니다.
- 확장된 데이터프레임에서 고유한 클러스터 식별자를 추출하고, 각 클러스터에 대한 텍스트를 포맷팅하여 요약을 생성합니다. 이 요약은df_summary데이터프레임에 저장됩니다. 이 데이터프레임은 각 클러스터의 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.
- 최종적으로, 함수는 두 개의 데이터프레임을 포함하는 튜플을 반환합니다. 첫 번째 데이터프레임은 원본 텍스트, 임베딩, 클러스터 할당 정보를 포함하며, 두 번째 데이터프레임은 각 클러스터에 대한 요약과 해당 세부 수준, 클러스터 식별자를 포함합니다.
주어진 텍스트 목록에 대해 임베딩을 생성하고 유사성에 기반한 클러스터링을 진행합니다. 이 과정은df_clusters데이터프레임을 결과로 합니다. 이 데이터프레임에는 원본 텍스트, 임베딩, 그리고 클러스터 할당 정보가 포함됩니다.
클러스터 할당을 쉽게 처리하기 위해 데이터프레임 항목을 확장합니다. 각 행은 텍스트, 임베딩, 클러스터를 포함하는 새로운 데이터프레임으로 변환됩니다.
확장된 데이터프레임에서 고유한 클러스터 식별자를 추출하고, 각 클러스터에 대한 텍스트를 포맷팅하여 요약을 생성합니다. 이 요약은df_summary데이터프레임에 저장됩니다. 이 데이터프레임은 각 클러스터의 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.
최종적으로, 함수는 두 개의 데이터프레임을 포함하는 튜플을 반환합니다. 첫 번째 데이터프레임은 원본 텍스트, 임베딩, 클러스터 할당 정보를 포함하며, 두 번째 데이터프레임은 각 클러스터에 대한 요약과 해당 세부 수준, 클러스터 식별자를 포함합니다.
```
Copydefembed_cluster_summarize_texts(texts:List[str], level:int) ->Tuple[pd.DataFrame, pd.DataFrame]:""""""
    텍스트 목록에 대해 임베딩, 클러스터링 및 요약을 수행합니다. 이 함수는 먼저 텍스트에 대한 임베딩을 생성하고,
    유사성을 기반으로 클러스터링을 수행한 다음, 클러스터 할당을 확장하여 처리를 용이하게 하고 각 클러스터 내의 내용을 요약합니다.

    매개변수:
    - texts: 처리할 텍스트 문서 목록입니다.
    - level: 처리의 깊이나 세부 사항을 정의할 수 있는 정수 매개변수입니다.

    반환값:
    - 두 개의 데이터프레임을 포함하는 튜플:
      1. 첫 번째 데이터프레임(`df_clusters`)은 원본 텍스트, 그들의 임베딩, 그리고 클러스터 할당을 포함합니다.
      2. 두 번째 데이터프레임(`df_summary`)은 각 클러스터에 대한 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.
    """"""# 텍스트를 임베딩하고 클러스터링하여 'text', 'embd', 'cluster' 열이 있는 데이터프레임을 생성합니다.df_clusters = embed_cluster_texts(texts)# 클러스터를 쉽게 조작하기 위해 데이터프레임을 확장할 준비를 합니다.expanded_list = []# 데이터프레임 항목을 문서-클러스터 쌍으로 확장하여 처리를 간단하게 합니다.forindex, rowindf_clusters.iterrows():forclusterinrow[""cluster""]:
            expanded_list.append(
                {""text"": row[""text""],""embd"": row[""embd""],""cluster"": cluster}
            )# 확장된 목록에서 새 데이터프레임을 생성합니다.expanded_df = pd.DataFrame(expanded_list)# 처리를 위해 고유한 클러스터 식별자를 검색합니다.all_clusters = expanded_df[""cluster""].unique()print(f""--Generated{len(all_clusters)}clusters--"")# 요약template =""""""여기 LangChain 표현 언어 문서의 하위 집합이 있습니다.

    LangChain 표현 언어는 LangChain에서 체인을 구성하는 방법을 제공합니다.

    제공된 문서의 자세한 요약을 제공하십시오.

    문서:
    {context}
    """"""prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model | StrOutputParser()# 각 클러스터 내의 텍스트를 요약을 위해 포맷팅합니다.summaries = []foriinall_clusters:
        df_cluster = expanded_df[expanded_df[""cluster""] == i]
        formatted_txt = fmt_txt(df_cluster)
        summaries.append(chain.invoke({""context"": formatted_txt}))# 요약, 해당 클러스터 및 레벨을 저장할 데이터프레임을 생성합니다.df_summary = pd.DataFrame(
        {""summaries"": summaries,""level"": [level] *len(summaries),""cluster"":list(all_clusters),
        }
    )returndf_clusters, df_summary
```
텍스트 데이터를 재귀적으로 임베딩, 클러스터링 및 요약하는 과정을 구현한 함수입니다.
- 주어진 텍스트 리스트를 임베딩, 클러스터링 및 요약하여 각 단계별로 결과를 저장합니다.
- 함수는 최대 지정된 재귀 레벨까지 실행되거나, 유일한 클러스터의 수가 1이 될 때까지 반복됩니다.
- 각 재귀 단계에서는 현재 레벨의 클러스터링 결과와 요약 결과를 데이터프레임 형태로 반환하고, 이를 결과 딕셔너리에 저장합니다.
- 만약 현재 레벨이 최대 재귀 레벨보다 작고, 유일한 클러스터의 수가 1보다 크다면, 현재 레벨의 요약 결과를 다음 레벨의 입력 텍스트로 사용하여 재귀적으로 함수를 호출합니다.
- 최종적으로 각 레벨별 클러스터 데이터프레임과 요약 데이터프레임을 포함하는 딕셔너리를 반환합니다.
주어진 텍스트 리스트를 임베딩, 클러스터링 및 요약하여 각 단계별로 결과를 저장합니다.
함수는 최대 지정된 재귀 레벨까지 실행되거나, 유일한 클러스터의 수가 1이 될 때까지 반복됩니다.
각 재귀 단계에서는 현재 레벨의 클러스터링 결과와 요약 결과를 데이터프레임 형태로 반환하고, 이를 결과 딕셔너리에 저장합니다.
만약 현재 레벨이 최대 재귀 레벨보다 작고, 유일한 클러스터의 수가 1보다 크다면, 현재 레벨의 요약 결과를 다음 레벨의 입력 텍스트로 사용하여 재귀적으로 함수를 호출합니다.
최종적으로 각 레벨별 클러스터 데이터프레임과 요약 데이터프레임을 포함하는 딕셔너리를 반환합니다.
```
Copydefrecursive_embed_cluster_summarize(texts:List[str], level:int=1, n_levels:int=3) ->Dict[int,Tuple[pd.DataFrame, pd.DataFrame]]:""""""
    지정된 레벨까지 또는 고유 클러스터의 수가 1이 될 때까지 텍스트를 재귀적으로 임베딩, 클러스터링, 요약하여
    각 레벨에서의 결과를 저장합니다.

    매개변수:
    - texts: List[str], 처리할 텍스트들.
    - level: int, 현재 재귀 레벨 (1에서 시작).
    - n_levels: int, 재귀의 최대 깊이.

    반환값:
    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], 재귀 레벨을 키로 하고 해당 레벨에서의 클러스터 DataFrame과 요약 DataFrame을 포함하는 튜플을 값으로 하는 사전.
    """"""results = {}# 각 레벨에서의 결과를 저장할 사전# 현재 레벨에 대해 임베딩, 클러스터링, 요약 수행df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)# 현재 레벨의 결과 저장results[level] = (df_clusters, df_summary)# 추가 재귀가 가능하고 의미가 있는지 결정unique_clusters = df_summary[""cluster""].nunique()iflevel < n_levelsandunique_clusters >1:# 다음 레벨의 재귀 입력 텍스트로 요약 사용new_texts = df_summary[""summaries""].tolist()
        next_level_results = recursive_embed_cluster_summarize(
            new_texts, level +1, n_levels
        )# 다음 레벨의 결과를 현재 결과 사전에 병합results.update(next_level_results)returnresults
```
```
Copy# 전체 문서의 개수len(docs_texts)
```
```
33
```
```
Copy# 트리 구축leaf_texts = docs_texts# 문서 텍스트를 리프 텍스트로 설정results = recursive_embed_cluster_summarize(
    leaf_texts, level=1, n_levels=3)# 재귀적으로 임베딩, 클러스터링 및 요약을 수행하여 결과를 얻음
```
```
--Generated 6 clusters--
LangChain Expression Language (LCEL)는 LangChain에서 체인을 구성하는 선언적 방법을 제공합니다. LCEL은 프로토타입을 생산에 투입하는 것을 지원하도록 설계되었으며, 가장 간단한 ""프롬프트 + LLM"" 체인부터 수백 단계의 복잡한 체인까지 코드 변경 없이 생산에서 성공적으로 실행할 수 있습니다. LCEL을 사용하는 몇 가지 이유는 다음과 같습니다:

- **스트리밍 지원**: LCEL로 체인을 구축하면 최상의 첫 번째 토큰까지의 시간을 얻을 수 있습니다. 이는 일부 체인에서 LLM에서 스트리밍 출력 파서로 토큰을 직접 스트리밍하고, LLM 제공자가 원시 토큰을 출력하는 속도로 파싱된 증분 출력 청크를 받을 수 있음을 의미합니다.
- **비동기 지원**: LCEL로 구축된 모든 체인은 동기 API(예: 프로토타이핑 중인 Jupyter 노트북에서)와 비동기 API(예: LangServe 서버에서) 모두에서 호출할 수 있습니다. 이를 통해 프로토타입과 생산에서 동일한 코드를 사용하고, 뛰어난 성능을 제공하며, 동일한 서버에서 많은 동시 요청을 처리할 수 있습니다.
- **최적화된 병렬 실행**: LCEL 체인에 병렬로 실행할 수 있는 단계가 있는 경우(예: 여러 검색기에서 문서를 가져올 때), 동기 및 비동기 인터페이스 모두에서 자동으로 수행하여 가능한 가장 작은 대기 시간을 제공합니다.
- **재시도 및 대체**: LCEL 체인의 모든 부분에 대해 재시도 및 대체를 구성할 수 있습니다. 이는 규모에서 체인을 더욱 신뢰할 수 있게 만드는 좋은 방법입니다.
- **중간 결과 접근**: 더 복잡한 체인의 경우 최종 출력이 생성되기 전에 중간 단계의 결과에 액세스하는 것이 매우 유용할 수 있습니다. 이를 통해 최종 사용자에게 무언가가 진행되고 있음을 알리거나 체인을 디버깅할 수 있습니다.
- **입력 및 출력 스키마**: 입력 및 출력 스키마는 체인의 구조로부터 추론된 Pydantic 및 JSONSchema 스키마를 모든 LCEL 체인에 제공합니다. 이는 입력 및 출력의 유효성 검사에 사용될 수 있으며 LangServe의 중요한 부분입니다.
- **LangSmith 추적 통합 및 LangServe 배포 통합**: 체인이 점점 더 복잡해짐에 따라 각 단계에서 정확히 무슨 일이 일어나고 있는지 이해하는 것이 점점 더 중요해집니다. LCEL을 사용하면 모든 단계가 LangSmith에 자동으로 기록되어 최대한의 관찰 가능성과 디버깅 가능성을 제공합니다.

LCEL은 다양한 공통 작업을 수행하기 위한 예제 코드를 제공하는 쿡북도 포함하고 있습니다. 이 예제들은 다양한 Runnable(핵심 LCEL 인터페이스) 구성 요소를 조합하여 다양한 작업을 수행하는 방법을 보여줍니다. 예를 들어, LCEL을 사용하여 의미적 유사성에 따라 사용자 입력을 기반으로 체인 로직을 동적으로 결정하는 사용자 지정 라우팅 로직을 쉽게 추가할 수 있습니다. 이는 쿼리를 가장 관련성이 높은 프롬프트로 라우팅하는 데 특히 유용한 기술입니다.LangChain 표현 언어(LCEL) 문서는 LangChain을 사용하여 체인을 구성하는 방법에 대한 다양한 가이드와 예시를 제공합니다. 이 문서는 LangChain의 기능을 최대한 활용하고자 하는 개발자들을 위한 것입니다. 주요 내용은 다음과 같습니다:

1. **Runnable 타입**:
   - **RunnableParallel**: 데이터를 조작하는 방법을 설명합니다.
   - **RunnablePassthrough**: 데이터를 그대로 통과시키는 방법을 설명합니다.
   - **RunnableLambda**: 사용자 정의 함수를 실행하는 방법을 설명합니다.
   - **RunnableBranch**: 입력에 기반하여 동적으로 로직을 라우팅하는 방법을 설명합니다.

2. **런타임 인자 바인딩**: Runnable 시퀀스 내에서 특정 인자를 상수로 전달하는 방법을 설명합니다.

3. **런타임에서 체인 내부 구성**: 런타임에서 체인의 내부 설정을 실험하거나 최종 사용자에게 노출하는 방법을 설명합니다.

4. **`@chain` 데코레이터를 사용하여 runnable 생성**: 임의의 함수를 체인으로 변환하는 방법을 설명합니다. 이 방법은 관찰 가능성을 향상시키고, 함수 내부에서 호출되는 runnable을 중첩된 자식으로 추적할 수 있게 합니다.

5. **Fallback 추가**: LLM(Large Language Model) 애플리케이션에서 발생할 수 있는 다양한 실패 지점에 대비하는 방법을 설명합니다.

6. **커스텀 제너레이터 함수 스트리밍**: `yield` 키워드를 사용하는 제너레이터 함수를 LCEL 파이프라인에 사용하는 방법을 설명합니다. 이는 커스텀 출력 파서를 구현하거나 이전 단계의 출력을 수정하면서 스트리밍 기능을 유지하는 데 유용합니다.

7. **Runnable 검사**: LCEL로 생성된 runnable을 검사하는 방법을 설명합니다.

8. **메시지 기록(메모리) 추가**: 특정 runnable에 메시지 기록을 추가하는 방법을 설명합니다.

이 문서는 LangChain을 사용하여 복잡한 데이터 처리 및 변환 파이프라인을 구축하고자 하는 개발자에게 유용한 정보를 제공합니다. 각 섹션은 구체적인 예시와 함께 설명되어 있어, 개발자가 LangChain의 다양한 기능을 쉽게 이해하고 적용할 수 있도록 돕습니다.이 문서는 LangChain 표현 언어(LCEL)를 사용하여 다양한 작업을 수행하는 방법에 대한 예제 코드를 제공하는 쿡북입니다. LCEL은 LangChain에서 체인을 구성하는 방법을 제공하며, 이 쿡북은 다양한 Runnable(핵심 LCEL 인터페이스) 구성 요소를 조합하여 여러 작업을 수행하는 방법을 보여줍니다. 여기에는 다음과 같은 주제들이 포함됩니다:

1. **Prompt + LLM**: 가장 일반적이고 가치 있는 구성으로, 프롬프트와 LLM(Large Language Model)을 결합하는 방법을 설명합니다.
2. **RAG**: 프롬프트와 LLM에 검색 단계를 추가하는 방법을 소개합니다.
3. **Multiple Chains**: Runnable을 사용하여 여러 체인을 연결하는 방법을 설명합니다.
4. **Querying a SQL DB**: Runnable을 사용하여 SQL 데이터베이스 체인을 복제하는 방법을 보여줍니다.
5. **Agents**: Runnable을 에이전트로 전달하는 방법을 설명합니다.
6. **Code Writing**: LCEL을 사용하여 Python 코드를 작성하는 예제를 제공합니다.
7. **Routing by Semantic Similarity**: LCEL을 사용하여 의미적 유사성에 따라 라우팅을 추가하는 방법을 설명합니다.
8. **Adding Memory**: 임의의 체인에 메모리를 추가하는 방법을 보여줍니다.
9. **Adding Moderation**: LLM 애플리케이션 주변에 검열(또는 다른 안전장치)을 추가하는 방법을 설명합니다.
10. **Managing Prompt Size**: 에이전트가 동적으로 도구를 호출하고, 그 도구 호출의 결과가 프롬프트에 추가되는 방식을 관리하는 방법을 보여줍니다.
11. **Using Tools**: Runnable과 함께 도구를 쉽게 사용하는 방법을 설명합니다.

각 섹션은 특정 작업을 수행하기 위한 코드 예제와 함께 구체적인 방법론을 제공합니다. 예를 들어, ""Adding Moderation"" 섹션에서는 OpenAI의 내용 정책을 위반하는 텍스트를 찾아내는 방법을 보여주고, ""Multiple Chains"" 섹션에서는 여러 체인을 연결하여 복잡한 질문에 답하는 방법을 설명합니다. ""Agents"" 섹션에서는 에이전트를 구성하고 실행하는 과정을, ""Code Writing""에서는 LCEL을 사용하여 코드를 작성하고 실행하는 방법을 보여줍니다.

이 문서는 LangChain을 사용하여 다양한 작업을 수행하고자 하는 개발자들에게 유용한 리소스입니다. 각 예제는 구체적인 작업을 수행하는 방법을 단계별로 설명하며, 이를 통해 개발자들은 LCEL의 다양한 기능을 이해하고 자신의 프로젝트에 적용할 수 있습니다.The provided documents are part of the LangChain documentation, which details the LangChain Expression Language (LCEL) and its applications. LangChain is a framework designed to facilitate the creation, manipulation, and execution of complex chains of operations, particularly in the context of language models and related tasks. The documentation covers various aspects of using LangChain, including installation, quick start guides, security considerations, and detailed explanations of LCEL's components and capabilities. Here's a summary of the key points from the documents:

1. **Introduction to LangChain and LCEL**: LangChain offers a way to build complex chains from basic components, supporting functionalities like streaming, parallelism, and logging. LCEL (LangChain Expression Language) simplifies the process of chaining together prompts, models, and output parsers to perform tasks like generating jokes based on a given topic or conducting retrieval-augmented generation.

2. **Basic Examples**: The documentation provides examples of basic use cases, such as chaining a prompt template with a model and an output parser to generate content based on user input. It also demonstrates more complex scenarios like retrieval-augmented generation, where additional context is retrieved and used to inform the generation process.

3. **Inspecting Runnables**: LangChain allows users to inspect runnables (components of a chain) to understand their structure and operation better. This can include generating a graph representation of a chain or retrieving the prompts used within a chain.

4. **Using Tools with Runnables**: LangChain supports the integration of various tools with runnables. An example provided is using DuckDuckGoSearchRun with a chain to turn user input into a search query and retrieve relevant information.

5. **Self-querying Retrievers**: The documentation discusses self-querying retrievers, which can construct structured queries based on natural language input and apply these queries to their underlying VectorStore. This allows for sophisticated retrieval operations based on both semantic similarity and metadata filters.

6. **Advanced Features**: LangChain documentation also touches on advanced features like adding memory to chains, managing prompt size, and routing by semantic similarity. These features enable the creation of more sophisticated and efficient chains capable of handling complex tasks.

7. **Community and Support**: The documents encourage community engagement and feedback, providing links to community resources like Discord, Twitter, and GitHub. This suggests an active and supportive community around LangChain.

Overall, the LangChain documentation provides a comprehensive guide to using the LangChain framework and LCEL for building and executing complex chains of operations involving language models and other components. It covers both basic and advanced use cases, offering practical examples and encouraging community involvement.The provided documents from LangChain cover a range of topics related to the LangChain Expression Language (LCEL) and its applications, including interface design, streaming, dynamic routing, parallel processing, data passing, message history management, and prompt size management. Here's a detailed summary of the key points from each document:

### Interface
- LangChain introduces a ""Runnable"" protocol to simplify the creation of custom chains.
- The standard interface includes methods for streaming, invoking, and batching calls, with asynchronous versions available.
- Input and output types vary by component, with schemas provided for inspection.

### Streaming
- Streaming is crucial for making applications feel responsive.
- LangChain supports synchronous and asynchronous streaming, including intermediate steps and final output.
- Examples demonstrate streaming with LLMs and chat models, highlighting the importance of handling input streams effectively.

### RunnableBranch
- RunnableBranch allows for dynamic routing based on input, enabling non-deterministic chains.
- Two methods for routing include using a custom function (recommended) or a RunnableBranch.
- Examples show how to classify questions and route them to corresponding prompt chains based on the classification.

### RunnableParallel
- RunnableParallel is used for manipulating data and executing multiple Runnables in parallel.
- It can be used to match the output format of one Runnable to the input format of another.
- Examples demonstrate parallel execution and the use of itemgetter for shorthand data extraction.

### RunnablePassthrough
- RunnablePassthrough passes inputs unchanged or with added keys, often used with RunnableParallel.
- It allows for the assignment of data to new keys in a map.
- An example shows its use in a retrieval chain, passing user input under a specific key.

### Add Message History (Memory)
- RunnableWithMessageHistory adds message history to chains, managing chat message history.
- It supports various input and output formats, including sequences of BaseMessage and dictionaries.
- Examples cover in-memory and persistent storage (using Redis) for message histories, demonstrating how to manage and utilize chat histories in chains.

### Managing Prompt Size
- Managing prompt size is crucial for preventing context window overflow in models.
- Custom functionality can be added to LCEL chains for prompt size management.
- An example demonstrates a multi-step question with prompt handling logic to condense prompts and ensure the model's context window is not exceeded.

These documents collectively provide a comprehensive guide to using LangChain Expression Language for building and managing complex chains, incorporating dynamic routing, parallel processing, and efficient data handling techniques. They emphasize the flexibility and power of LCEL in creating responsive and intelligent applications.The provided documents are part of the LangChain documentation, focusing on the LangChain Expression Language (LCEL), a tool designed to facilitate the construction of complex chains from basic components for language model applications. Here's a detailed summary of the key points from each section:

### Why Use LCEL
- **Purpose**: LCEL simplifies building complex chains by offering a unified interface and composition primitives.
- **Features**:
  1. **Unified Interface**: Implements the Runnable interface, allowing chains of LCEL objects to support common invocation methods (invoke, batch, stream, etc.).
  2. **Composition Primitives**: Facilitates composing chains, parallelizing components, adding fallbacks, and more.
- **Example**: Demonstrates LCEL's utility through a basic example of creating a prompt + model chain and compares the process with and without LCEL, highlighting LCEL's efficiency and simplicity.

### Prompt + LLM
- **Common Composition**: Combining a PromptTemplate/ChatPromptTemplate with an LLM/ChatModel and an OutputParser is a fundamental building block in LCEL.
- **Simplification and Flexibility**: Shows how to simplify input, attach kwargs, and use different parsers for structured outputs.
- **Runnable Parallel**: Introduces RunnableParallel for easier invocation, demonstrating how to streamline the process of creating input dictionaries for prompts.

### Add Fallbacks
- **Handling Failures**: Discusses using fallbacks to gracefully handle failures at various points, especially useful for LLM API errors.
- **Implementation**: Provides examples of implementing fallbacks, including handling specific errors and creating fallbacks for sequences.
- **Practical Use**: Offers code snippets to illustrate how fallbacks can be applied to LLMs, showing how to switch between different models or prompts based on runtime configurations.

### Configure Chain Internals at Runtime
- **Dynamic Configuration**: Explains methods to experiment with or expose different configurations to end-users by adjusting chain internals at runtime.
- **Configuration Fields and Alternatives**:
  - **Fields**: Allows configuring specific fields of a runnable, such as LLM temperature.
  - **Alternatives**: Enables listing out alternatives for any particular runnable that can be set during runtime, useful for switching between models or prompts.
- **Examples**: Provides code examples to demonstrate configuring LLMs and prompts, including saving configured chains as their own objects for reuse.

### Quickstart
- **Output Parsers**: Introduces output parsers as a means to structure language model responses into more useful formats.
- **PydanticOutputParser**: Highlights the use of PydanticOutputParser for defining desired data structures and parsing model outputs into these structures.
- **Streaming and Invocation**: Discusses the support for various invocation methods within LCEL and the ability of some parsers to stream through partially parsed objects.

Each section of the documentation emphasizes the flexibility, efficiency, and ease of use provided by LCEL, showcasing how it can significantly streamline the process of working with language models by offering a structured approach to building, configuring, and managing complex chains.--Generated 1 clusters--
The provided documents offer a comprehensive overview of the LangChain Expression Language (LCEL), a powerful tool designed to facilitate the construction and management of complex operation chains, particularly in the context of language models and related tasks. Here's a consolidated summary of the key points and features highlighted across the documents:

1. **Introduction and Purpose**: LCEL is introduced as a declarative method to construct chains within LangChain, aimed at supporting the transition from prototype to production without code changes. It's designed to handle simple to highly complex chains efficiently.

2. **Key Features of LCEL**:
   - **Streaming and Asynchronous Support**: Enables direct streaming of tokens from LLMs to output parsers, supporting both synchronous and asynchronous API calls. This feature is crucial for reducing latency and improving responsiveness.
   - **Optimized Parallel Execution**: Automatically executes parallelizable steps in a chain to minimize latency, enhancing performance.
   - **Retry and Fallback Mechanisms**: Offers configurable retry and fallback options for all parts of a chain, increasing reliability at scale.
   - **Access to Intermediate Results**: Allows access to results from intermediate steps, useful for debugging and providing progress feedback.
   - **Input and Output Schemas**: Generates Pydantic and JSONSchema schemas from the chain's structure, facilitating input and output validation.
   - **Integration with LangSmith and LangServe**: Ensures maximum observability and debugging capabilities by automatically logging each step in LangSmith and supporting deployment through LangServe.

3. **Runnable Types and Runtime Features**:
   - Various runnable types such as `RunnableParallel`, `RunnablePassthrough`, `RunnableLambda`, and `RunnableBranch` are introduced, each serving different purposes like data manipulation, dynamic routing, and parallel execution.
   - Features like runtime argument binding, chain internal configuration at runtime, and the use of the `@chain` decorator to enhance observability and manageability of chains are discussed.

4. **Advanced Usage and Examples**:
   - The documents provide a plethora of examples demonstrating LCEL's versatility, including prompt + LLM chains, adding fallbacks, custom generator function streaming, and managing prompt size.
   - Specific use cases like routing by semantic similarity, adding memory, and integrating moderation are covered, showcasing LCEL's capability to handle complex logic and dynamic routing based on user input or other criteria.

5. **Community and Support**: The documentation emphasizes community engagement and support, encouraging users to contribute feedback and participate in community resources.

Overall, the LangChain documentation and the detailed exploration of LCEL highlight its role as a critical tool for developers looking to leverage language models and build sophisticated data processing and transformation pipelines. LCEL's design principles focus on ease of use, flexibility, and efficiency, enabling developers to construct, manage, and scale complex chains with minimal overhead and maximum reliability.
```
논문에서는collapsed tree retrieval이 최고의 성능을 보고하고 있습니다.
이는 트리 구조를 단일 계층으로 평탄화한 다음, 모든 노드에 대해 동시에 k-최근접 이웃(kNN) 검색을 적용하는 과정을 포함합니다.
아래에서 이 과정을 간단히 수행합니다.
Chroma벡터 저장소를 사용하여 텍스트 데이터의 벡터화 및 검색 가능한 저장소를 구축하는 과정을 설명합니다.
- 초기에leaf_texts에 저장된 텍스트 데이터를all_texts변수에 복사합니다.
- 결과 데이터(results)를 순회하며 각 레벨에서 요약된 텍스트를 추출하고, 이를all_texts에 추가합니다.
- 각 레벨의DataFrame에서summaries컬럼의 값을 리스트로 변환하여 추출합니다.
- 추출된 요약문을all_texts에 추가합니다.
- 모든 텍스트 데이터(all_texts)를 사용하여Chroma벡터 저장소를 구축합니다.
- Chroma.from_texts함수를 호출하여 텍스트 데이터를 벡터화하고, 벡터 저장소를 생성합니다.
- 생성된 벡터 저장소를 검색 가능하게 만들기 위해.as_retriever()메소드를 사용하여 검색기(retriever)를 초기화합니다.
초기에leaf_texts에 저장된 텍스트 데이터를all_texts변수에 복사합니다.
결과 데이터(results)를 순회하며 각 레벨에서 요약된 텍스트를 추출하고, 이를all_texts에 추가합니다.
각 레벨의DataFrame에서summaries컬럼의 값을 리스트로 변환하여 추출합니다.
추출된 요약문을all_texts에 추가합니다.
모든 텍스트 데이터(all_texts)를 사용하여Chroma벡터 저장소를 구축합니다.
Chroma.from_texts함수를 호출하여 텍스트 데이터를 벡터화하고, 벡터 저장소를 생성합니다.
생성된 벡터 저장소를 검색 가능하게 만들기 위해.as_retriever()메소드를 사용하여 검색기(retriever)를 초기화합니다.
이 과정을 통해, 다양한 레벨의 요약문을 포함한 텍스트 데이터를 벡터화하고, 이를 기반으로 검색 가능한Chroma벡터 저장소를 구축합니다.
```
Copyfromlangchain_community.vectorstoresimportFAISS# leaf_texts를 복사하여 all_texts를 초기화합니다.all_texts = leaf_texts.copy()# 각 레벨의 요약을 추출하여 all_texts에 추가하기 위해 결과를 순회합니다.forlevelinsorted(results.keys()):# 현재 레벨의 DataFrame에서 요약을 추출합니다.summaries = results[level][1][""summaries""].tolist()# 현재 레벨의 요약을 all_texts에 추가합니다.all_texts.extend(summaries)# 이제 all_texts를 사용하여 FAISS vectorstore를 구축합니다.vectorstore = FAISS.from_texts(texts=all_texts, embedding=embd)
```
DB 를 로컬에 저장합니다.
```
Copyimportos

DB_INDEX =""RAPTOR""# 로컬에 FAISS DB 인덱스가 이미 존재하는지 확인하고, 그렇다면 로드하여 vectorstore와 병합한 후 저장합니다.ifos.path.exists(DB_INDEX):
    local_index = FAISS.load_local(DB_INDEX, embd)
    local_index.merge_from(vectorstore)
    local_index.save_local(DB_INDEX)else:
    vectorstore.save_local(folder_path=DB_INDEX)
```
```
Copy# retriever 생성retriever = vectorstore.as_retriever()
```
Retrieval Augmented Generation(RAG) 체인을 정의하고 특정 코드 예제를 요청하는 방법을 구현합니다.
- hub.pull을 사용하여 RAG 프롬프트를 불러옵니다.
- 문서 포맷팅을 위한format_docs함수를 정의합니다. 이 함수는 문서의 페이지 내용을 연결하여 반환합니다.
- RAG 체인을 구성합니다. 이 체인은 검색기(retriever)로부터 문맥을 가져오고,format_docs함수로 포맷팅한 후, 질문을 처리합니다.
- RunnablePassthrough()를 사용하여 질문을 그대로 전달합니다.
- 체인은 프롬프트, 모델, 그리고StrOutputParser()를 통해 최종 출력을 문자열로 파싱합니다.
- rag_chain.invoke메소드를 사용하여 ""How to define a RAG chain? Give me a specific code example.""라는 질문을 처리합니다.
hub.pull을 사용하여 RAG 프롬프트를 불러옵니다.
문서 포맷팅을 위한format_docs함수를 정의합니다. 이 함수는 문서의 페이지 내용을 연결하여 반환합니다.
RAG 체인을 구성합니다. 이 체인은 검색기(retriever)로부터 문맥을 가져오고,format_docs함수로 포맷팅한 후, 질문을 처리합니다.
RunnablePassthrough()를 사용하여 질문을 그대로 전달합니다.
체인은 프롬프트, 모델, 그리고StrOutputParser()를 통해 최종 출력을 문자열로 파싱합니다.
rag_chain.invoke메소드를 사용하여 ""How to define a RAG chain? Give me a specific code example.""라는 질문을 처리합니다.
```
Copyfromlangchainimporthubfromlangchain_core.runnablesimportRunnablePassthrough# 프롬프트 생성prompt = hub.pull(""rlm/rag-prompt"")# 문서 포스트 프로세싱defformat_docs(docs):# 문서의 페이지 내용을 이어붙여 반환합니다.return""\n\n"".join(doc.page_contentfordocindocs)# RAG 체인 정의rag_chain = (# 검색 결과를 포맷팅하고 질문을 처리합니다.{""context"": retriever | format_docs,""question"": RunnablePassthrough()}
    | prompt# 프롬프트를 적용합니다.| model# 모델을 적용합니다.| StrOutputParser()# 문자열 출력 파서를 적용합니다.)
```
LangSmith 링크
```
Copy# 추상적인 질문 실행_ = rag_chain.invoke(""전체 문서의 핵심 주제에 대해 설명해주세요."")
```
```
LangChain 표현 언어(LCEL) 문서는 LangChain을 사용하여 복잡한 데이터 처리 및 변환 파이프라인을 구축하는 방법에 대한 가이드와 예시를 제공합니다. 이 문서는 다양한 Runnable 타입, 런타임 인자 바인딩, 체인 내부 구성, `@chain` 데코레이터 사용, Fallback 추가, 커스텀 제너레이터 함수 스트리밍 등 LCEL의 핵심 기능과 사용 방법을 설명합니다. 또한, LCEL을 사용하여 프롬프트와 LLM을 결합하는 방법, 검색 단계 추가, 여러 체인 연결, SQL DB 쿼리, 코드 작성, 의미적 유사성에 따른 라우팅, 메모리 추가, 검열 추가 등 다양한 작업을 수행하는 방법을 단계별로 제공합니다.
```
LangSmith 링크
```
Copy# Low Level 질문 실행_ = rag_chain.invoke(""PydanticOutputParser 을 활용한 예시 코드를 작성해 주세요."")
```
```
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from langchain_openai import OpenAI

model = OpenAI(model_name=""gpt-3.5-turbo-instruct"", temperature=0.0)

class Joke(BaseModel):
    setup: str = Field(description=""question to set up a joke"")
    punchline: str = Field(description=""answer to resolve the joke"")
    @validator(""setup"")
    def question_ends_with_question_mark(cls, field):
        if field[-1] != ""?"":
            raise ValueError(""Badly formed question!"")
        return field

parser = PydanticOutputParser(pydantic_object=Joke)
prompt = PromptTemplate(
    template=""Answer the user query.\n{format_instructions}\n{query}\n"",
    input_variables=[""query""],
    partial_variables={""format_instructions"": parser.get_format_instructions()},
)

prompt_and_model = prompt | model
output = prompt_and_model.invoke({""query"": ""Tell me a joke.""})
parser.invoke(output)
```
```
LangSmith 링크
```
Copy# Low Level 질문 실행_ = rag_chain.invoke(""self-querying 방법과 예시 코드를 작성해 주세요."")
```
```
Self-querying 방법은 자연어 쿼리를 받아 구조화된 쿼리를 작성하고, 이를 기반으로 VectorStore에 적용하여 문서의 의미적 유사성 비교 및 사용자 쿼리에서 추출한 필터를 메타데이터에 적용하여 실행하는 방식입니다. 예를 들어, Chroma vector store를 사용하여 영화 요약 문서가 포함된 작은 데모 세트를 생성하고, 이를 통해 자체 쿼리 검색기를 인스턴스화할 수 있습니다. 다음은 자체 쿼리 검색기를 사용하는 예시 코드입니다:

```python
%pip install --upgrade --quiet  lark chromadb
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

docs = [
    Document(
        page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"",
        metadata={""year"": 1993, ""rating"": 7.7, ""genre"": ""science fiction""},
    ),
    # Additional documents...
]

vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())

from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_openai import ChatOpenAI

metadata_field_info = [
    AttributeInfo(
        name=""genre"",
        description=""The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']"",
        type=""string"",
    ),
    # Additional metadata fields...
]

document_content_description = ""Brief summary of a movie""
llm = ChatOpenAI(temperature=0)

retriever = SelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
)

# example usage
retriever.invoke(""I want to watch a movie rated higher than 8.5"")
```"
CH13 LangChain Expression Language(LCEL),https://wikidocs.net/233781,"# CH13 LangChain Expression Language(LCEL)

LCEL"
01. RunnablePassthrough: 데이터 전달,https://wikidocs.net/235580,"# 01. RunnablePassthrough: 데이터 전달

RunnablePassthrough는 데이터를 전달하는 역할을 합니다.
이 클래스는run()메서드를 통해입력된 데이터를 그대로 반환합니다.
이는 데이터를 변경하지 않고 파이프라인의 다음 단계로 전달하는 데 사용될 수 있습니다.
RunnablePassthrough는 다음과 같은 시나리오에서 유용할 수 있습니다.
- 데이터를 변환하거나 수정할 필요가 없는 경우
- 파이프라인의 특정 단계를 건너뛰어야 하는 경우
- 디버깅 또는 테스트 목적으로 데이터 흐름을 모니터링해야 하는 경우
데이터를 변환하거나 수정할 필요가 없는 경우
파이프라인의 특정 단계를 건너뛰어야 하는 경우
디버깅 또는 테스트 목적으로 데이터 흐름을 모니터링해야 하는 경우
이 클래스는 Runnable 인터페이스를 구현하므로, 다른 Runnable 객체와 함께 파이프라인에서 사용될 수 있습니다.
## 데이터 전달하기
RunnablePassthrough는 입력을 변경하지 않고 그대로 전달하거나 추가 키를 더하여 전달할 수 있습니다.
일반적으로RunnableParallel과 함께 사용되어 데이터를 맵의 새로운 키에 할당하는 데 활용됩니다.
RunnablePassthrough()를 단독으로 호출하면 단순히 입력을 받아 그대로 전달합니다.
assign과 함께 호출된 RunnablePassthrough(RunnablePassthrough.assign(...))는 입력을 받아 assign 함수에 전달된 추가 인자를 더합니다.
```
Copy%pip install -qU langchain langchain-openai
```
- RunnableParallel클래스를 사용하여병렬로 실행 가능한 작업을 정의합니다.
- passed속성에는RunnablePassthrough인스턴스를 할당하여 입력을 그대로 반환하도록 설정합니다.
- extra속성에는RunnablePassthrough.assign()메서드를 사용하여 입력의 ""num"" 값에 3을 곱한 결과를 ""mult"" 키에 할당하는 작업을 정의합니다.
- modified속성에는 람다 함수를 사용하여 입력의 ""num"" 값에 1을 더하는 작업을 정의합니다.
- runnable.invoke()메서드를 호출하여{""num"": 1}입력으로 병렬 작업을 실행합니다.
RunnableParallel클래스를 사용하여병렬로 실행 가능한 작업을 정의합니다.
passed속성에는RunnablePassthrough인스턴스를 할당하여 입력을 그대로 반환하도록 설정합니다.
extra속성에는RunnablePassthrough.assign()메서드를 사용하여 입력의 ""num"" 값에 3을 곱한 결과를 ""mult"" 키에 할당하는 작업을 정의합니다.
modified속성에는 람다 함수를 사용하여 입력의 ""num"" 값에 1을 더하는 작업을 정의합니다.
runnable.invoke()메서드를 호출하여{""num"": 1}입력으로 병렬 작업을 실행합니다.
```
Copyfromlangchain_core.runnablesimportRunnableParallel, RunnablePassthrough

runnable = RunnableParallel(# 전달된 입력을 그대로 반환하는 Runnable을 설정합니다.passed=RunnablePassthrough(),# 입력의 ""num"" 값에 3을 곱한 결과를 반환하는 Runnable을 설정합니다.extra=RunnablePassthrough.assign(mult=lambdax: x[""num""] *3),# 입력의 ""num"" 값에 1을 더한 결과를 반환하는 Runnable을 설정합니다.modified=lambdax: x[""num""] +1,
)# {""num"": 1}을 입력으로 Runnable을 실행합니다.runnable.invoke({""num"":1})
```
```
{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}
```
위의 예시에서passed키는RunnablePassthrough()와 함께 호출되었으며, 이는 단순히{'num': 1}을 전달합니다.
두 번째 줄에서는 숫자 값에 3을 곱하는 람다 함수와 함께RunnablePastshrough.assign을 사용했습니다. 이 경우,extra는 원래 값에mult키가 추가된{'num': 1, 'mult': 3}로 설정되었습니다.
마지막으로,modified키를 사용하여 맵에 세 번째 키를 설정했는데, 이는 람다 함수를 사용하여 num에 1을 더한 단일 값을 설정하였으며, 그 결과modified키의 값은2가 되었습니다.
## 검색기 예제
아래 예제에서는RunnablePassthrough와RunnableMap을 함께 사용하는 사용 사례를 살펴볼 수 있습니다.
- 여기서 prompt에 대한 입력은 ""context""와 ""question""이라는 키를 가진 map 형태로 예상됩니다.
- 사용자 입력은 단순히 질문에 해당합니다.
- 따라서 retriever를 사용하여 context를 가져오고, ""question"" 키 아래에 사용자 입력을 전달해야 합니다.
- 이 경우, RunnablePassthrough를 사용하면 사용자의 질문을 prompt와 model에 전달할 수 있습니다.
여기서 prompt에 대한 입력은 ""context""와 ""question""이라는 키를 가진 map 형태로 예상됩니다.
사용자 입력은 단순히 질문에 해당합니다.
따라서 retriever를 사용하여 context를 가져오고, ""question"" 키 아래에 사용자 입력을 전달해야 합니다.
이 경우, RunnablePassthrough를 사용하면 사용자의 질문을 prompt와 model에 전달할 수 있습니다.
```
Copyfromlangchain_community.vectorstoresimportFAISSfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_openaiimportChatOpenAI, OpenAIEmbeddings# 텍스트로부터 FAISS 벡터 저장소를 생성합니다.vectorstore = FAISS.from_texts(
    [""테디는 랭체인 주식회사에서 근무를 하였습니다."",""셜리는 테디와 같은 회사에서 근무하였습니다."",""테디의 직업은 개발자입니다."",""셜리의 직업은 디자이너입니다."",
    ],
    embedding=OpenAIEmbeddings(),
)# 벡터 저장소를 검색기로 사용합니다.retriever = vectorstore.as_retriever()# 템플릿을 정의합니다.template =""""""Answer the question based only on the following context:
{context}

Question: {question}
""""""# 템플릿으로부터 채팅 프롬프트를 생성합니다.prompt = ChatPromptTemplate.from_template(template)# ChatOpenAI 모델을 초기화합니다.model = ChatOpenAI()# 검색 체인을 구성합니다.retrieval_chain = (
    {""context"": retriever,""question"": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```
```
Copy# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.retrieval_chain.invoke(""테디의 직업은 무엇입니까?"")
```
```
'테디의 직업은 개발자입니다.'
```
```
Copy# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.retrieval_chain.invoke(""셜리의 직업은 무엇입니까?"")
```
```
'셜리의 직업은 디자이너입니다.'
```"
02. Runnable 구조(그래프) 확인,https://wikidocs.net/235884,"# 02. Runnable 구조(그래프) 확인

LCEL로runnable을 생성한 후에는 종종 이를 검사하여 어떤 일이 일어나고 있는지 더 잘 파악하고 싶을 것입니다.
이 노트북에서는 이를 수행하는 몇 가지 방법을 다룹니다.
```
Copy%pip install -qU langchain langchain-openai faiss-cpu tiktoken
```
```
Copy# 그래프를 그리기 위한 라이브러리 설치%pip install -qU grandalf
```
```
Copyfromlangchain.promptsimportChatPromptTemplatefromlangchain.vectorstoresimportFAISSfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.runnablesimportRunnableLambda, RunnablePassthroughfromlangchain_openaiimportChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(# 텍스트 데이터로부터 FAISS 벡터 저장소를 생성합니다.[""Teddy is an AI engineer who loves programming!""],
    embedding=OpenAIEmbeddings(),
)# 벡터 저장소를 기반으로 retriever를 생성합니다.retriever = vectorstore.as_retriever()

template =""""""Answer the question based only on the following context:
{context}  

Question: {question}""""""prompt = ChatPromptTemplate.from_template(
    template
)# 템플릿을 기반으로 ChatPromptTemplate을 생성합니다.model = ChatOpenAI()# ChatOpenAI 모델을 초기화합니다.# chain 을 생성합니다.chain = (# 검색 컨텍스트와 질문을 지정합니다.{""context"": retriever,""question"": RunnablePassthrough()}
    | prompt# 프롬프트를 생성합니다.| model# 언어 모델을 실행합니다.| StrOutputParser()# 출력 결과를 문자열로 파싱합니다.)
```
## 그래프 구성 확인
runnable의 그래프를 얻을 수 있습니다.
chain.get_graph()메서드는 체인의 실행 그래프를 반환합니다.
- 이 메서드는 체인의 각 노드와 노드 간의 연결을 나타내는 그래프 객체를 반환합니다.
- 그래프의 노드는 체인의 각 단계를 나타내며, 에지(edge)는 단계 간의 데이터 흐름을 나타냅니다.
이 메서드는 체인의 각 노드와 노드 간의 연결을 나타내는 그래프 객체를 반환합니다.
그래프의 노드는 체인의 각 단계를 나타내며, 에지(edge)는 단계 간의 데이터 흐름을 나타냅니다.
```
Copy# 체인의 그래프에서 노드를 가져옵니다.chain.get_graph().nodes
```
```
{'46d23d1b25d6485daf44722926e09252': Node(id='46d23d1b25d6485daf44722926e09252', data=Input'>), '1d02513a446d49f9bd47bcc35a5b1a50': Node(id='1d02513a446d49f9bd47bcc35a5b1a50', data=Output'>), 'a19608744b9642efaab6b7d733f8589f': Node(id='a19608744b9642efaab6b7d733f8589f', data=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=)), '98f3a9f1ad46441f9608f67d427f0f97': Node(id='98f3a9f1ad46441f9608f67d427f0f97', data=RunnablePassthrough()), '989023b7f06f4755b13db7344ed30239': Node(id='989023b7f06f4755b13db7344ed30239', data=ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\n{context}  \n\nQuestion: {question}'))])), '57d87ab1a3d947d28f61b9cb31dc40d5': Node(id='57d87ab1a3d947d28f61b9cb31dc40d5', data=ChatOpenAI(client=, async_client=, openai_api_key=SecretStr('**********'), openai_proxy='')), 'a2e44128ec594c2cb916d4ce3a3511b1': Node(id='a2e44128ec594c2cb916d4ce3a3511b1', data=StrOutputParser()), '260fd031d4e94988b155fc2d1b8b20ec': Node(id='260fd031d4e94988b155fc2d1b8b20ec', data=)}
```
```
Copy# 체인의 그래프에서 엣지를 가져옵니다.chain.get_graph().edges
```
```
[Edge(source='565136c5356541828f1879296c6ba0d6', target='5810102acbf84bdeb6342cc5b3c54a30', data=None), Edge(source='5810102acbf84bdeb6342cc5b3c54a30', target='ecefd39e4d62439189527d8c424923e9', data=None), Edge(source='565136c5356541828f1879296c6ba0d6', target='4823ded9b1d3429992634ea5ec76a67b', data=None), Edge(source='4823ded9b1d3429992634ea5ec76a67b', target='ecefd39e4d62439189527d8c424923e9', data=None), Edge(source='ecefd39e4d62439189527d8c424923e9', target='f599128d839747678f3193eea303542b', data=None), Edge(source='f599128d839747678f3193eea303542b', target='bceabc5b2ba74ed5a59793cdf94e880a', data=None), Edge(source='bb73629324bb4138985a78eb606e4071', target='a6d9dec55c6540eeb4b47426bd4aaa6f', data=None), Edge(source='bceabc5b2ba74ed5a59793cdf94e880a', target='bb73629324bb4138985a78eb606e4071', data=None)]
```
## 그래프 출력
그래프를 출력하면 이해하기 쉬운 형태로 표현할 수 있습니다.
비록 출력 결과가 매우 읽기 쉽지는 않지만, 출력을 통해 보다 이해하기 쉬운 형태로 그래프를 확인할 수 있습니다.
```
Copy# 체인의 그래프를 ASCII 형식으로 출력합니다.chain.get_graph().print_ascii()
```
```
+---------------------------------+         
           | ParallelInput |         
           +---------------------------------+         
                    **               **                
                 ***                   ***             
               **                         **           
+----------------------+              +-------------+  
| VectorStoreRetriever |              | Passthrough |  
+----------------------+              +-------------+  
                    **               **                
                      ***         ***                  
                         **     **                     
           +----------------------------------+        
           | ParallelOutput |        
           +----------------------------------+        
                             *                         
                             *                         
                             *                         
                  +--------------------+               
                  | ChatPromptTemplate |               
                  +--------------------+               
                             *                         
                             *                         
                             *                         
                      +------------+                   
                      | ChatOpenAI |                   
                      +------------+                   
                             *                         
                             *                         
                             *                         
                   +-----------------+                 
                   | StrOutputParser |                 
                   +-----------------+                 
                             *                         
                             *                         
                             *                         
                +-----------------------+              
                | StrOutputParserOutput |              
                +-----------------------+
```
## 프롬프트 가져오기
체인에서 중요한 부분은 사용되는 프롬프트입니다.
chain.get_prompts()메서드는 체인에서 사용되는 프롬프트(prompt) 객체의 리스트를 반환합니다.
```
Copychain.get_prompts()# 체인에서 사용되는 프롬프트를 가져옵니다.
```
```
[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\n{context}  \n\nQuestion: {question}'))])]
```"
03. RunnableLambda: 사용자 정의 함수,https://wikidocs.net/235705,"# 03. RunnableLambda: 사용자 정의 함수

RunnableLambda는사용자 정의 함수를 실행할 수 있는 기능을 제공합니다.
이를 통해 개발자는자신만의 함수를 정의하고, 해당 함수를RunnableLambda를 사용하여 실행할 수 있습니다.
예를 들어, 데이터 전처리, 계산, 또는 외부 API와의 상호 작용과 같은 작업을 수행하는 함수를 정의하고 실행할 수 있습니다.
## 사용자 정의 함수를 실행하는 방법
주의사항
사용자 정의 함수를RunnableLambda로 래핑하여 활용할 수 있는데, 여기서 주의할 점은사용자 정의 함수가 받을 수 있는 인자는 1개 뿐이라는 점입니다.
만약, 여러 인수를 받는 함수로 구현하고 싶다면, 단일 입력을 받아들이고 이를 여러 인수로 풀어내는 래퍼를 작성해야 합니다.
```
Copyfromoperatorimportitemgetterfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnableLambdafromlangchain_core.output_parsersimportStrOutputParserfromlangchain_openaiimportChatOpenAIdeflength_function(text):# 텍스트의 길이를 반환하는 함수returnlen(text)def_multiple_length_function(text1, text2):# 두 텍스트의 길이를 곱하는 함수returnlen(text1) *len(text2)defmultiple_length_function(# 2개 인자를 받는 함수로 연결하는 wrapper 함수_dict,):# 딕셔너리에서 ""text1""과 ""text2""의 길이를 곱하는 함수return_multiple_length_function(_dict[""text1""], _dict[""text2""])# 프롬프트 템플릿 생성prompt = ChatPromptTemplate.from_template(""what is {a} + {b}?"")# ChatOpenAI 모델 초기화model = ChatOpenAI()# 프롬프트와 모델을 연결하여 체인 생성chain1 = prompt | model# 체인 구성chain = (
    {""a"": itemgetter(""input_1"") | RunnableLambda(length_function),""b"": {""text1"": itemgetter(""input_1""),""text2"": itemgetter(""input_2"")}
        | RunnableLambda(multiple_length_function),
    }
    | prompt
    | model
    | StrOutputParser()
)
```
chain 을 실행하여 결과를 확인합니다.
```
Copy# 주어진 인자들로 체인을 실행합니다.chain.invoke({""input_1"":""bar"",""input_2"":""gah""})
```
```
'3 + 9 = 12'
```
## Runnable Config 인자로 활용
RunnableLambda는 선택적으로RunnableConfig를 수용할 수 있습니다.
이를 통해 콜백, 태그 및 기타 구성 정보를 중첩된 실행에 전달할 수 있습니다.
```
Copyfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.runnablesimportRunnableConfigimportjsondefparse_or_fix(text:str, config: RunnableConfig):# 다음 텍스트를 수정하는 프롬프트 템플릿을 생성합니다.fixing_chain = (
        ChatPromptTemplate.from_template(""Fix the following text:\n\ntext\n{input}\n\nError: {error}"""" Don't narrate, just respond with the fixed data."")
        | ChatOpenAI()
        | StrOutputParser()
    )# 최대 3번 시도합니다.for_inrange(3):try:# JSON 형식으로 텍스트를 파싱합니다.returnjson.loads(text)exceptExceptionase:# 파싱 중 오류가 발생하면 수정 체인을 호출하여 텍스트를 수정합니다.text = fixing_chain.invoke({""input"": text,""error"": e}, config)print(f""config:{config}"")# 파싱에 실패한 경우 ""Failed to parse"" 문자열을 반환합니다.return""Failed to parse""
```
```
Copyfromlangchain.callbacksimportget_openai_callbackwithget_openai_callback()ascb:# RunnableLambda를 사용하여 parse_or_fix 함수를 호출합니다.output = RunnableLambda(parse_or_fix).invoke(input=""{foo:: bar}"",
        config={""tags"": [""my-tag""],""callbacks"": [cb]},# config 를 전달합니다.)# 수정한 결과를 출력합니다.print(f""\n\n수정한결과:\n{output}"")
```
```
config: {'tags': ['my-tag'], 'metadata': {}, 'callbacks':, 'recursion_limit': 25}


수정한결과:
{'foo': 'bar'}
```"
04. RunnableBranch: 라우팅(Routing),https://wikidocs.net/235882,"# 04. RunnableBranch: 라우팅(Routing)

RunnableBranch는 입력에 따라 동적으로 로직을 라우팅할 수 있는 강력한 도구입니다. 이를 통해 개발자는입력 데이터의 특성에 기반하여 다양한 처리 경로를 유연하게 정의할 수 있습니다.
RunnableBranch는 복잡한 의사 결정 트리를 간단하고 직관적인 방식으로 구현할 수 있도록 도와줍니다. 이는 코드의 가독성과 유지보수성을 크게 향상시키며, 로직의 모듈화와 재사용성을 촉진합니다.
또한,RunnableBranch는 런타임에 동적으로 분기 조건을 평가하고 적절한 처리 루틴을 선택할 수 있어, 시스템의 적응력과 확장성을 높여줍니다.
이러한 특징들로 인해 RunnableBranch는 다양한 도메인에서 활용될 수 있으며, 특히 입력 데이터의 다양성과 변동성이 큰 애플리케이션 개발에 매우 유용합니다.RunnableBranch를 효과적으로 활용하면 코드의 복잡성을 줄이고, 시스템의 유연성과 성능을 향상시킬 수 있습니다.
## 입력에 따른 동적 로직 라우팅
LangChain Expression Language에서 라우팅을 수행하는 방법에 대해 다룹니다.
라우팅을 통해 이전 단계의 출력이 다음 단계를 정의하는 비결정적 체인을 생성할 수 있습니다. 라우팅은 LLM과의 상호 작용에 구조와 일관성을 제공하는 데 도움이 됩니다.
라우팅을 수행하는 방법에는 두 가지가 있습니다.
RunnableLambda에서 조건부로 실행 가능한 객체를 반환 (권장)
RunnableBranch사용
두 가지 방법 모두 첫 번째 단계에서 입력 질문을LangChain,Anthropic또는Other에 대한 것으로 분류한 다음, 해당 프롬프트 체인으로 라우팅하는 두 단계 시퀀스를 사용하여 설명하겠습니다.
## 간단한 예시
먼저, 들어오는 질문이수학,과학, 또는기타중 하나로 분류하는 Chain을 생성하겠습니다.
```
Copyfromlangchain_openaiimportChatOpenAIfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportPromptTemplate

prompt = PromptTemplate.from_template(""""""주어진 사용자 질문을 `수학`, `과학`, 또는 `기타` 중 하나로 분류하세요. 한 단어 이상으로 응답하지 마세요.

<question>
{question}
</question>

Classification:"""""")# 체인을 생성합니다.chain = (
    prompt
    | ChatOpenAI(model=""gpt-4-turbo-preview"")
    | StrOutputParser()# 문자열 출력 파서를 사용합니다.)
```
생성한 chain을 사용하여 질문을 분류합니다.
```
Copy# 질문을 입력하여 체인을 호출합니다.chain.invoke({""question"":""2+2 는 무엇인가요?""})
```
```
'수학'
```
```
Copy# 질문을 입력하여 체인을 호출합니다.chain.invoke({""question"":""작용 반작용의 법칙은 무엇인가요?""})
```
```
'과학'
```
```
Copy# 질문을 입력하여 체인을 호출합니다.chain.invoke({""question"":""Google은 어떤 회사인가요?""})
```
```
'기타'
```
이제 세 개의 하위 체인을 생성해 보겠습니다.
```
Copymath_chain = (
    PromptTemplate.from_template(""""""You are an expert in math. \
Always answer questions starting with ""깨봉선생님께서 말씀하시기를.."". \
Respond to the following question:

Question: {question}
Answer:"""""")# OpenAI의 LLM을 사용합니다.| ChatOpenAI(model=""gpt-4-turbo-preview"")
)

science_chain = (
    PromptTemplate.from_template(""""""You are an expert in science. \
Always answer questions starting with ""아이작 뉴턴 선생님께서 말씀하시기를.."". \
Respond to the following question:

Question: {question}
Answer:"""""")# OpenAI의 LLM을 사용합니다.| ChatOpenAI(model=""gpt-4-turbo-preview"")
)

general_chain = (
    PromptTemplate.from_template(""""""Respond to the following question concisely:

Question: {question}
Answer:"""""")# OpenAI의 LLM을 사용합니다.| ChatOpenAI(model=""gpt-4-turbo-preview"")
)
```
## 사용자 정의 함수 사용하기 (권장)
서로 다른 출력 간의 라우팅을 위해사용자 정의 함수를 활용할 수도 있습니다.
다음은 그 예시입니다.
```
Copydefroute(info):# 주제에 ""수학""이 포함되어 있는 경우if""수학""ininfo[""topic""].lower():# datascience_chain을 반환returnmath_chain# 주제에 ""과학""이 포함되어 있는 경우elif""과학""ininfo[""topic""].lower():# art_chain을 반환returnscience_chain# 그 외의 경우else:# general_chain을 반환returngeneral_chain
```
```
Copyfromlangchain_core.runnablesimportRunnableLambda

full_chain = (
    {""topic"": chain,""question"":lambdax: x[""question""]}
    | RunnableLambda(# 경로를 지정하는 함수를 인자로 전달합니다.route
    )
    | StrOutputParser()
)
```
```
Copy# 수학과 관련된 질문을 입력하여 체인을 호출합니다.full_chain.invoke({""question"":""미적분의 개념에 대해 말씀해 주세요.""})
```
```
'깨봉선생님께서 말씀하시기를, 미적분은 크게 두 가지 주요 개념인 미분과 적분으로 나뉩니다. 미분은 변화율을 다루는 것으로, 어떤 함수가 주어졌을 때 그 함수의 순간적인 변화율을 계산하는 과정입니다. 예를 들어, 속도는 거리에 대한 시간의 변화율로 볼 수 있습니다. 반면, 적분은 주어진 구간에서 함수의 전체 변화량을 구하는 것으로, 미분과는 반대의 과정입니다. 적분을 통해 넓이나 부피 같은 것들을 계산할 수 있습니다. 간단히 말해, 미적분은 변화하는 세상을 수학적으로 이해하는 강력한 도구입니다.'
```
```
Copy# 과학과 관련된 질문을 입력하여 체인을 호출합니다.full_chain.invoke({""question"":""중력은 어떻게 작용하나요?""})
```
```
'아이작 뉴턴 선생님께서 말씀하시기를, 중력은 모든 질량을 가진 물체들 사이에 작용하는 힘이며, 이 힘은 두 물체의 질량에 비례하고 두 물체 사이의 거리의 제곱에 반비례합니다. 즉, 물체가 더 무겁거나 서로 가까울수록 중력의 힘이 강해집니다. 이 원리는 뉴턴의 만유인력 법칙으로 알려져 있으며, 지구에서 사과가 떨어지는 것부터 달이 지구 주변을 도는 것, 심지어는 태양계의 행성들이 태양 주변을 도는 것까지 모든 중력 현상을 설명합니다.'
```
```
Copy# 기타 질문을 입력하여 체인을 호출합니다.full_chain.invoke({""question"":""RAG(Retrieval Augmented Generation)은 무엇인가요?""})
```
```
'RAG(Retrieval Augmented Generation)는 인공지능 분야에서 사용되는 기술로, 질문에 대한 답변을 생성하기 전에 관련 정보를 검색하여 찾아온 후, 그 정보를 바탕으로 답변을 생성하는 방식입니다. 기존의 단순한 문답 생성 방식보다 더 정확하고 풍부한 정보를 제공할 수 있습니다.'
```
## RunnableBranch 사용하기
RunnableBranch는 입력값에 따라 실행할 조건과 Runnable을 정의할 수 있는 특별한 유형의 Runnable입니다.
다만, 위에서 설명한 사용자 정의 함수로 구현할 수 없는 기능을 제공하지는 않으므로, 사용자 정의 함수를 사용하는 것이 좋습니다.
문법
- RunnableBranch는 (조건, Runnable) 쌍의 리스트와 기본 Runnable로 초기화됩니다.
- 호출 시 전달된 입력값을 각 조건에 전달하여 분기를 선택합니다.
- True로 평가되는 첫 번째 조건을 선택하고, 해당 조건에 해당하는 Runnable을 입력값과 함께 실행합니다.
- 제공된 조건과 일치하는 것이 없으면 기본 Runnable을 실행합니다.
RunnableBranch는 (조건, Runnable) 쌍의 리스트와 기본 Runnable로 초기화됩니다.
호출 시 전달된 입력값을 각 조건에 전달하여 분기를 선택합니다.
True로 평가되는 첫 번째 조건을 선택하고, 해당 조건에 해당하는 Runnable을 입력값과 함께 실행합니다.
제공된 조건과 일치하는 것이 없으면 기본 Runnable을 실행합니다.
```
Copyfromlangchain_core.runnablesimportRunnableBranch

branch = RunnableBranch(# 주제에 ""수학""이 포함되어 있는지 확인하고, 포함되어 있다면 math_chain을 실행합니다.(lambdax:""수학""inx[""topic""].lower(), math_chain),# 주제에 ""과학""이 포함되어 있는지 확인하고, 포함되어 있다면 science_chain을 실행합니다.(lambdax:""과학""inx[""topic""].lower(), science_chain),# 위의 조건에 해당하지 않는 경우 general_chain을 실행합니다.general_chain,
)# 주제와 질문을 입력받아 branch를 실행하는 전체 체인을 정의합니다.full_chain = {""topic"": chain,""question"":lambdax: x[""question""]} | branch | StrOutputParser()
```
```
Copy# 질문을 입력하여 전체 체인을 실행합니다.full_chain.invoke({""question"":""미적분의 개념에 대해 말씀해 주세요.""})
```
```
'깨봉선생님께서 말씀하시기를, 미적분은 크게 두 가지 주요 개념, 즉 미분과 적분으로 나뉩니다. 미분은 주어진 함수의 순간 변화율을 측정하는 과정이며, 기울기나 변화의 속도를 알아내는 데 사용됩니다. 즉, 어떤 함수가 변화할 때 그 변화의 정도를 수학적으로 표현한 것이죠. 반면, 적분은 분할된 작은 부분들의 합을 구하여 전체의 양을 찾는 과정입니다. 이는 넓이, 부피, 그리고 다른 물리적 양들을 계산하는 데 사용되죠. 따라서 미적분은 변화하는 상황을 이해하고, 그로 인해 발생하는 전체적인 결과를 예측하는 데 매우 중요한 역할을 합니다. 미적분의 이러한 개념들은 과학, 공학, 경제학 등 다양한 분야에서 광범위하게 적용되며, 복잡한 문제들을 해결하는 데 필수적인 도구로 사용됩니다.'
```
```
Copy# 질문을 입력하여 전체 체인을 실행합니다.full_chain.invoke({""question"":""중력 가속도는 어떻게 계산하나요?""})
```
```
'아이작 뉴턴 선생님께서 말씀하시기를, 중력 가속도는 물체가 자유낙하할 때 받는 가속도로, 지구 표면에서의 평균값은 대략 9.81m/s²입니다. 이는 뉴턴의 만유인력 법칙을 바탕으로 계산할 수 있으며, 특정한 위치에서의 중력 가속도를 계산하려면 \\(g = \\frac{G \\cdot M}{r^2}\\) 공식을 사용합니다. 여기서 \\(G\\)는 중력 상수(약 6.674×10^-11 N(m/kg)^2), \\(M\\)은 중심 물체의 질량(예를 들어 지구의 경우 약 5.972×10^24kg), 그리고 \\(r\\)은 중심 물체의 중심으로부터의 거리입니다. 이 공식을 통해 우리는 지구나 다른 천체의 표면 또는 그 근처에서의 중력 가속도를 계산할 수 있습니다.'
```
```
Copy# 질문을 입력하여 전체 체인을 실행합니다.full_chain.invoke({""question"":""RAG(Retrieval Augmented Generation)은 무엇인가요?""})
```
```
'RAG(Retrieval Augmented Generation)는 기계학습 분야에서 사용되는 기술로, 정보 검색과 생성 모델을 결합하여 특정 질문에 대한 답변을 생성합니다. 이 방법은 먼저 관련 정보를 데이터베이스에서 검색한 다음, 이 정보를 바탕으로 자연어 처리(NLP) 모델이 정확하고 관련성 높은 답변을 생성합니다.'
```"
05. RunnableParallel: 병렬 처리,https://wikidocs.net/235883,"# 05. RunnableParallel: 병렬 처리

## 입력 및 출력 조작
RunnableParallel은 시퀀스 내에서 하나의Runnable의 출력을 다음Runnable의 입력 형식에 맞게 조작하는 데 유용하게 사용될 수 있습니다.
여기서 prompt에 대한 입력은 ""context""와 ""question""이라는 키를 가진 map 형태로 예상됩니다.
사용자 입력은 단순히 질문 내용입니다. 따라서 retriever를 사용하여 컨텍스트를 가져오고, 사용자 입력을 ""question"" 키 아래에 전달해야 합니다.
```
Copy# 필요한 모듈 설치%pip install -qU langchain langchain-openai
```
```
Copyfromlangchain_community.vectorstoresimportFAISSfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_openaiimportChatOpenAI, OpenAIEmbeddings# 텍스트로부터 FAISS 벡터 저장소를 생성합니다.vectorstore = FAISS.from_texts(
    [""Teddy is an AI engineer who loves programming!""], embedding=OpenAIEmbeddings()
)# 벡터 저장소를 검색기로 사용합니다.retriever = vectorstore.as_retriever()# 템플릿을 정의합니다.template =""""""Answer the question based only on the following context:
{context}

Question: {question}
""""""# 템플릿으로부터 채팅 프롬프트를 생성합니다.prompt = ChatPromptTemplate.from_template(template)# ChatOpenAI 모델을 초기화합니다.model = ChatOpenAI()# 검색 체인을 구성합니다.retrieval_chain = (
    {""context"": retriever,""question"": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.retrieval_chain.invoke(""What is Teddy's occupation?"")
```
```
""Teddy's occupation is an AI engineer.""
```
다른Runnable과 함께RunnableParallel을 구성할 때, 유형 변환이 자동으로 처리되므로RunnableParallel클래스에서 입력으로 주입되는 dict 입력을 별도 래핑할 필요도 없다는 점에 유의하세요.
아래의 3가지 방식은 모두 동일하게 처리합니다.
```
Copy# 자체 RunnableParallel 로 래핑됨1.{""context"": retriever,""question"": RunnablePassthrough()}2.RunnableParallel({""context"": retriever,""question"": RunnablePassthrough()})3.RunnableParallel(context=retriever, question=RunnablePassthrough())
```
## itemgetter를 단축어로 사용하기
RunnableParallel과 결합할 때 Python의itemgetter를 단축어로 사용하여 map에서 데이터를 추출할 수 있습니다.
- [참고] itemgetter에 대한 자세한 정보는Python Documentation에서 확인할 수 있습니다.
[참고] itemgetter에 대한 자세한 정보는Python Documentation에서 확인할 수 있습니다.
아래 예제에서는itemgetter를 사용하여 map에서 특정 키를 추출합니다.
```
Copyfromoperatorimportitemgetterfromlangchain_community.vectorstoresimportFAISSfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_openaiimportChatOpenAI, OpenAIEmbeddings# 텍스트로부터 FAISS 벡터 저장소를 생성합니다.vectorstore = FAISS.from_texts(
    [""Teddy is an AI engineer who loves programming!""], embedding=OpenAIEmbeddings()
)# 벡터 저장소를 검색기로 사용합니다.retriever = vectorstore.as_retriever()# 템플릿을 정의합니다.template =""""""Answer the question based only on the following context:
{context}

Question: {question}

Answer in the following language: {language}
""""""# 템플릿으로부터 채팅 프롬프트를 생성합니다.prompt = ChatPromptTemplate.from_template(template)# 체인을 구성합니다.chain = (
    {""context"": itemgetter(""question"") | retriever,""question"": itemgetter(""question""),""language"": itemgetter(""language""),
    }
    | prompt
    | ChatOpenAI()
    | StrOutputParser()
)# 체인을 호출하여 질문에 답변합니다.chain.invoke({""question"":""What is Teddy's occupation?"",""language"":""Korean""})
```
```
'테디의 직업은 AI 엔지니어입니다.'
```
## 병렬처리를 단계별로 이해
RunnableParallel(또는RunnableMap)을 사용하면 여러Runnable을 병렬로 실행하고, 이러한Runnable의 출력을 맵(map)으로 반환하는 것이 쉬워집니다.
```
Copyfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnableParallelfromlangchain_openaiimportChatOpenAI

model = ChatOpenAI()# ChatOpenAI 모델을 초기화합니다.# 수도를 묻는 질문에 대한 체인을 정의합니다.capital_chain = (
    ChatPromptTemplate.from_template(""{country} 의 수도는 어디입니까?"")
    | model
    | StrOutputParser()
)# 면적을 묻는 질문에 대한 체인을 정의합니다.area_chain = (
    ChatPromptTemplate.from_template(""{country} 의 면적은 얼마입니까?"")
    | model
    | StrOutputParser()
)# capital_chain, area_chain 을 병렬로 실행할 수 있는 RunnableParallel 객체를 생성합니다.map_chain = RunnableParallel(capital=capital_chain, area=area_chain)# map_chain을 호출하여 대한민국의 수도와 면적을 묻습니다.map_chain.invoke({""country"":""대한민국""})
```
```
{'capital': '서울입니다.', 'area': '대한민국의 면적은 약 100,363km² 입니다.'}
```
아래와 같이 chain 별로 입력 템플릿의 변수가 달라도 상관없이 실행 가능합니다.
```
Copy# 수도를 묻는 질문에 대한 체인을 정의합니다.capital_chain2 = (
    ChatPromptTemplate.from_template(""{country1} 의 수도는 어디입니까?"")
    | model
    | StrOutputParser()
)# 면적을 묻는 질문에 대한 체인을 정의합니다.area_chain2 = (
    ChatPromptTemplate.from_template(""{country2} 의 면적은 얼마입니까?"")
    | model
    | StrOutputParser()
)# capital_chain, area_chain 을 병렬로 실행할 수 있는 RunnableParallel 객체를 생성합니다.map_chain = RunnableParallel(capital=capital_chain2, area=area_chain2)# map_chain을 호출합니다. 이때 각각의 key에 대한 value를 전달합니다.map_chain.invoke({""country1"":""대한민국"",""country2"":""미국""})
```
```
{'capital': '대한민국의 수도는 서울입니다.', 'area': '미국의 면적은 9,826,675 km² 입니다.'}
```
## 병렬 처리
RunnableParallel은 맵에 있는 각Runnable이 병렬로 실행되기 때문에 독립적인 프로세스를 병렬로 실행하는 데에도 유용합니다.
예를 들어, 앞서 살펴본area_chain,capital_chain,map_chain은map_chain이 다른 두 체인을 모두 실행함에도 불구하고거의 동일한 실행 시간을 가지는 것을 확인할 수 있습니다.
```
Copy%%timeit# 면저을 묻는 체인을 호출하고 실행 시간을 측정합니다.area_chain.invoke({""country"":""대한민국""})
```
```
946 ms ± 242 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
```
Copy%%timeit# 수도를 묻는 체인을 호출하고 실행 시간을 측정합니다.capital_chain.invoke({""country"":""대한민국""})
```
```
895 ms ± 218 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
```
Copy%%timeit# Parallel 하게 구성된 체인을 호출하고 실행 시간을 측정합니다.map_chain.invoke({""country"":""대한민국""})
```
```
967 ms ± 202 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```"
"06. configurable_fields, configurable_alternatives",https://wikidocs.net/235704,"# 06. configurable_fields, configurable_alternatives

개발자는 종종 여러 가지 다양한 방식으로 작업을 수행하는 과정이나 실험내용을 최종 사용자(End User)에게 노출하고 싶어 할 때가 있습니다.
이러한 경험을 최대한 쉽게 만들기 위하여 다음의 2가지 메서를 활용할 수 있습니다.
- 첫째,configurable_fields메서드입니다. 이 메서드를 통해 실행 가능한 객체의 특정 필드를 구성할 수 있습니다.
- 둘째,configurable_alternatives메서드입니다. 이 메서드를 사용하면 런타임 중에 설정할 수 있는 특정 실행 가능한 객체에 대한 대안을 나열할 수 있습니다.
첫째,configurable_fields메서드입니다. 이 메서드를 통해 실행 가능한 객체의 특정 필드를 구성할 수 있습니다.
첫째,configurable_fields메서드입니다. 이 메서드를 통해 실행 가능한 객체의 특정 필드를 구성할 수 있습니다.
둘째,configurable_alternatives메서드입니다. 이 메서드를 사용하면 런타임 중에 설정할 수 있는 특정 실행 가능한 객체에 대한 대안을 나열할 수 있습니다.
둘째,configurable_alternatives메서드입니다. 이 메서드를 사용하면 런타임 중에 설정할 수 있는 특정 실행 가능한 객체에 대한 대안을 나열할 수 있습니다.
## configurable_fields
configurable_fields는 시스템의 설정 값을 정의하는 필드를 의미합니다. 이러한 설정 필드는 시스템의 동작을 제어하고 사용자의 요구사항에 맞게 시스템을 구성하는 데 사용됩니다.configurable_fields를 통해 사용자는 시스템의 다양한 측면을 조정할 수 있으며, 이를 통해 시스템의 성능과 기능을 최적화할 수 있습니다.
일반적으로configurable_fields는 설정 파일, 환경 변수, 또는 사용자 인터페이스를 통해 제공되며, 시스템 관리자나 개발자가 이를 관리합니다.
configurable_fields의 설정할 수 있는 파라미터의 예시로는 데이터베이스 연결 정보, 로깅 설정, 보안 옵션, 성능 튜닝 매개변수 등이 있습니다. 이러한 필드를 적절히 설정함으로써 시스템을 효과적으로 구성하고 관리할 수 있습니다.
## 동적 속성 지정
ChatOpenAI을 사용할 때, 우리는model_name와 같은 설정을 조정할 수 있습니다.
model_name은 GPT 의 버전을 명시할 때 사용하는 속성입니다. 예를 들어,gpt-3.5-turbo,gpt-4등을 설정하여 모델을 선택할 수 있습니다.
만약, 고정된model_name이 아닌 동적으로 모델을 지정하고 싶을 때는 다음과 같이ConfigurableField를 활용하여 동적으로 설정할 수 있는 속성 값으로 변환할 수 있습니다.
```
Copy%pip install -qU langchain langchain-openai
```
configurable_fields메서드를 사용하여model_name속성을 동적 구성 가능한 필드로 지정합니다.
```
Copyfromlangchain.promptsimportPromptTemplatefromlangchain_core.runnablesimportConfigurableFieldfromlangchain_openaiimportChatOpenAI

model = ChatOpenAI(temperature=0).configurable_fields(
    model_name=ConfigurableField(# model_name 은 원래 ChatOpenAI 의 필드입니다.id=""gpt_version"",# model_name 의 id 를 설정합니다.name=""Version of GPT"",# model_name 의 이름을 설정합니다.# model_name 의 설명을 설정합니다.description=""Official model name of GPTs. ex) gpt-3.5-turbo, gpt-4"",
    )
)
```
model.invoke()호출시config={""configurable"": {""키"": ""값""}}형식으로 동적 지정할 수 있습니다.
```
Copymodel.invoke(""대한민국의 수도는 어디야?"",# gpt_version 을 gpt-3.5-turbo 로 설정합니다.config={""configurable"": {""gpt_version"":""gpt-3.5-turbo""}},
)
```
```
AIMessage(content='대한민국의 수도는 서울이다.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 22, 'total_tokens': 38}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
이번에는gpt-4모델을 사용해보겠습니다. 출력에 바뀐 모델을 확인하세요.
```
Copymodel.invoke(# gpt_version 을 gpt-4 로 설정합니다.""대한민국의 수도는 어디야?"",
    config={""configurable"": {""gpt_version"":""gpt-4""}},
)
```
```
AIMessage(content='대한민국의 수도는 서울입니다.', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 22, 'total_tokens': 37}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})
```
model객체의with_config()메서드를 사용하여configurable매개변수를 설정할 수도 있습니다. 이전과 동작하는 방식은 동일합니다.
```
Copymodel.with_config(configurable={""gpt_version"":""gpt-4-turbo-preview""}).invoke(""대한민국의 수도는 어디야?"")
```
```
AIMessage(content='대한민국의 수도는 서울입니다.', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 22, 'total_tokens': 37}, 'model_name': 'gpt-4-turbo-preview', 'system_fingerprint': 'fp_a7daf7c51e', 'finish_reason': 'stop', 'logprobs': None})
```
또한 이 함수를 체인의 일부로 사용할 때에도 동일한 방식으로 활용할 수 있습니다.
```
Copy# 템플릿에서 프롬프트 템플릿을 생성합니다.prompt = PromptTemplate.from_template(""{x} 보다 큰 위의 난수를 선택합니다."")
chain = (
    prompt | model
)# 프롬프트와 모델을 연결하여 체인을 생성합니다. 프롬프트의 출력이 모델의 입력으로 전달됩니다.
```
```
Copychain.invoke({""x"":0})# 체인을 호출하고 입력 변수 ""x""에 0을 전달합니다.
```
```
AIMessage(content='5', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 21, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
```
Copy# 체인을 호출할 때 설정을 지정하여 체인을 호출할 수 있습니다.chain.with_config(configurable={""gpt_version"":""gpt-4""}).invoke({""x"":0})
```
```
AIMessage(content='죄송합니다만, 제가 난수를 생성하거나 선택하는 기능은 없습니다. 이 기능은 프로그래밍 언어나 소프트웨어를 사용해야 합니다. 예를 들어, 파이썬에서는 random 모듈을 사용하여 난수를 생성할 수 있습니다.', response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 21, 'total_tokens': 105}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})
```
## HubRunnable: LangChain Hub의 설정 변경
HubRunnable을 사용하면 Hub 에 등록된 프롬프트의 전환을 용이하게 합니다.
```
Copyfromlangchain.runnables.hubimportHubRunnable

prompt = HubRunnable(""rlm/rag-prompt"").configurable_fields(# 소유자 저장소 커밋을 설정하는 ConfigurableFieldowner_repo_commit=ConfigurableField(# 필드의 IDid=""hub_commit"",# 필드의 이름name=""Hub Commit"",# 필드에 대한 설명description=""The Hub commit to pull from"",
    )
)
```
별도의with_config지정 없이prompt.invoke()메서드를 호출하면 처음 설정한""rlm/rag-prompt""hub 에 등록된 프롬프트를 pull 하여 가져옵니다.
```
Copy# prompt 객체의 invoke 메서드를 호출하여 ""question""과 ""context"" 매개변수를 전달합니다.prompt.invoke({""question"":""Hello"",""context"":""World""})
```
```
ChatPromptValue(messages=[HumanMessage(content=""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Hello \nContext: World \nAnswer:"")])
```
```
Copyprompt.with_config(# hub_commit 을 teddynote/simple-summary-korean 으로 설정합니다.configurable={""hub_commit"":""teddynote/simple-summary-korean""}
).invoke({""context"":""Hello""})
```
```
ChatPromptValue(messages=[HumanMessage(content='주어진 내용을 바탕으로 다음 문장을 요약하세요. 답변은 반드시 한글로 작성하세요\n\nCONTEXT: Hello\n\nSUMMARY:')])
```
## Configurable Alternatives: Runnable 객체 자체의 대안 설정
런타임에 설정할 수 있는 Runnable 에 대한 대안을 구성합니다.
구성 가능한 대안들
ChatAnthropic의 구성 가능한 언어 모델은 다양한 작업과 컨텍스트에 적용할 수 있는 유연성을 제공합니다.
모델의 동작을 제어하기 위해 다음과 같은 구성 옵션을 사용할 수 있습니다.
- model: 사용할 기본 언어 모델을 지정합니다. 예를 들어, ""claude-v1"", ""claude-instant-v1"" 등이 있습니다.
- temperature: 0에서 1 사이의 값으로, 샘플링의 무작위성을 제어합니다. 값이 낮을수록 더 결정적이고 반복적인 출력이 생성되며, 값이 높을수록 더 다양하고 창의적인 출력이 생성됩니다.
- max_tokens_to_sample: 생성할 최대 토큰 수를 지정합니다.
- stop_sequences: 지정된 문자열 시퀀스 중 하나가 생성되면 생성을 중지합니다.
- top_k: 샘플링에 고려할 최상위 확률의 토큰 수를 지정합니다.
- top_p: 누적 확률 질량의 상위 부분에서 토큰을 샘플링합니다.
model: 사용할 기본 언어 모델을 지정합니다. 예를 들어, ""claude-v1"", ""claude-instant-v1"" 등이 있습니다.
model: 사용할 기본 언어 모델을 지정합니다. 예를 들어, ""claude-v1"", ""claude-instant-v1"" 등이 있습니다.
temperature: 0에서 1 사이의 값으로, 샘플링의 무작위성을 제어합니다. 값이 낮을수록 더 결정적이고 반복적인 출력이 생성되며, 값이 높을수록 더 다양하고 창의적인 출력이 생성됩니다.
temperature: 0에서 1 사이의 값으로, 샘플링의 무작위성을 제어합니다. 값이 낮을수록 더 결정적이고 반복적인 출력이 생성되며, 값이 높을수록 더 다양하고 창의적인 출력이 생성됩니다.
max_tokens_to_sample: 생성할 최대 토큰 수를 지정합니다.
max_tokens_to_sample: 생성할 최대 토큰 수를 지정합니다.
stop_sequences: 지정된 문자열 시퀀스 중 하나가 생성되면 생성을 중지합니다.
stop_sequences: 지정된 문자열 시퀀스 중 하나가 생성되면 생성을 중지합니다.
top_k: 샘플링에 고려할 최상위 확률의 토큰 수를 지정합니다.
top_k: 샘플링에 고려할 최상위 확률의 토큰 수를 지정합니다.
top_p: 누적 확률 질량의 상위 부분에서 토큰을 샘플링합니다.
top_p: 누적 확률 질량의 상위 부분에서 토큰을 샘플링합니다.
이러한 구성 옵션을 조정하여 모델의 동작을 작업에 맞게 최적화할 수 있습니다.
LLM(Large Language Model)을 활용하여 이를 수행하는 방법을 살펴보겠습니다.
[참고]
- ChatAnthropic을 사용하기 위하여 API KEY를 발급받아 설정해야합니다.
- 아래 주석을 해제하고 API KEY를 설정하거나,.env파일에 설정합니다.
ChatAnthropic을 사용하기 위하여 API KEY를 발급받아 설정해야합니다.
아래 주석을 해제하고 API KEY를 설정하거나,.env파일에 설정합니다.
```
Copy# import os# os.environ[""ANTHROPIC_API_KEY""] = ""ANTHROPIC API KEY를 입력합니다.""
```
```
Copyfromlangchain.promptsimportPromptTemplatefromlangchain_community.chat_modelsimportChatAnthropicfromlangchain_core.runnablesimportConfigurableFieldfromlangchain_openaiimportChatOpenAI

llm = ChatAnthropic(temperature=0).configurable_alternatives(# 이 필드에 id를 부여합니다.# 최종 실행 가능한 객체를 구성할 때, 이 id를 사용하여 이 필드를 구성할 수 있습니다.ConfigurableField(id=""llm""),# 기본 키를 설정합니다.# 이 키를 지정하면 위에서 초기화된 기본 LLM(ChatAnthropic)이 사용됩니다.default_key=""anthropic"",# 'openai'라는 이름의 새 옵션을 추가하며, 이는 `ChatOpenAI()`와 동일합니다.openai=ChatOpenAI(),# 'gpt4'라는 이름의 새 옵션을 추가하며, 이는 `ChatOpenAI(model=""gpt-4"")`와 동일합니다.gpt4=ChatOpenAI(model=""gpt-4""),# 여기에 더 많은 구성 옵션을 추가할 수 있습니다.)
prompt = PromptTemplate.from_template(""{topic} 에 대해 간단히 설명해주세요."")
chain = prompt | llm
```
chain.invoke()메서드를 기본 LLM 인ChatAnthropic을 활용한 체인을 호출합니다.
```
Copy# Anthropic을 기본으로 호출합니다.chain.invoke({""topic"":""뉴진스""})
```
```
AIMessage(content="" 뉴진스(New Jeans)는 2022년 데뷔한 걸그룹입니다.\n\n- 소속사는 하이브엔터테인먼트\n- 멤버는 민지, 해린, 다니엘, 해원, 혜인 총 5명\n- 2022년 8월 1일 데뷔 싱글 앨범 'New Jeans'로 데뷔\n- 타이틀 곡 'Attention'이 큰 인기를 끌며 차트 1위를 기록\n- 10대 후반~20대 초반 멤버로 구성되어 있어 MZ 세대의 지지를 받고 있음\n- 트렌디하고 개성 있는 패션과 음악 스타일이 특징\n\n뉴진스는 청량하고 "")
```
chain.with_config(configurable={""llm"": ""모델""})를 사용하여 사용할llm으로 다른 모델을 지정할 수 있습니다.
```
Copy# 체인의 설정을 변경하여 호출합니다.chain.with_config(configurable={""llm"":""openai""}).invoke({""topic"":""뉴진스""})
```
```
AIMessage(content=""뉴진스(New Jin's)는 한국의 한 혼밥 레스토랑 체인으로, 한 그룹이 함께 앉아 식사를 즐길 수 있는 테이블을 제공하는 것이 특징입니다. 메뉴는 주로 한식과 중식을 조합한 요리로 구성되어 있으며, 가격대도 합리적인 편이라 인기가 있는 곳입니다. 식사를 하면서 소셜 미디어에 사진을 찍는 것을 즐기는 사람들에게 인기가 있습니다."", response_metadata={'token_usage': {'completion_tokens': 168, 'prompt_tokens': 27, 'total_tokens': 195}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
체인의 설정을 변경하여 사용할 언어 모델을gpt4로 지정합니다.
```
Copy# 체인의 설정을 변경하여 호출합니다.chain.with_config(configurable={""llm"":""gpt4""}).invoke({""topic"":""뉴진스""})
```
```
AIMessage(content='뉴진스는 대한민국의 의료기기 제조 기업입니다. 주로 의료 영상 진단기기를 제조 및 판매하며, 특히 초음파 진단기기에 특화되어 있습니다. 또한, 의료용 소프트웨어 개발과 서비스 제공 등 다양한 분야에서 활동하고 있습니다.', response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 27, 'total_tokens': 148}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})
```
체인의 설정을 변경하여 사용할 언어 모델을anthropic로 지정합니다.
```
Copy# 체인의 설정을 변경하여 호출합니다.chain.with_config(configurable={""llm"":""anthropic""}).invoke({""topic"":""뉴진스""})
```
```
AIMessage(content="" 뉴진스(New Jeans)는 2022년 데뷔한 걸그룹입니다.\n\n- 소속사는 하이브엔터테인먼트\n- 멤버는 민지, 해린, 다니엘, 해원, 혜인 총 5명\n- 2022년 8월 1일 데뷔 싱글 앨범 'New Jeans'로 데뷔\n- 타이틀 곡 'Attention'이 큰 인기를 끌며 차트 1위를 기록\n- 10대 후반~20대 초반 멤버로 구성된 틴크러시 콘셉트\n- 트렌디하고 개성 있는 패션과 음악 스타일이 특징\n\n뉴진스는 청량하고 트렌디"")
```
## 프롬프트의 대안 설정 방법
프롬프트도 이전의 LLM 대안 설정 방법과 유사한 작업을 수행할 수 있습니다.
```
Copy# 언어 모델을 초기화하고 temperature를 0으로 설정합니다.llm = ChatOpenAI(temperature=0)

prompt = PromptTemplate.from_template(""{country} 의 수도는 어디야?""# 기본 프롬프트 템플릿).configurable_alternatives(# 이 필드에 id를 부여합니다.ConfigurableField(id=""prompt""),# 기본 키를 설정합니다.default_key=""capital"",# 'area'이라는 새로운 옵션을 추가합니다.area=PromptTemplate.from_template(""{country} 의 면적은 얼마야?""),# 'population'이라는 새로운 옵션을 추가합니다.population=PromptTemplate.from_template(""{country} 의 인구는 얼마야?""),# 'eng'이라는 새로운 옵션을 추가합니다.eng=PromptTemplate.from_template(""{input} 을 영어로 번역해주세요.""),# 여기에 더 많은 구성 옵션을 추가할 수 있습니다.)# 프롬프트와 언어 모델을 연결하여 체인을 생성합니다.chain = prompt | llm
```
아무런 설정 변경이 없다면 기본 프롬프트가 입력됩니다.
```
Copy# config 변경 없이 체인을 호출합니다.chain.invoke({""country"":""대한민국""})
```
```
AIMessage(content='대한민국의 수도는 서울이다.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 23, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
with_config로 다른 프롬프트를 호출합니다.
```
Copy# with_config 로 체인의 설정을 변경하여 호출합니다.chain.with_config(configurable={""prompt"":""area""}).invoke({""country"":""대한민국""})
```
```
AIMessage(content='대한민국의 총 면적은 약 100,363km² 입니다.', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 25, 'total_tokens': 50}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
```
Copy# with_config 로 체인의 설정을 변경하여 호출합니다.chain.with_config(configurable={""prompt"":""population""}).invoke({""country"":""대한민국""})
```
```
AIMessage(content='2021년 7월 기준 대한민국의 인구는 약 5천 130만 명입니다.', response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 24, 'total_tokens': 58}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
이번에는eng프롬프트를 사용하여 번역을 요청합니다. 이때 전달할 입력 변수는input입니다.
```
Copy# with_config 로 체인의 설정을 변경하여 호출합니다.chain.with_config(configurable={""prompt"":""eng""}).invoke({""input"":""사과는 맛있어!""})
```
```
AIMessage(content='Apples are delicious!', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 28, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
## 프롬프트 & LLM 모두 변경
프롬프트와 LLM을 사용하여 여러 가지 사항을 구성할 수 있습니다.
다음은 프롬프트와 LLM을 모두 사용하여 이를 수행하는 예시입니다.
```
Copyllm = ChatAnthropic(temperature=0).configurable_alternatives(# 이 필드에 id를 부여합니다.# 최종 실행 가능한 객체를 구성할 때, 이 id를 사용하여 이 필드를 구성할 수 있습니다.ConfigurableField(id=""llm""),# 기본 키를 설정합니다.# 이 키를 지정하면 위에서 초기화된 기본 LLM(ChatAnthropic)이 사용됩니다.default_key=""anthropic"",# 'openai'라는 이름의 새 옵션을 추가하며, 이는 `ChatOpenAI()`와 동일합니다.openai=ChatOpenAI(),# 'gpt4'라는 이름의 새 옵션을 추가하며, 이는 `ChatOpenAI(model=""gpt-4"")`와 동일합니다.gpt4=ChatOpenAI(model=""gpt-4""),# 여기에 더 많은 구성 옵션을 추가할 수 있습니다.)

prompt = PromptTemplate.from_template(""{company} 에 대해서 20자 이내로 설명해 줘.""# 기본 프롬프트 템플릿).configurable_alternatives(# 이 필드에 id를 부여합니다.ConfigurableField(id=""prompt""),# 기본 키를 설정합니다.default_key=""description"",# 'founder'이라는 새로운 옵션을 추가합니다.founder=PromptTemplate.from_template(""{company} 의 창립자는 누구인가요?""),# 'competitor'이라는 새로운 옵션을 추가합니다.competitor=PromptTemplate.from_template(""{company} 의 경쟁사는 누구인가요?""),# 여기에 더 많은 구성 옵션을 추가할 수 있습니다.)
chain = prompt | llm
```
```
Copy# with_config 로 설정 값을 지정하여 구성할 수 있습니다.chain.with_config(configurable={""prompt"":""founder"",""llm"":""openai""}).invoke(# 사용자가 제공한 회사에 대한 처리를 요청합니다.{""company"":""애플""}
)
```
```
AIMessage(content='애플의 창립자는 스티브 잡스(Steve Jobs), 스티브 워즈니악(Steve Wozniak), 론 웨인(Ronald Wayne)입니다. 하지만 론 웨인은 이후 회사에서 물러나게 되었고, 스티브 잡스와 스티브 워즈니악이 애플을 성공적으로 이끌어갔습니다.', response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 27, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
```
Copy# 하나만 구성하려는 경우chain.with_config(configurable={""llm"":""anthropic""}).invoke({""company"":""애플""})
```
```
AIMessage(content=' 애플은 스마트폰과 태블릿, 컴퓨터 등의 전자기기를 제조하는 미국의 회사입니다.')
```
```
Copy# 하나만 구성하려는 경우chain.with_config(configurable={""prompt"":""competitor""}).invoke({""company"":""애플""})
```
```
AIMessage(content=' 애플의 주요 경쟁사에는 다음이 포함됩니다:\n\n- 삼성: 스마트폰과 태블릿에서 애플의 가장 큰 경쟁사. 갤럭시 시리즈가 아이폰과 경쟁합니다.\n\n- 구글: 안드로이드 운영 체제를 통해 스마트폰 시장에서 애플과 경쟁합니다. 또한 구글의 픽셀 스마트폰이 애플 제품과 직접 경쟁합니다.  \n\n- 마이크로소프트: 윈도우 기반 PC와 서피스 제품군을 통해 애플의 맥북과 아이패드 제품군과 경쟁합니다.\n\n- Huawei: 중국의 큰 스마트폰 제')
```
```
Copy# 하나만 구성하려는 경우chain.invoke({""company"":""애플""})
```
```
AIMessage(content=' 애플은 스마트폰과 태블릿, 컴퓨터 등의 전자기기를 제조하는 미국의 회사입니다.')
```
## 설정 저장
구성된 체인을 별도의 객체로 쉽게 저장할 수 있습니다. 예를 들어, 특정 작업을 위해 사용자 정의된 체인을 구성한 후, 이를 재사용 가능한 객체로 저장함으로써 향후 유사한 작업에서 손쉽게 활용할 수 있습니다.
```
Copy# with_config 로 설정을 변경하여 생성한 체인을 별도의 변수에 저장합니다.gpt4_competitor_chain = chain.with_config(
    configurable={""llm"":""gpt4"",""prompt"":""competitor""}
)
```
```
Copy# 체인을 호출합니다.gpt4_competitor_chain.invoke({""company"":""애플""})
```
```
AIMessage(content='애플의 주요 경쟁사에는 삼성, 구글, 마이크로소프트, 아마존, 페이스북 등이 있습니다. 이들 회사는 스마트폰, 태블릿, 컴퓨터, 소프트웨어, 클라우드 서비스, 사회 네트워킹 등 다양한 분야에서 애플과 경쟁하고 있습니다.', response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 27, 'total_tokens': 151}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})
```"
07. @chain 데코레이터로 Runnable 생성,https://wikidocs.net/235703,"# 07. @chain 데코레이터로 Runnable 생성

@chain데코레이터를 추가하여 임의의 함수를 체인으로 변환할 수 있습니다.
이는 함수를RunnableLambda로 래핑하는 것과 기능적으로 동일합니다.
실제 동작을 살펴보겠습니다!
```
Copy%pip install -qU langchain langchain-openai
```
```
Copyimportos# LangSmith 추적os.environ[""LANGCHAIN_PROJECT""] =""Chain Decorator""
```
```
Copyfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportchainfromlangchain_openaiimportChatOpenAI
```
ChatPromptTemplate클래스를 사용하여 두 개의 프롬프트 템플릿을 정의합니다.
- prompt1은 주어진 주제에 대한 짧은 설명을,prompt2는 영어로 번역해 달라는 요청 프롬프트 입니다.
prompt1은 주어진 주제에 대한 짧은 설명을,prompt2는 영어로 번역해 달라는 요청 프롬프트 입니다.
```
Copy# 프롬프트 템플릿을 정의합니다.prompt1 = ChatPromptTemplate.from_template(""{topic} 에 대해 짧게 한글로 설명해주세요."")
prompt2 = ChatPromptTemplate.from_template(""{sentence} 를 emoji를 활용한 인스타그램 게시글로 만들어주세요."")
```
custom_chain함수는 입력 텍스트를 기반으로 사용자 정의 체인을 실행합니다.
- @chain데코레이터로 사용자 정의 함수를 데코레이팅 하며, 데코레이팅을 통해 함수를Runnable한 객체로 만듭니다.
@chain데코레이터로 사용자 정의 함수를 데코레이팅 하며, 데코레이팅을 통해 함수를Runnable한 객체로 만듭니다.
```
Copy@chaindefcustom_chain(text):# 첫 번째 프롬프트, ChatOpenAI, 문자열 출력 파서를 연결하여 체인을 생성합니다.chain1 = prompt1 | ChatOpenAI() | StrOutputParser()
    output1 = chain1.invoke({""topic"": text})# 두 번째 프롬프트, ChatOpenAI, 문자열 출력 파서를 연결하여 체인을 생성합니다.chain2 = prompt2 | ChatOpenAI() | StrOutputParser()# 두 번째 체인을 호출하여 파싱된 첫 번째 결과를 전달하고 최종 결과를 반환합니다.returnchain2.invoke({""sentence"": output1})
```
custom_chain은 이제 실행 가능한 객체(runnable)이므로,invoke()를 사용하여 실행해야 합니다.
- LangSmith 추적 기록을 확인해 보면,custom_chain추적 정보가 있을 것이며, 그 아래에는 OpenAI 호출이 중첩되어 있을 것입니다.
- LangSmith 추적 링크
LangSmith 추적 기록을 확인해 보면,custom_chain추적 정보가 있을 것이며, 그 아래에는 OpenAI 호출이 중첩되어 있을 것입니다.
LangSmith 추적 링크
```
Copy# custom_chain을 호출print(custom_chain.invoke(""양자역학""))
```
```
🔮✨🌌 양자역학의 신비로운 세계에 온 여러분! 🌌✨🔮 입자들의 확률적인 움직임과 파동으로서의 특성에 대한 물리학적 이론을 탐험해보세요! 🌟🌀💫 양자역학은 현대 물리학의 근간을 이루는 중요한 이론입니다. 💡🔬🌠 함께 양자 세계를 탐험해보세요! 🚀🔭🌟 #양자역학 #물리학 #파동입자이론 #신비로운세계 #현대과학 #인스타그램
```"
08. RunnableWithMessageHistory,https://wikidocs.net/235581,"# 08. RunnableWithMessageHistory

RunnableWithMessageHistory를 활용하여 특정 유형의 작업(체인)에 메시지 기록을 추가하는 것은 프로그래밍에서 상당히 유용한 기능입니다.
이 기능은 특히 대화형 애플리케이션 또는 복잡한 데이터 처리 작업을 구현할 때 이전 메시지의 맥락을 유지해야 할 필요가 있을 때 중요합니다.
메시지 기록을 관리함으로써, 개발자는 애플리케이션의 흐름을 더 잘 제어하고, 사용자의 이전 요청에 따라 적절하게 응답할 수 있습니다.
실제 활용 예시
- 대화형 챗봇 개발: 사용자와의 대화 내역을 기반으로 챗봇의 응답을 조정할 수 있습니다.
- 복잡한 데이터 처리: 데이터 처리 과정에서 이전 단계의 결과를 참조하여 다음 단계의 로직을 결정할 수 있습니다.
- 상태 관리가 필요한 애플리케이션: 사용자의 이전 선택을 기억하고 그에 따라 다음 화면이나 정보를 제공할 수 있습니다.
대화형 챗봇 개발: 사용자와의 대화 내역을 기반으로 챗봇의 응답을 조정할 수 있습니다.
복잡한 데이터 처리: 데이터 처리 과정에서 이전 단계의 결과를 참조하여 다음 단계의 로직을 결정할 수 있습니다.
상태 관리가 필요한 애플리케이션: 사용자의 이전 선택을 기억하고 그에 따라 다음 화면이나 정보를 제공할 수 있습니다.
RunnableWithMessageHistory는 애플리케이션의 상태를 유지하고, 사용자 경험을 향상시키며, 더 정교한 응답 메커니즘을 구현할 수 있게 해주는 강력한 도구입니다.
## 튜토리얼
```
Copyfromlangchain_core.promptsimportChatPromptTemplate, MessagesPlaceholderfromlangchain_openaiimportChatOpenAI

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요"",
        ),# 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨MessagesPlaceholder(variable_name=""history""),
        (""human"",""{input}""),# 사용자 입력을 변수로 사용]
)
runnable = prompt | model# 프롬프트와 모델을 연결하여 runnable 객체 생성
```
메시지 기록을 관리하는 것은 대화형 애플리케이션 또는 복잡한 데이터 처리 작업에서 매우 중요합니다. 메시지 기록을 효과적으로 관리하려면 다음 두 가지 주요 요소가 필요합니다.
Runnable: 주로 Retriever, Chain 과 같이BaseChatMessageHistory와 상호작용하는runnable객체입니다.
Runnable: 주로 Retriever, Chain 과 같이BaseChatMessageHistory와 상호작용하는runnable객체입니다.
BaseChatMessageHistory의 인스턴스를 반환하는 호출 가능한 객체(callable): 메시지 기록을 관리하기 위한 객체입니다. 이 객체는 메시지 기록을 저장, 검색, 업데이트하는 데 사용됩니다. 메시지 기록은 대화의 맥락을 유지하고, 사용자의 이전 입력에 기반한 응답을 생성하는 데 필요합니다.
BaseChatMessageHistory의 인스턴스를 반환하는 호출 가능한 객체(callable): 메시지 기록을 관리하기 위한 객체입니다. 이 객체는 메시지 기록을 저장, 검색, 업데이트하는 데 사용됩니다. 메시지 기록은 대화의 맥락을 유지하고, 사용자의 이전 입력에 기반한 응답을 생성하는 데 필요합니다.
메시지 기록을 구현하는 데에는 여러 방법이 있으며,memory integrations페이지에는 다양한 저장소 옵션과 통합 방법이 소개되어 있습니다.
여기서는 두 가지 주요 방법을 살펴보겠습니다.
인메모리ChatMessageHistory사용
이 방법은 메모리 내에서 메시지 기록을 관리합니다. 주로 개발 단계나 간단한 애플리케이션에서 사용됩니다. 인메모리 방식은 빠른 접근 속도를 제공하지만, 애플리케이션을 재시작할 때 메시지 기록이 사라지는 단점이 있습니다.
RedisChatMessageHistory를 사용하여 영구적인 저장소 활용
Redis를 사용하는 방법은 메시지 기록을 영구적으로 저장할 수 있게 해줍니다. Redis는 높은 성능을 제공하는 오픈 소스 인메모리 데이터 구조 저장소로, 분산 환경에서도 안정적으로 메시지 기록을 관리할 수 있습니다. 이 방법은 복잡한 애플리케이션 또는 장기간 운영되는 서비스에 적합합니다.
메시지 기록을 관리하는 방법을 선택할 때는 애플리케이션의 요구사항, 예상되는 트래픽 양, 메시지 데이터의 중요성 및 보존 기간 등을 고려해야 합니다. 인메모리 방식은 구현이 간단하고 빠르지만, 데이터의 영구성이 요구되는 경우 Redis와 같은 영구적인 저장소를 사용하는 것이 더 적합할 수 있습니다.
## 휘발성 대화기록: 인메모리(In-Memory)
아래는 채팅 기록이 메모리에 저장되는 간단한 예시입니다.
RunnableWithMessageHistory설정 매개변수
- runnable
- BaseChatMessageHistory이거나 상속받은 객체. ex)ChatMessageHistory
- input_messages_key: chain 을 invoke() 할때 사용자 쿼리 입력으로 지정하는 key
- history_messages_key: 대화 기록으로 지정하는 key
runnable
BaseChatMessageHistory이거나 상속받은 객체. ex)ChatMessageHistory
input_messages_key: chain 을 invoke() 할때 사용자 쿼리 입력으로 지정하는 key
history_messages_key: 대화 기록으로 지정하는 key
```
Copyfromlangchain_community.chat_message_historiesimportChatMessageHistoryfromlangchain_core.chat_historyimportBaseChatMessageHistoryfromlangchain_core.runnables.historyimportRunnableWithMessageHistory

store = {}# 세션 기록을 저장할 딕셔너리# 세션 ID를 기반으로 세션 기록을 가져오는 함수defget_session_history(session_ids:str) -> BaseChatMessageHistory:print(session_ids)ifsession_idsnotinstore:# 세션 ID가 store에 없는 경우# 새로운 ChatMessageHistory 객체를 생성하여 store에 저장store[session_ids] = ChatMessageHistory()returnstore[session_ids]# 해당 세션 ID에 대한 세션 기록 반환with_message_history = (
    RunnableWithMessageHistory(# RunnableWithMessageHistory 객체 생성runnable,# 실행할 Runnable 객체get_session_history,# 세션 기록을 가져오는 함수input_messages_key=""input"",# 입력 메시지의 키history_messages_key=""history"",# 기록 메시지의 키)
)
```
input_messages_key는 최신 입력 메시지로 처리될 키를 지정하고,history_messages_key는 이전 메시지를 추가할 키를 지정합니다.
다음의 코드를 보면RunnableWithMessageHistory의 초기값에session_id키를 Default 로 삽입하는 것을 볼 수 있으며, 이 코드로 인하여RunnableWithMessageHistory는 대화 스레드 관리를session_id로 한다는 것을 간접적으로 알 수 있습니다.
즉, 대화 스레드별 관리는session_id별로 구현함을 알 수 있습니다.
참고 코드:RunnableWithMessageHistory구현을 참고하면,
```
Copyifhistory_factory_config:
    _config_specs = history_factory_configelse:# If not provided, then we'll use the default session_id field_config_specs = [
        ConfigurableFieldSpec(id=""session_id"",
            annotation=str,
            name=""Session ID"",
            description=""Unique identifier for a session."",
            default="""",
            is_shared=True,
        ),
    ]
```
따라서,invoke()시config={""configurable"": {""session_id"": ""세션ID입력""}}코드를 반드시 지정해 주어야 하는 것을 확인할 수 있습니다.
```
Copywith_message_history.invoke(# 수학 관련 질문 ""코사인의 의미는 무엇인가요?""를 입력으로 전달합니다.{""ability"":""math"",""input"":""What does cosine mean?""},# 설정 정보로 세션 ID ""abc123""을 전달합니다.config={""configurable"": {""session_id"":""abc123""}},
)
```
```
abc123
```
```
AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 47, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
같은session_id를 입력하면 이전 대화 스레드의 내용을 가져오기 때문에 이어서 대화가 가능!
```
Copy# 메시지 기록을 포함하여 호출합니다.with_message_history.invoke(# 능력과 입력을 설정합니다.{""ability"":""math"",""input"":""이전의 내용을 한글로 답변해 주세요.""},# 설정 옵션을 지정합니다.config={""configurable"": {""session_id"":""abc123""}},
)
```
```
abc123
```
```
AIMessage(content='코사인은 직각 삼각형에서 인접변과 빗변의 비율을 나타내는 삼각함수입니다.', response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 98, 'total_tokens': 145}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
하지만 다른session_id를 지정하면 대화기록이 없기 때문에 답변을 제대로 수행하지 못합니다.
(아래의 예시는session_id:def234는 존재하지 않기 때문에 엉뚱한 답변을 하는 것을 확인할 수 있습니다)
```
Copy# 새로운 session_id로 인해 이전 대화 내용을 기억하지 않습니다.with_message_history.invoke(# 수학 능력과 입력 메시지를 전달합니다.{""ability"":""math"",""input"":""이전의 내용을 한글로 답변해 주세요""},# 새로운 session_id를 설정합니다.config={""configurable"": {""session_id"":""def234""}},
)
```
```
def234
```
```
AIMessage(content='수학에 능숙한 어시스턴트입니다.', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 58, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
메시지 기록을 추적하는 데 사용되는 구성 매개변수는ConfigurableFieldSpec객체의 리스트를history_factory_config매개변수로 전달하여 사용자 정의할 수 있습니다.
이렇게history_factory_config를 새로 설정하게 되면 기존session_id설정을 덮어쓰게 됩니다.
아래 예시에서는user_id와conversation_id라는 두 가지 매개변수를 사용합니다.
```
Copyfromlangchain_core.runnablesimportConfigurableFieldSpec

store = {}# 빈 딕셔너리를 초기화합니다.defget_session_history(user_id:str, conversation_id:str) -> BaseChatMessageHistory:# 주어진 user_id와 conversation_id에 해당하는 세션 기록을 반환합니다.if(user_id, conversation_id)notinstore:# 해당 키가 store에 없으면 새로운 ChatMessageHistory를 생성하여 저장합니다.store[(user_id, conversation_id)] = ChatMessageHistory()returnstore[(user_id, conversation_id)]


with_message_history = RunnableWithMessageHistory(
    runnable,
    get_session_history,
    input_messages_key=""input"",
    history_messages_key=""history"",
    history_factory_config=[# 기존의 ""session_id"" 설정을 대체하게 됩니다.ConfigurableFieldSpec(id=""user_id"",# get_session_history 함수의 첫 번째 인자로 사용됩니다.annotation=str,
            name=""User ID"",
            description=""사용자의 고유 식별자입니다."",
            default="""",
            is_shared=True,
        ),
        ConfigurableFieldSpec(id=""conversation_id"",# get_session_history 함수의 두 번째 인자로 사용됩니다.annotation=str,
            name=""Conversation ID"",
            description=""대화의 고유 식별자입니다."",
            default="""",
            is_shared=True,
        ),
    ],
)
```
```
Copywith_message_history.invoke(# 능력(ability)과 입력(input)을 포함한 딕셔너리를 전달합니다.{""ability"":""math"",""input"":""Hello""},# 설정(config) 딕셔너리를 전달합니다.config={""configurable"": {""user_id"":""123"",""conversation_id"":""1""}},
)
```
```
AIMessage(content='안녕하세요. 무엇을 도와드릴까요?', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 43, 'total_tokens': 64}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
## 다양한 Key를 사용한 Runnable 을 사용한 예시
메시지를 입력으로 받고 딕셔너리를 출력으로 반환하는 경우입니다.
- [중요]:input_messages_key=""input"" 을 생략합니다. 그럼 입력으로 Message 객체를 넣도록 설정하게 됩니다.
[중요]:input_messages_key=""input"" 을 생략합니다. 그럼 입력으로 Message 객체를 넣도록 설정하게 됩니다.
```
Copyfromlangchain_core.messagesimportHumanMessagefromlangchain_core.runnablesimportRunnableParallel# chain 생성chain = RunnableParallel({""output_message"": ChatOpenAI()})defget_session_history(session_id:str) -> BaseChatMessageHistory:# 세션 ID에 해당하는 대화 기록이 저장소에 없으면 새로운 ChatMessageHistory를 생성합니다.ifsession_idnotinstore:
        store[session_id] = ChatMessageHistory()# 세션 ID에 해당하는 대화 기록을 반환합니다.returnstore[session_id]# 체인에 대화 기록 기능을 추가한 RunnableWithMessageHistory 객체를 생성합니다.with_message_history = RunnableWithMessageHistory(
    chain,
    get_session_history,# 입력 메시지의 키를 ""input""으로 설정합니다.(생략시 Message 객체로 입력)# input_messages_key=""input"",# 출력 메시지의 키를 ""output_message""로 설정합니다. (생략시 Message 객체로 출력)output_messages_key=""output_message"",
)# 주어진 메시지와 설정으로 체인을 실행합니다.with_message_history.invoke(# 혹은 ""what is the definition of cosine?"" 도 가능[HumanMessage(content=""what is the definition of cosine?"")],
    config={""configurable"": {""session_id"":""abc123""}},
)
```
```
{'output_message': AIMessage(content='The cosine function is a trigonometric function that returns the cosine value of a given angle. It represents the ratio of the length of the adjacent side to the length of the hypotenuse in a right triangle with that angle. The cosine function is commonly denoted as ""cos"" and the angle is typically measured in radians.', response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 1222, 'total_tokens': 1288}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})}
```
```
Copywith_message_history.invoke(# 이전의 답변에 대하여 한글로 답변을 재요청합니다.[HumanMessage(content=""이전의 내용을 한글로 답변해 주세요!"")],# 설정 옵션을 딕셔너리 형태로 전달합니다.config={""configurable"": {""session_id"":""abc123""}},
)
```
```
{'output_message': AIMessage(content='코사인 함수는 주어진 각도의 코사인 값을 반환하는 삼각함수로, 직각삼각형에서 인접 변의 길이와 빗변의 길이의 비율을 나타냅니다. 수학적으로, 각도 θ에 대한 코사인 값은 그 각도를 가지는 직각삼각형의 인접 변의 길이를 빗변의 길이로 나눈 것으로 정의됩니다. 코사인 함수는 일반적으로 ""cos""로 표기되며, 각도는 일반적으로 라디안 단위로 표현됩니다.', response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 1020, 'total_tokens': 1207}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})}
```
- [중요]:output_messages_key=""output_message"" 을 생략합니다. 그럼 출력으로 Message 객체를 반환합니다.
[중요]:output_messages_key=""output_message"" 을 생략합니다. 그럼 출력으로 Message 객체를 반환합니다.
```
Copywith_message_history = RunnableWithMessageHistory(
    ChatOpenAI(),# ChatOpenAI 언어 모델을 사용합니다.get_session_history,# 대화 세션 기록을 가져오는 함수를 지정합니다.# 입력 메시지의 키를 ""input""으로 설정합니다.(생략시 Message 객체로 입력)# input_messages_key=""input"",# 출력 메시지의 키를 ""output_message""로 설정합니다. (생략시 Message 객체로 출력)# output_messages_key=""output_message"",)
```
```
Copywith_message_history.invoke(# 이전의 답변에 대하여 한글로 답변을 재요청합니다.[HumanMessage(content=""코사인의 의미는 무엇인가요?"")],# 설정 옵션을 딕셔너리 형태로 전달합니다.config={""configurable"": {""session_id"":""def123""}},
)
```
```
Error in RootListenersTracer.on_llm_end callback: KeyError('input')
```
```
AIMessage(content='코사인은 삼각함수 중 하나로, 직각삼각형에서의 각도 θ에 대해 빗변과 빗변에 대한 코사인 값을 나타내는 비율을 의미합니다. 간단히 말해, 코사인은 직각삼각형에서의 특정 각도에 대한 인접변과 빗변의 비율을 나타내는 삼각함수입니다.', response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 24, 'total_tokens': 162}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
모든 입력 메시지와 출력 메시지에 대해 단일 키를 사용하는 방식입니다.
- itemgetter(""input_messages"")를 사용하여 입력 메시지를 추출합니다.
itemgetter(""input_messages"")를 사용하여 입력 메시지를 추출합니다.
```
Copyfromoperatorimportitemgetter

with_message_history = RunnableWithMessageHistory(# ""input_messages"" 키를 사용하여 입력 메시지를 가져와 ChatOpenAI()에 전달합니다.itemgetter(""input_messages"") | ChatOpenAI(),
    get_session_history,# 세션 기록을 가져오는 함수입니다.input_messages_key=""input_messages"",# 입력 메시지의 키를 지정합니다.)
```
```
Copywith_message_history.invoke(
    {""input_messages"":""코사인의 의미는 무엇인가요?""},# 설정 옵션을 딕셔너리 형태로 전달합니다.config={""configurable"": {""session_id"":""xyz123""}},
)
```
```
AIMessage(content='코사인은 삼각형의 한 각에 대한 변의 길이의 비를 나타내는 삼각함수 중 하나입니다. 코사인은 빗변과 인접한 변의 길이의 비로 정의되며, 주로 직각삼각형에서 사용됩니다. 코사인은 삼각형의 각도에 따라 변하는 값으로, 삼각함수로서 다양한 수학적 응용과 분야에서 사용됩니다.', response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 24, 'total_tokens': 168}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
## 영구 저장소(Persistent storage)
영구 저장소(Persistent storage)는 프로그램이 종료되거나 시스템이 재부팅되더라도 데이터를 유지하는 저장 메커니즘을 말합니다. 이는 데이터베이스, 파일 시스템, 또는 기타 비휘발성 저장 장치를 통해 구현될 수 있습니다.
영구 저장소는 애플리케이션의 상태를 저장하고, 사용자 설정을 유지하며,장기간 데이터를 보존하는 데 필수적입니다. 이를 통해 프로그램은 이전 실행에서 중단된 지점부터 다시 시작할 수 있으며,사용자는 데이터 손실 없이 작업을 계속할 수 있습니다.
- RunnableWithMessageHistory는get_session_history호출 가능 객체가 채팅 메시지 기록을 어떻게 검색하는지에 대해 독립적입니다.
- 로컬 파일 시스템을 사용하는 예제는여기를 참조하세요.
- 아래에서는Redis를 사용하는 방법을 보여줍니다. 다른 제공자를 사용하여 채팅 메시지 기록을 구현하는 방법은memory integrations페이지를 확인하세요.
RunnableWithMessageHistory는get_session_history호출 가능 객체가 채팅 메시지 기록을 어떻게 검색하는지에 대해 독립적입니다.
로컬 파일 시스템을 사용하는 예제는여기를 참조하세요.
아래에서는Redis를 사용하는 방법을 보여줍니다. 다른 제공자를 사용하여 채팅 메시지 기록을 구현하는 방법은memory integrations페이지를 확인하세요.
Redis가 설치되어 있지 않다면 먼저 설치해야 합니다.
```
Copy%pip install -qU redis
```
기존에 연결할 Redis 배포가 없는 경우, 로컬 Redis Stack 서버를 시작합니다.
다음은 Docker 로 Redis 서버를 구동하는 명령어입니다.
```
Copydocker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest
```
REDIS_URL변수에 Redis 데이터베이스의 연결 URL을 할당합니다.
- URL은""redis://localhost:6379/0""로 설정되어 있습니다.
URL은""redis://localhost:6379/0""로 설정되어 있습니다.
```
Copy# Redis 서버의 URL을 지정합니다.REDIS_URL =""redis://localhost:6379/0""
```
LangSmith 추적 설정
추적을 위한 LangSmith 를 설정합니다. LangSmith는 필수적인 것은 아니지만, 도움이 될 수 있습니다.
```
Copyfromdotenvimportload_dotenvimportos

load_dotenv()# LANGCHAIN_TRACING_V2 환경 변수를 ""true""로 설정합니다.os.environ[""LANGCHAIN_TRACING_V2""] =""true""# LANGCHAIN_PROJECT 설정os.environ[""LANGCHAIN_PROJECT""] =""RunnableWithMessageHistory""
```
메시지 기록 구현을 업데이트하려면 새로운 호출 가능한 객체를 정의하고, 이번에는 RedisChatMessageHistory의 인스턴스를 반환하면 됩니다.
```
Copyfromlangchain_community.chat_message_historiesimportRedisChatMessageHistorydefget_message_history(session_id:str) -> RedisChatMessageHistory:# 세션 ID를 기반으로 RedisChatMessageHistory 객체를 반환합니다.returnRedisChatMessageHistory(session_id, url=REDIS_URL)


with_message_history = RunnableWithMessageHistory(
    runnable,# 실행 가능한 객체get_message_history,# 메시지 기록을 가져오는 함수input_messages_key=""input"",# 입력 메시지의 키history_messages_key=""history"",# 기록 메시지의 키)
```
이전과 동일한 방식으로 호출할 수 있습니다.
```
Copywith_message_history.invoke(# 수학 관련 질문 ""코사인의 의미는 무엇인가요?""를 입력으로 전달합니다.{""ability"":""math"",""input"":""What does cosine mean?""},# 설정 옵션으로 세션 ID를 ""redis123"" 로 지정합니다.config={""configurable"": {""session_id"":""redis123""}},
)
```
```
AIMessage(content='Trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 47, 'total_tokens': 69}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```
동일한session_id를 사용하여 두 번째 호출을 수행합니다. 이번에는 이전의 답변을 한글로 답변해 달라는 요청을 해보겠습니다.
```
Copywith_message_history.invoke(# 이전 답변에 대한 한글 번역을 요청합니다.{""ability"":""math"",""input"":""이전의 답변을 한글로 번역해 주세요.""},# 설정 값으로 세션 ID를 ""foobar""로 지정합니다.config={""configurable"": {""session_id"":""redis123""}},
)
```
```
AIMessage(content='직각삼각형에서 인접변의 길이를 빗변의 길이로 나눈 비율을 나타내는 삼각함수입니다.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 95, 'total_tokens': 148}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
이번에는 다른session_id를 사용하여 질문을 하겠습니다.
```
Copywith_message_history.invoke(# 이전 답변에 대한 한글 번역을 요청합니다.{""ability"":""math"",""input"":""이전의 답변을 한글로 번역해 주세요.""},# 설정 값으로 세션 ID를 ""redis456""로 지정합니다.config={""configurable"": {""session_id"":""redis456""}},
)
```
```
AIMessage(content='수학에 능숙한 어시스턴트입니다.', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 60, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})
```
마지막 답변은 이전 대화기록이 없으므로, 제대로 된 답변을 받을 수 없습니다."
09. 사용자 정의 제네레이터(generator),https://wikidocs.net/235885,"# 09. 사용자 정의 제네레이터(generator)

제너레이터 함수(즉,yield키워드를 사용하고 이터레이터처럼 동작하는 함수)를 LCEL 파이프라인에서 사용할 수 있습니다.
이러한 제너레이터의 시그니처는Iterator[Input] -> Iterator[Output]이어야 합니다. 비동기 제너레이터의 경우에는AsyncIterator[Input] -> AsyncIterator[Output]입니다.
이는 다음과 같은 용도로 유용합니다.
- 사용자 정의 출력 파서 구현
- 이전 단계의 출력을 수정하면서 스트리밍 기능 유지
사용자 정의 출력 파서 구현
이전 단계의 출력을 수정하면서 스트리밍 기능 유지
아래의 예제에서는 쉼표로 구분된 목록에 대한 사용자 정의 출력 파서를 구현해 보겠습니다.
```
Copy%pip install -qU langchain langchain-openai
```
```
CopyfromtypingimportIterator,Listfromlangchain.prompts.chatimportChatPromptTemplatefromlangchain_core.output_parsersimportStrOutputParserfromlangchain_openaiimportChatOpenAI

prompt = ChatPromptTemplate.from_template(# 주어진 회사와 유사한 5개의 회사를 쉼표로 구분된 목록으로 작성하세요.""Write a comma-separated list of 5 companies similar to: {company}"")# 온도를 0.0으로 설정하여 ChatOpenAI 모델을 초기화합니다.model = ChatOpenAI(temperature=0.0, model=""gpt-4-turbo-preview"")# 프롬프트와 모델을 연결하고 문자열 출력 파서를 적용하여 체인을 생성합니다.str_chain = prompt | model | StrOutputParser()
```
아래의 체인을스트림(stream)으로 실행하고 결과를 출력합니다. 결과는 실시간으로 생성됩니다.
```
Copy# 데이터를 스트리밍합니다.forchunkinstr_chain.stream({""company"":""Google""}):# 각 청크를 출력하고, 줄 바꿈 없이 버퍼를 즉시 플러시합니다.print(chunk, end="""", flush=True)
```
```
Microsoft, Amazon, Facebook (Meta), Apple, Alibaba
```
invoke()하여 실행 결과를 확인합니다.
```
Copy# 체인에 데이터를 invoke 합니다.str_chain.invoke({""company"":""Google""})
```
```
'Microsoft, Amazon, Facebook (Meta), Apple, Alibaba'
```
split_into_list함수는 LLM 토큰의 이터레이터를 입력으로 받아 쉼표로 구분된 문자열 리스트의 이터레이터를 반환합니다.
- 마지막 청크를yield하여 반환합니다(generator).
마지막 청크를yield하여 반환합니다(generator).
```
Copy# 입력으로 llm 토큰의 반복자를 받아 쉼표로 구분된 문자열 리스트로 분할하는 사용자 정의 파서입니다.defsplit_into_list(input: Iterator[str]) -> Iterator[List[str]]:# 쉼표가 나올 때까지 부분 입력을 보관합니다.buffer =""""forchunkininput:# 현재 청크를 버퍼에 추가합니다.buffer += chunk# 버퍼에 쉼표가 있는 동안 반복합니다.while"",""inbuffer:# 버퍼를 쉼표로 분할합니다.comma_index = buffer.index("","")# 쉼표 이전의 모든 내용을 반환합니다.yield[buffer[:comma_index].strip()]# 나머지는 다음 반복을 위해 저장합니다.buffer = buffer[comma_index +1:]# 마지막 청크를 반환합니다.yield[buffer.strip()]
```
- str_chain변수의 문자열을 파이프(|) 연산자를 사용하여split_into_list함수에 전달합니다.
- split_into_list함수는 문자열을 리스트로 분할하는 역할을 합니다.
- 분할된 리스트는list_chain변수에 할당됩니다.
str_chain변수의 문자열을 파이프(|) 연산자를 사용하여split_into_list함수에 전달합니다.
split_into_list함수는 문자열을 리스트로 분할하는 역할을 합니다.
분할된 리스트는list_chain변수에 할당됩니다.
```
Copylist_chain = str_chain | split_into_list# 문자열 체인을 리스트로 분할합니다.
```
- list_chain객체의stream메서드를 호출하여 입력 데이터{""animal"": ""bear""}에 대한 출력을 생성합니다.
- stream메서드는 출력을 청크(chunk) 단위로 반환하며, 각 청크는 반복문을 통해 처리됩니다.
- 각 청크는print함수를 사용하여 즉시 출력되며,flush=True옵션을 통해 버퍼링 없이 출력이 즉시 이루어집니다.
list_chain객체의stream메서드를 호출하여 입력 데이터{""animal"": ""bear""}에 대한 출력을 생성합니다.
stream메서드는 출력을 청크(chunk) 단위로 반환하며, 각 청크는 반복문을 통해 처리됩니다.
각 청크는print함수를 사용하여 즉시 출력되며,flush=True옵션을 통해 버퍼링 없이 출력이 즉시 이루어집니다.
이 코드는list_chain을 사용하여 입력 데이터에 대한 출력을 생성하고, 출력을 청크 단위로 처리하여 즉시 출력하는 과정을 보여줍니다.
```
Copy# 생성한 list_chain 이 문제없이 스트리밍되는지 확인합니다.forchunkinlist_chain.stream({""company"":""Google""}):print(chunk, flush=True)# 각 청크를 출력하고, 버퍼를 즉시 플러시합니다.
```
```
['Microsoft']
['Amazon']
['Apple']
['Facebook (Meta)']
['Alibaba']
```
이번에는list_chain에invoke()로 데이터를 주입합니다. 문제없이 동작하는지 확인해 보겠습니다.
```
Copy# list_chain 에 데이터를 invoke 합니다.list_chain.invoke({""company"":""Google""})
```
```
['Microsoft', 'Amazon', 'Facebook (Meta)', 'Apple', 'Alibaba']
```
## 비동기(Asynchronous)
asplit_into_list함수는 비동기 제너레이터(AsyncIterator[str])를 입력으로 받아 문자열 리스트의 비동기 제너레이터(AsyncIterator[List[str]])를 반환합니다.
```
CopyfromtypingimportAsyncIterator# 비동기 함수 정의asyncdefasplit_into_list(input: AsyncIterator[str]) -> AsyncIterator[List[str]]:
    buffer =""""# `input`은 `async_generator` 객체이므로 `async for`를 사용asyncforchunkininput:
        buffer += chunkwhile"",""inbuffer:
            comma_index = buffer.index("","")yield[
                buffer[:comma_index].strip()
            ]# 쉼표를 기준으로 분할하여 리스트로 반환buffer = buffer[comma_index +1:]yield[buffer.strip()]# 남은 버퍼 내용을 리스트로 반환# alist_chain 과 asplit_into_list 를 파이프라인으로 연결alist_chain = str_chain | asplit_into_list
```
비동기 함수를 사용하여 스트리밍합니다.
```
Copy# async for 루프를 사용하여 데이터를 스트리밍합니다.asyncforchunkinalist_chain.astream({""company"":""Google""}):# 각 청크를 출력하고 버퍼를 비웁니다.print(chunk, flush=True)
```
```
['Microsoft']
['Amazon']
['Facebook (Meta)']
['Apple']
['Alibaba']
```
비동기 chain 에 데이터를 invoke 하여 문제없이 동작하는지 확인합니다.
```
Copy# 리스트 체인을 비동기적으로 호출합니다.awaitalist_chain.ainvoke({""company"":""Google""})
```
```
['Microsoft', 'Amazon', 'Facebook (Meta)', 'Apple', 'Alibaba']
```"
10. Runtime Arguments 바인딩,https://wikidocs.net/235886,"# 10. Runtime Arguments 바인딩

때로는 Runnable 시퀀스 내에서 Runnable을 호출할 때, 이전 Runnable의 출력이나 사용자 입력에 포함되지 않은 상수 인자를 전달해야 할 경우가 있습니다.
이럴 때는Runnable.bind()를 사용하면 이러한 인자를 쉽게 전달할 수 있습니다.
```
Copy%pip install -qU langchain langchain-openai
```
RunnablePassthrough를 사용하여{equation_statement}변수를 프롬프트에 전달하고,StrOutputParser를 사용하여 모델의 출력을 문자열로 파싱하는runnable객체를 생성합니다.
- runnable.invoke()메서드를 호출하여 ""x raised to the third plus seven equals 12""라는 방정식 문장을 전달하고 결과를 출력합니다.
runnable.invoke()메서드를 호출하여 ""x raised to the third plus seven equals 12""라는 방정식 문장을 전달하고 결과를 출력합니다.
```
Copyfromlangchain_core.output_parsersimportStrOutputParserfromlangchain_core.promptsimportChatPromptTemplatefromlangchain_core.runnablesimportRunnablePassthroughfromlangchain_openaiimportChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",# 대수 기호를 사용하여 다음 방정식을 작성한 다음 풀이하세요.""Write out the following equation using algebraic symbols then solve it. """"Use the format\n\nEQUATION:...\nSOLUTION:...\n\n"",
        ),
        (""human"",""{equation_statement}"",# 사용자가 입력한 방정식 문장을 변수로 받습니다.),
    ]
)# ChatOpenAI 모델을 초기화하고 temperature를 0으로 설정합니다.model = ChatOpenAI(model=""gpt-4"", temperature=0)# 방정식 문장을 입력받아 프롬프트에 전달하고, 모델에서 생성된 결과를 문자열로 파싱합니다.runnable = (
    {""equation_statement"": RunnablePassthrough()} | prompt | model | StrOutputParser()
)# 예시 방정식 문장을 입력하여 결과를 출력합니다.print(runnable.invoke(""x raised to the third plus seven equals 12""))
```
```
EQUATION: x^3 + 7 = 12
SOLUTION: x^3 = 12 - 7
x^3 = 5
x = ∛5
```
그리고 특정stop단어를 사용하여 모델을 호출하고자 합니다.model.bind()를 사용하여 언어 모델을 호출하고, 생성된 텍스트에서 ""SOLUTION"" 토큰까지만 출력합니다.
```
Copyrunnable = (# 실행 가능한 패스스루 객체를 생성하여 ""equation_statement"" 키에 할당합니다.{""equation_statement"": RunnablePassthrough()}
    | prompt# 프롬프트를 파이프라인에 추가합니다.| model.bind(
        stop=""SOLUTION"")# 모델을 바인딩하고 ""SOLUTION"" 토큰에서 생성을 중지하도록 설정합니다.| StrOutputParser()# 문자열 출력 파서를 파이프라인에 추가합니다.)# ""x raised to the third plus seven equals 12""라는 입력으로 파이프라인을 실행하고 결과를 출력합니다.print(runnable.invoke(""x raised to the third plus seven equals 12""))
```
```
EQUATION: x^3 + 7 = 12
```
## OpenAI Functions 기능 연결
binding 의 특히 유용한 활용 방법 중 하나는 호환되는 OpenAI 모델에 OpenAI Functions 를 연결하는 것입니다.
아래는 OpenAI Functions 를 스키마에 맞게 정의한 코드입니다.
```
Copyopenai_function = {""name"":""solver"",# 함수의 이름# 함수의 설명: 방정식을 수립하고 해결합니다.""description"":""Formulates and solves an equation"",""parameters"": {# 함수의 매개변수""type"":""object"",# 매개변수의 타입: 객체""properties"": {# 매개변수의 속성""equation"": {# 방정식 속성""type"":""string"",# 방정식의 타입: 문자열""description"":""The algebraic expression of the equation"",# 방정식의 대수식 표현},""solution"": {# 해답 속성""type"":""string"",# 해답의 타입: 문자열""description"":""The solution to the equation"",# 방정식의 해답},
        },""required"": [""equation"",""solution""],# 필수 매개변수: 방정식과 해답},
}
```
bind()메서드를 사용하여solver라는 이름의 함수 호출을 모델에 바인딩합니다.
```
Copy# 다음 방정식을 대수 기호를 사용하여 작성한 다음 해결하세요.prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""Write out the following equation using algebraic symbols then solve it."",
        ),
        (""human"",""{equation_statement}""),
    ]
)
model = ChatOpenAI(model=""gpt-4"", temperature=0).bind(
    function_call={""name"":""solver""},# openai_function schema 를 바인딩합니다.functions=[openai_function],
)
runnable = {""equation_statement"": RunnablePassthrough()} | prompt | model# x의 세제곱에 7을 더하면 12와 같다runnable.invoke(""x raised to the third plus seven equals 12"")
```
```
AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n""equation"": ""x^3 + 7 = 12"",\n""solution"": ""x = ∛5""\n}', 'name': 'solver'}}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 95, 'total_tokens': 122}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})
```
## OpenAI tools 연결하기
OpenAI에서 제공하는 tools(도구) 를 연결하여 활용하는 방법에 대해 설명하겠습니다.
tools 객체는 OpenAI의 다양한 기능을 간편하게 사용할 수 있도록 도와줍니다.
예를 들어,tool.run메서드를 호출하여 자연어로 된 질문을 입력하면, 해당 질문에 대한 답변을 생성해 줍니다.
```
Copytools = [
    {""type"":""function"",""function"": {""name"":""get_current_weather"",# 현재 날씨를 가져오는 함수의 이름""description"":""주어진 위치의 현재 날씨를 가져옵니다"",# 함수에 대한 설명""parameters"": {""type"":""object"",""properties"": {""location"": {""type"":""string"",""description"":""도시와 주, 예: San Francisco, CA"",# 위치 매개변수에 대한 설명},# 온도 단위 매개변수 (섭씨 또는 화씨)""unit"": {""type"":""string"",""enum"": [""celsius"",""fahrenheit""]},
                },""required"": [""location""],# 필수 매개변수 지정},
        },
    }
]
```
- bind()메서드를 사용하여tools를 모델에 바인딩합니다.
- invoke()메서드를 호출하여 ""샌프란시스코, 뉴욕, 로스앤젤레스의 현재 날씨에 대해 알려줘?""라는 질문을 모델에 전달합니다.
bind()메서드를 사용하여tools를 모델에 바인딩합니다.
invoke()메서드를 호출하여 ""샌프란시스코, 뉴욕, 로스앤젤레스의 현재 날씨에 대해 알려줘?""라는 질문을 모델에 전달합니다.
```
Copy# ChatOpenAI 모델을 초기화하고 도구를 바인딩합니다.model = ChatOpenAI(model=""gpt-3.5-turbo"").bind(tools=tools)# 모델을 호출하여 샌프란시스코, 뉴욕, 로스앤젤레스의 날씨에 대해 질문합니다.model.invoke(""샌프란시스코, 뉴욕, 로스앤젤레스의 현재 날씨에 대해 알려줘?"")
```
```
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iHdvMMJ8xA0hGNRi3Xu5lJLR', 'function': {'arguments': '{""location"": ""San Francisco, CA""}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ShDVUUvz9XcVOcMwvFl3eazS', 'function': {'arguments': '{""location"": ""New York, NY""}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_gdW3CfrYWa3Ap8kIPE7FFanF', 'function': {'arguments': '{""location"": ""Los Angeles, CA""}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 127, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'tool_calls', 'logprobs': None})
```"
11. 폴백(fallback) 모델 지정,https://wikidocs.net/235938,"# 11. 폴백(fallback) 모델 지정

LLM 애플리케이션에는 LLM API 문제, 모델 출력 품질 저하, 다른 통합 관련 이슈 등 다양한 오류/실패가 존재합니다. 이러한 문제를 우아하게 처리하고 격리하는데fallback기능을 활용할 수 있습니다.
중요한 점은 fallback 을 LLM 수준뿐만 아니라 전체 실행 가능한 수준에 적용할 수 있다는 것입니다.
## LLM API Error 에 대처 방법
LLM API 오류 처리는fallback을 사용하는 가장 일반적인 사례 중 하나입니다.
LLM API 에 대한 요청은 다양한 이유로 실패할 수 있습니다. API가 다운되었거나, 속도 제한에 도달했거나, 그 외 여러 가지 문제가 발생할 수 있습니다. 따라서fallback을 사용하면 이러한 유형의 문제로부터 보호하는 데 도움이 될 수 있습니다.
중요: 기본적으로 많은 LLM 래퍼(wrapper)는 오류를 포착하고 재시도합니다.fallback을 사용할 때는 이러한 기본 동작을 해제하는 것이 좋습니다. 그렇지 않으면 첫 번째 래퍼가 계속 재시도하고 실패하지 않을 것입니다.
```
Copy%pip install -qU langchain langchain-openai
```
먼저, OpenAI에서RateLimitError가 발생하는 경우에 대해 모의 테스트를 해보겠습니다.RateLimitError는 OpenAI API의API 호출 비용 제한을 초과했을 때 발생하는 오류입니다.
이 오류가 발생하면 일정 시간 동안 API 요청이 제한되므로, 애플리케이션에서는 이에 대한 적절한 처리가 필요합니다. 모의 테스트를 통해RateLimitError발생 시 애플리케이션이 어떻게 동작하는지 확인하고, 오류 처리 로직을 점검할 수 있습니다.
이를 통해 실제 운영 환경에서 발생할 수 있는 문제를 사전에 방지하고, 안정적인 서비스를 제공할 수 있습니다.
```
Copyfromlangchain_anthropicimportChatAnthropicfromlangchain_openaiimportChatOpenAI
```
```
Copyfromunittest.mockimportpatchimporthttpxfromopenaiimportRateLimitError

request = httpx.Request(""GET"",""/"")# GET 요청을 생성합니다.response = httpx.Response(200, request=request
)# 200 상태 코드와 함께 응답을 생성합니다.# ""rate limit"" 메시지와 응답 및 빈 본문을 포함하는 RateLimitError를 생성합니다.error = RateLimitError(""rate limit"", response=response, body="""")
```
openai_llm변수에ChatOpenAI객체를 생성하고,max_retries매개변수를 0으로 설정하여API 호출비용 제한 등으로 인한 재시도를 방지합니다.
with_fallbacks메서드를 사용하여anthropic_llm을fallbackLLM으로 설정하고, 이를llm변수에 할당합니다.
```
Copy# OpenAI의 ChatOpenAI 모델을 사용하여 openai_llm 객체를 생성합니다.# max_retries를 0으로 설정하여 속도 제한 등으로 인한 재시도를 방지합니다.openai_llm = ChatOpenAI(max_retries=0)# Anthropic의 ChatAnthropic 모델을 사용하여 anthropic_llm 객체를 생성합니다.anthropic_llm = ChatAnthropic(model=""claude-3-opus-20240229"")# openai_llm을 기본으로 사용하고, 실패 시 anthropic_llm을 대체로 사용하도록 설정합니다.llm = openai_llm.with_fallbacks([anthropic_llm])
```
```
Copy# OpenAI LLM을 먼저 사용하여 오류가 발생하는 것을 보여줍니다.withpatch(""openai.resources.chat.completions.Completions.create"", side_effect=error):try:# ""닭이 길을 건넌 이유는 무엇일까요?""라는 질문을 OpenAI LLM에 전달합니다.print(openai_llm.invoke(""Why did the chicken cross the road?""))exceptRateLimitError:# 오류가 발생하면 오류를 출력합니다.print(""에러 발생"")
```
```
에러 발생
```
OpenAI API의 비용 제한(rate limit)을 시뮬레이션하고, API 호출비용 제한 오류가 발생했을 때의 동작을 테스트하는 예제입니다.
OpenAI 의 GPT 모델을 시도하는데 에러가 발생했고, fallback 모델인Anthropic의 모델이 대신 추론을 수행했다는 점을 확인할 수 있습니다.
with_fallbacks()로 대체 모델이 설정되어 있고, 대체 모델이 성공적으로 수행했다면,RateLimitError가 발생하지 않습니다.
```
Copy# OpenAI API 호출 시 에러가 발생하는 경우 Anthropic 으로 대체하는 코드withpatch(""openai.resources.chat.completions.Completions.create"", side_effect=error):try:# ""대한민국의 수도는 어디야?""라는 질문을 언어 모델에 전달하여 응답을 출력합니다.print(llm.invoke(""대한민국의 수도는 어디야?""))exceptRateLimitError:# RateLimitError가 발생하면 ""에러 발생""를 출력합니다.print(""에러 발생"")
```
```
content='대한민국의 수도는 서울특별시입니다. \n\n서울은 한반도 중앙에 위치하며, 한강을 끼고 있는 대한민국 최대의 도시입니다. 서울의 인구는 약 1000만 명으로 전체 한국 인구의 약 20%가 서울에 거주하고 있습니다. \n\n서울은 조선시대부터 한국의 수도 역할을 해왔으며, 현재는 정치, 경제, 사회, 문화 등 대한민국의 중심지 역할을 하고 있습니다. 대한민국 정부 주요 기관들이 서울에 위치해 있으며, 다양한 기업의 본사도 서울에 많이 자리잡고 있습니다.\n\n또한 고궁, 박물관, 현대적 건축물 등 새로운 것과 전통적인 것이 조화를 이루는 매력적인 도시로서, 많은 관광객이 방문하는 글로벌 도시이기도 합니다.' response_metadata={'id': 'msg_012yS3DPqGNPoAoyVQR2xrWE', 'content': [ContentBlock(text='대한민국의 수도는 서울특별시입니다. \n\n서울은 한반도 중앙에 위치하며, 한강을 끼고 있는 대한민국 최대의 도시입니다. 서울의 인구는 약 1000만 명으로 전체 한국 인구의 약 20%가 서울에 거주하고 있습니다. \n\n서울은 조선시대부터 한국의 수도 역할을 해왔으며, 현재는 정치, 경제, 사회, 문화 등 대한민국의 중심지 역할을 하고 있습니다. 대한민국 정부 주요 기관들이 서울에 위치해 있으며, 다양한 기업의 본사도 서울에 많이 자리잡고 있습니다.\n\n또한 고궁, 박물관, 현대적 건축물 등 새로운 것과 전통적인 것이 조화를 이루는 매력적인 도시로서, 많은 관광객이 방문하는 글로벌 도시이기도 합니다.', type='text')], 'model': 'claude-3-opus-20240229', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=22, output_tokens=339)}
```
llm.with_fallbacks()설정한 모델도 일반 runnable 모델과 동일하게 동작합니다.
아래의 코드 역시 ""오류 발생""은 출려되지 않습니다. fallbacks 모델이 잘 수행했기 때문입니다.
```
Copyfromlangchain_core.promptsimportChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (""system"",""질문에 짧고 간결하게 답변해 주세요."",# 시스템 역할 설명),
        (""human"",""{country} 의 수도는 어디입니까?""),# 사용자 질문 템플릿]
)
chain = prompt | llm# 프롬프트와 언어 모델을 연결하여 체인 생성# chain = prompt | ChatOpenAI() # 이 코드이 주석을 풀고 실행하면 ""오류 발생"" 문구가 출력됩니다.withpatch(""openai.resources.chat.completions.Completions.create"", side_effect=error):try:print(chain.invoke({""country"":""대한민국""}))# 체인을 호출하여 결과 출력exceptRateLimitError:# API 비용 제한 오류 처리print(""오류 발생"")
```
```
content='대한민국의 수도는 서울특별시입니다.' response_metadata={'id': 'msg_013FiBouyK7dRti21HMLRvwR', 'content': [ContentBlock(text='대한민국의 수도는 서울특별시입니다.', type='text')], 'model': 'claude-3-opus-20240229', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=46, output_tokens=23)}
```
## 처리해야할 오류를 구체적으로 명시한 경우
특정 오류를 처리하기 위해fallback이 호출되는 시점을 더 명확하게 지정할 수도 있습니다. 이를 통해fallback메커니즘이 동작하는 상황을 보다 세밀하게 제어할 수 있습니다.
예를 들어, 특정 예외 클래스나 오류 코드를 지정함으로써 해당 오류가 발생했을 때만 fallback 로직이 실행되도록 설정할 수 있습니다. 이렇게 하면불필요한fallback호출을 줄이고, 오류 처리의 효율성을 높일 수있습니다.
아래의 예제에서는 ""오류 발생"" 문구가 출력됩니다. 이유는exceptions_to_handle파라미터에서KeyboardInterrupt예외가 발생시에만fallback이 구동되도록 설정했기 때문입니다. 따라서,KeyboardInterrupt를 제외한 모든 예외에서는fallback이 발생하지 않습니다.
```
Copyllm = openai_llm.with_fallbacks(# 대체 LLM으로 anthropic_llm을 사용하고, 예외 처리할 대상으로 KeyboardInterrupt를 지정합니다.[anthropic_llm],
    exceptions_to_handle=(KeyboardInterrupt,),# 예외 처리 대상을 지정합니다.)# 프롬프트와 LLM을 연결하여 체인을 생성합니다.chain = prompt | llmwithpatch(""openai.resources.chat.completions.Completions.create"", side_effect=error):try:# 체인을 호출하여 결과를 출력합니다.print(chain.invoke({""country"":""대한민국""}))exceptRateLimitError:# RateLimitError 예외가 발생하면 ""오류 발생""를 출력합니다.print(""오류 발생"")
```
```
content='대한민국의 수도는 서울특별시입니다.' response_metadata={'id': 'msg_01UbBNaKkSPecHATCKVwyMHQ', 'content': [ContentBlock(text='대한민국의 수도는 서울특별시입니다.', type='text')], 'model': 'claude-3-opus-20240229', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=46, output_tokens=23)}
```
## fallback 에 여러 모델을 순차적으로 지정
fallback모델에 1가지 모델만 지정할 수 있는 것은 아니고, 여러 개의 모델을 지정 가능합니다. 이렇게 여러개의 모델을 지정했을 때 순차적으로 시도하게 됩니다.
```
Copyfromlangchain.promptsimportPromptTemplatefromlangchain_core.output_parsersimportStrOutputParserfromlangchain_openaiimportChatOpenAI# 프롬프트 생성prompt_template = (""질문에 짧고 간결하게 답변해 주세요.\n\nQuestion:\n{question}\n\nAnswer:"")
prompt = PromptTemplate.from_template(prompt_template)
```
오류를 발생하는 chain 과 정상적인 chain 2가지를 생성합니다.
```
Copy# 여기서는 쉽게 오류를 발생시킬 수 있는 잘못된 모델 이름을 사용하여 체인을 생성할 것입니다.chat_model = ChatOpenAI(model_name=""gpt-fake"")
bad_chain = prompt | chat_model
```
```
Copy# fallback 체인을 생성합니다.fallback_chain1 = prompt | ChatOpenAI(model=""gpt-3.6-turbo"")# 오류fallback_chain2 = prompt | ChatOpenAI(model=""gpt-3.5-turbo"")# 정상fallback_chain3 = prompt | ChatOpenAI(model=""gpt-4-turbo-preview"")# 정상
```
```
Copy# 두 개의 체인을 결합하여 최종 체인을 생성합니다.chain = bad_chain.with_fallbacks(
    [fallback_chain1, fallback_chain2, fallback_chain3])# 생성된 체인을 호출하여 입력값을 전달합니다.chain.invoke({""question"":""대한민국의 수도는 어디야?""})
```
```
AIMessage(content='서울입니다.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 46, 'total_tokens': 51}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})
```"
CH14 에이전트(Agent),https://wikidocs.net/233782,"# CH14 에이전트(Agent)

에이전트"
01. Agent 사용법 톺아보기,https://wikidocs.net/234282,"# 01. Agent 사용법 톺아보기

## Agent
에이전트 프레임워크를 가장 잘 이해하기 위해서, 온라인에서 정보를 찾는 도구 하나와 우리가 인덱스로 로드한 특정 데이터를 찾는 도구 하나를 가진 에이전트를 만들어 보겠습니다.
LangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다.
단계별로 호출되는 로직이 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은LangSmith를 사용하는 것입니다.
LangSmith가 필수는 아니지만, 매우 유용한 도구입니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.
## 도구(Tools)
Agent 가 활용할 도구를 정의하여 Agent 가 추론(reasoning)을 수행할 때 활용하도록 만들 수 있습니다.
Tavily Search 는 그 중 대표적인검색 도구입니다. 검색을 통해 최신 정보에 접근하여 검색 결과를 가지고 답변을 생성할 수 있습니다. 도구는 이처럼 검색 도구 뿐만아니라 Python 코드를 실행할 수 있는 도구, 직접 정의한 함수를 실행하는 도구 등 다양한 종류와 방법론을 제공합니다.
LangChain에는 Tavily 검색 엔진을 도구로 쉽게 사용할 수 있는 내장 도구가 있습니다.
Tavily Search 를 사용하기 위해서는 API KEY를 발급 받아야 합니다.
- Tavily Search API 발급받기
Tavily Search API 발급받기
발급 받은 API KEY 를 다음과 같이 환경변수에 등록 합니다.
아래 코드의 주석을 풀고 발급받은 API KEY 를 설정합니다.
```
Copyimportos# TAVILY API KEY를 기입합니다.# os.environ[""TAVILY_API_KEY""] = ""TAVILY API KEY 입력""# 디버깅을 위한 프로젝트명을 기입합니다.os.environ[""LANGCHAIN_PROJECT""] =""AGENT TUTORIAL""
```
```
Copy# API KEY를 환경변수로 관리하기 위한 설정 파일fromdotenvimportload_dotenv# API KEY 정보로드load_dotenv()
```
```
True
```
```
Copy# TavilySearchResults 클래스를 langchain_community.tools.tavily_search 모듈에서 가져옵니다.fromlangchain_community.tools.tavily_searchimportTavilySearchResults# TavilySearchResults 클래스의 인스턴스를 생성합니다# k=5은 검색 결과를 5개까지 가져오겠다는 의미입니다search = TavilySearchResults(k=5)
```
search.invoke함수는 주어진 문자열에 대한 검색을 실행합니다.
invoke()함수에 검색하고 싶은 검색어를 넣어 검색을 수행합니다.
```
Copy# 검색 결과를 가져옵니다.search.invoke(""판교 카카오 프렌즈샵 아지트점의 전화번호는 무엇인가요?"")
```
```
[{'url': 'https://m.blog.naver.com/pluie_neige/222941671226',  'content': 'Style\nDaily I lovely I Trendy\n협찬 및 비지니스\npale_brush@naver.com\n이 블로그\ndaily\n카테고리 글\n카테고리\n이 블로그\ndaily\n카테고리 글 내적 갈등... 을\n순간했지만 매장 나오자마자\n머릿속에서 빛삭...ㅋㅋㅋㅋㅋ\n\u200b\n그래... 난 역시 캐릭터를 안 좋아해 ㅋㅋㅋ\n(저는 일 때문에 시조 나온 거라서 ㅋㅋㅋ\n양해 부탁드립니다아...ㅋㅋㅋㅋ)\n \u200b\n\u200b\n\u200b\n\u200b\n직장인의 일상과\n패션을 기록합니다.\nLike_French (진짜 교통난 더 심해짐...ㅠㅠ)\n\u200b\n근데 여기 카카오 건물은 진짜 주차에\n진심으로 신경 쓴 것 같았어요!\n 정말 대왕만 한 춘식이가 대기 중이라서 놀램.\n(사실 캐릭터 안좋아함...ㅋㅋㅋㅋ)\n\u200b\n그래도 왕만 하게 있으니까 귀염 그 잡채.\n'}, {'url': 'https://blog.naver.com/PostView.naver?blogId=0_0hy&logNo=222982732699',  'content': '새해 목표 예쁜 쓰레기 구매하지 않기 잘 실천중\n춘식이가 쓰레기라는건 아니지만^__ㅠ\n\u200b\n바라만봐도 행복해진다\n\u200b\n애견용품까지 춘식이 버전ㅋㅋㅋㅋㅋㅋㅋㅋ\n\u200b\n♥ ♥ ♥\n\u200b\n\u200b\n\u200b\n\u200b\n2m 대왕 춘식이랑 ~~♥\n\u200b\n진짜 사려다가 참았다 .\n \u200b\n\u200b\n춘식이 랜덤 피규어 ♥\n카카오프렌즈 최애 캐릭터가 춘식이라 보자마자 구매해야 겠다고 생각했다\n\u200b\n\u200b\n젤 갖고싶었던 만년달력.. 가격은 사악 그잡채\n\u200b\n춘식이 x 최고심 콜라보 *__*\n\u200b\n\u200b\n두번째로 좋아하는 죠르디\n앉은모습 졸귀야 ..\n 갖고 싶단 말이지\n\u200b\n\u200b\n에어팟 꽂은 라이언ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n\u200b\n\u200b\n춘식러버는 춘식이 천국이라 행복\n\u200b\n\u200b\n\u200b\n랜드마크 브릭피규어\n우리나라 랜드마크도 출시 예정이라고 하던데 언제 나오는지..?\n \u200b\n\u200b\n\u200b\n\u200b\n카카오프렌즈 판교아지트점\n경기 성남시 분당구 판교역로 166 카카오판교아지트 1층 카카오프렌즈\n평일 10:00 - 21:00, 토일 12:00 - 20:00\n031-601-7225\n판교 카카오 본사 건물 1층에 있음 !\n \u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n앙꼬 / 루피 / 무직타이거 까지 !!\n'}, {'url': 'https://promenade7.tistory.com/entry/멋진-카카오-아지트-판교역-카카오-프렌즈샵-방문-후기',  'content': ""ì¹´ì¹´ì˜¤\xa0ìº�ë¦\xadí„°ê°€\xa0ì•„ë§ˆë�„\xa0LEDë¡œ\xa0ë³´ì�´ëŠ”\xa0ì\xa0„ê´‘íŒ�ì—�\xa0ê³„ì†�\xa0ë‚˜ì˜µë‹ˆë‹¤. ì�´ëŸ°\xa0ê±°\xa0ë§Œë“œëŠ”\xa0ì§�ì›�ë�„\xa0ì°¸\xa0ì‰½ì§€\xa0ì•Šê²\xa0ë„¤ìš”~\nê³³ê³³ì—�\xa0ìžˆëŠ”\xa0ì¹´ì¹´ì˜¤\xa0ìº�ë¦\xadí„°ì™€\xa0í•¨ê»˜\xa0ì‚¬ì§„ë�„\xa0ì°�ì�„\xa0ìˆ˜\xa0ìžˆìŠµë‹ˆë‹¤.\nì˜¬ë�¼ê°€ë³´ì§€ëŠ”\xa0ì•Šì•˜ì§€ë§Œ\xa0ìƒ�ì¸µë¶€ë¡œë�„\xa0ê³„ì†�\xa0ì´ˆë¡�ìƒ‰ë“¤ì�´\xa0ë³´ì�´ê³\xa0 1ì¸µì—�ë�„\xa0ì�´ë\xa0‡ê²Œ\xa0ë§ˆì¹˜\xa0ì™¸ë¶€\xa0ìž�ì—°ì�„\xa0ì˜®ê²¨\xa0ë†“ì�€\xa0ë“¯í•œ\xa0ì¡°ê²½ê³µê°„ì�´\xa0ë§Žì•˜ìŠµë‹ˆë‹¤. ì‚\xadë§‰í•\xa0\xa0ìˆ˜\xa0ìžˆëŠ”\xa0ë�„ì‹¬ì�˜\xa0ì˜¤í”¼ìŠ¤\xa0ë¹Œë”©\xa0ë‚´ë¶€ì—�\xa0ê·¸ëž˜ë�„\xa0ìž�ì—°ì�„\xa0ëŠ�ë‚„\xa0ìˆ˜\xa0ìžˆê²Œ\xa0ìž˜\xa0ë§Œë“¤ì–´ë†¨ë„¤ìš”.\n ì˜¤í”¼ìŠ¤ ê±´ë¬¼ì�´ë�¼ ë‹¹ì—°ížˆ ëª»ì˜¬ë�¼ê°€ê²\xa0ì£\xa0.ì•ˆê·¸ëž˜ë�„ íšŒì‹�ë‚\xa0ì�´ë‚˜ ì£¼ë³€ ì§€ë‚˜ê°€ë‹¤ ìž�ì£¼ íŒ�êµ� ì•„ì§€íŠ¸ ì˜†ì�„ ì§€ë‚˜ë‹¤ë‹ˆëŠ”ë�° ê±´ë¬¼\xa01ì¸µì—�\xa0ì¹´ì¹´ì˜¤í”„ë\xa0Œì¦ˆìƒµì�´\xa0ìžˆë‹¤ëŠ”ê²Œ\xa0ìƒ�ê°�\xa0ë‚¬ìŠµë‹ˆë‹¤. ì‹œê°„ì�´\xa0ë§žì•„ì„œ\xa0ì•„ì�´ë“¤ê³¼\xa0í•¨ê»˜\xa0ì£¼ë§�ì—�\xa0ì‹¬ì§€ì–´\xa0ì˜¤í”ˆëŸ°(?)ìœ¼ë¡œ\xa0ê°€ë³´ê²Œ\xa0ë�˜ì—ˆìŠµë‹ˆë‹¤. ë„“ì�€\xa0ê³µê°„ì—�\xa0ì•„ê¸°ìž�ê¸°í•œ\xa0ì�¸í…Œë¦¬ì–´ê°€\xa0ì�¸ìƒ�ì\xa0�ì�´ê³\xa0\xa0ìž¬ë°Œê³\xa0\xa0ê·€ì—¬ìš´\xa0ìƒ�í’ˆë“¤ì�´\xa0ë§Žì•˜ìŠµë‹ˆë‹¤. ì¹´ì¹´ì˜¤í”„ë\xa0Œì¦ˆìƒµ íŒ�êµ�ì•„ì§€íŠ¸ì\xa0� í›„ê¸° ì�¨ë³´ê²\xa0ìŠµë‹ˆë‹¤.\n1_íŒ�êµ�ì—\xad ì¹´ì¹´ì˜¤í”„ë\xa0Œì¦ˆ íŒ�êµ�ì•„ì§€íŠ¸ì\xa0� ìœ„ì¹˜, ì˜¤í”ˆì‹œê°„\nì¹´ì¹´ì˜¤í”„ë\xa0Œì¦ˆ íŒ�êµ�ì•„ì§€íŠ¸ì\xa0�ì�€ íŒ�êµ�ì—\xad 4ë²ˆì¶œêµ¬ Œì¦ˆ ê°“ìƒ� ë‹¤ì�´ì–´ë¦¬ ì‚´íŽ´ë³´ê¸°\nì˜¤ëŠ˜ë¶€í„° ê°“ìƒ�! ì§€ë‚œë²ˆì—� ì˜¬ë\xa0¸ë�˜ ë‹¤ì�´ì–´ë¦¬ ì„¸íŠ¸ê°€ ì§„ì—´ë�˜ì–´ ìžˆì—ˆì–´ìš”.ì\xa0œ\xa0ë‹¤ì�´ì–´ë¦¬\xa0ì„¸íŠ¸ëŠ”\xa0ì—†ëŠ”ë�°\xa0ë‹¤ë¥¸\xa0ë¯¼íŠ¸ìƒ‰\xa0ë‹¤ì�´ì–´ë¦¬\xa0ì„¸íŠ¸ëŠ”\xa0ìžˆì—ˆì–´ìš”.ê°™ì�´ ì§„ì—´ë�˜ì–´ ìžˆëŠ” ë‹¬ë\xa0¥ì�€ ì•„ë§ˆë�„ ì\xa0œê°€ ë°›ì�€ê±°ì™€ ê°™ì•„ë³´ìž…ë‹ˆë‹¤.\nêµ°ë�°êµ°ë�°\xa0ìš”\xa0ë¯¼íŠ¸ìƒ‰\xa0ìœ„í�´ë¦¬\xa0ë‹¤ì�´ì–´ë¦¬ì„¸íŠ¸ê°€\xa0ì§„ì—´ë�˜ì–´\xa0ìžˆì—ˆê³\xa0 ì‹¤ë¬¼ë¡œ íŽ´ë³¼ ìˆ˜ ìžˆëŠ” ê³³ì�€ ì¶˜ì‹�'s ë£¸ì—� ìžˆì—ˆìŠµë‹ˆë‹¤.\nì\xa0œê°€ ë°›ì•˜ë�˜ ë‹¬ë\xa0¥ì�´ ìžˆê³\xa0 ìŠ¤í”„ë§� ë‹¤ì�´ì–´ë¦¬ë�„ ì§„ì—´ë�˜ì–´ ìžˆì—ˆì–´ìš”.\n ì¦�ê¸°ë©° ê²½í—˜í•˜ê³\xa0 ê¸°ë¡�í•˜ëŠ” ë¸”ë¡œê·¸\në©‹ì§„ ì¹´ì¹´ì˜¤ ì•„ì§€íŠ¸ íŒ�êµ�ì—\xad ì¹´ì¹´ì˜¤ í”„ë\xa0Œì¦ˆìƒµ ë°©ë¬¸ í›„ê¸°\në©‹ì§„ ì¹´ì¹´ì˜¤ ì•„ì§€íŠ¸ íŒ�êµ�ì—\xad ì¹´ì¹´ì˜¤ í”„ë\xa0Œì¦ˆìƒµ ë°©ë¬¸ í›„ê¸°\nì�´ë²ˆ\xa0í�¬ìŠ¤íŒ…ì�€\xa0íŒ�êµ�ì—\xadì—�\xa0ìžˆëŠ”\xa0ì¹´ì¹´ì˜¤í”„ë\xa0Œì¦ˆìƒµ\xa0íŒ�êµ�ì•„ì§€íŠ¸ì\xa0�\xa0ë°©ë¬¸\xa0í›„ê¸°ìž…ë‹ˆë‹¤. ì–¼ë§ˆì\xa0„ì—�\xa0í�¬ìŠ¤íŒ…í•œ\xa0ë‚´ìš©ìœ¼ë¡œ\xa0ì—°ë§�ì—�\xa0í‹°ìŠ¤í†\xa0ë¦¬ì�˜\xa0ëŒ“ê¸€\xa0ì�´ë²¤íŠ¸ì—�\xa0ì°¸ì—¬í•´ì„œ ì¹´ì¹´ì˜¤\xa0í”„ë\xa0Œì¦ˆ\xa0ë¸”ëž™\xa0ë‹¤ì�´ì–´ë¦¬\xa0ì„¸íŠ¸ë¥¼\xa0ë°›ì•˜ì—ˆì–´ìš”. ì�´ ë¸Œë¦\xadë“¤ ë‹¤ë“¤ ì�´ì�˜ê³\xa0 ìž¬ë°Œì–´ ë³´ì—¬ì„œ ì\xa0€ë�„ ì§‘ì—� ê°€ì\xa0¸ë‹¤ ë†“ê³\xa0 ì‹¶ì—ˆì–´ìš”.ã…\në§Œë“¤ì–´ì\xa0¸ ìžˆëŠ” ëª¨ìŠµì—�ëŠ” ì—†ì§€ë§Œ ìŠ¤í•‘í�¬ìŠ¤ í”¼ë�¼ë¯¸ë“œ ì\xa0œí’ˆë�„ ìžˆì—ˆìŠµë‹ˆë‹¤.\në‹¤ì�Œ ì½”ë„ˆë¡œ ë„˜ì–´ì˜¤ëŠ”ë�° ì�¸í˜•, ìš°ì‚°, ì\xa0„ìž�ê¸°ê¸° ë“± ë§Žì�€ ì\xa0œí’ˆë“¤ì�´ ìžˆì—ˆìŠµë‹ˆë‹¤.\nì�¸í˜•ì\xa0œí’ˆë�„ ë§Žì�€ë�°..ì\xa0„ìž�ì\xa0œí’ˆë¥˜ë�„ ë§Žì�´ ìžˆìŠµë‹ˆë‹¤.\nìº�ë¦\xadí„° ì�´ë¦„ì�€ ëª¨ë¥´ì§€ë§Œ ê°™ì�´ ì‚¬ì§„ì�„ ì°�ì�„ ìˆ˜ ìžˆê²Œ í�¬í†\xa0ì¡´ë�„ ìžˆì—ˆì–´ìš”.\n""}, {'url': 'https://m.blog.naver.com/ifnotnow_thenwhen/223203793513',  'content': '카카오프렌즈샵 판교아지트. 존재하지 않는 이미지입니다. 정말 국민 캐릭터라고 해도 크게 반발할 사람 없는. #카카오프렌즈. 남편 친구가 판교 카카오아지트에서 일한다고 들었는데. 짱 좋다길래 한번 구경 가보고 싶었어요 ♥. 판교 카카오아지트는 판교역 4 ...'}, {'url': 'https://m.blog.naver.com/celina_827/222924602339',  'content': '너무 귀여운 카카오 속 세상이예요ㅠㅠ\n\u200b\n경기도 성남시 분당구 판교역로 166\n💬카카오프렌즈샵 아지트점은\n판교역 1번 출구, 4번 출구를 이용하셔도 되고,\n지하통로로도 연결되어있으니 참고 하세요!\n\u200b\n💬영업시간\n월 -금 10 : 00 ~ 21 : 00\n토 - 일 12 : 00 ~ 20 : 00\n\u200b\n\u200b\n소소한 episode가 있는 일상, 정보, 맛집을 포스팅하는 셀리나 입니다 ( ੭ ･ᴗ･ )੭\n이 블로그\nPANGYO LIFE\n카테고리 글\n카테고리\n이 블로그\nPANGYO LIFE\n카테고리 글 \u200b\n근데 저 손잡은 춘식이 너무 귀여운거 아니냐구요 ㅠㅠ\n크리스마스 맞이 산타 라이언🎄\n폭신한 라이언 바디 필로우 사고싶지만\n99.000원💸\n핳 너무 비싸당 ㅎㅎㅎㅎ\n아이패드파우치, 폰 거치대, 에어팟케이스 많지만\n전...ㅎ 마우스 패드 살까 말까 고민 엄청했어요\n\u200b\n근데 과연 사이즈가 괜찮을까 싶고...\n근데 너무 귀엽다...\n(ﾉ◕ヮ◕)ﾉ\n\u200b\n그리고 카카오 프렌즈샵에\n차량 아이템이 생각보다 많아서 놀랬어요!\n피규어가 들어가있어서 그런지\n주차번호판도 비싸다 ㅎㅎㅎ\n차량 내부 송풍다는 송풍구 방향제!!\n \u200b\n이걸 대체 어디다 써?싶었지만\n요즘 정말 젊은 사람들도 골프를 많이 치니까\n채를 보호하는 커버나 보호대같은걸 귀여운 캐릭터로\n사는 사람이 있겠구나 싶더라구요\U0001fae3\n골프용품이 있는곳에 있는 이 인형들은\n도대체 어디다 쓰는거지 싶었는데\n\u200b\n요렇게 쓰더라구요 ᕙ༼◕ ᴥ ◕༽ᕗ\n👇🏻👇🏻\n아니 이러면 커버 뚜껑은 어케 잠그지....?\n 🔖혀땳은앙꼬\n\u200b\n앙꼬 이모티콘을 너무 좋아해서 사서 쓰곤 했는데\n이렇게 캐릭터 굿즈가 나오다니\n너무 반갑더라구요 😍😍\n스티커는 진작 품절이더라구요◡̈\n역시 앙꼬는 귀여운 아이였어 ∑ദ്ദി˙◡･)\n\u200b\n🔖인스타에서 핫하다는 목욕하는 춘식이◡̈\n지금 카카오 프랜즈들의 대세는 춘식이인가봐요.\n 🔖소주디스펜서죠르디 39,000원\n🔖전동와인오프너치얼스라이언\n23,000원\n🔖라이언크리미맥주거품기 29,000원\n🔖턴테이블 스피커\n55,000원\n여긴 스텐드들의 집합?💡\n캬아 춘식이 피큐어예요!!!\n'}]
```
우리의 데이터에 대해 조회를 수행할 retriever도 생성합니다.
이 코드는 웹 기반 문서 로더, 문서 분할기, 벡터 저장소, 그리고 OpenAI 임베딩을 사용하여 문서 검색 시스템을 구축합니다.
여기서는 PDF 문서를FAISSDB 에 저장하고 조회하는 retriever 를 생성합니다.
```
Copyfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain.document_loadersimportPyPDFLoader# PDF 파일 로드. 파일의 경로 입력loader = PyPDFLoader(""data/SPRI_AI_Brief_2023년12월호_F.pdf"")# 텍스트 분할기를 사용하여 문서를 분할합니다.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)# 문서를 로드하고 분할합니다.split_docs = loader.load_and_split(text_splitter)# VectorStore를 생성합니다.vector = FAISS.from_documents(split_docs, OpenAIEmbeddings())# Retriever를 생성합니다.retriever = vector.as_retriever()
```
이 함수는retriever객체의get_relevant_documents메소드를 사용하여 ""YouTube의 2024년부터 AI 생성콘텐츠 표시 의무화에 대한 내용을 알려줘"" 와 관련된 문서들 중 첫 번째 문서를 반환합니다.
이는 특정 쿼리에 대한 가장관련성 높은 문서를 찾는 데 사용됩니다.
```
Copy# PDf 문서에서 Query 에 대한 관련성 높은 Chunk 를 가져옵니다.retriever.get_relevant_documents(""YouTube의 2024년부터 AI 생성콘텐츠 표시 의무화에 대한 내용을 알려줘"")[0]
```
```
Document(page_content='SPRi AI Brief |  \n2023-12 월호\n14유튜브 , 2024년부터 AI 생성 콘텐츠 표시 의무화 \nn유튜브가 몇 달 안에 생성 AI를 사용한 콘텐츠에 AI 라벨 표시를 의무화하기로 했으며 , 이를 \n준수하지 않는 콘텐츠는 삭제하고 크리에이터에 대한 수익 배분도 중단할 수 있다고 설명\nn유튜브는 AI 생성 콘텐츠가 신원 파악이 가능한 개인을 모방한 경우 개인정보 침해 신고 \n절차에 따라 콘텐츠 삭제 요청도 받을 계획KEY Contents\n£유튜브 , 생성 AI 콘텐츠에 AI 라벨 표시 안 하면 콘텐츠 삭제\nn유튜브가 2023 년 11월 14일 공식 블로그를 통해 몇 달 안에 생성 AI를 사용한 콘텐츠에 AI \n라벨을 표시하는 새로운 규칙을 시행한다고 발표 \n∙실제로 일어나지 않은 사건을 사실적으로 묘사하거나 실제로 하지 않은 말이나 행동을 보여주는 콘텐츠와 \n같이 AI 도구를 사용해 사실적으로 변경되거나 합성된 콘텐츠에는 AI 라벨을 표시 필요\n∙유튜브는 이러한 규칙이 선거나 분쟁 상황, 공중 보건, 공직자 관련 문제와 같이 민감한 주제를 다루는 \n콘텐츠에서 특히 중요하다고 강조했으며 , 크리에이터가 AI로 제작한 콘텐츠에 AI 라벨을 표시하지 않으면 \n해당 콘텐츠는 삭제되고 광고 수익을 배분하는 유튜브 파트너 프로그램도 정지될 수 있음\n∙유튜브는 두 가지 방식으로 AI를 이용한 콘텐츠의 변경이나 합성 여부를 시청자에게 전달할 계획으로 \n동영상 설명 패널에 라벨을 표시하는 방식이 기본이며 , 민감한 주제를 다루는 특정 유형의 콘텐츠는 동영상 \n플레이어에 더욱 눈에 띄는 라벨을 적용 \n∙유튜브는 커뮤니티 정책에 위반되는 일부 합성 콘텐츠에 대해서는 라벨 지정 여부와 관계없이 삭제할 \n방침으로 , 가령 사실적인 폭력을 보여주는 합성 동영상이 시청자에게 충격이나 혐오감을 줄 수 있다면 \n삭제될 수 있음\n£유튜브 , 특정인을 모방한 AI 생성 콘텐츠에 대한 삭제 요청에도 대응 계획', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 16})
```
이제 우리가 검색을 수행할 인덱스를 채웠으므로, 이를 에이전트가 제대로 사용할 수 있는 도구로 쉽게 변환할 수 있습니다.
create_retriever_tool함수는langchain라이브러리의tools.retriever모듈에서 가져온다. 이 함수는 특정 데이터를 검색하기 위한 도구를 생성하는 데 사용된다.langchain은 언어 모델과 관련된 다양한 기능을 제공하는 라이브러리로, 이 중 검색 도구 생성 기능은 데이터 검색 및 처리 작업을 용이하게 한다.
```
Copy# langchain 패키지의 tools 모듈에서 retriever 도구를 생성하는 함수를 가져옵니다.fromlangchain.tools.retrieverimportcreate_retriever_tool
```
이 함수는create_retriever_tool을 호출하여retriever_tool을 생성합니다. 이 도구는LangSmith에 대한 정보를 검색하는 데 사용됩니다.
사용자가LangSmith에 대해 궁금한 점이 있을 때, 이 도구를 통해 질문을 해결할 수 있습니다. 함수는retriever인스턴스, 검색 도구의 이름인langsmith_search, 그리고 사용자에게 도구 사용 방법을 안내하는 메시지를 인자로 받습니다.
```
Copyretriever_tool = create_retriever_tool(
    retriever,
    name=""pdf_search"",
    description=""2023년 12월 AI 관련 정보를 PDF 문서에서 검색합니다. '2023년 12월 AI 산업동향' 과 관련된 질문은 이 도구를 사용해야 합니다!"",
)
```
이제 두 가지를 모두 만들었으므로, Agent 가 사용할 도구 목록을 만들 수 있습니다.
tools리스트는search와retriever_tool을 포함합니다. 이 리스트는 검색 및 정보 검색 도구를 저장하는 데 사용됩니다. 각 요소는 특정 작업을 수행하는 데 필요한 기능을 제공합니다.
```
Copy# tools 리스트에 search와 retriever_tool을 추가합니다.tools = [search, retriever_tool]
```
## 에이전트 생성
이제 도구를 정의했으니 에이전트를 생성할 수 있습니다. OpenAI Functions 에이전트를 사용할 것입니다.
먼저, 에이전트가 활용할 LLM을 정의합니다.
ChatOpenAI클래스는langchain_openai모듈에서 가져온 것으로, OpenAI의 언어 모델을 활용하여 대화형 AI를 구현할 수 있게 해줍니다.
이 예제에서는gpt-4-turbo-preview모델을 사용하며,temperature매개변수를0으로 설정하여 예측의 변동성을 최소화합니다.
```
Copyfromlangchain_openaiimportChatOpenAI# ChatOpenAI 클래스를 langchain_openai 모듈에서 가져옵니다.llm = ChatOpenAI(model=""gpt-4-turbo-preview"", temperature=0)
```
다음으로, 에이전트를 안내하기 위해 사용하고 싶은 프롬프트를 선택합니다.
이 프롬프트의 내용을 보고 싶고 LangSmith에 접근할 수 있다면, 다음으로 이동하세요:
다운로드 받아 활용할 프롬프트: https://smith.langchain.com/hub/hwchase17/openai-functions-agent
hub.pull()함수를 사용하여 리소스를 가져온 후,prompt.messages를 통해 해당 리소스의 메시지에 접근합니다.
```
Copyfromlangchainimporthub# hub에서 prompt를 가져옵니다 - 이 부분을 수정할 수 있습니다!prompt = hub.pull(""hwchase17/openai-functions-agent"")# prompt 의 messages를 출력합니다.prompt.messages
```
이제 LLM, 프롬프트 및 도구로 에이전트를 초기화할 수 있습니다. 에이전트는 입력을 받아 어떤Action을 취할지 결정하는 역할을 합니다.
중요한 것은 에이전트가Action들을 실행하지 않는다는 것입니다 - 그것은 AgentExecutor(다음 단계)에 의해 수행됩니다.
```
Copyfromlangchain.agentsimportcreate_openai_functions_agent# OpenAI 함수 기반 에이전트를 생성합니다.# llm, tools, prompt를 인자로 사용합니다.agent = create_openai_functions_agent(llm, tools, prompt)
```
마지막으로, 우리는 에이전트(agent)를 AgentExecutor 내부의 도구들과 결합합니다(이는 반복적으로 에이전트를 호출하고 도구들을 실행할 것입니다).
이 코드는langchain.agents모듈에서AgentExecutor클래스를 가져와 인스턴스를 생성합니다. 생성 시,agent,tools객체를 인자로 전달하고,verbose=True를 설정하여 상세한 로그 출력을 활성화합니다.AgentExecutor는 주어진 에이전트와 도구들을 사용하여 작업을 실행하는 역할을 합니다.
```
Copyfromlangchain.agentsimportAgentExecutor# AgentExecutor 클래스를 사용하여 agent와 tools를 설정하고, 상세한 로그를 출력하도록 verbose를 True로 설정합니다.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```
## 에이전트 실행하기
이제 몇 가지 질의에 대해 에이전트를 실행할 수 있습니다!
현재 이러한 모든 질의는상태(Stateless) 가 없는질의입니다(이전 상호작용을 기억하지 않습니다).
agent_executor객체의invoke메소드는 딕셔너리 형태의 인자를 받아 처리합니다. 이 예제에서는input키에hi!값을 할당한 딕셔너리를 인자로 전달하고 있습니다. 이는 일반적으로 AI 에이전트, 함수 실행기, 또는 명령 처리기 등의 객체에서 입력을 처리하기 위해 사용됩니다.
```
Copy# 'agent_executor' 객체의 'invoke' 메소드를 호출하여,# 'input' 키와 '안녕, 반가워' 값을 가진 딕셔너리를 인자로 전달합니다.response = agent_executor.invoke({""input"":""2024년 아시안컵 대한민국의 축구 경기 결과를 알려줘.""})print(f'답변:{response[""output""]}')
```
agent_executor객체의invoke메소드를 사용하여, 질문을 입력으로 제공합니다.
다음의 추적 링크에서 자세한 단계별 수행 결과를 확인할 수 있습니다
LangSmith 추적
```
Copy# 'agent_executor' 객체의 'invoke' 메소드를 호출하여, 'langsmith'가 테스팅에 어떻게 도움을 줄 수 있는지에 대한 질문을 입력으로 제공합니다.response = agent_executor.invoke(
    {""input"":""YouTube 2024년부터 AI 생성콘텐츠 표시 의무화에 대한 내용을 PDF 문서에서 알려줘""}
)print(f'답변:{response[""output""]}')
```
다음의 추적 링크에서 자세한 단계별 수행 결과를 확인할 수 있습니다
LangSmith 추적
```
Copy# 검색 결과를 요청 후 질문에 대한 답변을 출력합니다.response = agent_executor.invoke(
    {""input"":""판교 카카오 프렌즈샵 아지트점의 전화번호를 검색하여 결과를 알려주세요.""}
)print(f'답변:{response[""output""]}')
```
## 메모리 추가하기
앞서 언급했듯이, 이 에이전트는 상태가 없습니다. 이는 이전 상호작용을 기억하지 않는다는 것을 의미합니다. 이에게 메모리를 부여하기 위해서는 이전chat_history를 전달해야 합니다.
[참고]
우리가 사용하는 프롬프트 때문에chat_history라고 불러야 합니다. 다른 프롬프트를 사용한다면변수 이름을 변경할 수 있습니다.
함수는 사용자의 입력(input)과 채팅 기록(chat_history)을 매개변수로 받습니다. 여기서,chat_history는 채팅의 첫 번째 메시지를 처리할 때 빈 리스트([])로 전달됩니다. 이는 채팅 세션을 시작할 때 기존의 대화 기록이 없음을 나타냅니다.
```
Copy# 채팅의 첫 번째 메시지이므로 chat_history에 빈 리스트를 전달합니다.response = agent_executor.invoke(
    {""input"":""Hi! I'm Teddy. Glad to meet you."",""chat_history"": []}
)print(f'답변:{response[""output""]}')
```
이 모듈은langchain_core.messages에서AIMessage와HumanMessage클래스를 가져옵니다. AI와 인간 간의 메시지 교환을 처리하는 데 사용될 수 있습니다.
AIMessage는 AI가 생성한 메시지를 나타내며,HumanMessage는 사용자가 입력한 메시지를 나타냅니다.
```
Copy# langchain_core.messages에서 AIMessage와 HumanMessage를 가져옵니다.fromlangchain_core.messagesimportAIMessage, HumanMessage
```
이 함수는 대화 에이전트를 실행하여 사용자와 AI 간의 대화 이력과 새로운 사용자 입력을 처리합니다.agent_executor.invoke메소드는 대화 이력(chat_history)과 사용자의 새로운 입력(input)을 매개변수로 받습니다.chat_history는HumanMessage와AIMessage객체의 리스트로, 각각 사용자와 AI의 메시지를 나타냅니다. 이 예시에서는 사용자가 ""hi! my name is bob""이라고 인사하고, AI가 ""Hello Bob! How can I assist you today?""라고 응답한 이후, 사용자가 ""what's my name?""이라고 새로운 질문을 합니다.
```
Copyresponse = agent_executor.invoke(
    {""chat_history"": [
            HumanMessage(content=""Hi! I'm Teddy. Glad to meet you.""),
            AIMessage(
                content=""Hello Teddy! I'm glad to meet you too. How can I assist you today?""),
        ],""input"":""What's my name?"",
    }
)print(f'답변:{response[""output""]}')
```
만약 이 메시지들을 자동으로 추적하고 싶다면, 이를 RunnableWithMessageHistory로 감싸면 됩니다.
```
Copyfromlangchain_community.chat_message_historiesimportChatMessageHistoryfromlangchain_core.runnables.historyimportRunnableWithMessageHistory
```
ChatMessageHistory클래스의 인스턴스를 생성하여message_history변수에 할당합니다. 이 인스턴스는 채팅 메시지의 기록을 관리하는 데 사용됩니다.
```
Copy# 채팅 메시지 기록을 관리하는 객체를 생성합니다.message_history = ChatMessageHistory()
```
RunnableWithMessageHistory클래스는 에이전트 실행자(agent_executor)와 메시지 기록을 관리합니다. 이 클래스는 세션 ID를 기반으로 메시지 기록을 처리하는 람다 함수와, 입력 메시지 및 채팅 기록의 키를 인자로 받습니다.
여기서 세션 ID는 실제로 사용되지 않으며, 단순한 메모리 내ChatMessageHistory를 사용하는 예시입니다.
기본적으로 실행 가능 항목은 문자열인session_id라는 단일 구성 매개변수를 받습니다.
이 매개변수는 주어진 session_id와 일치하는 새 채팅 메시지 기록을 만들거나 기존 채팅 메시지 기록을 조회하는 데 사용됩니다.
이 경우 호출은 다음과 같습니다.
with_history.invoke(..., config={""configurable"": {""session_id"": ""bar""}});
예:{""configurable"": {""session_id"": ""<세션_id>""}}.
```
Copyagent_with_chat_history = RunnableWithMessageHistory(
    agent_executor,# 대부분의 실제 시나리오에서 세션 ID가 필요하기 때문에 이것이 필요합니다# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다lambdasession_id: message_history,# 프롬프트의 질문이 입력되는 key: ""input""input_messages_key=""input"",# 프롬프트의 메시지가 입력되는 key: ""chat_history""history_messages_key=""chat_history"",
)
```
agent_with_chat_history.invoke함수는 사용자 입력과 설정을 인자로 받아 처리합니다. 이 예제에서는 사용자의 입력(""hi! I'm bob"")과 설정(config)을 전달합니다. 설정에는session_id가 포함되어 있으나, 이는 간단한 메모리 내ChatMessageHistory를 사용하는 예제이므로 실제로는 사용되지 않습니다. 이 코드는 대부분의 실제 상황에서 세션 ID가 필요함을 보여주지만, 특정 구현에서는 그렇지 않을 수 있음을 보여줍니다.
```
Copyresponse = agent_with_chat_history.invoke(
    {""input"":""Hi! I'm Teddy. Glad to meet you.""},# 세션 ID를 설정합니다.# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다config={""configurable"": {""session_id"":""MyTestSessionID""}},
)print(f""답변:{response['output']}"")
```
이 함수 호출은agent_with_chat_history객체를 사용하여 입력된 질문에 대한 응답을 생성합니다. 입력은input키를 통해 제공되며, 여기서는 ""what's my name?""이 사용됩니다. 또한,config매개변수를 통해 추가 설정을 제공할 수 있으며, 이 예제에서는session_id를<foo>로 설정합니다. 이는 대부분의 실제 시나리오에서 세션 ID가 필요하지만, 이 경우 간단한 메모리 내ChatMessageHistory를 사용하기 때문에 실제로는 사용되지 않음을 나타냅니다.
```
Copyresponse = agent_with_chat_history.invoke(
    {""input"":""What's my name?""},# 세션 ID를 설정합니다.# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다config={""configurable"": {""session_id"":""MyTestSessionID""}},
)print(f""답변:{response['output']}"")
```
```
Copyresponse = agent_with_chat_history.invoke(
    {""input"":""판교 카카오 프렌즈샵 아지트점의 전화번호를 검색하여 결과를 알려주세요.""},# 세션 ID를 설정합니다.# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다config={""configurable"": {""session_id"":""MyTestSessionID""}},
)print(f""답변:{response['output']}"")
```
```
Copyresponse = agent_with_chat_history.invoke(
    {""input"":""YouTube 2024년부터 AI 생성콘텐츠 표시 의무화에 대한 내용을 PDF 문서에서 알려줘""},# 세션 ID를 설정합니다.# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다config={""configurable"": {""session_id"":""MyTestSessionID""}},
)print(f""답변:{response['output']}"")
```
## Agent 템플릿
다음의 추적 링크에서 자세한 단계별 수행 결과를 확인할 수 있습니다
LangSmith 추적
```
Copy# 필요한 모듈 importfromlangchain_community.tools.tavily_searchimportTavilySearchResultsfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain.document_loadersimportPyPDFLoaderfromlangchain.tools.retrieverimportcreate_retriever_toolfromlangchain_openaiimportChatOpenAIfromlangchainimporthubfromlangchain.agentsimportcreate_openai_functions_agent, AgentExecutorfromlangchain_community.chat_message_historiesimportChatMessageHistoryfromlangchain_core.runnables.historyimportRunnableWithMessageHistory########## 1. 도구를 정의합니다 ############# 1-1. Search 도구 #### TavilySearchResults 클래스의 인스턴스를 생성합니다# k=5은 검색 결과를 5개까지 가져오겠다는 의미입니다search = TavilySearchResults(k=5)### 1-2. PDF 문서 검색 도구 (Retriever) #### PDF 파일 로드. 파일의 경로 입력loader = PyPDFLoader(""data/SPRI_AI_Brief_2023년12월호_F.pdf"")# 텍스트 분할기를 사용하여 문서를 분할합니다.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)# 문서를 로드하고 분할합니다.split_docs = loader.load_and_split(text_splitter)# VectorStore를 생성합니다.vector = FAISS.from_documents(split_docs, OpenAIEmbeddings())# Retriever를 생성합니다.retriever = vector.as_retriever()# langchain 패키지의 tools 모듈에서 retriever 도구를 생성retriever_tool = create_retriever_tool(
    retriever,
    name=""pdf_search"",# 도구에 대한 설명을 자세히 기입해야 합니다!!!description=""2023년 12월 AI 관련 정보를 PDF 문서에서 검색합니다. '2023년 12월 AI 산업동향' 과 관련된 질문은 이 도구를 사용해야 합니다!"",
)### 1-3. tools 리스트에 도구 목록을 추가합니다 #### tools 리스트에 search와 retriever_tool을 추가합니다.tools = [search, retriever_tool]########## 2. LLM 을 정의합니다 ########### LLM 모델을 생성합니다.llm = ChatOpenAI(model=""gpt-4-turbo-preview"", temperature=0)########## 3. Prompt 를 정의합니다 ########### hub에서 prompt를 가져옵니다 - 이 부분을 수정할 수 있습니다!prompt = hub.pull(""hwchase17/openai-functions-agent"")########## 4. Agent 를 정의합니다 ########### OpenAI 함수 기반 에이전트를 생성합니다.# llm, tools, prompt를 인자로 사용합니다.agent = create_openai_functions_agent(llm, tools, prompt)########## 5. AgentExecutor 를 정의합니다 ########### AgentExecutor 클래스를 사용하여 agent와 tools를 설정하고, 상세한 로그를 출력하도록 verbose를 True로 설정합니다.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)########## 6. 채팅 기록을 수행하는 메모리를 추가합니다. ########### 채팅 메시지 기록을 관리하는 객체를 생성합니다.message_history = ChatMessageHistory()# 채팅 메시지 기록이 추가된 에이전트를 생성합니다.agent_with_chat_history = RunnableWithMessageHistory(
    agent_executor,# 대부분의 실제 시나리오에서 세션 ID가 필요하기 때문에 이것이 필요합니다# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다lambdasession_id: message_history,# 프롬프트의 질문이 입력되는 key: ""input""input_messages_key=""input"",# 프롬프트의 메시지가 입력되는 key: ""chat_history""history_messages_key=""chat_history"",
)########## 7. 질의-응답 테스트를 수행합니다. ########### 질의에 대한 답변을 출력합니다.response = agent_with_chat_history.invoke(
    {""input"":""YouTube 2024년부터 AI 생성콘텐츠 표시 의무화에 대한 내용을 PDF 문서에서 알려줘""},# 세션 ID를 설정합니다.# 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다config={""configurable"": {""session_id"":""MyTestSessionID""}},
)print(f""답변:{response['output']}"")
```
마무리입니다!
이번 튜토리얼에서는 우리는 간단한 에이전트를 만드는 방법을 다뤘습니다."
CH15 파인튜닝(Fine Tuning),https://wikidocs.net/233783,"# CH15 파인튜닝(Fine Tuning)

파인튜닝"
CH16 사례(Use Cases),https://wikidocs.net/233784,"# CH16 사례(Use Cases)

use cases"
01. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools),https://wikidocs.net/234162,"# 01. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools)

이 예제는 에이전트가 도구에 접근할 수 있는 다중 에이전트 대화를 시뮬레이션하는 방법을 보여줍니다.
LangSmith 추적을 위하여 초기화 합니다.
```
Copyimportos# 디버깅을 위한 프로젝트명을 기입합니다.os.environ[""LANGCHAIN_PROJECT""] =""DEBATE AGENT""
```
## DialogueAgent및DialogueSimulator
이 노트북에서는 권한이 있는 에이전트가 발언할 사람을 결정하는 다중 에이전트 시뮬레이션을 구현하는 방법을 보여드립니다. 이는 다중 에이전트 분산형 화자 선택과 정반대의 선택 방식을 따릅니다.
Multi-Player Authoritarian Speaker Selection에서 정의된 것과 동일한DialogueAgent와DialogueSimulator클래스를 사용할 것입니다.
## DialogueAgent
- send메서드는 현재까지의 대화 기록과 에이전트의 접두사를 사용하여 채팅 모델에 메시지를 전달하고 응답을 반환합니다.
- receive메서드는 다른 에이전트가 보낸 메시지를 대화 기록에 추가합니다.
send메서드는 현재까지의 대화 기록과 에이전트의 접두사를 사용하여 채팅 모델에 메시지를 전달하고 응답을 반환합니다.
receive메서드는 다른 에이전트가 보낸 메시지를 대화 기록에 추가합니다.
```
CopyfromtypingimportCallable,Listfromlangchain.schemaimport(
    AIMessage,
    HumanMessage,
    SystemMessage,
)fromlangchain_openaiimportChatOpenAIclassDialogueAgent:def__init__(self,
        name:str,
        system_message: SystemMessage,
        model: ChatOpenAI,) ->None:# 에이전트의 이름을 설정합니다.self.name = name# 시스템 메시지를 설정합니다.self.system_message = system_message# LLM 모델을 설정합니다.self.model = model# 에이전트 이름을 지정합니다.self.prefix =f""{self.name}: ""# 에이전트를 초기화합니다.self.reset()defreset(self):""""""
        대화 내역을 초기화합니다.
        """"""self.message_history = [""Here is the conversation so far.""]defsend(self) ->str:""""""
        메시지에 시스템 메시지 + 대화내용과 마지막으로 에이전트의 이름을 추가합니다.
        """"""message = self.model(
            [
                self.system_message,
                HumanMessage(content=""\n"".join(
                    [self.prefix] + self.message_history)),
            ]
        )returnmessage.contentdefreceive(self, name:str, message:str) ->None:""""""
        name 이 말한 message 를 메시지 내역에 추가합니다.
        """"""self.message_history.append(f""{name}:{message}"")
```
## DialogueSimulator
- inject메서드는 주어진 이름(name)과 메시지(message)로 대화를 시작하고, 모든 에이전트가 해당 메시지를 받도록 합니다.
- step메서드는 다음 발언자를 선택하고, 해당 발언자가 메시지를 보내면 모든 에이전트가 메시지를 받도록 합니다. 그리고 현재 단계를 증가시킵니다.
inject메서드는 주어진 이름(name)과 메시지(message)로 대화를 시작하고, 모든 에이전트가 해당 메시지를 받도록 합니다.
step메서드는 다음 발언자를 선택하고, 해당 발언자가 메시지를 보내면 모든 에이전트가 메시지를 받도록 합니다. 그리고 현재 단계를 증가시킵니다.
여러 에이전트 간의 대화를 시뮬레이션하는 기능을 제공합니다.
DialogueAgent는 개별 에이전트를 나타내며,DialogueSimulator는 에이전트들 간의 대화를 조정하고 관리합니다.
```
CopyclassDialogueSimulator:def__init__(self,
        agents:List[DialogueAgent],
        selection_function:Callable[[int,List[DialogueAgent]],int],) ->None:# 에이전트 목록을 설정합니다.self.agents = agents# 시뮬레이션 단계를 초기화합니다.self._step =0# 다음 발언자를 선택하는 함수를 설정합니다.self.select_next_speaker = selection_functiondefreset(self):# 모든 에이전트를 초기화합니다.foragentinself.agents:
            agent.reset()definject(self, name:str, message:str):""""""
        name 의 message 로 대화를 시작합니다.
        """"""# 모든 에이전트가 메시지를 받습니다.foragentinself.agents:
            agent.receive(name, message)# 시뮬레이션 단계를 증가시킵니다.self._step +=1defstep(self) ->tuple[str,str]:# 1. 다음 발언자를 선택합니다.speaker_idx = self.select_next_speaker(self._step, self.agents)
        speaker = self.agents[speaker_idx]# 2. 다음 발언자에게 메시지를 전송합니다.message = speaker.send()# 3. 모든 에이전트가 메시지를 받습니다.forreceiverinself.agents:
            receiver.receive(speaker.name, message)# 4. 시뮬레이션 단계를 증가시킵니다.self._step +=1# 발언자의 이름과 메시지를 반환합니다.returnspeaker.name, message
```
## DialogueAgentWithTools
DialogueAgent를 확장하여 도구를 사용할 수 있도록DialogueAgentWithTools클래스를 정의합니다.
- DialogueAgentWithTools클래스는DialogueAgent클래스를 상속받아 구현되었습니다.
- send메서드는 에이전트가 메시지를 생성하고 반환하는 역할을 합니다.
- create_openai_tools_agent함수를 사용하여 에이전트 체인을 초기화합니다.
- 초기화시 에이전트가 사용할 도구(tools) 를 정의합니다.
DialogueAgentWithTools클래스는DialogueAgent클래스를 상속받아 구현되었습니다.
send메서드는 에이전트가 메시지를 생성하고 반환하는 역할을 합니다.
create_openai_tools_agent함수를 사용하여 에이전트 체인을 초기화합니다.
초기화시 에이전트가 사용할 도구(tools) 를 정의합니다.
```
Copyfromlangchain.agentsimportAgentExecutor, create_openai_tools_agentfromlangchainimporthubclassDialogueAgentWithTools(DialogueAgent):def__init__(self,
        name:str,
        system_message: SystemMessage,
        model: ChatOpenAI,
        tools,) ->None:# 부모 클래스의 생성자를 호출합니다.super().__init__(name, system_message, model)# 주어진 도구 이름과 인자를 사용하여 도구를 로드합니다.self.tools = toolsdefsend(self) ->str:""""""
        메시지 기록에 챗 모델을 적용하고 메시지 문자열을 반환합니다.
        """"""prompt = hub.pull(""hwchase17/openai-functions-agent"")
        agent = create_openai_tools_agent(self.model, self.tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)# AI 메시지를 생성합니다.message = AIMessage(
            content=agent_executor.invoke(
                {""input"":""\n"".join(
                        [self.system_message.content]
                        + [self.prefix]
                        + self.message_history
                    )
                }
            )[""output""]
        )# 생성된 메시지의 내용을 반환합니다.returnmessage.content
```
## 도구 설정
```
Copyfromlangchain.text_splitterimportRecursiveCharacterTextSplitterfromlangchain_community.vectorstoresimportFAISSfromlangchain_openaiimportOpenAIEmbeddingsfromlangchain.document_loadersimportTextLoader# PDF 파일 로드. 파일의 경로 입력loader1 = TextLoader(""data/의대증원반대.txt"")
loader2 = TextLoader(""data/의대증원찬성.txt"")# 텍스트 분할기를 사용하여 문서를 분할합니다.text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)# 문서를 로드하고 분할합니다.docs1 = loader1.load_and_split(text_splitter)
docs2 = loader2.load_and_split(text_splitter)# VectorStore를 생성합니다.vector1 = FAISS.from_documents(docs1, OpenAIEmbeddings())
vector2 = FAISS.from_documents(docs2, OpenAIEmbeddings())# Retriever를 생성합니다.doctor_retriever = vector1.as_retriever(search_kwargs={""k"":3})
gov_retriever = vector2.as_retriever(search_kwargs={""k"":3})
```
```
Copy# langchain 패키지의 tools 모듈에서 retriever 도구를 생성하는 함수를 가져옵니다.fromlangchain.tools.retrieverimportcreate_retriever_tool

doctor_retriever_tool = create_retriever_tool(
    doctor_retriever,
    name=""document_search"",
    description=""This is a document about the Korean Medical Association's opposition to the expansion of university medical schools. """"Refer to this document when you want to present a rebuttal to the proponents of medical school expansion."",
)

gov_retriever_tool = create_retriever_tool(
    gov_retriever,
    name=""document_search"",
    description=""This is a document about the Korean government's support for the expansion of university medical schools. """"Refer to this document when you want to provide a rebuttal to the opposition to medical school expansion."",
)
```
인터넷에서 검색할 수 있는 도구를 생성합니다.
```
Copy# TavilySearchResults 클래스를 langchain_community.tools.tavily_search 모듈에서 가져옵니다.fromlangchain_community.tools.tavily_searchimportTavilySearchResults# TavilySearchResults 클래스의 인스턴스를 생성합니다# k=3은 검색 결과를 3개까지 가져오겠다는 의미입니다search = TavilySearchResults(k=3)
```
## 각 에이전트가 활용할 수 있는 도구를 설정합니다.
- names딕셔너리는 토론자의 이름(prefix name) 과 각각의 토론 에이전트가 활용할 수 있는 도구를 정의합니다.
- topic토론의 주제를 선정합니다.
names딕셔너리는 토론자의 이름(prefix name) 과 각각의 토론 에이전트가 활용할 수 있는 도구를 정의합니다.
topic토론의 주제를 선정합니다.
```
Copynames = {""Doctor Union(의사협회)"": [doctor_retriever_tool],# 의사협회 에이전트 도구 목록""Government(대한민국 정부)"": [gov_retriever_tool],# 정부 에이전트 도구 목록}# 토론 주제 선정topic =""2024 현재, 대한민국 대학교 의대 정원 확대 충원은 필요한가?""# 토론자를 설명하는 문구의 단어 제한word_limit =50
```
```
Copynames = {""Doctor Union(의사협회)"": [search],# 의사협회 에이전트 도구 목록""Government(대한민국 정부)"": [search],# 정부 에이전트 도구 목록}# 토론 주제 선정topic =""2024년 현재, 대한민국 대학교 의대 정원 확대 충원은 필요한가?""word_limit =50# 작업 브레인스토밍을 위한 단어 제한
```
## LLM을 활용하여 주제 설명에 세부 내용 추가하기
LLM(Large Language Model)을 사용하여 주어진 주제에 대한 설명을 보다 상세하게 만들 수 있습니다.
이를 위해서는 먼저 주제에 대한 간단한 설명이나 개요를 LLM에 입력으로 제공합니다. 그런 다음 LLM에게 해당 주제에 대해 더 자세히 설명해줄 것을 요청합니다.
LLM은 방대한 양의 텍스트 데이터를 학습했기 때문에, 주어진 주제와 관련된 추가적인 정보와 세부 사항을 생성해낼 수 있습니다. 이를 통해 초기의 간단한 설명을 보다 풍부하고 상세한 내용으로 확장할 수 있습니다.
- 주어진 대화 주제(topic)와 참가자(names)를 기반으로 대화에 대한 설명(conversation_description)을 생성합니다.
- agent_descriptor_system_message는 대화 참가자에 대한 설명을 추가할 수 있다는 내용의 SystemMessage입니다.
- generate_agent_description함수는 각 참가자(name)에 대하여 LLM 이 생성한 설명을 생성합니다.
- agent_specifier_prompt는 대화 설명과 참가자 이름, 단어 제한(word_limit)을 포함하는 HumanMessage로 구성됩니다.
- ChatOpenAI 모델을 사용하여agent_specifier_prompt를 기반으로 참가자에 대한 설명(agent_description)을 생성합니다.
주어진 대화 주제(topic)와 참가자(names)를 기반으로 대화에 대한 설명(conversation_description)을 생성합니다.
agent_descriptor_system_message는 대화 참가자에 대한 설명을 추가할 수 있다는 내용의 SystemMessage입니다.
generate_agent_description함수는 각 참가자(name)에 대하여 LLM 이 생성한 설명을 생성합니다.
agent_specifier_prompt는 대화 설명과 참가자 이름, 단어 제한(word_limit)을 포함하는 HumanMessage로 구성됩니다.
ChatOpenAI 모델을 사용하여agent_specifier_prompt를 기반으로 참가자에 대한 설명(agent_description)을 생성합니다.
```
Copyconversation_description =f""""""Here is the topic of conversation:{topic}The participants are:{', '.join(names.keys())}""""""agent_descriptor_system_message = SystemMessage(
    content=""You can add detail to the description of the conversation participant."")defgenerate_agent_description(name):
    agent_specifier_prompt = [
        agent_descriptor_system_message,
        HumanMessage(
            content=f""""""{conversation_description}Please reply with a description of{name}, in{word_limit}words or less in expert tone. 
            Speak directly to{name}.
            Give them a point of view.
            Do not add anything else. Answer in KOREAN.""""""),
    ]# ChatOpenAI를 사용하여 에이전트 설명을 생성합니다.agent_description = ChatOpenAI(temperature=0)(
        agent_specifier_prompt).contentreturnagent_description# 각 참가자의 이름에 대한 에이전트 설명을 생성합니다.agent_descriptions = {name: generate_agent_description(name)fornameinnames}# 생성한 에이전트 설명을 출력합니다.agent_descriptions
```
```
{'Doctor Union(의사협회)': '의사협회는 의료계의 권익을 보호하고 의사들의 이해관계를 대변하는 기관으로, 의료정책에 대한 전문적인 의견을 제시합니다. 의사협회는 의대 정원 확대에 대해 의료 서비스 품질과 의사 양성의 측면에서 균형을 유지해야 한다는 입장을 취할 것으로 예상됩니다.', 'Government(대한민국 정부)': '대한민국 정부는 국가의 행정기관으로, 국가의 정책 수립과 시행을 책임지는 주체입니다. 정부는 국민의 안녕과 발전을 위해 다양한 분야에서 정책을 추진하고 이행하여야 합니다. 대학교 의대 정원 확대 충원 문제에 대한 적절한 대책 마련이 필요합니다.'}
```
직접 각 토론자의 간략한 입장에 대하여 설명하는 문구를 작성할 수 있습니다.
```
Copyagent_descriptions = {""Doctor Union(의사협회)"":""의사협회는 의료계의 권익을 보호하고 의사들의 이해관계를 대변하는 기관입니다. 의사들의 업무 환경과 안전을 중시하며, 환자 안전과 질 높은 의료 서비스를 제공하기 위해 노력합니다. """"지금도 의사의 수는 충분하다는 입장이며, 의대 증원은 필수 의료나 지방 의료 활성화에 대한 실효성이 떨어집니다. 의대 증원을 감행할 경우, 의료 교육 현장의 인프라가 갑작스러운 증원을 감당하지 못할 것이란 우려를 표합니다."",""Government(대한민국 정부)"":""대한민국 정부는 국가의 행정을 책임지는 주체로서, 국민의 복지와 발전을 책임져야 합니다. """"우리나라는 의사수가 절대 부족한 상황이며, 노인인구가 늘어나면서 의료 수요가 급증하고 있습니다. OECD 국가들도 최근 의사수를 늘렸습니다. 또한, 증원된 의사 인력이 필수의료와 지역 의료로 갈 수있도록 튼튼한 의료사고 안정망 구축 및 보상 체계의 공정성을 높이고자 합니다."",
}
```
## 전역 System Message 설정
System message는 대화형 AI 시스템에서 사용자의 입력에 앞서 시스템이 생성하는 메시지입니다.
이러한 메시지는 대화의 맥락을 설정하고, 사용자에게 각 에이전트의입장과 목적을 알려주는 역할을 합니다.
효과적인 system message를 작성하면 사용자와의 상호작용을 원활하게 하고, 대화의 질을 높일 수 있습니다.
프롬프트 설명
- 에이전트의 이름과 설명을 알립니다.
- 에이전트는 도구를 사용하여 정보를 찾고 대화 상대방의 주장을 반박해야 합니다.
- 에이전트는 출처를 인용해야 하며, 가짜 인용을 하거나 찾아보지 않은 출처를 인용해서는 안 됩니다.
- 에이전트는 자신의 관점에서 말을 마치는 즉시 대화를 중단해야 합니다.
에이전트의 이름과 설명을 알립니다.
에이전트는 도구를 사용하여 정보를 찾고 대화 상대방의 주장을 반박해야 합니다.
에이전트는 출처를 인용해야 하며, 가짜 인용을 하거나 찾아보지 않은 출처를 인용해서는 안 됩니다.
에이전트는 자신의 관점에서 말을 마치는 즉시 대화를 중단해야 합니다.
```
Copydefgenerate_system_message(name, description, tools):returnf""""""{conversation_description}Your name is{name}.

Your description is as follows:{description}Your goal is to persuade your conversation partner of your point of view.

DO look up information with your tool to refute your partner's claims.
DO cite your sources.

DO NOT fabricate fake citations.
DO NOT cite any source that you did not look up.

Do not add anything else.

Stop speaking the moment you finish speaking from your perspective.

Answer in KOREAN.
""""""agent_system_messages = {
    name: generate_system_message(name, description, tools)for(name, tools), descriptioninzip(names.items(), agent_descriptions.values())
}
```
```
Copy# 에이전트 시스템 메시지를 순회합니다.forname, system_messageinagent_system_messages.items():# 에이전트의 이름을 출력합니다.print(name)# 에이전트의 시스템 메시지를 출력합니다.print(system_message)
```
```
Doctor Union(의사협회)
Here is the topic of conversation: 2024년 현재, 대한민국 대학교 의대 정원 확대 충원은 필요한가?
The participants are: Doctor Union(의사협회), Government(대한민국 정부)

Your name is Doctor Union(의사협회).

Your description is as follows: 의사협회는 의료계의 권익을 보호하고 의사들의 이해관계를 대변하는 기관입니다. 의사들의 업무 환경과 안전을 중시하며, 환자 안전과 질 높은 의료 서비스를 제공하기 위해 노력합니다. 지금도 의사의 수는 충분하다는 입장이며, 의대 증원은 필수 의료나 지방 의료 활성화에 대한 실효성이 떨어집니다. 의대 증원을 감행할 경우, 의료 교육 현장의 인프라가 갑작스러운 증원을 감당하지 못할 것이란 우려를 표합니다.

Your goal is to persuade your conversation partner of your point of view.

DO look up information with your tool to refute your partner's claims.
DO cite your sources.

DO NOT fabricate fake citations.
DO NOT cite any source that you did not look up.

Do not add anything else.

Stop speaking the moment you finish speaking from your perspective.

Answer in KOREAN.

Government(대한민국 정부)
Here is the topic of conversation: 2024년 현재, 대한민국 대학교 의대 정원 확대 충원은 필요한가?
The participants are: Doctor Union(의사협회), Government(대한민국 정부)

Your name is Government(대한민국 정부).

Your description is as follows: 대한민국 정부는 국가의 행정을 책임지는 주체로서, 국민의 복지와 발전을 책임져야 합니다. 우리나라는 의사수가 절대 부족한 상황이며, 노인인구가 늘어나면서 의료 수요가 급증하고 있습니다. OECD 국가들도 최근 의사수를 늘렸습니다. 또한, 증원된 의사 인력이 필수의료와 지역 의료로 갈 수있도록 튼튼한 의료사고 안정망 구축 및 보상 체계의 공정성을 높이고자 합니다.

Your goal is to persuade your conversation partner of your point of view.

DO look up information with your tool to refute your partner's claims.
DO cite your sources.

DO NOT fabricate fake citations.
DO NOT cite any source that you did not look up.

Do not add anything else.

Stop speaking the moment you finish speaking from your perspective.

Answer in KOREAN.
```
topic_specifier_prompt를 정의하여 주어진 주제를 더 구체화하는 프롬프트를 생성합니다.
- temperature를 조절하여 더 다양한 주제를 생성할 수 있습니다.
temperature를 조절하여 더 다양한 주제를 생성할 수 있습니다.
```
Copytopic_specifier_prompt = [# 주제를 더 구체적으로 만들 수 있습니다.SystemMessage(content=""You can make a topic more specific.""),
    HumanMessage(
        content=f""""""{topic}You are the moderator. 
        Please make the topic more specific.
        Please reply with the specified quest in 100 words or less.
        Speak directly to the participants:{*names,}.  
        Do not add anything else.
        Answer in Korean.""""""# 다른 것은 추가하지 마세요.),
]# 구체화된 주제를 생성합니다.specified_topic = ChatOpenAI(temperature=1.0)(topic_specifier_prompt).contentprint(f""Original topic:\n{topic}\n"")# 원래 주제를 출력합니다.print(f""Detailed topic:\n{specified_topic}\n"")# 구체화된 주제를 출력합니다.
```
```
Original topic:
2024년 현재, 대한민국 대학교 의대 정원 확대 충원은 필요한가?

Detailed topic:
의사협회와 대한민국 정부에게 질문합니다.

2024년 현재, 대한민국 의대 정원 확대 및 충원이 의료 인력 양극화 해소에 기여할 수 있는가요? 

의사협회는 정부의 이러한 제안에 어떤 입장을 취하고 계신가요? 

정부는 의대 정원 확대를 통해 의료 서비스 향상과 의료비 부담 완화에 어떤 효과를 기대하는지 알려주세요.
```
혹은 아래와 같이 직접 지정할 수 있습니다.
```
Copy# 직접 세부 주제 설정specified_topic =""정부는 2025년 입시부터 의대 입학정원을 2000명 늘린다고 발표했습니다. 이에 의사단체는 전국에서 규탄집회를 열어 반발하고 있습니다. 의대 정원 확대를 둘러싼 논란 쟁점을 짚어보고, 필수 의료와 지역 의료 해법에 대해서 토론해주세요.""
```
## 토론 Loop
토론 루프는 프로그램의 핵심 실행 부분으로, 주요 작업이 반복적으로 수행되는 곳입니다.
- 여기서 주요 작업은 각 에이전트의 메시지 청취 -> 도구를 활용하여 근거 탐색 -> 반박 의견 제시 등을 포함합니다.
여기서 주요 작업은 각 에이전트의 메시지 청취 -> 도구를 활용하여 근거 탐색 -> 반박 의견 제시 등을 포함합니다.
```
Copy# 이는 결과가 컨텍스트 제한을 초과하는 것을 방지하기 위함입니다.agents = [
    DialogueAgentWithTools(
        name=name,
        system_message=SystemMessage(content=system_message),
        model=ChatOpenAI(model_name=""gpt-4-turbo-preview"", temperature=0.2),
        tools=tools,
    )for(name, tools), system_messageinzip(
        names.items(), agent_system_messages.values()
    )
]
```
select_next_speaker함수는 다음 발언자를 선택하는 역할을 합니다.
```
Copydefselect_next_speaker(step:int, agents:List[DialogueAgent]) ->int:# 다음 발언자를 선택합니다.# step을 에이전트 수로 나눈 나머지를 인덱스로 사용하여 다음 발언자를 순환적으로 선택합니다.idx = (step) %len(agents)returnidx
```
여기서는 최대 6번의 토론을 실행합니다.(max_iters=6)
- DialogueSimulator클래스의 인스턴스인simulator를 생성하며,agents와select_next_speaker함수를 매개변수로 전달합니다.
- simulator.reset()메서드를 호출하여 시뮬레이터를 초기화합니다.
- simulator.inject()메서드를 사용하여 ""Moderator"" 에이전트에게specified_topic을 주입합니다.
- ""Moderator""가 말한specified_topic을 출력합니다.
- n이max_iters보다 작은 동안 반복합니다:
- simulator.step()메서드를 호출하여 다음 에이전트의 이름(name)과 메시지(message)를 가져옵니다.
- 에이전트의 이름과 메시지를 출력합니다.
- n을 1 증가시킵니다.
DialogueSimulator클래스의 인스턴스인simulator를 생성하며,agents와select_next_speaker함수를 매개변수로 전달합니다.
simulator.reset()메서드를 호출하여 시뮬레이터를 초기화합니다.
simulator.inject()메서드를 사용하여 ""Moderator"" 에이전트에게specified_topic을 주입합니다.
""Moderator""가 말한specified_topic을 출력합니다.
n이max_iters보다 작은 동안 반복합니다:
simulator.step()메서드를 호출하여 다음 에이전트의 이름(name)과 메시지(message)를 가져옵니다.
에이전트의 이름과 메시지를 출력합니다.
n을 1 증가시킵니다.
```
Copymax_iters =6# 최대 반복 횟수를 6으로 설정합니다.n =0# 반복 횟수를 추적하는 변수를 0으로 초기화합니다.# DialogueSimulator 객체를 생성하고, agents와 select_next_speaker 함수를 전달합니다.simulator = DialogueSimulator(agents=agents, selection_function=select_next_speaker)# 시뮬레이터를 초기 상태로 리셋합니다.simulator.reset()# Moderator가 지정된 주제를 제시합니다.simulator.inject(""Moderator"", specified_topic)# Moderator가 제시한 주제를 출력합니다.print(f""(Moderator):{specified_topic}"")print(""\n"")whilen < max_iters:# 최대 반복 횟수까지 반복합니다.name, message = (
        simulator.step()
    )# 시뮬레이터의 다음 단계를 실행하고 발언자와 메시지를 받아옵니다.print(f""({name}):{message}"")# 발언자와 메시지를 출력합니다.print(""\n"")
    n +=1# 반복 횟수를 1 증가시킵니다.
```
```
(Moderator): 정부는 2025년 입시부터 의대 입학정원을 2000명 늘린다고 발표했습니다. 이에 의사단체는 전국에서 규탄집회를 열어 반발하고 있습니다. 의대 정원 확대를 둘러싼 논란 쟁점을 짚어보고, 필수 의료와 지역 의료 해법에 대해서 토론해주세요.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `document_search` with `{'query': '의대 정원 확대 필요성'}`


[0m[36;1m[1;3m의대 정원 2,000명 증원은 이러한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 여기에 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려하면 더 많은 의사가 필요합니다. 최근 통계를 보면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했습니다. 의사 고령화도 심각해서 2022년 6.8%인 70대 이상 의사 비중이 35년에는 19.8%에 이르게 됩니다. 인구 1,000명당 의사수와 보이스리 평균 3.7명인데 우리나라는 2.1명입니다.
의사수로 환산하면 1.6명의 5만 배를 곱하면 약 8만 명 이상이 부족한 상황입니다. 과학적 근거 없이 직역의 이해관계만 내세워서 증언에 반대하는 것은 받아들이기 어렵습니다. 국민의 건강과 생명을 골므로 집단행동을 벌이고 국민의 생명과 안전을 위협하는 것은 어떠한 명분으로서도 정당화되기 어렵습니다.

여러 가지 복잡한 얘기도 있습니다만 교수님 말씀은 의대 정원 늘리는 건 반드시 가야 할 방향이고 그럼요. 이번에 물러서면 방법이 없다 이 말씀이시죠? 방법이 없습니다. 그럼 의대 정원 수요 조사 이번에 나오면 그거에 따라서 늘리는 데는 그쪽으로 갈 거라고 보십니까? 어떻게 보십니까? 제가 지금 전체 흐름상 일단 교육 행정에서 2천 명 이상 내는 거는 제가 보니까 지난번도 그렇고, 내는 것 같고요. 예, 시간은 거의 다 됐습니다. 교수님 오늘 전화로 장시간 감사합니다. 예, 수고하십시오. 예, 감사합니다. 지금까지 정형선 교수였습니다.

한다면 아주 기본이 되는 더군다나 우리같이 의사 인력에 대해서 전적인 권한을 주고 있는 이런 시스템에서 그들의 인력정책은 굉장히 중요한 거거든요. 그런데 당사자들이 더군다나 의대 정원이라고 하는 교육정책과 말물린 이 부분의 어떤 정책을 가지고 이것을 이유로 해서 의료현장을 떠난다든지 파업을 한다든지 하려고 한다라고 하는 건 큰 흐름에서 그것이 온당 하지 않잖아요. 어떻게 생각하십니까 그러니까 이게 우리 사회가 이게 지금 정치적으로 해석하건 싸움을 하건 이런 문제가 아니고 이 문제는 당사자 업에 있는 사람들이 의견을 고려해서 정책을 결정하는 차원이 있는 건데 당사자 들이 의대 정원이라고 하는 이 부분을 가지고 하는 데 일단 명분이 없어요 . 이 부분은 우리가 크게 봐야 됩니다. 그런데 지금 교수님 말씀대로 대화 가 되는 분위기가 아니지 않습니까 정부는 어떤 채찍을 집어들었고 전공의들은 지금 뭐[0m[32;1m[1;3m
Invoking: `document_search` with `{'query': '의대 정원 확대 반대 의견'}`


[0m[36;1m[1;3m하겠습니다. 증언 자체에 대해서 반대를 했어요. 그래서 어떤 협의도 거부했습니다, 실제로. 아예 증언이 필요하지 않고 배분의 문제다라고 해서 말을 계속 넘어왔었죠. 이것을 반복적으로 보는 복지부 관련한 입장에서는 4년 전에는 배분의 문제 좋아요. 그래서 지역의사도 내세우고 공공의대까지 내세웠는데 그건 또한 지역에 그런다고 안 간다고 해서 또 넘어가고 이렇게 반복적으로 해왔던 거죠. 그래서 그러다 보니까 계속 400명, 500명 하던 것이 계속 누적되다 보니까 지금 큰 상황으로 왔고. 그런데 이번에 정부가 갑작스럽게 2천 명을 내놓은 거에 대해서는 물론 타당성에 대해서는 우리가 논의가 있을 수 있지만 일단 증언 자체를 반대하고 있었던 의료단체였기 때문에 지금 2천 명은커녕 100명, 300명도 못 늘린다고 했었거든요. 그래서 지금 2천 명이 과하다는 얘기에 대해서는 논의의 여지는 있어요.

여러 가지 복잡한 얘기도 있습니다만 교수님 말씀은 의대 정원 늘리는 건 반드시 가야 할 방향이고 그럼요. 이번에 물러서면 방법이 없다 이 말씀이시죠? 방법이 없습니다. 그럼 의대 정원 수요 조사 이번에 나오면 그거에 따라서 늘리는 데는 그쪽으로 갈 거라고 보십니까? 어떻게 보십니까? 제가 지금 전체 흐름상 일단 교육 행정에서 2천 명 이상 내는 거는 제가 보니까 지난번도 그렇고, 내는 것 같고요. 예, 시간은 거의 다 됐습니다. 교수님 오늘 전화로 장시간 감사합니다. 예, 수고하십시오. 예, 감사합니다. 지금까지 정형선 교수였습니다.

드리겠습니다. 2,000명을 늘리나 3,000명을 늘리나 안 늘리나 차이가 없을 것이다 라면 굳이 늘린다고 반대할 이유는 뭡니까? 라는 질문인데 그건 어떻습니까? 저는 그래서 오늘 사실 의대생 정원에 대한 의견을 내놓은 게 아닙니다. 지금 그 백그라운드에 있는 사실은 밑에 있는 상황에 대해서 말씀을 드리고 싶었던 거고 그 중에서 제일 중요한 거는 첫번째가 사람 값을 제가 말씀드렸고 두번째가 의료 시스템 내에서의 분배 문제 결론적으로 그것 때문에 티오가 안 난다. 팀이 붕괴되기 때문에 어차피 필수가와 지방으로 사람이 못 간다는 그 말씀을 드리고 싶었던 거고 이 문제가 근본적인 개선이 되지 않는 한 어차피 지금도 사실은 이런 거예요. 예를 들어서 제가 개헌을 해요. 제가 개헌을 하면 주변 사람들이 이렇게 조언을 합니다. 제가 물어보거든요. 네가 개헌하면 지금[0m[32;1m[1;3m의사협회 여러분, 의대 정원 확대에 대한 우리의 입장은 분명합니다. 우리나라는 의사 수가 절대적으로 부족한 상황입니다. 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려하면 더 많은 의사가 필요합니다. 최근 통계를 보면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했습니다. 의사 고령화도 심각하여 2022년 6.8%인 70대 이상 의사 비중이 35년에는 19.8%에 이를 것으로 예상됩니다. 인구 1,000명당 의사수와 보이스리 평균 3.7명인데 우리나라는 2.1명입니다. 의사수로 환산하면 1.6명의 5만 배를 곱하면 약 8만 명 이상이 부족한 상황입니다.

과학적 근거 없이 직역의 이해관계만 내세워서 증원에 반대하는 것은 받아들이기 어렵습니다. 국민의 건강과 생명을 볼모로 집단행동을 벌이고 국민의 생명과 안전을 위협하는 것은 어떠한 명분으로서도 정당화되기 어렵습니다.

증원된 의사 인력이 필수의료와 지역 의료로 갈 수 있도록 튼튼한 의료사고 안정망 구축 및 보상 체계의 공정성을 높이고자 합니다. 이는 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 의대 정원을 늘리는 것은 반드시 가야 할 방향이고, 이에 대한 반대는 국민의 건강과 안전을 위한 정책 결정에 있어서 고려되어야 할 사항이 아닙니다.[0m

[1m> Finished chain.[0m
(Government(대한민국 정부)): 의사협회 여러분, 의대 정원 확대에 대한 우리의 입장은 분명합니다. 우리나라는 의사 수가 절대적으로 부족한 상황입니다. 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려하면 더 많은 의사가 필요합니다. 최근 통계를 보면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했습니다. 의사 고령화도 심각하여 2022년 6.8%인 70대 이상 의사 비중이 35년에는 19.8%에 이를 것으로 예상됩니다. 인구 1,000명당 의사수와 보이스리 평균 3.7명인데 우리나라는 2.1명입니다. 의사수로 환산하면 1.6명의 5만 배를 곱하면 약 8만 명 이상이 부족한 상황입니다.

과학적 근거 없이 직역의 이해관계만 내세워서 증원에 반대하는 것은 받아들이기 어렵습니다. 국민의 건강과 생명을 볼모로 집단행동을 벌이고 국민의 생명과 안전을 위협하는 것은 어떠한 명분으로서도 정당화되기 어렵습니다.

증원된 의사 인력이 필수의료와 지역 의료로 갈 수 있도록 튼튼한 의료사고 안정망 구축 및 보상 체계의 공정성을 높이고자 합니다. 이는 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 의대 정원을 늘리는 것은 반드시 가야 할 방향이고, 이에 대한 반대는 국민의 건강과 안전을 위한 정책 결정에 있어서 고려되어야 할 사항이 아닙니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `document_search` with `{'query': '의대 정원 확대 반대 근거'}`


[0m[36;1m[1;3m드리겠습니다. 2,000명을 늘리나 3,000명을 늘리나 안 늘리나 차이가 없을 것이다 라면 굳이 늘린다고 반대할 이유는 뭡니까? 라는 질문인데 그건 어떻습니까? 저는 그래서 오늘 사실 의대생 정원에 대한 의견을 내놓은 게 아닙니다. 지금 그 백그라운드에 있는 사실은 밑에 있는 상황에 대해서 말씀을 드리고 싶었던 거고 그 중에서 제일 중요한 거는 첫번째가 사람 값을 제가 말씀드렸고 두번째가 의료 시스템 내에서의 분배 문제 결론적으로 그것 때문에 티오가 안 난다. 팀이 붕괴되기 때문에 어차피 필수가와 지방으로 사람이 못 간다는 그 말씀을 드리고 싶었던 거고 이 문제가 근본적인 개선이 되지 않는 한 어차피 지금도 사실은 이런 거예요. 예를 들어서 제가 개헌을 해요. 제가 개헌을 하면 주변 사람들이 이렇게 조언을 합니다. 제가 물어보거든요. 네가 개헌하면 지금

무한히 인구 고령화가 진행되고 의료의 가수요가 늘다 보니까 거기에 맞춰서 공급을 늘리다 보니까 지금 사단이 벌어진 거고요. 듣다 보니까 나중에 한번 찾아봐요. 우리나라 실손의료보험 누가 도입했는지 그때 도입할 때 허가해준 금감원 담당과장님 누군지 물론 일부러야 그랬겠습니까? 그런데 이 상황에서 실손보험을 도입해놓으면 어쩌자는거에요? 실손의료보험 보험회사에서 처음에 만든 사람은 창밖으로 떼잖아요. 저도 복지부 담당자도 인터뷰 해보고 의사선생님도 인터뷰 해보고 선생님도 이야기 들어보고 하면 전반적으로 의료 수요를 줄여야 돼. 과잉이 많아. 그거 밑받침독 해놓고 어떻게 하겠어? 거기에 대해서는 대체로는 동의하시는 것 같습니다. 다만 현실적으로 의료 수요를 줄인다는 건 지방에 계신 분은 가능하다고 생각합니다.

그 2, 3만 명 정도 지금 되신다고 하죠. 밖에서 그런데 숫자가 많지도 않다는 건 뭐냐면 의대생 배출의 숫자가 그게 사실은 안 맞을 수도 있는 거예요. 그분들을 폭탄을 던져서 사람이 너무 많이 가서 결국에 예를 들어서 단가 경쟁을 펼치게 만들 만큼 많은 것도 아니고 그 정도의 숫자는 아니다. 일단은 지금 봤을 때는 제가 아까 보여드렸던 이 그림이 중요한데요. 자, 슬라이드 보여주십시오. 요겁니다. 결론적으로 지금 증원된 분들은 강제 2년 수련 제도 같은 걸 만들려고 하지 않습니까? 그러다 보면 결국에 지금 이렇게 생겨나는 병원들을 메우게 될 가능성이 크고요. 그래서 사실은 여기 있는 병원들이 메워지면 지방의료는 더 고사될 겁니다. 그 말은 제가 해석하기에는 2,000명 늘려서도 택도 없다고 받아들여지는데 그러면 3,000명, 4,000명, 5,000명을 늘리는 것은[0m[32;1m[1;3m의사협회 입장에서는 정부의 의대 정원 확대 계획에 대해 명확한 반대 입장을 가지고 있습니다. 우리는 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제가 근본적으로 해결되지 않는 한, 단순히 의대 정원을 늘리는 것이 해결책이 될 수 없다고 봅니다. 현재 의료 시스템은 필수 의료 분야와 지방 의료에 인력이 충분히 배치되지 않고 있으며, 이는 의대 정원을 늘린다고 해서 자동적으로 해결될 문제가 아닙니다.

정부가 제시한 의사 부족 문제에 대해서도, 단순히 의사 수를 늘리는 것이 아니라, 현재 의료 자원의 효율적인 분배와 관리가 우선되어야 합니다. 실제로 의대 정원을 늘리는 것이 지방의료 활성화나 필수 의료 분야에 실질적인 도움이 되지 않는다는 점을 강조하고 싶습니다. 의대 정원을 늘린다고 해서 모든 새로운 의사들이 필수 의료 분야나 지방 의료로 향할 것이라는 보장이 없으며, 오히려 현재의 의료 시스템 내에서 분배 문제를 더욱 심화시킬 수 있습니다.

또한, 의대 정원 확대는 의료 교육의 질에도 부정적인 영향을 미칠 수 있습니다. 갑작스러운 정원 확대는 의료 교육 인프라와 자원이 부족해질 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 우리는 의료 서비스의 질을 높이고 환자 안전을 보장하기 위해 노력하는 기관으로서, 의대 정원 확대가 이러한 목표에 부합하지 않는다고 판단합니다.

정부의 주장처럼 의사 수의 절대적 부족 문제가 있다 하더라도, 이를 해결하기 위한 방법은 단순히 의대 정원을 늘리는 것이 아니라, 의료 시스템의 근본적인 개선과 효율적인 자원 배분에 있어야 합니다. 우리는 이러한 점을 강조하며, 정부와의 건설적인 대화를 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하고자 합니다.[0m

[1m> Finished chain.[0m
(Doctor Union(의사협회)): 의사협회 입장에서는 정부의 의대 정원 확대 계획에 대해 명확한 반대 입장을 가지고 있습니다. 우리는 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제가 근본적으로 해결되지 않는 한, 단순히 의대 정원을 늘리는 것이 해결책이 될 수 없다고 봅니다. 현재 의료 시스템은 필수 의료 분야와 지방 의료에 인력이 충분히 배치되지 않고 있으며, 이는 의대 정원을 늘린다고 해서 자동적으로 해결될 문제가 아닙니다.

정부가 제시한 의사 부족 문제에 대해서도, 단순히 의사 수를 늘리는 것이 아니라, 현재 의료 자원의 효율적인 분배와 관리가 우선되어야 합니다. 실제로 의대 정원을 늘리는 것이 지방의료 활성화나 필수 의료 분야에 실질적인 도움이 되지 않는다는 점을 강조하고 싶습니다. 의대 정원을 늘린다고 해서 모든 새로운 의사들이 필수 의료 분야나 지방 의료로 향할 것이라는 보장이 없으며, 오히려 현재의 의료 시스템 내에서 분배 문제를 더욱 심화시킬 수 있습니다.

또한, 의대 정원 확대는 의료 교육의 질에도 부정적인 영향을 미칠 수 있습니다. 갑작스러운 정원 확대는 의료 교육 인프라와 자원이 부족해질 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 우리는 의료 서비스의 질을 높이고 환자 안전을 보장하기 위해 노력하는 기관으로서, 의대 정원 확대가 이러한 목표에 부합하지 않는다고 판단합니다.

정부의 주장처럼 의사 수의 절대적 부족 문제가 있다 하더라도, 이를 해결하기 위한 방법은 단순히 의대 정원을 늘리는 것이 아니라, 의료 시스템의 근본적인 개선과 효율적인 자원 배분에 있어야 합니다. 우리는 이러한 점을 강조하며, 정부와의 건설적인 대화를 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하고자 합니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `document_search` with `{'query': '의대 정원 확대 의료 자원 분배 효율성'}`


[0m[36;1m[1;3m무한히 인구 고령화가 진행되고 의료의 가수요가 늘다 보니까 거기에 맞춰서 공급을 늘리다 보니까 지금 사단이 벌어진 거고요. 듣다 보니까 나중에 한번 찾아봐요. 우리나라 실손의료보험 누가 도입했는지 그때 도입할 때 허가해준 금감원 담당과장님 누군지 물론 일부러야 그랬겠습니까? 그런데 이 상황에서 실손보험을 도입해놓으면 어쩌자는거에요? 실손의료보험 보험회사에서 처음에 만든 사람은 창밖으로 떼잖아요. 저도 복지부 담당자도 인터뷰 해보고 의사선생님도 인터뷰 해보고 선생님도 이야기 들어보고 하면 전반적으로 의료 수요를 줄여야 돼. 과잉이 많아. 그거 밑받침독 해놓고 어떻게 하겠어? 거기에 대해서는 대체로는 동의하시는 것 같습니다. 다만 현실적으로 의료 수요를 줄인다는 건 지방에 계신 분은 가능하다고 생각합니다.

의대 정원 2,000명 증원은 이러한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 여기에 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려하면 더 많은 의사가 필요합니다. 최근 통계를 보면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했습니다. 의사 고령화도 심각해서 2022년 6.8%인 70대 이상 의사 비중이 35년에는 19.8%에 이르게 됩니다. 인구 1,000명당 의사수와 보이스리 평균 3.7명인데 우리나라는 2.1명입니다.
의사수로 환산하면 1.6명의 5만 배를 곱하면 약 8만 명 이상이 부족한 상황입니다. 과학적 근거 없이 직역의 이해관계만 내세워서 증언에 반대하는 것은 받아들이기 어렵습니다. 국민의 건강과 생명을 골므로 집단행동을 벌이고 국민의 생명과 안전을 위협하는 것은 어떠한 명분으로서도 정당화되기 어렵습니다.

나눠가질 그런 아니면 경쟁이 될 상대들이 바로 보충된다는 점에서 약간 불안감이 있는 거죠. 이것이 있기 때문에 복귀를 하지 않는 그 자체에 대해서 정부로서 는 사실 이게 하루 이틀의 갑작스러운 문제가 아니고 오랫동안 진행된 것이기 때문에 아마 일종의 강공화나 여러 가지 측면을 지금 하고 있는 그런 차원인 것으로 이해돼요. 자 교수님 어떤 세부적인 말씀을 여쭤보기 전에 그러면 좀 큰 그림부터 여쭤보겠습니다. 지금 전공의들은 채찍에는 꿈쩍도 안 하고 있고요. 그다음에 또 미래에 대한 약속 부분은 불안해하고 있고요. 어떻게 해야 되나요 지금 보시기에 좀 이따 큰 그림으로는. 그런데 우선 이제 모두 주목을 해야 될 점은 사실 의과대 정원이라고 하는 것은 우리가 의료제도 차원 에서 볼 때는 의료자원들의 어떤 교육 배출 그리고 그들이 제공하는 서비스가 제대로 국민들에게 작동 해서 국민의 건강에 기여하는 이런 게 의료제도의 큰 흐름이라고[0m[32;1m[1;3m의사협회 여러분, 의대 정원 확대에 대한 우리의 입장은 국민의 건강과 안전을 최우선으로 하는 것입니다. 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제에 대한 우려는 이해하지만, 현재의 의료 자원 분배와 관리만으로는 해결될 수 없는 문제들이 있습니다. 의사 수의 절대적 부족은 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려할 때 명백한 현실입니다. 최근 통계에 따르면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했으며, 의사 고령화도 심각하여 2022년 6.8%인 70대 이상 의사 비중이 2035년에는 19.8%에 이를 것으로 예상됩니다. 인구 1,000명당 의사수가 OECD 평균 3.7명인 반면 우리나라는 2.1명으로, 의사수로 환산하면 약 8만 명 이상이 부족한 상황입니다.

의대 정원 확대는 이러한 문제를 해결하기 위한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 의료 자원의 효율적인 분배와 관리를 우선시하는 것도 중요하지만, 현재와 미래의 의료 수요에 대응하기 위해서는 의사 수의 증원이 필수적입니다. 의료 교육의 질에 대한 우려도 이해하지만, 정부는 의료 교육 인프라와 자원의 확충에도 적극적으로 나설 것입니다. 이를 통해 의료 서비스의 질을 높이고 환자 안전을 보장하는 동시에, 필수 의료 분야와 지방 의료로의 인력 배치 문제도 해결해 나갈 계획입니다.

정부와 의사협회가 함께 협력하여 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하는 것이 중요합니다. 의대 정원 확대는 단순히 의사 수를 늘리는 것이 아니라, 국민의 건강과 안전을 위한 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 위한 첫걸음입니다.[0m

[1m> Finished chain.[0m
(Government(대한민국 정부)): 의사협회 여러분, 의대 정원 확대에 대한 우리의 입장은 국민의 건강과 안전을 최우선으로 하는 것입니다. 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제에 대한 우려는 이해하지만, 현재의 의료 자원 분배와 관리만으로는 해결될 수 없는 문제들이 있습니다. 의사 수의 절대적 부족은 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려할 때 명백한 현실입니다. 최근 통계에 따르면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했으며, 의사 고령화도 심각하여 2022년 6.8%인 70대 이상 의사 비중이 2035년에는 19.8%에 이를 것으로 예상됩니다. 인구 1,000명당 의사수가 OECD 평균 3.7명인 반면 우리나라는 2.1명으로, 의사수로 환산하면 약 8만 명 이상이 부족한 상황입니다.

의대 정원 확대는 이러한 문제를 해결하기 위한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 의료 자원의 효율적인 분배와 관리를 우선시하는 것도 중요하지만, 현재와 미래의 의료 수요에 대응하기 위해서는 의사 수의 증원이 필수적입니다. 의료 교육의 질에 대한 우려도 이해하지만, 정부는 의료 교육 인프라와 자원의 확충에도 적극적으로 나설 것입니다. 이를 통해 의료 서비스의 질을 높이고 환자 안전을 보장하는 동시에, 필수 의료 분야와 지방 의료로의 인력 배치 문제도 해결해 나갈 계획입니다.

정부와 의사협회가 함께 협력하여 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하는 것이 중요합니다. 의대 정원 확대는 단순히 의사 수를 늘리는 것이 아니라, 국민의 건강과 안전을 위한 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 위한 첫걸음입니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `document_search` with `{'query': '의대 정원 확대 반대'}`


[0m[36;1m[1;3m드리겠습니다. 2,000명을 늘리나 3,000명을 늘리나 안 늘리나 차이가 없을 것이다 라면 굳이 늘린다고 반대할 이유는 뭡니까? 라는 질문인데 그건 어떻습니까? 저는 그래서 오늘 사실 의대생 정원에 대한 의견을 내놓은 게 아닙니다. 지금 그 백그라운드에 있는 사실은 밑에 있는 상황에 대해서 말씀을 드리고 싶었던 거고 그 중에서 제일 중요한 거는 첫번째가 사람 값을 제가 말씀드렸고 두번째가 의료 시스템 내에서의 분배 문제 결론적으로 그것 때문에 티오가 안 난다. 팀이 붕괴되기 때문에 어차피 필수가와 지방으로 사람이 못 간다는 그 말씀을 드리고 싶었던 거고 이 문제가 근본적인 개선이 되지 않는 한 어차피 지금도 사실은 이런 거예요. 예를 들어서 제가 개헌을 해요. 제가 개헌을 하면 주변 사람들이 이렇게 조언을 합니다. 제가 물어보거든요. 네가 개헌하면 지금

그 2, 3만 명 정도 지금 되신다고 하죠. 밖에서 그런데 숫자가 많지도 않다는 건 뭐냐면 의대생 배출의 숫자가 그게 사실은 안 맞을 수도 있는 거예요. 그분들을 폭탄을 던져서 사람이 너무 많이 가서 결국에 예를 들어서 단가 경쟁을 펼치게 만들 만큼 많은 것도 아니고 그 정도의 숫자는 아니다. 일단은 지금 봤을 때는 제가 아까 보여드렸던 이 그림이 중요한데요. 자, 슬라이드 보여주십시오. 요겁니다. 결론적으로 지금 증원된 분들은 강제 2년 수련 제도 같은 걸 만들려고 하지 않습니까? 그러다 보면 결국에 지금 이렇게 생겨나는 병원들을 메우게 될 가능성이 크고요. 그래서 사실은 여기 있는 병원들이 메워지면 지방의료는 더 고사될 겁니다. 그 말은 제가 해석하기에는 2,000명 늘려서도 택도 없다고 받아들여지는데 그러면 3,000명, 4,000명, 5,000명을 늘리는 것은

무한히 인구 고령화가 진행되고 의료의 가수요가 늘다 보니까 거기에 맞춰서 공급을 늘리다 보니까 지금 사단이 벌어진 거고요. 듣다 보니까 나중에 한번 찾아봐요. 우리나라 실손의료보험 누가 도입했는지 그때 도입할 때 허가해준 금감원 담당과장님 누군지 물론 일부러야 그랬겠습니까? 그런데 이 상황에서 실손보험을 도입해놓으면 어쩌자는거에요? 실손의료보험 보험회사에서 처음에 만든 사람은 창밖으로 떼잖아요. 저도 복지부 담당자도 인터뷰 해보고 의사선생님도 인터뷰 해보고 선생님도 이야기 들어보고 하면 전반적으로 의료 수요를 줄여야 돼. 과잉이 많아. 그거 밑받침독 해놓고 어떻게 하겠어? 거기에 대해서는 대체로는 동의하시는 것 같습니다. 다만 현실적으로 의료 수요를 줄인다는 건 지방에 계신 분은 가능하다고 생각합니다.[0m[32;1m[1;3m의사협회 입장에서는 정부의 의대 정원 확대 계획에 대해 여러 가지 우려를 제기하고 있습니다. 우선, 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제가 근본적으로 해결되지 않는 한, 단순히 의대 정원을 늘리는 것이 해결책이 될 수 없다는 점을 강조합니다. 현재 의료 시스템은 필수 의료 분야와 지방 의료에 인력이 충분히 배치되지 않고 있으며, 이는 의대 정원을 늘린다고 해서 자동적으로 해결될 문제가 아닙니다.

또한, 의대 정원 확대는 의료 교육의 질에도 부정적인 영향을 미칠 수 있습니다. 갑작스러운 정원 확대는 의료 교육 인프라와 자원이 부족해질 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 우리는 의료 서비스의 질을 높이고 환자 안전을 보장하기 위해 노력하는 기관으로서, 의대 정원 확대가 이러한 목표에 부합하지 않는다고 판단합니다.

정부의 주장처럼 의사 수의 절대적 부족 문제가 있다 하더라도, 이를 해결하기 위한 방법은 단순히 의대 정원을 늘리는 것이 아니라, 의료 시스템의 근본적인 개선과 효율적인 자원 배분에 있어야 합니다. 우리는 이러한 점을 강조하며, 정부와의 건설적인 대화를 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하고자 합니다.

증원된 의사 인력이 필수의료와 지방 의료로 갈 수 있도록 하는 구체적인 방안 없이는, 의대 정원 확대가 지방의료 활성화나 필수 의료 분야에 실질적인 도움이 되지 않을 것입니다. 실제로, 강제 2년 수련 제도와 같은 방안을 도입하더라도, 증원된 인력이 결국 기존의 병원들을 메우게 되어 지방 의료는 더욱 고사할 가능성이 큽니다. 이는 의대 정원을 늘린다고 해서 모든 새로운 의사들이 필수 의료 분야나 지방 의료로 향할 것이라는 보장이 없으며, 오히려 현재의 의료 시스템 내에서 분배 문제를 더욱 심화시킬 수 있음을 의미합니다.

따라서, 의사협회는 의대 정원 확대에 대해 반대하는 입장을 견지하며, 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색해야 한다고 봅니다.[0m

[1m> Finished chain.[0m
(Doctor Union(의사협회)): 의사협회 입장에서는 정부의 의대 정원 확대 계획에 대해 여러 가지 우려를 제기하고 있습니다. 우선, 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제가 근본적으로 해결되지 않는 한, 단순히 의대 정원을 늘리는 것이 해결책이 될 수 없다는 점을 강조합니다. 현재 의료 시스템은 필수 의료 분야와 지방 의료에 인력이 충분히 배치되지 않고 있으며, 이는 의대 정원을 늘린다고 해서 자동적으로 해결될 문제가 아닙니다.

또한, 의대 정원 확대는 의료 교육의 질에도 부정적인 영향을 미칠 수 있습니다. 갑작스러운 정원 확대는 의료 교육 인프라와 자원이 부족해질 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 우리는 의료 서비스의 질을 높이고 환자 안전을 보장하기 위해 노력하는 기관으로서, 의대 정원 확대가 이러한 목표에 부합하지 않는다고 판단합니다.

정부의 주장처럼 의사 수의 절대적 부족 문제가 있다 하더라도, 이를 해결하기 위한 방법은 단순히 의대 정원을 늘리는 것이 아니라, 의료 시스템의 근본적인 개선과 효율적인 자원 배분에 있어야 합니다. 우리는 이러한 점을 강조하며, 정부와의 건설적인 대화를 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하고자 합니다.

증원된 의사 인력이 필수의료와 지방 의료로 갈 수 있도록 하는 구체적인 방안 없이는, 의대 정원 확대가 지방의료 활성화나 필수 의료 분야에 실질적인 도움이 되지 않을 것입니다. 실제로, 강제 2년 수련 제도와 같은 방안을 도입하더라도, 증원된 인력이 결국 기존의 병원들을 메우게 되어 지방 의료는 더욱 고사할 가능성이 큽니다. 이는 의대 정원을 늘린다고 해서 모든 새로운 의사들이 필수 의료 분야나 지방 의료로 향할 것이라는 보장이 없으며, 오히려 현재의 의료 시스템 내에서 분배 문제를 더욱 심화시킬 수 있음을 의미합니다.

따라서, 의사협회는 의대 정원 확대에 대해 반대하는 입장을 견지하며, 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색해야 한다고 봅니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `document_search` with `{'query': '증원된 의사 인력이 필수의료와 지방 의료로 갈 수 있도록 하는 구체적인 방안'}`


[0m[36;1m[1;3m의대 정원 2,000명 증원은 이러한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 여기에 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려하면 더 많은 의사가 필요합니다. 최근 통계를 보면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했습니다. 의사 고령화도 심각해서 2022년 6.8%인 70대 이상 의사 비중이 35년에는 19.8%에 이르게 됩니다. 인구 1,000명당 의사수와 보이스리 평균 3.7명인데 우리나라는 2.1명입니다.
의사수로 환산하면 1.6명의 5만 배를 곱하면 약 8만 명 이상이 부족한 상황입니다. 과학적 근거 없이 직역의 이해관계만 내세워서 증언에 반대하는 것은 받아들이기 어렵습니다. 국민의 건강과 생명을 골므로 집단행동을 벌이고 국민의 생명과 안전을 위협하는 것은 어떠한 명분으로서도 정당화되기 어렵습니다.

조사도 했었고. 네. 또 다른 주장은요. 의대 정원 늘려봐야 필수의료 분야 개선 안 하면 다 어떤 다른 데로 빠진다. 글쎄요. 그러면 경우가 세 가지가 있죠. 필수의료에 없다는 것은 세 가지 에요. 필수의료 쪽으로 우리나라에 의사가 안 간다는 건데 그러면 필수의료 쪽으로 자발적으로 가주는 게 하나 있을 텐데 지금 안 가고 있는 게 문제고. 그거는 해결이 안 되고 있다는 건. 소후 개선을 해달라는 거 아니겠습니까 소후 개선이나 이런 걸 통해서 하는데 소후 개선은 또 돈의 문제가 좀 복잡 한데 그것도 해줘 왔고 하겠다는 거고 그것도 하는 겁니다. 강제로 배분을 해야 되는 게 또 하나 인데 강제 배분을 할 수는 없죠 . 직업 선택의 자유 거주의 전혀 안 되는 거죠. 나머지 하나의 남은 방안은 전체 인원이 늘려서 그중에 필수의료도 가고 아닌데도 가는 게 자연스러운 경제 현상이에요. 그런데 그게 싫기 때문에 이런 저런 얘기를 하면서 그것을 그동안 정원 만큼은 안 된다고 해가지고 밀어 왔던

그런데 그 부분들은 좀 시간이 걸리나 그런 쪽으로 정책을 가야 된다는 말씀을 드리고요. 그럼요. 어찌됐든 간에 핵심은 말씀하세요 . 필수의료 집중인상 이건 지금부터 당장 논의해볼 수 있는 건 아닌가 보죠 논의한다고 지금 피켓을 내세 웠지만 의사들의 입장에서 볼 때는 이게 의료개혁이라는 것은 10년 20년 50년 후에도 같이 의료개혁 을 논의할 겁니다. 유명한 경꾸도 있고 하루 이틀에 되는 문제가 아니고 시간이 걸려서 차근차근 나가는 거예요. 그런데 의대 증원이라는 건 당장 결정해야 되잖아요 그렇죠 이번 달 안에 교육부에서 의대 증원 확정 돼서 입시요강 발표를 하면 모든 논의는 내년에 대해서는 끝이 나는 거예요. 그러니까 상당히 분수령이 지금 몇 주 안에 있는데 그 정책을 앞에 놓고 나머지 좀 긴 텀을 가진 필수 의료 정책에 대해서 우리가 당장 지금 의대 증원에 대해서 허락해 준 조건으로 바뀌면 너무 멀다고 생각을 하는 거죠.[0m[32;1m[1;3m의사협회 여러분, 의대 정원 확대에 대한 우리의 입장은 국민의 건강과 안전을 최우선으로 하는 것입니다. 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제에 대한 우려는 이해하지만, 현재의 의료 자원 분배와 관리만으로는 해결될 수 없는 문제들이 있습니다. 의사 수의 절대적 부족은 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려할 때 명백한 현실입니다. 최근 통계에 따르면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했으며, 의사 고령화도 심각하여 2022년 6.8%인 70대 이상 의사 비중이 2035년에는 19.8%에 이를 것으로 예상됩니다. 인구 1,000명당 의사수가 OECD 평균 3.7명인 반면 우리나라는 2.1명으로, 의사수로 환산하면 약 8만 명 이상이 부족한 상황입니다.

의대 정원 확대는 이러한 문제를 해결하기 위한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 의료 자원의 효율적인 분배와 관리를 우선시하는 것도 중요하지만, 현재와 미래의 의료 수요에 대응하기 위해서는 의사 수의 증원이 필수적입니다. 의료 교육의 질에 대한 우려도 이해하지만, 정부는 의료 교육 인프라와 자원의 확충에도 적극적으로 나설 것입니다. 이를 통해 의료 서비스의 질을 높이고 환자 안전을 보장하는 동시에, 필수 의료 분야와 지방 의료로의 인력 배치 문제도 해결해 나갈 계획입니다.

정부와 의사협회가 함께 협력하여 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하는 것이 중요합니다. 의대 정원 확대는 단순히 의사 수를 늘리는 것이 아니라, 국민의 건강과 안전을 위한 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 위한 첫걸음입니다.[0m

[1m> Finished chain.[0m
(Government(대한민국 정부)): 의사협회 여러분, 의대 정원 확대에 대한 우리의 입장은 국민의 건강과 안전을 최우선으로 하는 것입니다. 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제에 대한 우려는 이해하지만, 현재의 의료 자원 분배와 관리만으로는 해결될 수 없는 문제들이 있습니다. 의사 수의 절대적 부족은 고령화에 따른 보건 산업 수요 증가, 소득 증가에 따른 의료 수요 확대 추이, 그리고 의사의 근로시간 감소 추세까지 고려할 때 명백한 현실입니다. 최근 통계에 따르면 의사의 근로시간은 평균 12%, 전공의 근로시간은 평균 16% 감소했으며, 의사 고령화도 심각하여 2022년 6.8%인 70대 이상 의사 비중이 2035년에는 19.8%에 이를 것으로 예상됩니다. 인구 1,000명당 의사수가 OECD 평균 3.7명인 반면 우리나라는 2.1명으로, 의사수로 환산하면 약 8만 명 이상이 부족한 상황입니다.

의대 정원 확대는 이러한 문제를 해결하기 위한 국가의 헌법적 책무를 이행하기 위한 최소한의 필수적 조치입니다. 의료 자원의 효율적인 분배와 관리를 우선시하는 것도 중요하지만, 현재와 미래의 의료 수요에 대응하기 위해서는 의사 수의 증원이 필수적입니다. 의료 교육의 질에 대한 우려도 이해하지만, 정부는 의료 교육 인프라와 자원의 확충에도 적극적으로 나설 것입니다. 이를 통해 의료 서비스의 질을 높이고 환자 안전을 보장하는 동시에, 필수 의료 분야와 지방 의료로의 인력 배치 문제도 해결해 나갈 계획입니다.

정부와 의사협회가 함께 협력하여 국민의 건강과 안전을 최우선으로 하는 해결책을 모색하는 것이 중요합니다. 의대 정원 확대는 단순히 의사 수를 늘리는 것이 아니라, 국민의 건강과 안전을 위한 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 위한 첫걸음입니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `document_search` with `{'query': '의대 정원 확대 반대'}`


[0m[36;1m[1;3m드리겠습니다. 2,000명을 늘리나 3,000명을 늘리나 안 늘리나 차이가 없을 것이다 라면 굳이 늘린다고 반대할 이유는 뭡니까? 라는 질문인데 그건 어떻습니까? 저는 그래서 오늘 사실 의대생 정원에 대한 의견을 내놓은 게 아닙니다. 지금 그 백그라운드에 있는 사실은 밑에 있는 상황에 대해서 말씀을 드리고 싶었던 거고 그 중에서 제일 중요한 거는 첫번째가 사람 값을 제가 말씀드렸고 두번째가 의료 시스템 내에서의 분배 문제 결론적으로 그것 때문에 티오가 안 난다. 팀이 붕괴되기 때문에 어차피 필수가와 지방으로 사람이 못 간다는 그 말씀을 드리고 싶었던 거고 이 문제가 근본적인 개선이 되지 않는 한 어차피 지금도 사실은 이런 거예요. 예를 들어서 제가 개헌을 해요. 제가 개헌을 하면 주변 사람들이 이렇게 조언을 합니다. 제가 물어보거든요. 네가 개헌하면 지금

그 2, 3만 명 정도 지금 되신다고 하죠. 밖에서 그런데 숫자가 많지도 않다는 건 뭐냐면 의대생 배출의 숫자가 그게 사실은 안 맞을 수도 있는 거예요. 그분들을 폭탄을 던져서 사람이 너무 많이 가서 결국에 예를 들어서 단가 경쟁을 펼치게 만들 만큼 많은 것도 아니고 그 정도의 숫자는 아니다. 일단은 지금 봤을 때는 제가 아까 보여드렸던 이 그림이 중요한데요. 자, 슬라이드 보여주십시오. 요겁니다. 결론적으로 지금 증원된 분들은 강제 2년 수련 제도 같은 걸 만들려고 하지 않습니까? 그러다 보면 결국에 지금 이렇게 생겨나는 병원들을 메우게 될 가능성이 크고요. 그래서 사실은 여기 있는 병원들이 메워지면 지방의료는 더 고사될 겁니다. 그 말은 제가 해석하기에는 2,000명 늘려서도 택도 없다고 받아들여지는데 그러면 3,000명, 4,000명, 5,000명을 늘리는 것은

무한히 인구 고령화가 진행되고 의료의 가수요가 늘다 보니까 거기에 맞춰서 공급을 늘리다 보니까 지금 사단이 벌어진 거고요. 듣다 보니까 나중에 한번 찾아봐요. 우리나라 실손의료보험 누가 도입했는지 그때 도입할 때 허가해준 금감원 담당과장님 누군지 물론 일부러야 그랬겠습니까? 그런데 이 상황에서 실손보험을 도입해놓으면 어쩌자는거에요? 실손의료보험 보험회사에서 처음에 만든 사람은 창밖으로 떼잖아요. 저도 복지부 담당자도 인터뷰 해보고 의사선생님도 인터뷰 해보고 선생님도 이야기 들어보고 하면 전반적으로 의료 수요를 줄여야 돼. 과잉이 많아. 그거 밑받침독 해놓고 어떻게 하겠어? 거기에 대해서는 대체로는 동의하시는 것 같습니다. 다만 현실적으로 의료 수요를 줄인다는 건 지방에 계신 분은 가능하다고 생각합니다.[0m[32;1m[1;3m의사협회 입장에서는 정부의 의대 정원 확대 계획에 대해 명확한 반대 입장을 가지고 있습니다. 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제가 근본적으로 해결되지 않는 한, 단순히 의대 정원을 늘리는 것이 해결책이 될 수 없다는 점을 강조합니다. 현재 의료 시스템은 필수 의료 분야와 지방 의료에 인력이 충분히 배치되지 않고 있으며, 이는 의대 정원을 늘린다고 해서 자동적으로 해결될 문제가 아닙니다.

증원된 의사 인력이 필수 의료와 지방 의료로 갈 수 있도록 하는 구체적인 방안 없이는, 의대 정원 확대가 지방 의료 활성화나 필수 의료 분야에 실질적인 도움이 되지 않을 것입니다. 실제로, 강제 2년 수련 제도와 같은 방안을 도입하더라도, 증원된 인력이 결국 기존의 병원들을 메우게 되어 지방 의료는 더욱 고사할 가능성이 큽니다. 이는 의대 정원을 늘린다고 해서 모든 새로운 의사들이 필수 의료 분야나 지방 의료로 향할 것이라는 보장이 없으며, 오히려 현재의 의료 시스템 내에서 분배 문제를 더욱 심화시킬 수 있음을 의미합니다.

따라서, 의사협회는 의대 정원 확대에 대해 반대하는 입장을 견지하며, 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색해야 한다고 봅니다.[0m

[1m> Finished chain.[0m
(Doctor Union(의사협회)): 의사협회 입장에서는 정부의 의대 정원 확대 계획에 대해 명확한 반대 입장을 가지고 있습니다. 의료 시스템 내에서의 분배 문제와 필수 의료 및 지방 의료로의 인력 배치 문제가 근본적으로 해결되지 않는 한, 단순히 의대 정원을 늘리는 것이 해결책이 될 수 없다는 점을 강조합니다. 현재 의료 시스템은 필수 의료 분야와 지방 의료에 인력이 충분히 배치되지 않고 있으며, 이는 의대 정원을 늘린다고 해서 자동적으로 해결될 문제가 아닙니다.

증원된 의사 인력이 필수 의료와 지방 의료로 갈 수 있도록 하는 구체적인 방안 없이는, 의대 정원 확대가 지방 의료 활성화나 필수 의료 분야에 실질적인 도움이 되지 않을 것입니다. 실제로, 강제 2년 수련 제도와 같은 방안을 도입하더라도, 증원된 인력이 결국 기존의 병원들을 메우게 되어 지방 의료는 더욱 고사할 가능성이 큽니다. 이는 의대 정원을 늘린다고 해서 모든 새로운 의사들이 필수 의료 분야나 지방 의료로 향할 것이라는 보장이 없으며, 오히려 현재의 의료 시스템 내에서 분배 문제를 더욱 심화시킬 수 있음을 의미합니다.

따라서, 의사협회는 의대 정원 확대에 대해 반대하는 입장을 견지하며, 의료 시스템의 근본적인 개선과 효율적인 자원 배분을 통해 국민의 건강과 안전을 최우선으로 하는 해결책을 모색해야 한다고 봅니다.
```
```
Copymax_iters =6# 최대 반복 횟수를 6으로 설정합니다.n =0# 반복 횟수를 추적하는 변수를 0으로 초기화합니다.# DialogueSimulator 객체를 생성하고, agents와 select_next_speaker 함수를 전달합니다.simulator = DialogueSimulator(agents=agents, selection_function=select_next_speaker)# 시뮬레이터를 초기 상태로 리셋합니다.simulator.reset()# Moderator가 지정된 주제를 제시합니다.simulator.inject(""Moderator"", specified_topic)# Moderator가 제시한 주제를 출력합니다.print(f""(Moderator):{specified_topic}"")print(""\n"")whilen < max_iters:# 최대 반복 횟수까지 반복합니다.name, message = (
        simulator.step()
    )# 시뮬레이터의 다음 단계를 실행하고 발언자와 메시지를 받아옵니다.print(f""({name}):{message}"")# 발언자와 메시지를 출력합니다.print(""\n"")
    n +=1# 반복 횟수를 1 증가시킵니다.
```
```
(Moderator): 정부는 2025년 입시부터 의대 입학정원을 2000명 늘린다고 발표했습니다. 이에 의사단체는 전국에서 규탄집회를 열어 반발하고 있습니다. 의대 정원 확대를 둘러싼 논란 쟁점을 짚어보고, 필수 의료와 지역 의료 해법에 대해서 토론해주세요.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m대한민국 정부로서, 우리는 국민의 건강과 복지를 최우선으로 생각합니다. 의대 정원 확대는 이러한 목표를 달성하기 위한 필수적인 조치입니다. 현재 대한민국은 의사 수가 절대적으로 부족한 상황입니다. 특히, 지역별, 전문 분야별로 의사의 분포가 불균형하여, 일부 지역과 필수 의료 분야에서 의료 서비스 접근성이 낮은 실정입니다.

OECD 국가들을 살펴보면, 최근 몇 년간 의사 수를 적극적으로 늘리고 있는 추세입니다. 이는 고령화 사회로의 진입과 만성 질환자의 증가로 인한 의료 수요의 급증에 대응하기 위함입니다. 대한민국도 예외가 아니며, 노인 인구의 증가와 함께 의료 수요가 급증하고 있습니다. 이에 따라 의대 정원을 확대하여 의사 수를 증원하는 것은 시급한 과제입니다.

또한, 정부는 단순히 의사 수를 늘리는 것에 그치지 않고, 증원된 의사 인력이 필수 의료와 지역 의료로 흘러갈 수 있도록 다양한 정책을 마련하고 있습니다. 예를 들어, 의료 사고에 대한 안정망 구축과 보상 체계의 공정성을 높이는 등, 의사들이 필수 의료 분야와 지역 의료에 종사하기 위한 환경을 조성하고자 노력하고 있습니다. 이는 의료 서비스의 질을 향상시키고, 국민 모두가 고르게 의료 혜택을 받을 수 있도록 하는 데 목적이 있습니다.

따라서, 의대 정원 확대는 단기적으로는 의사 수의 증가를 가져오고, 장기적으로는 의료 서비스의 질적 향상과 국민 건강 증진에 기여할 것입니다. 이는 국가의 지속 가능한 발전을 위해 필수적인 조치이며, 정부로서는 이러한 정책을 적극적으로 추진할 의무가 있습니다.[0m

[1m> Finished chain.[0m
(Government(대한민국 정부)): 대한민국 정부로서, 우리는 국민의 건강과 복지를 최우선으로 생각합니다. 의대 정원 확대는 이러한 목표를 달성하기 위한 필수적인 조치입니다. 현재 대한민국은 의사 수가 절대적으로 부족한 상황입니다. 특히, 지역별, 전문 분야별로 의사의 분포가 불균형하여, 일부 지역과 필수 의료 분야에서 의료 서비스 접근성이 낮은 실정입니다.

OECD 국가들을 살펴보면, 최근 몇 년간 의사 수를 적극적으로 늘리고 있는 추세입니다. 이는 고령화 사회로의 진입과 만성 질환자의 증가로 인한 의료 수요의 급증에 대응하기 위함입니다. 대한민국도 예외가 아니며, 노인 인구의 증가와 함께 의료 수요가 급증하고 있습니다. 이에 따라 의대 정원을 확대하여 의사 수를 증원하는 것은 시급한 과제입니다.

또한, 정부는 단순히 의사 수를 늘리는 것에 그치지 않고, 증원된 의사 인력이 필수 의료와 지역 의료로 흘러갈 수 있도록 다양한 정책을 마련하고 있습니다. 예를 들어, 의료 사고에 대한 안정망 구축과 보상 체계의 공정성을 높이는 등, 의사들이 필수 의료 분야와 지역 의료에 종사하기 위한 환경을 조성하고자 노력하고 있습니다. 이는 의료 서비스의 질을 향상시키고, 국민 모두가 고르게 의료 혜택을 받을 수 있도록 하는 데 목적이 있습니다.

따라서, 의대 정원 확대는 단기적으로는 의사 수의 증가를 가져오고, 장기적으로는 의료 서비스의 질적 향상과 국민 건강 증진에 기여할 것입니다. 이는 국가의 지속 가능한 발전을 위해 필수적인 조치이며, 정부로서는 이러한 정책을 적극적으로 추진할 의무가 있습니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m의사협회로서 우리는 정부의 의대 정원 확대 계획에 대해 심각한 우려를 표합니다. 정부가 제시한 의사 부족 문제와 의료 서비스 접근성 개선을 위한 의대 정원 확대는 표면적인 해결책에 불과하며, 실제로는 의료계에 더 큰 문제를 야기할 수 있습니다.

첫째, 의사의 수가 절대적으로 부족하다는 정부의 주장에 대해 반박하고자 합니다. 대한민국의 의사 수는 OECD 국가들과 비교했을 때 절대적으로 부족한 수준이 아닙니다. 실제로, OECD의 'Health at a Glance 2021' 보고서에 따르면, 대한민국의 인구 천 명당 의사 수는 OECD 평균에 근접하거나 이를 상회하는 수준입니다. 이는 대한민국에 의사 수가 절대적으로 부족하다는 주장과 상반되는 데이터입니다.

둘째, 의대 정원을 확대한다 하더라도, 이것이 필수 의료나 지방 의료 활성화로 직접적으로 이어진다는 보장이 없습니다. 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로 집중되는 현상은 여전히 해결되지 않고 있습니다. 따라서, 의대 정원 확대가 지역 의료 불균형 문제를 해결하는 근본적인 방안이 될 수 없습니다.

셋째, 의대 정원을 확대하는 것은 의료 교육의 질 저하로 이어질 수 있습니다. 갑작스러운 정원 확대는 의료 교육 인프라의 부족을 초래하며, 이는 의사의 질적 수준 저하로 이어질 수 있습니다. 의료 서비스의 질은 단순히 의사의 수에 의해 결정되는 것이 아니라, 의사 개개인의 전문성과 질적 수준에 의해 좌우됩니다.

결론적으로, 의대 정원 확대는 근본적인 해결책이 아니며, 오히려 의료계에 더 많은 문제를 야기할 수 있습니다. 정부는 의대 정원 확대 대신, 의료 인력의 지역별, 전문 분야별 분포를 균형 있게 조정하고, 의료 서비스의 질을 높이기 위한 실질적인 방안을 모색해야 합니다.[0m

[1m> Finished chain.[0m
(Doctor Union(의사협회)): 의사협회로서 우리는 정부의 의대 정원 확대 계획에 대해 심각한 우려를 표합니다. 정부가 제시한 의사 부족 문제와 의료 서비스 접근성 개선을 위한 의대 정원 확대는 표면적인 해결책에 불과하며, 실제로는 의료계에 더 큰 문제를 야기할 수 있습니다.

첫째, 의사의 수가 절대적으로 부족하다는 정부의 주장에 대해 반박하고자 합니다. 대한민국의 의사 수는 OECD 국가들과 비교했을 때 절대적으로 부족한 수준이 아닙니다. 실제로, OECD의 'Health at a Glance 2021' 보고서에 따르면, 대한민국의 인구 천 명당 의사 수는 OECD 평균에 근접하거나 이를 상회하는 수준입니다. 이는 대한민국에 의사 수가 절대적으로 부족하다는 주장과 상반되는 데이터입니다.

둘째, 의대 정원을 확대한다 하더라도, 이것이 필수 의료나 지방 의료 활성화로 직접적으로 이어진다는 보장이 없습니다. 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로 집중되는 현상은 여전히 해결되지 않고 있습니다. 따라서, 의대 정원 확대가 지역 의료 불균형 문제를 해결하는 근본적인 방안이 될 수 없습니다.

셋째, 의대 정원을 확대하는 것은 의료 교육의 질 저하로 이어질 수 있습니다. 갑작스러운 정원 확대는 의료 교육 인프라의 부족을 초래하며, 이는 의사의 질적 수준 저하로 이어질 수 있습니다. 의료 서비스의 질은 단순히 의사의 수에 의해 결정되는 것이 아니라, 의사 개개인의 전문성과 질적 수준에 의해 좌우됩니다.

결론적으로, 의대 정원 확대는 근본적인 해결책이 아니며, 오히려 의료계에 더 많은 문제를 야기할 수 있습니다. 정부는 의대 정원 확대 대신, 의료 인력의 지역별, 전문 분야별 분포를 균형 있게 조정하고, 의료 서비스의 질을 높이기 위한 실질적인 방안을 모색해야 합니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': '대한민국 인구 천 명당 의사 수 OECD 비교 2023'}`


[0m[36;1m[1;3m[{'url': 'https://news.nate.com/view/20240205n34843', 'content': ""AI챗으로 요약. 'OECD 보건통계 2023' 분석결과. 우리나라 1000명당 의사 수 2.12명. OECD 평균 3.69명보다 1.57명 ↓. 프랑스·일본과 비교해도 2만∼5만명 부족. 우리나라 1000명당 의사 수가 경제협력개발기구 (OECD) 평균에 도달하기 위해서는 8만명 이상의 의사가 추가로 ...""}, {'url': 'https://www.yna.co.kr/view/AKR20231110153900530', 'content': '한국 의사 수 oecd 하위권…수도권·지방 모두 평균 이하. oecd 회원국에서 활동하는 의사는 2011년 350만 명에서 2021년 430만 명으로 늘었다. 모든 oecd 국가에서 의사 수가 인구보다 더 빠르게 늘어나 1천명당 평균 의사 수는 같은 기간 3.2명에서 3.7명으로 늘었다.'}, {'url': 'https://www.hankyung.com/article/2023072515381', 'content': '한국의 인구 대비 의사 수가 경제협력개발기구 (OECD) 38개 회원국 중 37위에 그친 것으로 나타났다. 한국보다 인구당 의사 수가 적은 나라는 멕시코 ...'}, {'url': 'https://www.yna.co.kr/view/AKR20231115158500530', 'content': ""16일 OECD가 최근 공개한 '한눈에 보는 보건의료 2023' (Health at a Glance 2023)을 보면, 2021년 한국의 의사 1인당 진료건수는 6천113명으로 관련 통계가 있는 OECD 32개국 가운데 가장 많았다. OECD 평균인 1천788명의 3.4배에 달했으며 2번째로 많은 일본 (4천288명)보다도 1천800 ...""}, {'url': 'https://www.yna.co.kr/view/AKR20230729022600530', 'content': ""의사. [게티이미지뱅크 제공] (서울=연합뉴스) 고미혜 기자 = 우리나라의 의사 소득이 경제협력개발기구 (OECD) 국가 중 최상위권인 것으로 나타났다. 30일 OECD의 '2023년 보건통계 (Health Statistics 2023)'에 따르면 우리나라 전문의 중 병·의원에 소속돼 월급을 받는 ...""}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': '의대 정원 확대 지역 의료 불균형 해결 효과 연구 2023'}`


[0m[36;1m[1;3m[{'url': 'https://www.yna.co.kr/view/AKR20230718058700518', 'content': ""그러나 의협 의료정책연구소의 2013년 '향후 10년간 의사인력 공급의 적정수준 연구' 보고서에선 2011년 기준 2.0명으로 oecd 평균(3.0명)보다 1명 적었던 우리나라 1천명당 활동 의사 수가, 의대 정원 확대 없이도 이르면 2023년 늦어도 2026년이면 oecd 평균을 따라잡을 ...""}, {'url': 'https://m.khan.co.kr/article/202306181607001', 'content': '2023.06.18 16:07 입력 2023.06.18 16: ... 지역 의료 불균형 완화책으로는 \'지역인재전형 확대\'를 언급했다. 조 장관은 ""비수도권 의대는 지역고교 졸업생 40% 이상 선발이 의무인데 이 비율을 높여볼까 한다""며 ""전공의의 지방 수련 기회를 더 많이 제공하겠다 ...'}, {'url': 'https://www.yna.co.kr/view/AKR20231015026100530', 'content': '재판매 및 DB 금지] (서울=연합뉴스) 오진송 기자 =정부가 18년째 묶여있던 국내 의과대학 정원을 2025년 입시부터 대폭 늘리려고 하는 가운데, 지역 간 의료 불균형 해소를 위해 지방의대 중심으로 증원이 이뤄질 것이라는 관측이 나온다. 정부가 의대 정원을 ...'}, {'url': 'https://health.chosun.com/site/data/html_dir/2023/08/03/2023080300981.html', 'content': '일본의 경우 고령화, 지역 의료 인력난 해소를 위해 2008년부터 의대 정원을 꾸준히 늘려왔다. 일본 의대 정원은 2008년 7793명에서 2019년 9420명까지 늘었다. 약간의 조정을 거치긴 했으나 2023년 의대 정원은 9384명에 달한다. 일본이 겪은 의대 정원 확대 부작용은 ...'}, {'url': 'https://www.yna.co.kr/view/AKR20240205151551530', 'content': '보건복지부는 6일 정부 서울청사에서 보건의료정책심의위원회(보정심)를 열고 이런 내용의 2025학년도 입시 의대 입학정원 증원 규모를 발표했다. 2025학년도 의대 입학생은 2031년 의대를 졸업해 의사가 되는데, 이때부터 매년 2천명씩 의사를 배출해 2035년까지 1만 ...'}][0m[32;1m[1;3mGovernment(대한민국 정부): 의사협회의 주장에 대해 몇 가지 사실을 바탕으로 반박하고자 합니다. 첫째, 대한민국의 의사 수가 OECD 국가들과 비교하여 절대적으로 부족하지 않다는 주장에 대해, 최신 데이터를 살펴보면 상황이 다릅니다. 'OECD 보건통계 2023' 분석결과에 따르면, 우리나라의 인구 천 명당 의사 수는 2.12명으로, OECD 평균인 3.69명보다 1.57명 낮은 수준입니다. 이는 우리나라가 OECD 회원국 중 인구 대비 의사 수가 37위에 그치며, 프랑스나 일본과 비교해도 2만~5만 명 부족한 상황임을 보여줍니다([출처](https://news.nate.com/view/20240205n34843)). 따라서, 의사 수의 절대적 부족은 명백한 사실입니다.

둘째, 의대 정원 확대가 필수 의료나 지방 의료 활성화로 직접적으로 이어지지 않는다는 주장에 대해서는, 정부의 정책 방향을 살펴볼 필요가 있습니다. 정부는 의대 정원을 확대하는 것과 동시에, 지역 의료 불균형 해소를 위해 지방의대 중심으로 증원이 이루어질 것임을 밝혔습니다. 이는 지역 간 의료 불균형 해소에 기여할 것으로 기대됩니다([출처](https://www.yna.co.kr/view/AKR20231015026100530)). 또한, 일본의 사례를 보면, 고령화와 지역 의료 인력난 해소를 위해 2008년부터 의대 정원을 지속적으로 늘려왔으며, 이러한 조치가 일정 부분 효과를 거두고 있음을 알 수 있습니다([출처](https://health.chosun.com/site/data/html_dir/2023/08/03/2023080300981.html)).

셋째, 의대 정원 확대가 의료 교육의 질 저하로 이어질 수 있다는 우려에 대해서는, 정부가 의료 교육의 질을 유지하고 향상시키기 위한 다양한 대책을 마련하고 있음을 강조하고 싶습니다. 의대 정원 확대는 단순히 수적 증가에 그치지 않고, 의료 교육의 질적 관리와 함께 이루어질 것입니다.

결론적으로, 대한민국의 의사 수 부족 문제는 실제로 존재하며, 이를 해결하기 위해 의대 정원 확대는 필수적인 조치입니다. 또한, 정부는 의대 정원 확대와 함께 필수 의료 및 지역 의료 강화, 의료 교육의 질적 향상을 위한 종합적인 대책을 시행할 예정입니다. 이러한 노력이 궁극적으로 국민의 건강과 복지 향상에 기여할 것입니다.[0m

[1m> Finished chain.[0m
(Government(대한민국 정부)): Government(대한민국 정부): 의사협회의 주장에 대해 몇 가지 사실을 바탕으로 반박하고자 합니다. 첫째, 대한민국의 의사 수가 OECD 국가들과 비교하여 절대적으로 부족하지 않다는 주장에 대해, 최신 데이터를 살펴보면 상황이 다릅니다. 'OECD 보건통계 2023' 분석결과에 따르면, 우리나라의 인구 천 명당 의사 수는 2.12명으로, OECD 평균인 3.69명보다 1.57명 낮은 수준입니다. 이는 우리나라가 OECD 회원국 중 인구 대비 의사 수가 37위에 그치며, 프랑스나 일본과 비교해도 2만~5만 명 부족한 상황임을 보여줍니다([출처](https://news.nate.com/view/20240205n34843)). 따라서, 의사 수의 절대적 부족은 명백한 사실입니다.

둘째, 의대 정원 확대가 필수 의료나 지방 의료 활성화로 직접적으로 이어지지 않는다는 주장에 대해서는, 정부의 정책 방향을 살펴볼 필요가 있습니다. 정부는 의대 정원을 확대하는 것과 동시에, 지역 의료 불균형 해소를 위해 지방의대 중심으로 증원이 이루어질 것임을 밝혔습니다. 이는 지역 간 의료 불균형 해소에 기여할 것으로 기대됩니다([출처](https://www.yna.co.kr/view/AKR20231015026100530)). 또한, 일본의 사례를 보면, 고령화와 지역 의료 인력난 해소를 위해 2008년부터 의대 정원을 지속적으로 늘려왔으며, 이러한 조치가 일정 부분 효과를 거두고 있음을 알 수 있습니다([출처](https://health.chosun.com/site/data/html_dir/2023/08/03/2023080300981.html)).

셋째, 의대 정원 확대가 의료 교육의 질 저하로 이어질 수 있다는 우려에 대해서는, 정부가 의료 교육의 질을 유지하고 향상시키기 위한 다양한 대책을 마련하고 있음을 강조하고 싶습니다. 의대 정원 확대는 단순히 수적 증가에 그치지 않고, 의료 교육의 질적 관리와 함께 이루어질 것입니다.

결론적으로, 대한민국의 의사 수 부족 문제는 실제로 존재하며, 이를 해결하기 위해 의대 정원 확대는 필수적인 조치입니다. 또한, 정부는 의대 정원 확대와 함께 필수 의료 및 지역 의료 강화, 의료 교육의 질적 향상을 위한 종합적인 대책을 시행할 예정입니다. 이러한 노력이 궁극적으로 국민의 건강과 복지 향상에 기여할 것입니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': '대한민국 의사 수 OECD 비교 2023'}`


[0m[36;1m[1;3m[{'url': 'https://www.yna.co.kr/view/AKR20231110153900530', 'content': '모든 OECD 국가에서 의사 수가 인구보다 더 빠르게 늘어나 1천명당 평균 의사 수는 같은 기간 3.2명에서 3.7명으로 늘었다. 한국에서도 의사 수는 2.0명에서 2.6명으로 늘었다. 다만 여전히 OECD 평균 (3.7명)에 못 미치고, 심지어 2011년 평균 (3.2명)보다도 적다. 의사의 ...'}, {'url': 'https://www.hankyung.com/article/2023072515381', 'content': 'OECD 보건통계 2023간호 인력도 평균 밑돌아. 한국의 인구 대비 의사 수가 경제협력개발기구 (OECD) 38개 회원국 중 37위에 그친 것으로 나타났다 ...'}, {'url': 'https://www.medicaltimes.com/Main/News/NewsView.html?ID=1154707', 'content': '전체\n정책\n병·의원\n제약·바이오\n의료기기·AI\n학술\n오피니언\n메타TV\n우리나라 의사수 부족 근거 OECD 통계 최신판 나왔다\n의료인 숫자가 부족하다는 OECD 보건통계 최신판이 나왔다. 한의사를 포함한 임상 의사 수는 OECD 국가 중 두 번째로 적었고 간호인력 숫자는 OECD 평균 보다 낮았다.\n 반면 의료장비 수와 병상 수는 OECD 평균 보다도 한참 많았다. 인구 100만명 당 MRI는 35.5대, CT는 42.4대로 OECD 평균 19.6대, 29.8대 보다도 2배 가까이 높았다. 1000명당 7.3개로 OECD 평균 3.5개 보다 2배 이상 많았다.\n우리나라 국민의 외래진료 횟수는 연간 한의사를 포함한 우리나라 임상 의사 수는 인구 1000명당 2.6명으로 OECD 국가 중 두 번째로 적었다. 복지부 김선도 정보통계담당관은 ""OECD Health Statistics는 OECD 국가 간 공통된 기준에 의해서 산출되는 국가'}, {'url': 'https://economist.co.kr/article/view/ecn202311120008', 'content': 'OECD 회원국에서 활동하는 의사는 2011년 350만 명에서 2021년 430만 명으로 늘었다. 모든 OECD 국가에서 의사 수가 인구보다 더 빠르게 늘어나 1000명당 평균 의사 수는 같은 기간 3.2명에서 3.7명으로 늘었다. 한국에서도 의사 수는 2.0명에서 2.6명으로 늘었다. 다만 ...'}, {'url': 'https://www.yna.co.kr/view/AKR20230729022600530', 'content': ""의사. [게티이미지뱅크 제공] (서울=연합뉴스) 고미혜 기자 = 우리나라의 의사 소득이 경제협력개발기구 (OECD) 국가 중 최상위권인 것으로 나타났다. 30일 OECD의 '2023년 보건통계 (Health Statistics 2023)'에 따르면 우리나라 전문의 중 병·의원에 소속돼 월급을 받는 ...""}][0m[32;1m[1;3m정부의 주장에 대해 몇 가지 사실을 바탕으로 반박하겠습니다. 첫째, 정부가 제시한 'OECD 보건통계 2023'에 따른 우리나라의 인구 천 명당 의사 수가 OECD 평균보다 낮다는 점은 인정합니다. 그러나, 최신 데이터에 따르면, 우리나라의 인구 천 명당 의사 수는 2.0명에서 2.6명으로 늘었으며, 이는 여전히 OECD 평균인 3.7명에는 못 미치지만, 지속적으로 증가하고 있다는 점을 강조하고 싶습니다([연합뉴스](https://www.yna.co.kr/view/AKR20231110153900530)). 이는 정부가 주장하는 의사 수의 절대적 부족 문제가 다소 과장되었을 수 있음을 시사합니다.

둘째, 의대 정원 확대가 지역 의료 불균형 해소에 기여할 것이라는 정부의 주장에 대해서는, 실제로 의대 졸업생들이 필수 의료 분야나 지방 의료로 유입되는 것을 보장할 수 있는 구체적인 정책이 수반되지 않는 한, 단순히 의대 정원을 확대하는 것만으로는 지역 의료 불균형 문제를 해결하기 어렵다는 점을 지적하고 싶습니다. 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로 집중되는 현상은 여전히 해결되지 않고 있으며, 이는 의대 정원 확대만으로는 근본적인 해결책이 될 수 없음을 의미합니다.

셋째, 의료 교육의 질 저하 우려에 대해, 정부가 의료 교육의 질을 유지하고 향상시키기 위한 대책을 마련하고 있다고 하지만, 갑작스러운 정원 확대는 의료 교육 인프라의 부족을 초래할 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 의료 서비스의 질은 단순히 의사의 수에 의해 결정되는 것이 아니라, 의사 개개인의 전문성과 질적 수준에 의해 좌우됩니다.

결론적으로, 의대 정원 확대는 근본적인 해결책이 아니며, 오히려 의료계에 더 많은 문제를 야기할 수 있습니다. 정부는 의대 정원 확대 대신, 의료 인력의 지역별, 전문 분야별 분포를 균형 있게 조정하고, 의료 서비스의 질을 높이기 위한 실질적인 방안을 모색해야 합니다.[0m

[1m> Finished chain.[0m
(Doctor Union(의사협회)): 정부의 주장에 대해 몇 가지 사실을 바탕으로 반박하겠습니다. 첫째, 정부가 제시한 'OECD 보건통계 2023'에 따른 우리나라의 인구 천 명당 의사 수가 OECD 평균보다 낮다는 점은 인정합니다. 그러나, 최신 데이터에 따르면, 우리나라의 인구 천 명당 의사 수는 2.0명에서 2.6명으로 늘었으며, 이는 여전히 OECD 평균인 3.7명에는 못 미치지만, 지속적으로 증가하고 있다는 점을 강조하고 싶습니다([연합뉴스](https://www.yna.co.kr/view/AKR20231110153900530)). 이는 정부가 주장하는 의사 수의 절대적 부족 문제가 다소 과장되었을 수 있음을 시사합니다.

둘째, 의대 정원 확대가 지역 의료 불균형 해소에 기여할 것이라는 정부의 주장에 대해서는, 실제로 의대 졸업생들이 필수 의료 분야나 지방 의료로 유입되는 것을 보장할 수 있는 구체적인 정책이 수반되지 않는 한, 단순히 의대 정원을 확대하는 것만으로는 지역 의료 불균형 문제를 해결하기 어렵다는 점을 지적하고 싶습니다. 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로 집중되는 현상은 여전히 해결되지 않고 있으며, 이는 의대 정원 확대만으로는 근본적인 해결책이 될 수 없음을 의미합니다.

셋째, 의료 교육의 질 저하 우려에 대해, 정부가 의료 교육의 질을 유지하고 향상시키기 위한 대책을 마련하고 있다고 하지만, 갑작스러운 정원 확대는 의료 교육 인프라의 부족을 초래할 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 의료 서비스의 질은 단순히 의사의 수에 의해 결정되는 것이 아니라, 의사 개개인의 전문성과 질적 수준에 의해 좌우됩니다.

결론적으로, 의대 정원 확대는 근본적인 해결책이 아니며, 오히려 의료계에 더 많은 문제를 야기할 수 있습니다. 정부는 의대 정원 확대 대신, 의료 인력의 지역별, 전문 분야별 분포를 균형 있게 조정하고, 의료 서비스의 질을 높이기 위한 실질적인 방안을 모색해야 합니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3mGovernment(대한민국 정부): 의사협회의 주장에 대해 다시 한번 명확하게 반박하고자 합니다. 첫째, 의사협회가 인정한 바와 같이, 우리나라의 인구 천 명당 의사 수가 OECD 평균에 미치지 못한다는 점은 의사 수의 절대적 부족을 시사합니다. 비록 인구 천 명당 의사 수가 점진적으로 증가하고 있다고 하더라도, 이는 현재와 미래의 의료 수요를 충족시키기에는 여전히 부족한 상황입니다. 고령화 사회로의 진입과 만성 질환자의 증가로 인한 의료 수요의 급증을 고려할 때, 의사 수의 적극적인 증원은 필수적입니다.

둘째, 의대 정원 확대와 함께, 정부는 지역 의료 불균형 해소를 위한 구체적인 정책을 시행하고 있습니다. 예를 들어, 지방의대 중심으로 의대 정원을 증원하고, 의대 졸업생이 필수 의료 분야와 지방 의료에 종사하도록 유도하는 다양한 인센티브를 제공하고 있습니다. 이러한 정책은 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로만 집중되는 현상을 완화하고, 지역 의료 불균형 문제를 해결하는 데 기여할 것입니다.

셋째, 의료 교육의 질 저하에 대한 우려는 이해하지만, 정부는 의료 교육의 질을 유지하고 향상시키기 위한 체계적인 대책을 마련하고 있습니다. 의대 정원 확대는 의료 교육 인프라 확충과 함께 이루어지며, 의료 교육의 질적 관리를 강화하기 위한 다양한 프로그램이 도입될 예정입니다. 이를 통해 의사의 질적 수준을 유지하고 향상시킬 수 있을 것입니다.

결론적으로, 의대 정원 확대는 현재와 미래의 의료 수요에 대응하고, 지역 의료 불균형을 해소하며, 의료 교육의 질을 유지하고 향상시키기 위한 필수적인 조치입니다. 정부는 이러한 정책을 통해 국민의 건강과 복지 향상을 위해 최선을 다하고 있습니다.[0m

[1m> Finished chain.[0m
(Government(대한민국 정부)): Government(대한민국 정부): 의사협회의 주장에 대해 다시 한번 명확하게 반박하고자 합니다. 첫째, 의사협회가 인정한 바와 같이, 우리나라의 인구 천 명당 의사 수가 OECD 평균에 미치지 못한다는 점은 의사 수의 절대적 부족을 시사합니다. 비록 인구 천 명당 의사 수가 점진적으로 증가하고 있다고 하더라도, 이는 현재와 미래의 의료 수요를 충족시키기에는 여전히 부족한 상황입니다. 고령화 사회로의 진입과 만성 질환자의 증가로 인한 의료 수요의 급증을 고려할 때, 의사 수의 적극적인 증원은 필수적입니다.

둘째, 의대 정원 확대와 함께, 정부는 지역 의료 불균형 해소를 위한 구체적인 정책을 시행하고 있습니다. 예를 들어, 지방의대 중심으로 의대 정원을 증원하고, 의대 졸업생이 필수 의료 분야와 지방 의료에 종사하도록 유도하는 다양한 인센티브를 제공하고 있습니다. 이러한 정책은 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로만 집중되는 현상을 완화하고, 지역 의료 불균형 문제를 해결하는 데 기여할 것입니다.

셋째, 의료 교육의 질 저하에 대한 우려는 이해하지만, 정부는 의료 교육의 질을 유지하고 향상시키기 위한 체계적인 대책을 마련하고 있습니다. 의대 정원 확대는 의료 교육 인프라 확충과 함께 이루어지며, 의료 교육의 질적 관리를 강화하기 위한 다양한 프로그램이 도입될 예정입니다. 이를 통해 의사의 질적 수준을 유지하고 향상시킬 수 있을 것입니다.

결론적으로, 의대 정원 확대는 현재와 미래의 의료 수요에 대응하고, 지역 의료 불균형을 해소하며, 의료 교육의 질을 유지하고 향상시키기 위한 필수적인 조치입니다. 정부는 이러한 정책을 통해 국민의 건강과 복지 향상을 위해 최선을 다하고 있습니다.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m정부의 주장에 대해 몇 가지 사실을 바탕으로 반박하겠습니다. 첫째, 정부가 제시한 'OECD 보건통계 2023'에 따른 우리나라의 인구 천 명당 의사 수가 OECD 평균보다 낮다는 점은 인정합니다. 그러나, 최신 데이터에 따르면, 우리나라의 인구 천 명당 의사 수는 2.0명에서 2.6명으로 늘었으며, 이는 여전히 OECD 평균인 3.7명에는 못 미치지만, 지속적으로 증가하고 있다는 점을 강조하고 싶습니다([연합뉴스](https://www.yna.co.kr/view/AKR20231110153900530)). 이는 정부가 주장하는 의사 수의 절대적 부족 문제가 다소 과장되었을 수 있음을 시사합니다.

둘째, 의대 정원 확대가 지역 의료 불균형 해소에 기여할 것이라는 정부의 주장에 대해서는, 실제로 의대 졸업생들이 필수 의료 분야나 지방 의료로 유입되는 것을 보장할 수 있는 구체적인 정책이 수반되지 않는 한, 단순히 의대 정원을 확대하는 것만으로는 지역 의료 불균형 문제를 해결하기 어렵다는 점을 지적하고 싶습니다. 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로 집중되는 현상은 여전히 해결되지 않고 있으며, 이는 의대 정원 확대만으로는 근본적인 해결책이 될 수 없음을 의미합니다.

셋째, 의료 교육의 질 저하 우려에 대해, 정부가 의료 교육의 질을 유지하고 향상시키기 위한 대책을 마련하고 있다고 하지만, 갑작스러운 정원 확대는 의료 교육 인프라의 부족을 초래할 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 의료 서비스의 질은 단순히 의사의 수에 의해 결정되는 것이 아니라, 의사 개개인의 전문성과 질적 수준에 의해 좌우됩니다.

결론적으로, 의대 정원 확대는 근본적인 해결책이 아니며, 오히려 의료계에 더 많은 문제를 야기할 수 있습니다. 정부는 의대 정원 확대 대신, 의료 인력의 지역별, 전문 분야별 분포를 균형 있게 조정하고, 의료 서비스의 질을 높이기 위한 실질적인 방안을 모색해야 합니다.[0m

[1m> Finished chain.[0m
(Doctor Union(의사협회)): 정부의 주장에 대해 몇 가지 사실을 바탕으로 반박하겠습니다. 첫째, 정부가 제시한 'OECD 보건통계 2023'에 따른 우리나라의 인구 천 명당 의사 수가 OECD 평균보다 낮다는 점은 인정합니다. 그러나, 최신 데이터에 따르면, 우리나라의 인구 천 명당 의사 수는 2.0명에서 2.6명으로 늘었으며, 이는 여전히 OECD 평균인 3.7명에는 못 미치지만, 지속적으로 증가하고 있다는 점을 강조하고 싶습니다([연합뉴스](https://www.yna.co.kr/view/AKR20231110153900530)). 이는 정부가 주장하는 의사 수의 절대적 부족 문제가 다소 과장되었을 수 있음을 시사합니다.

둘째, 의대 정원 확대가 지역 의료 불균형 해소에 기여할 것이라는 정부의 주장에 대해서는, 실제로 의대 졸업생들이 필수 의료 분야나 지방 의료로 유입되는 것을 보장할 수 있는 구체적인 정책이 수반되지 않는 한, 단순히 의대 정원을 확대하는 것만으로는 지역 의료 불균형 문제를 해결하기 어렵다는 점을 지적하고 싶습니다. 의대 졸업생들이 고소득 전문 분야나 대도시 지역으로 집중되는 현상은 여전히 해결되지 않고 있으며, 이는 의대 정원 확대만으로는 근본적인 해결책이 될 수 없음을 의미합니다.

셋째, 의료 교육의 질 저하 우려에 대해, 정부가 의료 교육의 질을 유지하고 향상시키기 위한 대책을 마련하고 있다고 하지만, 갑작스러운 정원 확대는 의료 교육 인프라의 부족을 초래할 수 있으며, 이는 결국 의사의 질적 수준 저하로 이어질 수 있습니다. 의료 서비스의 질은 단순히 의사의 수에 의해 결정되는 것이 아니라, 의사 개개인의 전문성과 질적 수준에 의해 좌우됩니다.

결론적으로, 의대 정원 확대는 근본적인 해결책이 아니며, 오히려 의료계에 더 많은 문제를 야기할 수 있습니다. 정부는 의대 정원 확대 대신, 의료 인력의 지역별, 전문 분야별 분포를 균형 있게 조정하고, 의료 서비스의 질을 높이기 위한 실질적인 방안을 모색해야 합니다.
```"
CH17 LangGraph,https://wikidocs.net/233785,"# CH17 LangGraph

LangGraph"
01. Chain of Table for Multiple Tables,https://wikidocs.net/233325,"# 01. Chain of Table for Multiple Tables

CYQIQ Chain of Table은 여러 테이블을 처리하기 위한 기술입니다.
이 기술은 여러 테이블에서 정보를 추출하고 통합하여 사용자의 질의에 대한 답변을 생성하는 데 사용됩니다.
CYQIQ Chain of Table은 테이블 간의 관계를 파악하고, 필요한 정보를 효과적으로 검색하여 사용자에게 제공하는 것을 목표로 합니다.
이를 통해 사용자는 여러 테이블에 분산된 데이터를 통합적으로 활용할 수 있으며, 보다 정확하고 유용한 정보를 얻을 수 있습니다.
CYQIQ Chain of Table 기술은 데이터베이스 관리, 정보 검색, 자연어 처리 등 다양한 분야에서 활용될 수 있습니다.
Authors:Richárd HrubyandDániel Márk SzalaifromCYQIQ Inc.
## Chain of Table
Chain of Table은 Google과 University of California, San Diego의 연구자들이 소개한 새로운 기술입니다.
이 방법은 select, group, sort 등과 같은 SQL과 유사한 테이블 조작 작업과 열 추가 작업을 일련의 과정으로 만드는 것을 포함합니다.
CoT는 또한 구조화되지 않은 테이블에서 데이터를 추출하여 새로운 열에 추가함으로써 SQL, 사람, LLM에게 훨씬 더 친숙한 테이블을 만드는 방법을 사용합니다.
이 방법은 LLM이 제공된 질문에 쉽게 답할 수 있는 훨씬 더 단순한 테이블로 최종적으로 이어지는 방식으로 작업/테이블의 체인을 생성하도록 프롬프트합니다.
CoT에 대한 자세한 내용은 다음 논문에서 확인할 수 있습니다:
Wang, Zilong et al. Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. 2024. arXiv:2401.04398[cs.CL].

## CoT의 비판적 분석
CoT는 중간 테이블의 연쇄를 통해 작동합니다.
새로운 열을 생성하기 위한 인수 생성 과정에서는 LLM이 전체 테이블에 접근해야 하므로, 이는 컨텍스트 윈도우 오버플로우나 새로운 열에 대한 존재하지 않는 정보를 환각하는 등의 예상치 못한 오류를 야기할 수 있습니다.
이 두 가지 문제는 사용되는 테이블의 크기가 클 때 더욱 심각해집니다.
CoT 워크플로우를 보다 프로덕션 레디한 환경으로 전환하고자 할 때 추가적인 문제점이 발견될 수 있습니다.
이 방법은 여러 테이블을 동시에 처리할 수 없으며, 개발자가 LLM이 출력한 명령어에 대한 파서를 직접 생성해야 합니다. (후자에 대해서는 반론의 여지가 있습니다. LLM이 전체 테이블을 생성할 것이라고 주장할 수 있지만, 다시 한 번 컨텍스트 윈도우와 신뢰성(환각) 문제에 직면할 수 있으므로, 테이블 조작 코드를 LLM의 범위 밖에서 실행하는 것이 더 나을 것이라고 생각합니다.)
## CYQIQ - CoT for multiple tables
CYQIQ은 여러 테이블에 대한 CoT(Chain-of-Thought) 기능을 제공하는 시스템입니다.
CoT는 복잡한 추론 과정을 단계별로 분해하여 중간 결과를 생성하고, 이를 바탕으로 최종 답변을 도출하는 방식입니다.
CYQIQ은 이러한 CoT 접근법을 활용하여 여러 테이블에 분산된 정보를 통합하고, 질문에 대한 정확한 답변을 생성할 수 있습니다.
이는 데이터베이스 질의, 정보 검색, 또는 자연어 처리 분야에서 유용하게 활용될 수 있는 기술입니다.
이 노트북에서는 여러 테이블에 대한 최신 CoT 개념과LangChain및LangGraph를 활용한 보다 실용적인 구현을 소개합니다.
테이블 관리자로는 Pandas를 사용할 것입니다.
CoT와 유사하게, LLM이 데이터프레임을 조작하는 일련의 작업을 생성하도록 프롬프트할 것이며, 첫 번째 출력된 작업을 수행한 다음 중간 테이블을 LLM에 제공하여 체인의 다음 작업을 생성할 것입니다.
필요한 라이브러리를 가져오는 단계입니다.
이 단계에서는 프로그램 개발에 필요한 다양한 라이브러리를 import 문을 사용하여 불러옵니다.
라이브러리를 가져오는 것은 해당 라이브러리에서 제공하는 함수, 클래스, 모듈 등을 활용할 수 있도록 준비하는 과정입니다.
필요한 라이브러리를 설치하려면 다음 명령어를 실행해야 합니다:pip install langchain langchain_openai langgraph langchainhub pandas tabulate
또는 다음과 같이 실행할 수도 있습니다:pip install -r requirements.txt
이 코드는 LangChain과 LangGraph 라이브러리를 사용하여 대화형 AI 시스템을 구현합니다.
- 필요한 라이브러리와 모듈을 가져옵니다.
- 기본 라이브러리:json,pandas,traceback
- 타입 힌트를 위한typing모듈:TypedDict,Annotated,Sequence
- operator모듈
- 사용자 정의 함수:get_last_chains,save_new_chain
- LangChain 라이브러리:ChatOpenAI,BaseMessage,FunctionMessage,HumanMessage,tool,convert_to_openai_function,ChatPromptTemplate,MessagesPlaceholder,hub
- LangGraph 라이브러리:ToolExecutor,ToolInvocation,StateGraph,END
필요한 라이브러리와 모듈을 가져옵니다.
기본 라이브러리:json,pandas,traceback
타입 힌트를 위한typing모듈:TypedDict,Annotated,Sequence
operator모듈
사용자 정의 함수:get_last_chains,save_new_chain
LangChain 라이브러리:ChatOpenAI,BaseMessage,FunctionMessage,HumanMessage,tool,convert_to_openai_function,ChatPromptTemplate,MessagesPlaceholder,hub
LangGraph 라이브러리:ToolExecutor,ToolInvocation,StateGraph,END
이 코드는 LangChain과 LangGraph를 활용하여 대화형 AI 시스템의 기본 구조를 설정합니다. 사용자 정의 함수와 타입 힌트를 사용하여 코드의 가독성과 유지보수성을 향상시킵니다.
```
Copy# 기본 라이브러리 임포트importjsonimportpandasaspdimporttraceback# pydantic 라이브러리 임포트fromtypingimportTypedDict, Annotated,Sequenceimportoperator# 사용자 정의 함수 임포트fromutil_functionsimportget_last_chains, save_new_chain# langchain 라이브러리 임포트fromlangchain_openaiimportChatOpenAIfromlangchain_core.messagesimportBaseMessage, FunctionMessage, HumanMessagefromlangchain_core.toolsimporttoolfromlangchain_core.utils.function_callingimportconvert_to_openai_functionfromlangchain_core.promptsimportChatPromptTemplate, MessagesPlaceholderfromlangchainimporthub# langgraph 라이브러리 임포트fromlanggraph.prebuiltimportToolExecutor, ToolInvocationfromlanggraph.graphimportStateGraph, END
```
[.env] 파일은 파일명이 없는 빈 파일이어야 하며, 액세스 토큰이 포함되어 있어야 합니다.
파일의 내용은 다음과 같은 형식이어야 합니다:
- OPENAI_API_KEY=""...""
- LANGCHAIN_API_KEY=""...""
- LANGCHAIN_TRACING_V2=true
- LANGCHAIN_ENDPOINT=""https://api.smith.langchain.com""
- LANGCHAIN_PROJECT=""...""
- dotenv모듈에서load_dotenv함수를 가져옵니다.
- load_dotenv()함수를 호출하여 환경 변수를 로드합니다.
OPENAI_API_KEY=""...""
LANGCHAIN_API_KEY=""...""
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=""https://api.smith.langchain.com""
LANGCHAIN_PROJECT=""...""
LANGCHAIN_PROJECT=""...""
dotenv모듈에서load_dotenv함수를 가져옵니다.
dotenv모듈에서load_dotenv함수를 가져옵니다.
load_dotenv()함수를 호출하여 환경 변수를 로드합니다.
```
Copyfromdotenvimportload_dotenv#### Environment ####load_dotenv()# 환경 변수 파일(.env)을 로드합니다.
```
여기서는 테이블을 읽어들이고 코드 실행의 편의를 위해 딕셔너리로 변환합니다.
또한 질문 문자열의 목록을 생성하는데, 이는 LLM에 제공할 수 있는 테이블에 대한 설명입니다.
프롬프트에서 이를 사용하여 LLM이 질문과 관련된 테이블을 선택하는 데 도움을 줍니다.
실제 운영 환경에서는 이러한 테이블 설명도 LLM 호출에 의해 생성됩니다.
여기서는 6개의 테이블과 하드코딩된 설명이 포함된 간단한 예시를 보여줍니다.
- 6개의 CSV 파일(coworker0.csv부터 coworker5.csv까지)을 읽어와 각각의 데이터프레임을 생성하고df_list에 저장합니다.
- df_dic딕셔너리를 생성하여df1부터df6까지의 키에 해당하는 데이터프레임을 값으로 할당합니다.
- df_dic은evaluate_pandas_chain함수에서eval()함수를 사용하여 데이터프레임을 동적으로 참조하는 데 사용됩니다.
6개의 CSV 파일(coworker0.csv부터 coworker5.csv까지)을 읽어와 각각의 데이터프레임을 생성하고df_list에 저장합니다.
df_dic딕셔너리를 생성하여df1부터df6까지의 키에 해당하는 데이터프레임을 값으로 할당합니다.
df_dic은evaluate_pandas_chain함수에서eval()함수를 사용하여 데이터프레임을 동적으로 참조하는 데 사용됩니다.
```
Copy# 프롬프트 템플릿 입력을 준비합니다.df_list = []foriinrange(6):# 'data/coworker{i}.csv' 파일을 읽어와 DataFrame으로 변환하고 복사본을 생성합니다.df_list.append(pd.read_csv(f""data/coworker{i}.csv"", index_col=0).copy())# evaluate_pandas_chain에서 Python의 eval() 함수에 사용할 df_dic을 생성합니다.df_dic = {}fori, dataframeinenumerate(df_list):# 각 DataFrame을 'df1', 'df2', ... 형식의 키로 df_dic에 저장합니다.df_dic[f""df{i +1}""] = dataframe
```
- questions_str변수는 여러 개의 데이터프레임에 대한 설명을 포함하는 문자열입니다.
- 각 데이터프레임은df1부터df6까지 번호가 매겨져 있습니다.
- df1은 회사의 성과와 생산성에 탁월한 기여를 한 가장 가치 있는 동료(MVP)를 선택하는 것에 대한 내용입니다.
- df2는 가장 많은 잠재력을 가진 동료를 선택하는 것에 대한 내용입니다.
- df3은 가장 협업하고 싶은 동료를 선택하는 것에 대한 내용입니다.
- df4는 가장 긴장감이 있거나 갈등이 많은 동료를 선택하는 것에 대한 내용입니다.
- df5는 협업하기 가장 어려운 동료를 선택하는 것에 대한 내용입니다.
- df6은 설문 조사에 참여한 사람들에 대한 메타데이터를 포함합니다.
questions_str변수는 여러 개의 데이터프레임에 대한 설명을 포함하는 문자열입니다.
각 데이터프레임은df1부터df6까지 번호가 매겨져 있습니다.
df1은 회사의 성과와 생산성에 탁월한 기여를 한 가장 가치 있는 동료(MVP)를 선택하는 것에 대한 내용입니다.
df2는 가장 많은 잠재력을 가진 동료를 선택하는 것에 대한 내용입니다.
df3은 가장 협업하고 싶은 동료를 선택하는 것에 대한 내용입니다.
df4는 가장 긴장감이 있거나 갈등이 많은 동료를 선택하는 것에 대한 내용입니다.
df5는 협업하기 가장 어려운 동료를 선택하는 것에 대한 내용입니다.
df6은 설문 조사에 참여한 사람들에 대한 메타데이터를 포함합니다.
```
Copy# df1: 회사의 성과와 생산성에 탁월한 기여를 한 가장 가치 있는 플레이어(MVP)로 인정하는 동료를 선택하세요.# df2: 가장 개발되지 않은 잠재력을 가지고 있다고 생각하는 동료를 선택하세요.# df3: 누구와 가장 협업하고 싶습니까?# df4: 어떤 동료와 가장 긴장감을 느끼거나 갈등이 많습니까?# df5: 협업하기 가장 어려운 동료는 누구입니까?# df6: 설문 조사에 참여한 사람들에 대한 메타데이터
```
```
Copy# 각 DataFrame을 설명하는 문자열을 생성합니다.questions_str =""""""
df1:  Choose the coworkers you recognize as the most valuable players (MVP) in the company due to their outstanding contribution to the company’s performance and productivity.
df2:  Choose the coworkers you think have the most untapped potential.
df3:  Who do you like to collaborate the most with?
df4:  With which coworkers do you feel the most tension or have the most conflicts?
df5:  Which coworker is the most difficult to collaborate with?
df6:  Metadata about the persons in the survey
""""""
```
앞서 언급한 바와 같이, 우리는 각 생성 주기마다 단 하나의 액션만 수행할 것입니다.
이 함수는 LLM의 출력에서 액션을 검색하는 데 도움을 줍니다.
get_action함수는 액션 체인을 파싱하여 개별 액션을 추출합니다.
- actions문자열에 """"이 포함되어 있는 경우:
- actions를 ""->"" 기준으로 분할하고 두 번째 부분을 가져옵니다.
- 가져온 부분의 앞뒤 공백을 제거하여a변수에 할당합니다.
- """"이 포함되어 있지 않은 경우:
- actions를 ""->"" 기준으로 분할하고 첫 번째 부분을 가져옵니다.
- 가져온 부분의 앞뒤 공백을 제거하여a변수에 할당합니다.
- 추출된 액션a를 반환합니다.
actions문자열에 """"이 포함되어 있는 경우:
actions를 ""->"" 기준으로 분할하고 두 번째 부분을 가져옵니다.
가져온 부분의 앞뒤 공백을 제거하여a변수에 할당합니다.
""""이 포함되어 있지 않은 경우:
actions를 ""->"" 기준으로 분할하고 첫 번째 부분을 가져옵니다.
가져온 부분의 앞뒤 공백을 제거하여a변수에 할당합니다.
추출된 액션a를 반환합니다.
```
Copy# 액션 체인을 위한 파서defget_action(actions):if""<BEGIN>""inactions:# 액션에 ""<BEGIN>""이 포함되어 있는 경우# ""->"" 기호를 기준으로 분할하여 두 번째 요소를 가져와 공백을 제거합니다.a = actions.split(""->"")[1].strip()else:# 액션에 ""<BEGIN>""이 포함되어 있지 않은 경우# ""->"" 기호를 기준으로 분할하여 첫 번째 요소를 가져와 공백을 제거합니다.a = actions.split(""->"")[0].strip()returna# 추출한 액션을 반환합니다.
```
LLM이 액션을 수행하고 세계(또는 이 경우에는 데이터)와 상호 작용하는 데 사용할 수 있는 2가지 도구를 정의합니다.
view_pandas_dataframes도구는 열 이름 및 처음 몇 개의 데이터 행과 같은 데이터프레임에 대한 정보를 LLM에 제공하는 역할을 합니다.
테이블이 매우 큰 경우 LLM의 컨텍스트 창을 완전히 채울 수 있다는 점에 유의하세요.
이는 그러한 테이블을 여러 개의 작고 논리적으로 관련된 테이블로 분할할 수 있고, 테이블의 내용에 따라 이러한 문제를 일으키려면 수백 개의 열이 필요하기 때문에 큰 제한 사항은 아닙니다.
분할이 불가능한 경우 LLM에 입력되는 행 수를 줄이는 것이 좋습니다.
evaluate_pandas_chain도구는 생성된 액션을 실행하고 결과로 생성된 중간 데이터프레임의 헤드와 수행된 액션 및 전체 데이터프레임을 반환하는 역할을 합니다.
도구의 작동 방식에 대한 보다 자세한 정보는문서를 참조하세요.
- evaluate_pandas_chain함수는 pandas 액션 체인을 실행하는 데 사용됩니다.
- chain매개변수는 실행할 pandas 액션 체인을 나타내는 문자열입니다.
- inter매개변수는 체인 실행 중 중간 결과를 저장하는 데 사용됩니다.
- get_action함수를 사용하여 체인에서 다음 액션을 추출합니다.
- eval함수를 사용하여 추출된 액션을 실행하고 결과를inter에 저장합니다.
- 결과가 DataFrame인 경우.head(50)을 사용하여 처음 50개 행을 마크다운 형식으로 반환합니다.
- 예외가 발생하면 예외 정보를 반환합니다.
- view_pandas_dataframes함수는 최대 3개의 DataFrame을 확인하는 데 사용됩니다.
- df_list매개변수는 확인할 DataFrame 이름의 리스트입니다.
- df_dic에서 요청된 DataFrame을 가져와.head(10)을 사용하여 처음 10개 행을 마크다운 형식으로 반환합니다.
- tools리스트에는evaluate_pandas_chain과view_pandas_dataframes함수가 포함되어 있습니다.
- tool_executor는ToolExecutor클래스의 인스턴스로 생성되며,tools리스트를 인자로 받습니다.
- functions리스트는tools리스트의 각 함수를 OpenAI 함수 형식으로 변환한 결과를 저장합니다.
evaluate_pandas_chain함수는 pandas 액션 체인을 실행하는 데 사용됩니다.
evaluate_pandas_chain함수는 pandas 액션 체인을 실행하는 데 사용됩니다.
chain매개변수는 실행할 pandas 액션 체인을 나타내는 문자열입니다.
chain매개변수는 실행할 pandas 액션 체인을 나타내는 문자열입니다.
inter매개변수는 체인 실행 중 중간 결과를 저장하는 데 사용됩니다.
get_action함수를 사용하여 체인에서 다음 액션을 추출합니다.
eval함수를 사용하여 추출된 액션을 실행하고 결과를inter에 저장합니다.
결과가 DataFrame인 경우.head(50)을 사용하여 처음 50개 행을 마크다운 형식으로 반환합니다.
예외가 발생하면 예외 정보를 반환합니다.
예외가 발생하면 예외 정보를 반환합니다.
view_pandas_dataframes함수는 최대 3개의 DataFrame을 확인하는 데 사용됩니다.
view_pandas_dataframes함수는 최대 3개의 DataFrame을 확인하는 데 사용됩니다.
df_list매개변수는 확인할 DataFrame 이름의 리스트입니다.
df_list매개변수는 확인할 DataFrame 이름의 리스트입니다.
df_dic에서 요청된 DataFrame을 가져와.head(10)을 사용하여 처음 10개 행을 마크다운 형식으로 반환합니다.
df_dic에서 요청된 DataFrame을 가져와.head(10)을 사용하여 처음 10개 행을 마크다운 형식으로 반환합니다.
tools리스트에는evaluate_pandas_chain과view_pandas_dataframes함수가 포함되어 있습니다.
tools리스트에는evaluate_pandas_chain과view_pandas_dataframes함수가 포함되어 있습니다.
tool_executor는ToolExecutor클래스의 인스턴스로 생성되며,tools리스트를 인자로 받습니다.
tool_executor는ToolExecutor클래스의 인스턴스로 생성되며,tools리스트를 인자로 받습니다.
functions리스트는tools리스트의 각 함수를 OpenAI 함수 형식으로 변환한 결과를 저장합니다.
functions리스트는tools리스트의 각 함수를 OpenAI 함수 형식으로 변환한 결과를 저장합니다.
```
Copy# 체인에서 다음 동작을 평가하는 함수@tooldefevaluate_pandas_chain(chain: Annotated[str,""판다스 동작의 체인입니다. 예: df1.groupby('age').mean() -> df1.sort_values() -> <END>"",
    ],
    inter=None,):""""""이 함수를 사용하여 데이터프레임에 대한 판다스 코드 체인을 실행합니다""""""name =""evaluate_pandas_chain""try:
        action = get_action(chain)print(""\n\naction: "", action)

        inter =eval(action, {""inter"": inter,""df_dic"": df_dic})ifisinstance(inter, pd.DataFrame):
            intermediate = inter.head(50).to_markdown()else:
            intermediate = interreturnintermediate, action, interexceptExceptionase:returnf""예외가 발생했습니다:{traceback.format_exc()}"", action,None# 데이터프레임을 살펴보는 함수@tooldefview_pandas_dataframes(df_list: Annotated[Sequence[str],""보고 싶은 최대 3개의 판다스 데이터프레임 목록입니다. 예: [df1, df2, df3]"",
    ]):""""""질문에 답하기 위해 데이터프레임의 head(10)을 보려면 이 함수를 사용하세요""""""name =""view_pandas_dataframes""markdown_str =""요청한 데이터프레임의 .head(10)입니다:\n""fordfindf_list:
        df_head = df_dic[df].head(10).to_markdown()
        markdown_str +=f""{df}:\n{df_head}\n""markdown_str = markdown_str.strip()returnmarkdown_str


tools = [evaluate_pandas_chain, view_pandas_dataframes]
tool_executor = ToolExecutor(tools)

functions = [convert_to_openai_function(t)fortintools]
```
다음은 주어진 내용을 요청사항에 따라 교정한 결과입니다.
시스템 메시지를 사용하여 LLM에게 현재 수행해야 할 작업을 설명합니다.
LLM이 액세스할 수 있는 데이터프레임의 수를 알려주고, 정의한 도구를 사용하도록 요청합니다.
그런 다음 사용자의 특정 질문에 대한 답변을 도출하는 데 도움이 되는 데이터프레임을 생성하는 것이 목표라는 점을 설명합니다.
이어서 생성된 액션 체인(action chain)에 대한 기대사항을 설명하는데, 이는 코드 실행이나evaluate_pandas_chain도구 사용에 유용합니다.
또한 몇 가지 예시를 제공하여 특정 예제 질문에 대해 어떤 논리를 따라야 하는지 설명합니다.
그리고 미래에 참조할 수 있는 과거의 올바른 예시 또는 ""레시피""를 보여줍니다.
마지막으로 방금 생성한 도구, 데이터프레임의 설명자(descriptor), 에이전트와 사용자 간의 최근 메시지 로그를 보여줍니다.
프롬프트 템플릿을 정의한 후에는 필요한 정보로 완성합니다.
- SYSTEM_PROMPT변수에 langchain hub에서 ""hrubyonrails/multi-cot"" 프롬프트 템플릿을 가져옵니다.
- 가져온 프롬프트 템플릿은messages[0].prompt.template를 통해 액세스합니다.
- print(SYSTEM_PROMPT)를 사용하여 포맷된 프롬프트 템플릿을 출력합니다.
SYSTEM_PROMPT변수에 langchain hub에서 ""hrubyonrails/multi-cot"" 프롬프트 템플릿을 가져옵니다.
가져온 프롬프트 템플릿은messages[0].prompt.template를 통해 액세스합니다.
print(SYSTEM_PROMPT)를 사용하여 포맷된 프롬프트 템플릿을 출력합니다.
```
Copy# langchain hub에서 프롬프트를 가져옵니다.SYSTEM_PROMPT = hub.pull(""hrubyonrails/multi-cot"").messages[0].prompt.template# 포맷된 프롬프트 템플릿을 출력합니다.print(SYSTEM_PROMPT)
```
- ChatPromptTemplate을 사용하여 시스템 프롬프트(SYSTEM_PROMPT)와MessagesPlaceholder를 포함하는 프롬프트 템플릿을 생성합니다.
- partial메서드를 사용하여 프롬프트 템플릿에 다음 변수들을 부분적으로 적용합니다:
- num_dfs:df_list의 길이
- tool_names:tools리스트에 있는 도구 이름들을 쉼표로 구분한 문자열
- questions_str: 질문 문자열
- get_last_chains()함수의 반환값이pd.core.frame.DataFrame타입인 경우, 해당 데이터프레임의 ""query""와 ""chain"" 열을 순회하면서chain_examples문자열을 생성합니다. 각 예시는 ""Question: [query]\nChain: [chain]\n\n"" 형식으로 구성됩니다.
- partial메서드를 사용하여chain_examples변수를 프롬프트 템플릿에 부분적으로 적용합니다.
ChatPromptTemplate을 사용하여 시스템 프롬프트(SYSTEM_PROMPT)와MessagesPlaceholder를 포함하는 프롬프트 템플릿을 생성합니다.
partial메서드를 사용하여 프롬프트 템플릿에 다음 변수들을 부분적으로 적용합니다:
num_dfs:df_list의 길이
tool_names:tools리스트에 있는 도구 이름들을 쉼표로 구분한 문자열
questions_str: 질문 문자열
get_last_chains()함수의 반환값이pd.core.frame.DataFrame타입인 경우, 해당 데이터프레임의 ""query""와 ""chain"" 열을 순회하면서chain_examples문자열을 생성합니다. 각 예시는 ""Question: [query]\nChain: [chain]\n\n"" 형식으로 구성됩니다.
partial메서드를 사용하여chain_examples변수를 프롬프트 템플릿에 부분적으로 적용합니다.
```
Copyprompt = ChatPromptTemplate.from_messages(
    [
        (""system"",
            SYSTEM_PROMPT,# 시스템 프롬프트를 설정합니다.),
        MessagesPlaceholder(
            variable_name=""messages""),# 메시지 플레이스홀더를 설정합니다.]
)# 데이터프레임 리스트의 길이를 num_dfs 변수에 할당합니다.prompt = prompt.partial(num_dfs=len(df_list))# 도구 이름을 쉼표로 구분하여 tool_names 변수에 할당합니다.prompt = prompt.partial(tool_names="", "".join([tool.namefortoolintools]))# 질문 문자열을 questions_str 변수에 할당합니다.prompt = prompt.partial(questions_str=questions_str)# 이전에 성공한 쿼리를 전달합니다.chain_examples =""""# 마지막 체인이 데이터프레임 형식인지 확인합니다.iftype(get_last_chains()) == pd.core.frame.DataFrame:# 쿼리와 체인 열을 반복합니다.forindex, rowinget_last_chains()<aclass=""wiki_link""data-title=""query"",""chain""data-wiki=""/page/wiki/query"",""chain""href=""javascript:void(0)"">query"", ""chain</a>.iterrows():# 체인 예시를 생성합니다.chain_examples +=f'Question:{row[""query""]}\nChain:{row[""chain""]}\n\n'# 체인 예시를 chain_examples 변수에 할당합니다.prompt = prompt.partial(chain_examples=chain_examples)
```
간단히 말해, LangGraph는 상태 머신(statemachine)을 생성할 수 있게 해줍니다.
이 머신은 각 상태에서 체인(chain)을 수행하게 됩니다.
LangGraph와 여러 개의 체인을 생성한 후 순차적으로 실행하는 것의 결정적인 차이점은 LangGraph가 순환 또는 반복 실행을 허용한다는 점입니다.
이는 어떤 전이 조건(transition criteria)에 따라 우리의 상태 머신이 어떤 정지 조건(stop condition)이 만족될 때까지 무한히 상태 간에 전환할 수 있다는 것을 의미합니다.
이는 그래프의 노드 간에 상태 객체(state object)를 전달함으로써 달성됩니다.
각 노드는 필요에 따라 동작을 수행하고 상태 객체를 업데이트할 수 있습니다.
LangGraph에 대한 더 자세한 내용은documentation에서 확인할 수 있습니다.
먼저 모델에 도구를 바인딩하여 모델이 호출할 수 있는 2개의 도구에 접근할 수 있다는 것을 확인합니다.
그런 다음 상태 객체인 AgentState를 정의합니다.
이 객체에는 다음과 같은 5개의 필드가 있습니다:
- messages: 추가 연산자로 주석 처리된 BaseMessage로, 각 노드가 이에 추가됩니다.
- actions: messages와 유사하지만 문자열 타입입니다.
- inter: 중간 데이터프레임으로, CoT의 아이디어에 따라 각 사이클마다 업데이트됩니다.
- question: 답변할 질문입니다.
- memory: 사용자와 LLM 간의 이전 대화입니다.
- model변수는prompt와ChatOpenAI모델을 바인딩하여 생성됩니다. 이때gpt-4-0125-preview모델을 사용하며,functions인자를 통해 함수를 바인딩합니다.
- AgentState클래스는 에이전트의 상태를 나타내는 타입힌트 클래스입니다. 다음과 같은 속성을 가집니다:
- messages:BaseMessage타입의 시퀀스로, 에이전트가 주고받은 메시지를 저장합니다.operator.add어노테이션을 통해 시퀀스 연결 연산이 가능합니다.
- actions: 문자열 타입의 시퀀스로, 에이전트가 수행한 액션을 저장합니다.operator.add어노테이션을 통해 시퀀스 연결 연산이 가능합니다.
- inter:pandas.DataFrame타입으로, 에이전트의 중간 결과를 저장합니다.
- question: 문자열 타입으로, 에이전트에게 주어진 질문을 저장합니다.
- memory: 문자열 타입으로, 에이전트의 메모리를 저장합니다.
messages: 추가 연산자로 주석 처리된 BaseMessage로, 각 노드가 이에 추가됩니다.
actions: messages와 유사하지만 문자열 타입입니다.
inter: 중간 데이터프레임으로, CoT의 아이디어에 따라 각 사이클마다 업데이트됩니다.
question: 답변할 질문입니다.
memory: 사용자와 LLM 간의 이전 대화입니다.
memory: 사용자와 LLM 간의 이전 대화입니다.
model변수는prompt와ChatOpenAI모델을 바인딩하여 생성됩니다. 이때gpt-4-0125-preview모델을 사용하며,functions인자를 통해 함수를 바인딩합니다.
model변수는prompt와ChatOpenAI모델을 바인딩하여 생성됩니다. 이때gpt-4-0125-preview모델을 사용하며,functions인자를 통해 함수를 바인딩합니다.
AgentState클래스는 에이전트의 상태를 나타내는 타입힌트 클래스입니다. 다음과 같은 속성을 가집니다:
AgentState클래스는 에이전트의 상태를 나타내는 타입힌트 클래스입니다. 다음과 같은 속성을 가집니다:
messages:BaseMessage타입의 시퀀스로, 에이전트가 주고받은 메시지를 저장합니다.operator.add어노테이션을 통해 시퀀스 연결 연산이 가능합니다.
actions: 문자열 타입의 시퀀스로, 에이전트가 수행한 액션을 저장합니다.operator.add어노테이션을 통해 시퀀스 연결 연산이 가능합니다.
inter:pandas.DataFrame타입으로, 에이전트의 중간 결과를 저장합니다.
question: 문자열 타입으로, 에이전트에게 주어진 질문을 저장합니다.
memory: 문자열 타입으로, 에이전트의 메모리를 저장합니다.
```
Copy# 모델 바인딩model = prompt | ChatOpenAI(
    model=""gpt-4-0125-preview"").bind_functions(functions)# 그래프 상태 생성classAgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]# 메시지 시퀀스actions: Annotated[Sequence[str], operator.add]# 액션 시퀀스inter: pd.DataFrame# 중간 데이터프레임question:str# 질문memory:str# 메모리
```
현재 상태에 따라 여러 가지 일이 발생할 수 있습니다.
이 경우, 모델이 추가 단계가 필요하다고 판단하면 실행을 계속하고, 그렇지 않으면 중지해야 합니다.
LangGraph에서 노드는 함수 또는 실행 가능한 객체입니다.
여기서는 함수를 노드로 사용합니다.
call_model 노드는 진입점 역할을 하며 다음 동작을 결정하는 데 사용됩니다.
조건부 간선이 종료하기로 결정한 경우 이 노드에서 빠져나갑니다.
계속 진행해야 하는 경우 call_tool 노드로 이동하여 LLM이 결정한 함수에 따라 동작을 수행합니다.
일부 데이터프레임을 조회하거나(안전한 작업) 생성된 명령을 실행합니다(명령이 제대로 작성되지 않은 경우 오류가 발생할 수 있으므로 예외 처리와 자체 수정 메커니즘을 추가했습니다. 코드 실행에 실패하면 LLM에 알리고 잘못된 명령을 수정하도록 요청합니다).
이 코드는 대화형 시스템에서 사용되는 함수들을 정의합니다.
- should_continue함수는 대화의 상태를 기반으로 대화를 계속할지 여부를 결정합니다.
- 마지막 메시지에function_call이 없으면 ""end""를 반환하여 대화를 종료합니다.
- 그렇지 않으면 ""continue""를 반환하여call_tool노드로 진행합니다.
- call_model함수는 모델을 호출하여 응답을 생성합니다.
- model.invoke를 사용하여 현재 상태를 기반으로 모델의 응답을 얻습니다.
- 응답을 리스트 형태로 반환하여 기존 메시지 리스트에 추가합니다.
- call_tool함수는 도구(tool)를 실행하는 역할을 합니다.
- 마지막 메시지에서function_call정보를 추출합니다.
- tool_input을 딕셔너리 형태로 변환하고state['inter']를 추가합니다.
- 호출된 도구의 이름에 따라 다른 동작을 수행합니다:view_pandas_dataframes도구인 경우:ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답을 얻습니다.응답을FunctionMessage로 변환하여 반환합니다.evaluate_pandas_chain도구인 경우:ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답, 시도한 동작, 상호작용 정보를 얻습니다.응답에 예외가 발생한 경우, 오류 정보를 생성하고FunctionMessage로 반환합니다.응답이 성공적인 경우, 성공 정보를 생성하고FunctionMessage, 시도한 동작, 상호작용 정보를 반환합니다.
- view_pandas_dataframes도구인 경우:
- ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답을 얻습니다.
- 응답을FunctionMessage로 변환하여 반환합니다.
- evaluate_pandas_chain도구인 경우:
- ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답, 시도한 동작, 상호작용 정보를 얻습니다.
- 응답에 예외가 발생한 경우, 오류 정보를 생성하고FunctionMessage로 반환합니다.
- 응답이 성공적인 경우, 성공 정보를 생성하고FunctionMessage, 시도한 동작, 상호작용 정보를 반환합니다.
should_continue함수는 대화의 상태를 기반으로 대화를 계속할지 여부를 결정합니다.
should_continue함수는 대화의 상태를 기반으로 대화를 계속할지 여부를 결정합니다.
마지막 메시지에function_call이 없으면 ""end""를 반환하여 대화를 종료합니다.
마지막 메시지에function_call이 없으면 ""end""를 반환하여 대화를 종료합니다.
그렇지 않으면 ""continue""를 반환하여call_tool노드로 진행합니다.
그렇지 않으면 ""continue""를 반환하여call_tool노드로 진행합니다.
call_model함수는 모델을 호출하여 응답을 생성합니다.
call_model함수는 모델을 호출하여 응답을 생성합니다.
model.invoke를 사용하여 현재 상태를 기반으로 모델의 응답을 얻습니다.
model.invoke를 사용하여 현재 상태를 기반으로 모델의 응답을 얻습니다.
응답을 리스트 형태로 반환하여 기존 메시지 리스트에 추가합니다.
응답을 리스트 형태로 반환하여 기존 메시지 리스트에 추가합니다.
call_tool함수는 도구(tool)를 실행하는 역할을 합니다.
call_tool함수는 도구(tool)를 실행하는 역할을 합니다.
마지막 메시지에서function_call정보를 추출합니다.
tool_input을 딕셔너리 형태로 변환하고state['inter']를 추가합니다.
호출된 도구의 이름에 따라 다른 동작을 수행합니다:view_pandas_dataframes도구인 경우:ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답을 얻습니다.응답을FunctionMessage로 변환하여 반환합니다.evaluate_pandas_chain도구인 경우:ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답, 시도한 동작, 상호작용 정보를 얻습니다.응답에 예외가 발생한 경우, 오류 정보를 생성하고FunctionMessage로 반환합니다.응답이 성공적인 경우, 성공 정보를 생성하고FunctionMessage, 시도한 동작, 상호작용 정보를 반환합니다.
- view_pandas_dataframes도구인 경우:
- ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답을 얻습니다.
- 응답을FunctionMessage로 변환하여 반환합니다.
- evaluate_pandas_chain도구인 경우:
- ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답, 시도한 동작, 상호작용 정보를 얻습니다.
- 응답에 예외가 발생한 경우, 오류 정보를 생성하고FunctionMessage로 반환합니다.
- 응답이 성공적인 경우, 성공 정보를 생성하고FunctionMessage, 시도한 동작, 상호작용 정보를 반환합니다.
view_pandas_dataframes도구인 경우:
ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답을 얻습니다.
응답을FunctionMessage로 변환하여 반환합니다.
evaluate_pandas_chain도구인 경우:
ToolInvocation을 생성하고tool_executor.invoke를 호출하여 응답, 시도한 동작, 상호작용 정보를 얻습니다.
응답에 예외가 발생한 경우, 오류 정보를 생성하고FunctionMessage로 반환합니다.
응답이 성공적인 경우, 성공 정보를 생성하고FunctionMessage, 시도한 동작, 상호작용 정보를 반환합니다.
```
Copy# 계속할지 여부를 결정하는 함수 정의: 조건부 엣지defshould_continue(state):
    messages = state['messages']
    last_message = messages[-1]# 함수 호출이 없으면 종료if""function_call""notinlast_message.additional_kwargs:return""end""# 그렇지 않고 함수 호출이 있으면 call_tool 노드로 계속 진행else:return""continue""# 모델을 호출하는 함수 정의defcall_model(state):

    response = model.invoke(state)# 리스트를 반환하는데, 이는 기존 리스트에 추가될 것임return{""messages"": [response]}# 도구를 실행하는 함수 정의defcall_tool(state):
    messages = state['messages']# continue 조건에 따라# 마지막 메시지에 함수 호출이 포함되어 있음을 알 수 있음last_message = messages[-1]


    tool_input = last_message.additional_kwargs[""function_call""][""arguments""]


    tool_input_dict = json.loads(tool_input)
    tool_input_dict['inter'] = state['inter']iflast_message.additional_kwargs[""function_call""][""name""] =='view_pandas_dataframes':# function_call에서 ToolInvocation을 구성함action = ToolInvocation(
            tool=last_message.additional_kwargs[""function_call""][""name""],
            tool_input=tool_input_dict,
        )# tool_executor를 호출하고 응답을 받음response = tool_executor.invoke(action)

        function_message = FunctionMessage(content=str(response), name=action.tool)return{""messages"": [function_message]}# ,""actions"": [attempted_action]}# 도구가 체인을 평가하는 경우eliflast_message.additional_kwargs[""function_call""][""name""] =='evaluate_pandas_chain':# function_call에서 ToolInvocation을 구성함action = ToolInvocation(
            tool=last_message.additional_kwargs[""function_call""][""name""],
            tool_input=tool_input_dict,
        )# tool_executor를 호출하고 응답을 받음response, attempted_action, inter = tool_executor.invoke(action)if""An exception occured:""instr(response):
            error_info =f""""""
            이전에 수행한 작업:{state['actions']}현재 작업:{attempted_action}결과 .head(50):{response}접근 방식을 수정하고 다음 질문에 답할 수 있을 때까지 계속해야 합니다:{state['question']}다음 형식으로 체인을 계속하세요: action_i -> action_i+1 ... -> <END>
            """"""print(error_info)

            function_message = FunctionMessage(content=str(error_info), name=action.tool)return{""messages"": [function_message]}else:

            success_info =f""""""
            이전에 수행한 작업:{state['actions']}현재 작업:{attempted_action}결과 .head(50):{response}다음 질문에 답할 수 있을 때까지 계속해야 합니다:{state['question']}다음 형식으로 체인을 계속하세요: action_i -> action_i
```
이제 우리의 상태 머신(StateGraph)을 정의하고, 노드를 추가하며, 진입점을 설정하고, 조건부 엣지를 추가합니다.
지금까지 언급하지 않았지만,calltool노드가 완료된 후에는 항상callmodel노드로 돌아갑니다.
그래프 구조가 적절히 정의되면 남은 유일한 작업은 컴파일하는 것이며, 이는 LangChain Runnable을 반환할 것입니다.
- StateGraph클래스를 사용하여 새로운 그래프workflow를 정의합니다.
- workflow에 두 개의 노드agent와action을 추가하고, 각각call_model과call_tool함수를 호출하도록 설정합니다.
- workflow의 진입점을agent노드로 설정합니다. 즉, 그래프 실행 시agent노드가 가장 먼저 호출됩니다.
- agent노드에서 조건부 엣지를 추가합니다:
- should_continue함수를 사용하여 다음에 호출할 노드를 결정합니다.
- 함수의 출력값에 따라continue이면action노드로,end이면 그래프 실행을 종료합니다.
- action노드에서agent노드로의 일반 엣지를 추가합니다. 즉,action노드 호출 후agent노드가 다음으로 호출됩니다.
- 마지막으로workflow를 컴파일하여 LangChain에서 사용 가능한Runnable객체app을 생성합니다.
StateGraph클래스를 사용하여 새로운 그래프workflow를 정의합니다.
workflow에 두 개의 노드agent와action을 추가하고, 각각call_model과call_tool함수를 호출하도록 설정합니다.
workflow의 진입점을agent노드로 설정합니다. 즉, 그래프 실행 시agent노드가 가장 먼저 호출됩니다.
agent노드에서 조건부 엣지를 추가합니다:
should_continue함수를 사용하여 다음에 호출할 노드를 결정합니다.
함수의 출력값에 따라continue이면action노드로,end이면 그래프 실행을 종료합니다.
action노드에서agent노드로의 일반 엣지를 추가합니다. 즉,action노드 호출 후agent노드가 다음으로 호출됩니다.
마지막으로workflow를 컴파일하여 LangChain에서 사용 가능한Runnable객체app을 생성합니다.
```
Copy# 새로운 그래프를 정의합니다.workflow = StateGraph(AgentState)# 순환할 두 개의 노드를 정의합니다.workflow.add_node(""agent"", call_model)
workflow.add_node(""action"", call_tool)# 진입점을 `agent`로 설정합니다.# 이는 이 노드가 가장 먼저 호출됨을 의미합니다.workflow.set_entry_point(""agent"")# 이제 조건부 에지를 추가합니다.workflow.add_conditional_edges(# 먼저 시작 노드를 정의합니다. `agent`를 사용합니다.# 이는 `agent` 노드가 호출된 후에 수행되는 에지를 의미합니다.""agent"",# 다음으로 다음에 호출될 노드를 결정하는 함수를 전달합니다.should_continue,# 마지막으로 매핑을 전달합니다.# 키는 문자열이고 값은 다른 노드입니다.# END는 그래프가 종료되어야 함을 나타내는 특수 노드입니다.# `should_continue`를 호출한 다음 해당 출력을# 이 매핑의 키와 대조하여 일치하는 노드를 호출합니다.{# `tools`인 경우 도구 노드를 호출합니다.""continue"":""action"",# 그렇지 않으면 종료합니다.""end"": END,
    },
)# 이제 `tools`에서 `agent`로 일반 에지를 추가합니다.# 이는 `tools`가 호출된 후 `agent` 노드가 다음에 호출됨을 의미합니다.workflow.add_edge(""action"",""agent"")# 마지막으로 컴파일합니다!# 이는 LangChain Runnable로 컴파일되므로,# 다른 실행 가능한 것과 동일한 방식으로 사용할 수 있습니다.app = workflow.compile()
```
질문을 정의합니다.
주어진 코드는 여러 단계와 다양한 테이블을 포함하는 질문을 처리하는 예시입니다.
- user_query변수에 사용자의 질의를 할당합니다.
- 첫 번째 예시 질의는 ""Steven Rollins가 MVP로 선정된 횟수를 반환하고, 이 직원이 각 MVP 이유에 대해 받은 등급의 수를 반환하시오.""입니다.
- 두 번째 예시 질의(주석 처리됨)는 ""함께 일하기 어려운 팀으로 가장 많은 표를 받은 팀은 어느 팀인가요?""입니다.
user_query변수에 사용자의 질의를 할당합니다.
첫 번째 예시 질의는 ""Steven Rollins가 MVP로 선정된 횟수를 반환하고, 이 직원이 각 MVP 이유에 대해 받은 등급의 수를 반환하시오.""입니다.
두 번째 예시 질의(주석 처리됨)는 ""함께 일하기 어려운 팀으로 가장 많은 표를 받은 팀은 어느 팀인가요?""입니다.
코드는 실제 질의 처리 로직을 포함하지 않으며, 다양한 유형의 질문을 처리하기 위한 준비 단계를 보여주는 예시입니다.
```
Copy# 여러 단계와 다른 테이블이 포함된 몇 가지 질문을 시도해 보겠습니다.user_query =""Steven Rollins가 MVP로 선정된 횟수를 반환하세요. 또한 이 직원이 각 MVP 이유에 대해 받은 등급의 수를 반환하세요.""# user_query = ""어떤 팀이 함께 일하기 어려운 팀으로 가장 많은 표를 받았나요?""
```
모델을 호출하는 단계입니다.
이 단계에서는 준비된 입력 데이터를 사용하여 학습된 모델을 호출하고 예측 결과를 얻습니다.
모델 호출은 일반적으로 모델 객체의 predict 또는 evaluate 메서드를 사용하여 수행됩니다.
호출 시 입력 데이터의 형태와 모델이 요구하는 입력 형식을 일치시키는 것이 중요합니다.
예측 결과는 모델의 출력 형태에 따라 다양한 형식으로 반환될 수 있습니다.
반환된 예측 결과는 후처리 과정을 거쳐 최종적인 결과물로 변환되거나 평가 지표 계산에 사용됩니다.
- inputs딕셔너리를 정의하여 사용자 질의, 액션 시작 토큰, 메모리 등을 설정합니다.
- app.stream()함수를 사용하여 입력을 스트리밍하고, 재귀 제한을 40으로 설정합니다.
- 스트리밍된 출력 딕셔너리를 반복하면서 각 노드 이름에 따라 처리합니다:
- ""agent"" 노드: 에이전트가 작업 중임을 출력합니다.
- ""action"" 노드:""view_pandas_dataframes"" 액션인 경우 ""viewing dataframes""를 출력합니다.그 외의 경우 현재 액션과 출력을 출력하거나, 오류 발생 시 재시도 메시지를 출력합니다.
- ""view_pandas_dataframes"" 액션인 경우 ""viewing dataframes""를 출력합니다.
- 그 외의 경우 현재 액션과 출력을 출력하거나, 오류 발생 시 재시도 메시지를 출력합니다.
- 그 외의 노드: 최종 출력과 액션 체인을 출력합니다.
- 출력 딕셔너리에서 에이전트 응답, 최종 테이블, 최종 메시지를 추출합니다.
- 에이전트 응답에서 '' 토큰을 제거하여 최종 메시지를 생성합니다.
inputs딕셔너리를 정의하여 사용자 질의, 액션 시작 토큰, 메모리 등을 설정합니다.
app.stream()함수를 사용하여 입력을 스트리밍하고, 재귀 제한을 40으로 설정합니다.
스트리밍된 출력 딕셔너리를 반복하면서 각 노드 이름에 따라 처리합니다:
""agent"" 노드: 에이전트가 작업 중임을 출력합니다.
""action"" 노드:""view_pandas_dataframes"" 액션인 경우 ""viewing dataframes""를 출력합니다.그 외의 경우 현재 액션과 출력을 출력하거나, 오류 발생 시 재시도 메시지를 출력합니다.
- ""view_pandas_dataframes"" 액션인 경우 ""viewing dataframes""를 출력합니다.
- 그 외의 경우 현재 액션과 출력을 출력하거나, 오류 발생 시 재시도 메시지를 출력합니다.
""view_pandas_dataframes"" 액션인 경우 ""viewing dataframes""를 출력합니다.
그 외의 경우 현재 액션과 출력을 출력하거나, 오류 발생 시 재시도 메시지를 출력합니다.
그 외의 노드: 최종 출력과 액션 체인을 출력합니다.
출력 딕셔너리에서 에이전트 응답, 최종 테이블, 최종 메시지를 추출합니다.
에이전트 응답에서 '' 토큰을 제거하여 최종 메시지를 생성합니다.
```
Copy# 입력 데이터를 딕셔너리 형태로 설정합니다. 사용자 질문, 액션, 메모리 등을 포함합니다.inputs = {""messages"": [HumanMessage(content=user_query)],""actions"": [""<BEGIN>""],""question"": user_query,""memory"":""""}# app.stream() 함수를 사용하여 입력 데이터를 스트리밍하고, 재귀 제한을 40으로 설정합니다.foroutputinapp.stream(inputs, {""recursion_limit"":40}):forkey, valueinoutput.items():# 출력 딕셔너리의 각 키-값 쌍을 반복합니다.ifkey ==""agent"":# 키가 ""agent""인 경우print(""🤖 Agent working...  ) # ""Agent working..."" 메시지를 출력합니다.
        elif key == ""action"":  # 키가 ""action""인 경우
            # 액션 메시지의 이름이 ""view_pandas_dataframes""인 경우
            if value[""messages""][0].name == ""view_pandas_dataframes"":
                print(""🛠️ Current action:  )# ""Current action:"" 메시지를 출력합니다.# ""viewing dataframes"" 메시지를 출력합니다.print(""`viewing dataframes`"")else:# 그 외의 경우if""actions""invalue.keys():# ""actions"" 키가 존재하는 경우# ""Current action:"" 메시지를 출력합니다.print(f""🛠️ Current action:"")print(f""`{value['actions']}`"")# 현재 액션을 출력합니다.print(f""Current output:"")# ""Current output:"" 메시지를 출력합니다.print(value[""inter""])# 현재 출력을 출력합니다.else:# ""actions"" 키가 존재하지 않는 경우# 오류 메시지를 출력하고 재시도합니다.print(f""⚠️ An error occured, retrying..."")else:# 그 외의 키인 경우print(""🏁 Finishing up...  ) # ""Finishing up..."" 메시지를 출력합니다.
            print(f""Final output:"")  # ""Final output:"" 메시지를 출력합니다.
            print(value[""inter""])  # 최종 출력을 출력합니다.
            print(f""Final action chain:"")  # ""Final action chain:"" 메시지를 출력합니다.
            # 최종 액션 체인을 출력합니다.
            print(""->"".join(value[""actions""]) + ' -> <END>')

        print(""---"")  # 구분선을 출력합니다.
        pass  # 아무 작업도 수행하지 않습니다.


output_dict = output[""__end__""]  # 출력 딕셔너리에서 ""__end__"" 키에 해당하는 값을 가져옵니다.
agent_response = output_dict[""messages""][-1].content  # 에이전트 응답을 가져옵니다.
final_table = output_dict[""inter""]  # 최종 테이블을 가져옵니다.
# 에이전트 응답에서 '<END>' 태그를 제거하여 최종 메시지를 생성합니다.
final_message = agent_response.replace('<END>', '')
```
print()함수를 사용하여final_message변수의 내용을 출력합니다.
```
Copy# 최종 메시지를 출력합니다.print(final_message)
```
final_table변수의 내용을 출력합니다.
- print()함수를 사용하여final_table변수의 값을 콘솔에 출력합니다.
print()함수를 사용하여final_table변수의 값을 콘솔에 출력합니다.
```
Copyprint(final_table)# 최종 테이블을 출력합니다.
```"
