{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47e94eb",
   "metadata": {},
   "source": [
    "# GPT4All\n",
    "\n",
    "![](./images/gpt4all.png)\n",
    "\n",
    "[GitHub:nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)은 코드, 스토리, 대화를 포함한 방대한 양의 깨끗한 어시스턴트 데이터로 학습된 오픈 소스 챗봇 생태계입니다.\n",
    "\n",
    "이 예제에서는 LangChain을 사용하여 `GPT4All` 모델과 상호 작용하는 방법에 대해 설명합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2927024",
   "metadata": {},
   "source": [
    "## 설치방법\n",
    "\n",
    "1. 먼저, 공식 홈페이지에 접속하여 설치파일을 다운로드 받아 설치합니다\n",
    "2. [공식 홈페이지](https://gpt4all.io/index.html) 바로가기\n",
    "3. 파이썬 패키지를 설치합니다.\n",
    "4. [pip 를 활용한 설치 방법](https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-bindings/python/README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73379234",
   "metadata": {},
   "source": [
    "`%pip` 매직 명령어를 사용하여 `gpt4all` 패키지를 최신 버전으로 업그레이드합니다.\n",
    "\n",
    "[참고]\n",
    "\n",
    "- `> /dev/null` 리다이렉션을 통해 설치 과정에서 발생하는 출력을 `/dev/null` 디바이스로 보내 출력을 숨깁니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5efa7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU gpt4all > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff73a4",
   "metadata": {},
   "source": [
    "## 모델 다운로드\n",
    "\n",
    "![](./images/gpt4all_models.png)\n",
    "\n",
    "[gpt4all 페이지](https://gpt4all.io/index.html)에는 `Model Explorer` 섹션이 있습니다.\n",
    "(더 많은 정보를 원하시면 https://github.com/nomic-ai/gpt4all 을 방문하세요.)\n",
    "\n",
    "1. [공식 홈페이지](https://gpt4all.io/index.html) 에서 다운로드 가능한 모델을 다운로드 받습니다. 본인의 PC 사양에서 구동가능한 모델을 선택하는 것이 좋습니다.\n",
    "2. 본 튜토리얼에서는 `nous-hermes-llama2-13b.Q4_0.gguf`(7.37GB) 모델을 다운로드 받아 진행하겠습니다.\n",
    "3. 다운로드 받은 모델은 `models` 폴더 생성 후 해당 폴더에 다운로드 받습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80118f",
   "metadata": {},
   "source": [
    "- `local_path` 변수에 로컬 파일 경로(\"./models/ggml-gpt4all-l13b-snoozy.bin\")를 할당합니다.\n",
    "- 이 경로는 사용자가 원하는 로컬 파일 경로로 대체할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d745bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\n",
    "    \"./models/nous-hermes-llama2-13b.Q4_0.gguf\"  # 원하는 로컬 파일 경로로 대체하세요.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3d3f5",
   "metadata": {},
   "source": [
    "## 모델 정보 설정\n",
    "\n",
    "로컬에서 실행하려면 호환되는 ggml 형식의 모델을 다운로드하세요.\n",
    "\n",
    "- 관심 있는 모델을 선택하세요.\n",
    "- UI를 사용하여 다운로드하고 `.bin` 파일을 `local_path`(아래 참고)로 이동시키세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95d0c2",
   "metadata": {},
   "source": [
    "### GPT4ALL 모델 활용\n",
    "\n",
    "`GPT4All`은 GPT-3와 유사한 대규모 언어 모델로, 다양한 자연어 처리 작업에 활용될 수 있습니다.\n",
    "\n",
    "이 모듈을 사용하면 GPT4All 모델을 간편하게 로드하고 추론에 활용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127159e7",
   "metadata": {},
   "source": [
    "- `GPT4All` 언어 모델을 사용하여 프롬프트에 대한 응답을 생성하는 `LLMChain`을 구현합니다.\n",
    "- `PromptTemplate`을 사용하여 프롬프트 템플릿을 정의합니다.\n",
    "- `StreamingStdOutCallbackHandler`를 사용하여 언어 모델의 출력을 실시간으로 스트리밍합니다.\n",
    "- 사용자 정의 모델을 사용하려면 `backend` 매개변수를 추가합니다. 지원되는 백엔드는 GPT4All Python 문서에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "243c4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are many companies that make smartphones, but some of the most popular ones include: 1. Apple Inc. (iPhone) 2. Samsung Electronics Co., Ltd. (Galaxy series) 3. Huawei Technologies Co., Ltd. (P30 and P20 series) 4. Xiaomi Corporation (Mi series) 5. Google LLC (Pixel series)."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Name any five companies which makes `{product}`?\",\n",
    ")\n",
    "\n",
    "# GPT4All 언어 모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "llm = GPT4All(\n",
    "    model=local_path,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    # backend=\"gpu\", # GPU 설정\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 질의 실행\n",
    "response = chain.invoke({\"product\": \"Smart phone\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
