{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd6d5d7",
   "metadata": {},
   "source": [
    "# 문맥 압축 검색기(ContextualCompressionRetriever)\n",
    "\n",
    "검색 시스템에서 직면하는 어려움 중 하나는 데이터를 시스템에 수집할 때 어떤 특정 질의를 처리해야 할지 미리 알 수 없다는 점입니다.\n",
    "\n",
    "이는 질의와 가장 관련성이 높은 정보가 많은 양의 무관한 텍스트를 포함한 문서에 묻혀 있을 수 있음을 의미합니다.\n",
    "\n",
    "이러한 전체 문서를 애플리케이션에 전달하면 더 비용이 많이 드는 LLM 호출과 품질이 낮은 응답으로 이어질 수 있습니다.\n",
    "\n",
    "`ContextualCompressionRetriever` 은 이 문제를 해결하기 위해 고안되었습니다.\n",
    "\n",
    "아이디어는 간단합니다. 검색된 문서를 그대로 즉시 반환하는 대신, 주어진 질의의 맥락을 사용하여 문서를 압축함으로써 관련 정보만 반환되도록 할 수 있습니다.\n",
    "\n",
    "여기서 \"압축\"은 개별 문서의 내용을 압축하는 것과 문서를 전체적으로 필터링하는 것 모두를 의미합니다.\n",
    "\n",
    "Contextual Compression Retriever를 사용하려면 다음이 필요합니다.\n",
    "\n",
    "- Base Retriever\n",
    "- Document Compressor\n",
    "\n",
    "`ContextualCompressionRetriever` 는 질의를 base retriever에 전달하고, 초기 문서를 가져와 Document Compressor를 통과시킵니다.\n",
    "\n",
    "Document Compressor는 문서 목록을 가져와 문서의 내용을 줄이거나 문서를 완전히 삭제하여 목록을 축소합니다.\n",
    "\n",
    "> 출처: https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v\n",
    "\n",
    "![](./images/01-Contextual-Compression.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca927c",
   "metadata": {},
   "source": [
    "`pretty_print_docs` 함수는 문서 리스트를 예쁘게 출력하는 헬퍼 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826a3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 예쁘게 출력하기 위한 도우미 함수\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cb273",
   "metadata": {},
   "source": [
    "## 바닐라 벡터 스토어 retriever 사용하기\n",
    "\n",
    "간단한 벡터 스토어 retriever를 초기화하고 텍스트 문서를 청크 단위로 저장하는 것부터 시작해 보겠습니다.\n",
    "\n",
    "예시 질문을 던졌을 때, retriever는 관련 있는 문서 1~2개와 관련 없는 문서 몇 개를 반환하는 것을 확인할 수 있습니다.\n",
    "\n",
    "심지어 관련 있는 문서조차도 많은 무관한 정보를 포함하고 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03be0a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066500cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 2:\n",
      "\n",
      "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
      "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
      "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
      "\n",
      "Page Rank\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 3:\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 4:\n",
      "\n",
      "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
      "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
      "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
      "\n",
      "데이터 마이닝\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# TextLoader를 사용하여 \"appendix-keywords.txt\" 파일에서 문서를 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 청크 크기 300자와 청크 간 중복 0으로 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "texts = loader.load_and_split(text_splitter)\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 FAISS 벡터 저장소를 생성하고 검색기로 변환합니다.\n",
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "# 쿼리에 질문을 정의하고 관련 문서를 검색합니다.\n",
    "docs = retriever.get_relevant_documents(\"Semantic Search 에 대해서 알려줘.\")\n",
    "\n",
    "# 검색된 문서를 예쁘게 출력합니다.\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77a320",
   "metadata": {},
   "source": [
    "## `LLMChainExtractor`를 사용하여 맥락적 압축 추가하기\n",
    "\n",
    "이제 기본 retriever를 `ContextualCompressionRetriever`로 감싸 보겠습니다.\n",
    "\n",
    "`LLMChainExtractor`를 추가할 것인데, 이는 초기에 반환된 문서들을 반복하면서 각 문서에서 질의와 관련된 내용만 추출하는 역할을 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb602a41",
   "metadata": {},
   "source": [
    "`ContextualCompressionRetriever` 를 사용하여 질문에 대한 관련 문서를 압축하고 검색하는 과정을 보여줍니다.\n",
    "\n",
    "- LLMChainExtractor를 사용하여 `compressor`를 생성합니다.\n",
    "- `compressor`와 `retriever`를 기반으로 `ContextualCompressionRetriever` 를 초기화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5464a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 2:\n",
      "\n",
      "검색 엔진, 데이터 검색, 정보 검색\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 3:\n",
      "\n",
      "검색 엔진 최적화, 웹 분석, 링크 분석\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)  # OpenAI 언어 모델 초기화\n",
    "\n",
    "# LLM을 사용하여 문서 압축기 생성\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    # 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(  # 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)  # 검색된 문서를 예쁘게 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35595f3",
   "metadata": {},
   "source": [
    "## 필터 사용\n",
    "\n",
    "### `LLMChainFilter`\n",
    "\n",
    "`LLMChainFilter`는 초기에 검색된 문서 중 어떤 문서를 필터링하고 어떤 문서를 반환할지 결정하기 위해 LLM 체인을 사용하는 보다 단순하지만 강력한 압축기입니다.\n",
    "\n",
    "이 필터는 문서 내용을 변경(압축)하지 않고 문서를 **선택적으로 반환** 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4226fc",
   "metadata": {},
   "source": [
    "- `LLMChainFilter` 클래스를 사용하여 `_filter` 객체를 생성합니다. 이 객체는 LLM을 기반으로 문서를 필터링하는 역할을 합니다.\n",
    "- `ContextualCompressionRetriever` 클래스를 사용하여 `compression_retriever` 객체를 생성합니다. 이 객체는 `_filter`를 기반 압축기로, `retriever`를 기반 검색기로 사용합니다.\n",
    "- `compression_retriever`의 `get_relevant_documents` 메서드를 호출하여 주어진 질문과 관련된 문서를 압축하여 가져옵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ec2a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 2:\n",
      "\n",
      "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
      "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
      "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
      "\n",
      "Page Rank\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "# LLM을 사용하여 LLMChainFilter 객체를 생성합니다.\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    # LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
    "    base_compressor=_filter,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\n",
    "    # 쿼리\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "pretty_print_docs(compressed_docs)  # 압축된 문서를 예쁘게 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8890e0",
   "metadata": {},
   "source": [
    "### `EmbeddingsFilter`\n",
    "\n",
    "각각의 검색된 문서에 대해 추가적인 LLM 호출을 수행하는 것은 비용이 많이 들고 속도가 느립니다.\n",
    "\n",
    "`EmbeddingsFilter`는 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환함으로써 더 저렴하고 빠른 옵션을 제공합니다.\n",
    "\n",
    "이를 통해 검색 결과의 관련성을 유지하면서도 계산 비용과 시간을 절약할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e39f1",
   "metadata": {},
   "source": [
    "`EmbeddingsFilter` 와 `ContextualCompressionRetriever` 를 사용하여 관련 문서를 압축하고 검색하는 과정입니다.\n",
    "\n",
    "- `EmbeddingsFilter` 를 초기화하며, 임베딩 객체와 **유사도 임계값(0.86)** 을 설정합니다.\n",
    "- ContextualCompressionRetriever를 초기화하며, EmbeddingsFilter를 기본 압축기로, 이전에 정의된 `retriever`를 기본 검색기로 사용합니다.\n",
    "- `compression_retriever.get_relevant_documents()` 메서드를 호출하여 주어진 질문과 관련된 문서를 압축하고 검색합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30565ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 유사도 임계값이 0.76인 EmbeddingsFilter 객체를 생성합니다.\n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings, similarity_threshold=0.86)\n",
    "\n",
    "# 기본 압축기로 embeddings_filter를, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "# ContextualCompressionRetriever 객체를 사용하여 관련 문서를 검색합니다.\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\n",
    "    # 쿼리\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "# 압축된 문서를 예쁘게 출력합니다.\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db1d1b3",
   "metadata": {},
   "source": [
    "## 파이프라인 생성(압축기+문서 변환기)\n",
    "\n",
    "`DocumentCompressorPipeline` 을 사용하면 여러 compressor를 순차적으로 결합할 수 있습니다.\n",
    "\n",
    "Compressor와 함께 `BaseDocumentTransformer`를 파이프라인에 추가할 수 있는데, 이는 맥락적 압축을 수행하지 않고 단순히 문서 집합에 대한 변환을 수행합니다.\n",
    "\n",
    "예를 들어, `TextSplitter`는 문서를 더 작은 조각으로 분할하기 위해 document transformer로 사용될 수 있으며, `EmbeddingsRedundantFilter`는 문서 간의 임베딩 유사성을 기반으로 중복 문서를 필터링하는 데 사용될 수 있습니다.\n",
    "\n",
    "아래에서는 먼저 문서를 더 작은 청크로 분할한 다음, 중복 문서를 제거하고, 쿼리와의 관련성을 기준으로 필터링하여 compressor pipeline을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21929ce1",
   "metadata": {},
   "source": [
    "`DocumentCompressorPipeline`을 사용하여 문서 압축 파이프라인을 구성합니다.\n",
    "\n",
    "- `CharacterTextSplitter`를 사용하여 문서를 분할합니다.\n",
    "- `EmbeddingsRedundantFilter`를 사용하여 중복된 내용을 필터링합니다.\n",
    "- `EmbeddingsFilter`를 사용하여 유사도 임계값이 0.86 이상인 관련 내용만 필터링합니다.\n",
    "- 위의 세 가지 변환기를 `DocumentCompressorPipeline`에 전달하여 파이프라인을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00fde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문자 기반 텍스트 분할기를 생성하고, 청크 크기를 300으로, 청크 간 중복을 0으로 설정합니다.\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 임베딩을 사용하여 중복 필터를 생성합니다.\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "\n",
    "# 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정합니다.\n",
    "relevant_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings, similarity_threshold=0.86)\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    # 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터를 변환기로 설정합니다.\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14e247",
   "metadata": {},
   "source": [
    "`ContextualCompressionRetriever`를 초기화하며, `base_compressor`로 `pipeline_compressor`를, `base_retriever`로 `retriever`를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "022626dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n"
     ]
    }
   ],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    # 기본 압축기로 pipeline_compressor를 사용하고, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever를 초기화합니다.\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\n",
    "    # 쿼리\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "# 압축된 문서를 예쁘게 출력합니다.\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
